
that if one is in a Spanish-speaking
country, it is also advisable not to
compliment a strong woman by
calling her macha.
w
At a loss for words, hockey puck?
Quote Shakespeare.
BULLETIN: The unparalleled king of insults
is not Don Rickles.
As has certainly not passed one’s notice, Shakespeare marshals
up gems of abuse that would whoosh right over the
average boor’s head. Therefore, appropriating The Bard’s
18
Y’wanna piece
of me, sweetie?
19
Shakespeare, Addressing Individual Mounds
of Foul, Undigested Lumps of Donkey Entrails:
for those of the female persuasion
Hag of hell, fat chuff, latten bilbo (brass shackles),
painted maypole, long-tongued babbling gossip,and
Amazonian trull.
For men who have fallen out of one’s favor
False hound, untutored churl, rank weed, insolent cracker, unlettered
small-knowing soul, odoriferous stench, pigeon-egg of
discretion, dilatory sloth, homely swain, clod of wayward marl,
dunghill groom, puke-stocking, improvident flea, ronyon (mangy
or scabby creature), roastmeat for worms, princox (fop), cacoethes
(one with insatiable desire, usually disreputable), mad mustachio’d
purple-hued maltworm, prick-eared cur of—(fill in the
name of town, school, or neighborhood the cur claims as home),
and whoreson.
for one’s boss
Old feeble carrion, scolding crookbank, embossed carbuncle,
white-livered-red-faced prince of fiends, cacodemon (evil spirit),
maggot pie, execrable wretch, beef-witted, or sodden-witted
implorer of unholy suits.
to verbally backhand group obnoxiousness
You rabble of vile confederates, herd of boils and plagues, petty
spirits of region low, strangely visited people, foul and pestilent
congregation of vapors, college of witcrackers, dissolute crew, or
base lackey peasants.
words to one’s own needs will serve a dual purpose. It
confounds the ignorant and catches the erudite off guard.
Hence:
cavil, censure, denunciation, disparagement, reproach,
Under certain circumstances, profanity provides a
relief denied even to prayer.
Mark Twain ab
21
Sacre Bleu:
Profanities and
Expletives
w
The “F” ing Word & Other Intensives
Veritable, sure enough, or bona-fide are perfectly respectable
intensifiers when one needs, well, emphasis. Unfortunately,
fucking seems to be the hands-down pejorative of
choice in modern society. This being the case, we believe a
little historical perspective couldn’t hurt . . .
No matter how many people believe it true, it is highly unlikely
that the word “fuck” is an acronym of For Unlawful
Carnal Knowledge or that other old chestnut, Fornicate
Under Consent of the King. Eric Partridge believed it
evolved from the German word ficken for “to strike.” Like
most, he found the word objectionable. He, however, categorized
it along with words that he considered sadistic representations
of the male’s part in copulation: clap, strike,
thump, nail, and, yes, bang.Webster’s offers the derivations,
fokken (Dutch, to breed) or fokka (Swedish, to copulate).
Others suggest the French word foutre, to thrust, and even
firk (English 1600’s), to beat or to lash. However it originated,
it has been in use and considered a vulgarity the better
part of a millennium. As an intensive, Webster’s calls it
meaningless. There are those who would disagree.
To avoid inciting an affronted swoon
by the more sensitive souls of society,
acronyms have been embraced in place
of a number of phrases that include
the “f” word. Specifically, we have
GFY, which instructs one to do something
anatomically impossible (Go
Fuck Yourself ); GFU, a moron (General
Fuck-Up); and NFW, an implausibility
(No Fucking Way). Related
acronyms include SNAFU, a cynical
expectation of any situation in which
the military is involved (Situation
Normal, All Fucked-Up); FUBAR,
unrecognizably mussed (Fucked-Up
Beyond All Recognition); and there is
the sarcastic BFD (Big Fucking Deal).
Additionally, when one has been indisputably wronged, one
has been RF—Royally Fucked (also known as the king’s
elevator—the royal shaft). Just for the record, a flying
fuck is what one does not give, not airborne copulation.
And abso-fucking-lutely means beyond a shadow of a
doubt.
22
w
Merde
The four-letter word for defecation has been in use for
eons—which allows that antiquity does not necessarily dictate
grand lexicon. It is possible to avoid the vulgarity of
the word shit completely, as feces, manure, and dung all
mean the same thing. (Small point of interest: feces refer to
human waste, manure and dung, animal.) Other selections
tend to be polysyllabic but are colorful—meadow dressing,
bovine excrement, horse apples, corral confetti, etc.
Granted, if one is discussing political matters, it may be impossible
to avoid using (or even shouting) bullshit. However,
if one does not want to compromise decorum completely,
that can be shortened to B.S. Or, call it hogwash,
heifer dust, or lip-gloss. Bull-chips might do in a pinch
but, in all probability, not what pops out of one’s mouth
when faced with ultimate doom (at which time one will
most likely be up Shit Creek). Indeed, sources report that
when the black boxes are recovered from airplane crash
sites, invariably the last words on the tape are “Uh-oh,”
“Fuck!” and “Oh, shit. ”
Of course, one can use the French, merde or
speak of “a short French expletive” which
would in fact allow one to perform a rather impressive
circumlocutory hat trick, a euphemism
for a euphemism for euphemism.
When one finds it necessary to point out the
limitations of another’s character via the
23
cavil, censure, denunciation, disparagement, reproach,
alimentary canal, it is our position that it is preferable to
enlist mock Latin such as excrementum cerebellum vincit
rather than call someone a shit-head.
Other expressions that would benefit such translation are:
shit list (a mental note of personae non gratae); the shitty
end of the stick (the bad end of a bargain—often known
as the shaft); to shit or get off the pot (or fish or cut bait).
To shit in high cotton is to have attained a higher standard
of living. But not knowing shit from Shinola—well, that
means . . . owing to stupidity one cannot tell feces from
shoe polish. Someone whose continued presence is an
24
annoyance sticks like shit to a shovel. Alternatively,
shit on wheels reflects an over-inflated opinion of oneself.
(We, however, could in no way determine how one could
deign this to be a self-compliment).
Shit a brick technically means discharging a copious and
compacted bowel movement, but colloquially it refers to
accomplishing the impossible. Lastly, to be so angry as to
perform said impossibility is engaging in a shit-fit (also
known as pitching a bitch). Certainly there are Latin
instructors standing by to assist us.
w Vexed
As ancient a word as is piss, it was not until the last century
that humankind found use for it beyond the single
verb or noun. Nowadays, if one is pissed off, one is actually
choleric (and undoubtedly with one’s panties in
a bunch or knickers in a knot). Shakespeare expressed
it thusly: “You do me the most insupportable vexation.”
Other urinary-based euphemisms and their more civilized
translations: full of piss and vinegar (effervescent), piss
away (squander the inheritance—leaving oneself without a
pot to piss in), and piss blood (work with extreme diligence).
A piss-ass is a worthless individual (occasionally called an
arseworm), to engage in a pissing match is an endeavor
that is certain to be unproductive, and if one is piss-poor,
one is monetarily disadvantaged (e.g., without cable).
25
cavil, censure, denunciation, disparagement, reproach,
26
Full a’ piss and
vinegar, ye’ are!
Piss ugly is extremely unattractive
and if piss-faced, one
is overly medicated by alcohol.
The heretofore unheard of, pissed as
a newt has come to our attention. As we
have not personally been confronted by an
outraged salamander, we are uncertain of the
etymology or history of this term. We can only
labor under the supposition that in this situation,
“pissed” does mean vexed, for we believe one even less
likely to come across a drunken newt than a mean one.
VNOTE: The colorful late U. S. Vice President, John
Nance Garner is oft quoted as saying the office of Vice
President was not worth a bucket of warm spit. Those who
knew the man insist he didn’t use the word, “spit.”
Oaths and General
Vituperation
If a potty mouth forsakes stock curses and lets fly with the
likes of Jumpin’ Jehosaphat, just imagine the stunned
silence. Likewise, pshaw, Land a Goshen, Lord love a
duck, criminey, Ye Gods and little Fishes, pish-tosh, My
Well, bugger my giddy aunt.
Great Aunt Gussie—or as Great Aunt Gussie might say,
hells bells and panther tracks! While we understand these
oaths are insufficiently obscene for some, calling someone a
pinhead instead of a fuck-head will neither get one ticketed
nor beat like a one-legged step-child.
w
TheAbodeof theWickedDead
Down, down to hell;
and say I sent thee thither
Shakespeare
Technically hell is the nether realm of
the devil in which the damned suffer
everlasting punishment. In other words, a real sticky
wicket. The word in and of itself is not naughty. Nevertheless,
everyone knows (or at least suspects) that damning
someone to it is considered a blasphemy.
Hence, it is not surprising that an entire cottage industry
of euphemistic splendor has erupted from that word’s
roots. Indeed, the lengths to which people go to say it without
“saying it” is quite remarkable: Hades, Hail Columbia,
blue blazes, Cain, tarnation, Sam Hill, Kingdom Come,
You Know Where, or any place that implies “down there,”
the hot place, netherworld, lower regions, etc. Although
28
the Victorians gave us heck, perdition has been a suitable
alternative since the 14th century. Today we seldom hear
the once popular Go to Helen B. Happy. A shame, really.
Hellacious is a multi-purpose adjective that can mean either:
exceptionally powerful, remarkably good, extremely
difficult or extraordinarily large. To be hell-bent is
recklessly determined come hell or high water.
w
Doggone and Up Yours
Oh darn, dang, confound, consarn, dagnab, dash, blank,
or blast followed by it, are all euphemistic replacements for
the word, damn. All denounce someone or something as
evil. Truly genteel society frowns on these seemingly benign
adjectives as well as bloody, bleeding,
blamed, all-fired, dad-gummed, dratted,
and cotton-picking.
At one time, a curse was serious business.
No one took lightly being consigned
by another to hell, which may
be why Go to the Devil morphed
into Kiss My Ass. Go jump in the
lake or take a long walk off a short
pier are only nicer sounding ways to
tell someone to fold it five ways and
shove it where the sun don’t shine.
censure, denunciation, disparagement, reproach,oppro-
29
Silent Disparagement
(The Bird and His
Friends)
Although many think of it as contemporary,
digitus infamis or digitus impudicus (infamous
or indecent finger) as a phallic symbol has
been referenced in literary works as early as
ancient Rome. Mad-as-a-hatter Caligula was
rumored to hold up his middle finger for supplicants
to kiss.
There is the obvious suggestion of genitalia
in fist and extended middle finger, but we
have heard that during early warfare, captured
enemy archers had their fingers removed so they
couldn’t draw a bow. Therefore, holding up two fingers
(index and middle) backwards to one’s enemy signified
one could still do them damage. One could premise
that’s pretty much saying “f--- you.”
From the fight scene in Romeo & Juliet, which
commenced with the snapping of thumbnail under
the front teeth, to Texas A&M’s upraised “Gig ‘Em
Aggies” thumb, we see the ultimate insult can be insinuated
by other than extended middle finger. In the Arab world,
palm down, middle finger waggling downward means the
same as raised middle finger in the West.
31
censure, denunciation, disparagement, reproach,oppro-
VFYI: The little finger offered as
suggestion of a, shall we say, modestly
proportioned male part is not of modern
origin. Seek ye the Bible. I Kings 12:10.
32
To express general disrespect, there is the Cock a Snook,
also known as Ann’s Fan or Pulling Bacon, which is the
thumb on nose, fingers waving. To grasp one elbow and
raise a fist is one of the commonest insult
found worldwide, but is not universal.
That title must go to displaying
one’s naked backside. Anthropologists
say mooning predates Braveheart and,
loosely translated, meant “eat shit.”
After the fall of Bagdad, we saw Iraqis
beating the tar out of portraits and statues
of Saddam Hussein with their
shoes, revealing to westerners one of the strongest insults
of their culture—that of sticking the sole of your shoe in
someone’s face.
As there are any number of variations of armpit, bicep, fist,
finger, thumb, nose, crotch and spit . . . maneuvers to express
disrespect in different cultures, if one must hail a cab,
say, in Greece or New York, do so with all due caution.
Circumlocution
Because we often toss them about willy-nilly, we may forget
that euphemisms serve a greater purpose than merely
keeping the ladies at a garden party from glaring at us over
the top of their spectacles. A glib turn of phrase can spare
wounded feelings, a few mincing words keep lawyers at bay.
Until the Victorian era, however, the euphemistic mother
lode had not really begun to be mined. Once Queen Victoria
was on her throne and her minions on high alert, there
was little that couldn’t be accused of having a sexual, and
therefore, evil, connotation. Everything had to be renamed.
Hence, a bull became a cow’s spouse and one’s buttocks,
sit-upons. One can only imagine how dicey it must have
been sitting down at Sunday dinner for some poor soul
trying to ask for a specific piece of chicken.
w
A Woman of Expansive Sensibilities
Paphos was an ancient city of Cypress known for worshipping
Aphrodite. The well-traveled, or at least well-read,
33
Victorian men found it quite sly to call a prostitute or her
doings, Paphian. Further 19th century circumlocution favored
demimondaine, academician, abbess, courtesan,
Fille de jolie (fun gal) or nymph du pave (streetwalker). The
term of choice for those whose professions or predilections
sought to save her soul: she was a fallen woman.
One rarely hears of a lady of certain description or
painted woman anymore, but one would have to be pretty
obtuse not to understand the meaning. A little more recent
is woman of the night, streetwalker, naughty girl, and
commercial sex worker. A quick check of our Yellow Pages
did not uncover services by call girls. However, escort,
model, and actress listings are numerous and offer “discreet
billing.”
As for the specific establishment where these shenanigans
take place, a century ago it was referred to as a leaping
academy, vaulting school, disorderly house, knocking
shop, or chamber of commerce. However dated that
sounds, one must agree that today’s snake ranch or slut
hut is not much of an improvement. Granted, whorehouse
is to the point, but just a tad crude. If compelled to speak
of it, polite society might say it is a brothel, place of accommodation,
bagnio, or seraglio. Or, depending on your
frame of mind, house of ill-repute.
Furthering the subject, we proffer that he who pimps prostitutes
is not a pussy peddler, hole-toller, buttock broker,
vent renter, or crack salesman, but a panderer, procurer,
or the French, sounteneur, and with or without pimpmobile,
undoubtedly, a louse (editorially speaking).
For he who thinks he is pulling the wool over by describing
she who is gyrating upon his lap as an exotic dancer rather
Dividing the spoils?
35
denunciation, disparagement, reproach, opprobrium,
than a stripper, be advised, he can go one step further in
self- (or wife) delusion. Employing the term ecdysiast is
even more oblique. Although ecdysis does sound like a
moderately uncomfortable medical procedure, it is actually
the molting or shedding of a skin like a snake.
w
Men Much Taken With Wenching
As lechery appears to be an
accepted major by college-age
males, modern vernacular has responded.
Nowadays, he who pursues
such activities with undue
vigor is a walking hard-on. If the
little stud muffin has seen more
tail than a toilet seat, when the
dean writes home of his expulsion,
he may be described as of
distempered blood and duteous
to vices. (Well, maybe if he was
at Oxford.)
We can call him a debaucher,
libertine, flesh-monger, incubus,
Lothario, insatiate, or roué,
but it doesn’t make him any less
36
irredeemable. Of course, if he insists he is a man of the
world by way of visiting three county fairs and a goatfucking,
he undoubtedly is a bon viveur.
w
The Prevaricator
Liar, Liar, Pants on Fire
At one time, to question a
person’s honesty was no trivial
matter. Such was its consequence;
one dare not bandy
the word liar about. Therefore,
the more innocent prevaricator
was often accused
of only spinning a windy or
embroidering the truth. A
mountebank would lie like
a rug, and a charlatan was crooked as a barrel of snakes.
To piss in someone’s pocket means one is feeding him a
pack of lies.
A lie can travel halfway around the world
while the truth is putting on its shoes.
Mark Twain
37
denunciation, disparagement, reproach, opprobrium, Honest
Bob’s
Used
Wagons
If you seek the finest for
the least, Honest Bob
can procure it for you.
38
It’s hard to believe that a man is telling the truth
when you know that you would lie
if you were in his place.
H.L. Mencken
Current euphemisms such as terminological inexactitude
and economy with the truth dilly-dally about. When last
we checked, thou shalt not bear false witness against one’s
neighbor was still the ninth commandment. So “I misspoke”
won’t cut it with the keeper of the pearly gates.
A lie is an abomination to the Lord
and a very present help in trouble.
Adlai Stevenson
Although not technically lying, pettifoggery fits into this
category for general unscrupulousness. Since the 16th century,
it has described the antics of two or more lawyers
haggling unceasingly about minute matters thereby inflating
their client’s bill—thus proving the old axiom about the
more things change, the more they stay the same.
Q. Why do lawyers wear neckties?
A. To keep the foreskin from crawling up their chins.
disparagement, reproach, opprobrium, reproof,stricture,
w Tuft Hunters and Suck-ups
In English colleges such as Oxford, the aristocrats wore
special tassels (tufts) on their mortarboard hats to denote
their status. The more obsequious among the student body
sought them out, ergo—tuft-hunters. To most, these truly
annoying suck-ups are sycophants.
If overtaken by an undeniable need to
publicly decry this character flaw, one
might whip out one’s French dictionary
and sniff “leche-cul” (butt-kisser) right in
the servile flatterer’s face. Once out of high
school, however, it is advisable to sling
more derogatory comments such as
bootlicker and brown-noser behind the
back. If you do cast this particular stone,
understand that bootlicker is associated
with the habit of kissing the feet of kings
and therefore conveys a modicum of respectability (only
barely). However, it is often overlooked that brown-noser
refers to the result of smooching another part of the anatomy.
Shakespeare called them all puling pickthanks.
40
41
disparagement, reproach, opprobrium, reproof,stricture,
Clothes make
the man. Naked
people have little
or no influence
on society.
Mark Twain
w
In the Altogether
By definition, if one finds oneself in dishabille, one is carelessly
attired. In truth, that French term is often nothing
less than an outright accusation of misconduct. Not only
has one been cited for having one’s clothes in a muss, but
also by having them become that way because one has
been fooling around. Standard advice: Gather whatever
dignity one is able to muster, deny everything and make
a brisk exit.
If caught without a stitch in the great outdoors, one is au
natural. If indoors and can strike a pose, one is nude. In
the case of being stark-ballock-naked and in a compromising
situation, one is nekkid.* No defense—beat a hasty retreat
without the bugle call.
w
The Part that Goes Over the Fence Last
It is never necessary to use that three-letter word regardless
whether it has been in use since the 12th century. Nor
the four-letter one short for buttocks. Just say buttocks.
Or bottom, behind, or rear end for heaven’s sake. If that
is just too simple and one feels the need to express oneself
more floridly, we suggest posterior, derriere, ampersand,
parts behind, prat (hence, pratfall), differential, fanny,
fleshy part of the thigh, blind cheeks, bum, or tushy.
However we would like not, one hears, of course, of
ying-yang, wazoo, and poop-chute. Or if you prefer the
cloak of Latin, gluteus maximus.
42
*This southern colloquialism, often preceded by “buck,” is differentiated
from naked thusly: “Naked” means you do not have any clothes on.
“Nekkid” means you don’t have any clothes on and you’re up to no good.
There once was a woman from Mass
Who had an enormously large ass
when asked does it wiggle
she replied with a giggle
No, but it occasionally does pass gas.
43
reproach, opprobrium, reproof, stricture, vitriol, epithets
w
The Endomorph
He sweats to death,
And lards the lean earth as he walks along.
Shakespeare
“A goodly bulk,” Shakespeare
also called it. But
even on those rare occasions
when an absolute description
is unavoidable,
however ample the avoirdupois,
we believe buffalobutt,
barge-assed, or
hopper-hipped are unnecessarily
mean. Weightadvantaged
would be discreet.
With corpulent,
obese, or endomorphic,
one gets the broad-beamed
picture. Callipygian or
Rubenesque are downright
complimentary.
It is a long-held defense for
having an amply fleshed
mate that one is assured of
optimum warmth in the winter
and shade in the summer.
Conversely, lore tells of a
guy of disturbingly low
reproach, opprobrium, reproof, stricture, vitriol, epithets
morals and poor initiative who only dates fat girls because he
figures, “They don’t have much willpower.”
We are seeking out the purveyors of these stories in order
to exact retribution.
VNOTE: Any abuse is allowable if it is indemnified by
the “bless her heart” clause. The only criteria for its application
is that one can either claim Southern heritage or
manage a credible Southern drawl when it is employed:
“That girl is so fat, bless her heart, if she sat on a bug it
would fossilize in five minutes.”
Clarification: In the South, a boy or a girl is anyone under
the age of 60.
w
Ill-Favored by Nature
Whether or not a person looks like they fell from the ugly
tree and hit every branch on the way down, one certainly
would not want to make this observation within their earshot.
If it becomes necessary to describe an unprepossessing
person to a third party and one does not want to be
Never try to teach a pig to sing,
it wastes your time and annoys the pig.
Proverb
out-right deceptive, said person might be described as
unlovely, disagreeable to the eye, or a bit homely. Do
avoid butt-ugly at all costs (impolitic remarks have a nasty
history of payback).
The Paper Bag Rule
If only one paper bag over the head is necessary to keep
from frightening children, one is uncomely. Two paper
bags, admittedly hard-featured. Three paper bags, o.k.,
butt-ugly. If the person in question is a close friend or relative,
said person is plain but has a good personality.
Postscript: If one would chew off an arm in the morning to
escape undetected from a one-nighter who looked all right
when they said, “Last Call,” that person is Coyote Ugly
(owing to a coyote’s supposed willingness to chew off a
limb to get out of a steel-jaw trap).
46
The Lord prefers common-looking people.
That is the reason why he made so many of them.
Abraham Lincoln
In our western regions, if one looks a bit worse for the
wear, one has been rode hard and put up wet. If this colloquialism
needs explaining, then it would be wise not to try
to work that dog won’t hunt into the conversation either.
w
Short Pockets
A small-statured person is not sawed-off nor suffering from
duck’s disease (short legs), but is vertically-challenged,
abbreviated, a bit close to the ground, compact, diminutive,
petite, slight, undersized, wee, or not tall. Alternately
one with exceedingly long legs may have high pockets and
run like a dromedary with the staggers, but it would be
kinder to describe him as lean, lanky, or rangy. She is statuesque,
unless, of course she is a carbon copy of Olive Oyl.
If this is the case, one might want to disregard bony, emaciated,
scrawny, living stick, or skeletal and rely on slender
or a bit spare.
w
Buck-Toothed
As to why the French describe someone with protruding
teeth as dents a l’anglaise, we shall, in the name of diplomacy,
not look to the British throne.
47
opprobrium, reproof, stricture, vitriol, epithetsand
Bacchus hath drowned
more men than Neptune
Dr. Thomas Fuller
And ale for
my steeds!
Worshipping at the
Shrine of Bacchus
w
Killing a Few Brain Cells
Webster’s first definition for bibulous is “highly absorbent,”
which is probably why its second definition describes one
who over-imbibes on alcoholic substances. Over-imbibers
are also: besotted, befuddled, bleary-eyed, blotto, soused,
bombed, Bosco Absoluto, adrip, afloat, wall-eyed, cupshot,
lit, likkered up, walking on rock socks, or stinking
drunk.
An oenophile is a lover of wine. With the addition of a
prepositional phrase such as “of legendary proportions,”
said drinker is a wino.
The difference between a drunk and an alcoholic
is that drunks don’t have to attend all
those meetings.
How we identify inebriates today is not half so eloquent as
did our forefathers. Their excessive quaffers were called
49
belch-guts, bibblers, biled owls, bloaters, boosey-cocks,
bubbing-culls, cadgers, fuddle-caps, fuddlers, groghounds,
gullions, guttles, large-heads (a hands-down
favorite), lick-spigots, lick-wimbles, moist-uns, plonkdots,
squiffs, and tosspots.
The productive drunk is the bane of moralists.
Anon
50
An alcoholic is someone you don’t
like who drinks as much as you.
Dylan Thomas
The trouble with jogging
is that the ice falls
out of your glass.
w Paying for It
crapulous \ kra-pye-les\ adj [LL crapulosus,
fr. L crapula intoxication, fr. Gk
kraipale ] (1536) 1 : marked by intemperance
esp. in eating or drinking
2 : sick from excessive indulgence in liquor
If not behind the wheel, intemperance can
be relatively benign. Indeed, a crying jag
is embarrassing but hardly lethal. Be forewarned,
even wearing beer goggles (optically
impaired by drink) can get a limb
chewed off (see coyote ugly). In one’s
armor (fighting drunk) is the best way to
get ass-whupped.We don’t even want to
talk about the infamous brewer’s droop (also known as
whiskey dick). While the morning after one may be spitting
feathers, visited by the brown bottle flu implies a trip
to Europe with Ralph and Earl in a Buick.
I always keep a stimulant handy in case
I see a snake, which I also keep handy.
W.C. Fields
The beezie-weezies sound kind of cute. If you have
them it means an array of colorful visitors from the
animal kingdom have come to call on you (pink
elephants, blue devils, red spiders, a black
dog, or snakes—of any hue—in one’s boots).
51
reproof, stricture, vitriol, epithets and vituperation
They are also synonymous with the screaming meemies, a
term a tad more accurate. But the presence of either means
the delirium tremens or DTs have invaded. And, if on the
wagon is not something the afflicted has yet contemplated,
clearly, the time is at hand.
VFYI: St. Bibiana, 4th century Spanish
Patron Saint of hangovers
52
You’re not drunk if
you can lie on the floor
without holding on.
its, disease, ill health, infirmity, breakdowns, affliction,
ailment, attacks, bugs, collapse, complaint, confinement,
convalescence, disability, disorder, disturbance, dose,
failing health, flu, indisposition, malady, malaise,
prostration, seizure, syndrome, a bit of unwell, and
what’s been going around
I need to see the
Duchess of York.
Indisposition
They do not fall under the canopy of saving face, litigation,
nor feelings. No, these situations have to be the reason
euphemisms were invented in the first place.
w
Gastro-Intestinal Disorder
Few of life’s miseries have escaped schoolyard ridicule,
occasionally even put to rhyme. Therefore,
it is not surprising that lower intestinal
disturbance inspired at least one school-age
ditty—“When you’re sliding into home and your
pants are full of foam, diarrhea, diarrhea . . .”
There is an array of frank terms that describe
not the bowel disorder itself, but the
rapid response it necessitates. Hence, far
too often we hear the runs, quickstep,
sprints, trots, scoots, scatters, etc. Yet,
admittedly, any of these are preferable to
excusing oneself to company by declaring
onset of the screaming shits. Additionally,
if on one’s vacation one has an attack of the
turistas, assigning specific ethnic blame
Going to Europe with Ralph and Earl in a Buick
If one is sick to one’s stomach, we believe that is all the
information one needs to share. Throwing up or vomiting
are also perfectly good descriptive terms. It has been
our experience, once that announcement has been made,
everyone pretty much gets out of your way on the way to
the lavatory.
We reduce ourselves to the indelicacy of delineating regurgitation
euphemisms for no other reason than it is an absolute
playground for onomatopoetic words such as gurk,
urp, and barf. With one’s head stuck down the big white
phone, one can talk to Earl, Ralph, or Cousin Sis, call
Hughie or cry Ruth.
Invariably, the most colorful are offered up by friends of the
vomitee recounting the entire event to avid listeners: flash
the hash, flay the fox, feed the fish, drive the Buick,
bow to the porcelain altar, hug the throne, toss
tacos, woof cookies, laugh at the carpet, launch
one’s lunch, de-food, bestow a Technicolor yawn,
heave Jonah, blow beets, park a custard, or go see
the Duchess of York.
Evidently, there is bovine sub-category provision for the escalation
of vomiting: to bison (be nauseated), yak (very
nauseated), or water buffalo (throw up one’s toenails).
57
Fits, disease, ill health, infirmity, breakdowns, affliction, ailment, such as Montezuma’s revenge, Dehli-belly, Mexican twostep,
Spanish squirts, Botswana bop, or Cairo crud does
nothing to improve international goodwill.
Let’s face it, unless one is sitting on the edge of an examining
table wearing nothing but a gaping hospital gown, “I
am unwell,” is pretty much all anyone needs to tell.
w
Pussyfooting around The Curse
When OTR (on the rag) or having that time
of the month, few occurrences engender more
verbal pussyfooting (again, no pun intended) than
women’s troubles.
Victorian ladies suffered from domestic affliction. So general
a term, however, could mean either the sink is stopped
up or one’s husband is a cur. Today we seldom hear of the
flowers, floods, vapors, wretched calendar, or high tide.
While weathering feminine complaint, then as now, not
only can one entertain the general or fly the red flag, one
can have the painters in, a wet weekend, endure wallflower
week, or a visit from Aunt flo. When the British
have landed (wearing red coats), the Captain is at home
and it is BENO time (there’ll be no fun).
Inevitably, the onset of one’s menstrual period requires
covering the waterfront by the wearing of a sanitary
58
product. It is preferable to specify
perineal pad or tampon by brand
name (Kotex, Tampax, etc.), else one
is left with a hopeless number of riding
analogies: the cotton bicycle, red stallion,
white sling, white horse, or
fanny mattress.
From a male point of view, this item is
identified as peter-cheater or manhole
cover which, while applicable, are in
poor taste. Pleasure garden padlock
sounds oh-so-refined, but we haven’t conjured
an occasion when this, as a topic of
general conversation, was.
w
Crawling Creatures
When once only an accusation one screamed at the opposite
sex at recess, cooties have become a renewed nuisance,
not only to school children, but to the population in general.
(There are those who blame this phenomenon entirely
on the hippie generation.) One would think such progress
would have birthed a parallel vocabulary. That seems not
the case. Euphemisms for pediculosis, while dated, are
interesting: light troops, active citizens, bosom chums,
familiars, walking dandruff, intimate friends, and seam
squirrels.
59
VNOTE: Lobby lice are found in hotels,
but of the two-legged variety, not eight.
60
Genital or crab lice are crotch pheasants and pants rabbits.
Lice are chats, hence, technically, a chatty person is
not loquacious, but slovenly.
That nightly admonition to not let them bite not withstanding,
few of us ever encounter bedbugs anymore. To the
Victorians, they were a fact of life, yet a troubling conundrum.
The more fastidious citizens of society refused to utter
the word “bug” because of its unfortunate connotation
(see The Love That Durst Not Speak Its Name). Hence,
the pesky critters were known as gentlemen in brown,
B-flats, or Norfolk Howard (which may or may not reference
either the War of the Roses or Flodden Field—far too
obscure for a non-Anglophile to ascertain).
w
Social Diseases
Disgraceful disorders refer specifically to gonorrhea
(the clap) and syphilis (the pox). Other substitutes are:
bad blood, nasty complaint, bone ache, foul disease, delicate
taint, pintle fever, fire down below, forget-me-not,
Venus’ curse, and infinite malady. Historically, however,
such misfortune appears to have incited unlimited opportunity
to disparage various ethnicities: French measles,
Neapolitan favor, Spanish gout, Irish mutton, and
Rangoon itch.
61
w Foul Emanations
There once were two men in black suits
who had trouble controlling their poots
At lunch one finally said
As the other nodded his head
We should switch now from beans to fruits
Breaking Wind
Should one befoul the
air with an unduly emphatic
noise, one has
committed a rouser.
If one got by, it was a
blind-fart also known
the acronym SBD—
silent but deadly. Anything
in between is
a backfire, backdoor
trumpet, bad powder,
buck-snort, or bathtub
bubble. In addition,
a whistle
britches can suffer
butter’s revenge or
pocket thunder.
ailment, attacks, bugs, collapse, complaint, confinement,convalescence,
62
Under these audible
circumstances (if the dog
is unavailable to blame),
someone might have stepped
on a frog, talked German
(supposed guttural reference),
cut a rusty, sliced the
cheese, or shot rabbits. If
any of these aforementioned
indiscretions occur and the
offender does not know to
look suspiciously at others,
then that person does not
deserve to inhabit polite society.
As already observed,
when one is beset by gastrointestinal
disorder, there is
little discretionary reaction
time. We shall assume any
sullying of the air, too, is
inadvertent, giving all transgressors
(you know who you
are) blanket clemency.
This is the rankest
compound of villainous smell that
ever offended nostril.
Shakespeare
63
convalescence, disability, disorder, disturbance, dose, failing
There once was a wonderful wizard
who had a great pain in his gizzard
So he ate wind and snow
at 50 below
and farted a forty day blizzard.
VFYI: Breaking wind was actually a great party trick
in the Renaissance. Even Dante wrote of a fartiste
who made a trumpet of his ass. At the turn of last
century, a French nightclub performer, Joseph Pujol,
reportedly plied his artistry in the Moulin Rouge. Although
known to play O Solo Mio on the ocarina, his
tour de force was an anal rendition of Claire de Lune.
Dog Breath
If one’s breath is strong enough to carry coal, could fell a
horse at twenty paces, or smells like the Chinese army
has walked through one’s mouth in their sweat socks,
one has halitosis.
Some small woodland creature sneaked
into his mouth and used it as a latrine.

65
In the Privy
w
Calls of Nature
In Elizabethan time, the place of ease was known as a jakes,
this was eventually corrupted to ajax. Derivation of another
more oft used term for the facilities, the loo remains under
disagreement. Some like l’eau (French for water), others
insist it lieu (as in “place”). Nonetheless, euphemisms for
the room that contains a toilet can fall into two categories.
In the first, based on the concept of contrary connotation,
we have bank, chapel, coffee shop, commons, counting
house, cottage, library, office, parliament, Spice Island,
or the temple.
The less verbally discriminating, however, relieve themselves
in a bog, cacatorium, can, compost hole, dilberry
creek, dunny, forakers, john, necessarian, place where
one coughs, siege-house, or stool of ease. In most places
in Europe, one seeks the W.C. (water closet), which seems
infinitely more reasonable than in America’s restroom
(where one may sit but does not necessarily rest).
VFYI: Yes, the story is apparently true, there actually
was a Thomas Crapper who invented a flush toilet.
w Wring Out One’s Socks
Our study has revealed a vast disparity
between the number of euphemisms for
male urination (lots) compared to those
for female (zilch). This may well fall to
the unquestionably finer sensibilities prevalent amongst the
lady-folk. Either that or if one sits to release one’s bladder,
it is a solitary, quiet event. There is very little associated
activity once one has made certain the toilet seat is down.
But he who has a penis with which to pee can even write
his name in the snow—well, for argument’s sake, we suppose
a woman could do it, but it would take a while.
Men can also take the snake for a gallop, siphon the
python, shake hands with the bishop, point Percy at the
porcelain, or train Terrance on the terracotta after which
they can shake the dew off the lily.
Either sex could give the Chinaman a music lesson, but in
that few use china pots in which to tinkle anymore, it is
generally obsolete.
As an exit excuse to relieve themselves, men go water the
horses, feed the goldfish, see how high the moon is, kill a
snake, chase a rabbit, drain the radiator, or check the ski
rack.Women seem to just go to the “Ladies” to powder
their noses (albeit a bit nonsensically, in pairs).
66
VNOTE: There was a hunt-themed restaurant that
initiated some baffled head-scratching among their patrons
by labeling their respective restrooms, Pointers and
Setters.
67
flu, indisposition, malady, malaise, prostration, seizure, syndrome,

Afflicted by Time’s
Wing’d Chariot
Be kind to your children, they will choose
your nursing home.
For those of us middle-aged (assuming everyone lives to
be 110), a person of maturity has the dwindles, is a bit
forward at the knees, long in the tooth, white-topped,
blue-haired, rusting out, old as the hills, in one’s dotage,
and no spring chicken, whiling away their time in God’s
waiting room.
CAUTION: Make very certain the senior citizen of
whom one speaks is deaf as a post before one utters any
of these little nuggets. Else, the person upon whom one
remarks is always distinguished.
He is alive, but only in the sense
that he can’t be legally buried.
Geoffrey Madan (subject of the observation unknown)
70
w
The Bucket Kick’d
Are there any grander occasions to pull out all the stops,
euphemistically speaking, than speculating on just where
the dearly departed’s place of eternal rest will be? The
late-lamented could land in Abraham’s bosom, be church
triumphant, called to a higher service, or, less optimistically,
stoking Lucifer’s fires. Non-ecumenically, a quietus
or an exitus could have occurred.
Better judgment would insist (at least insofar as the eulogy)
one avoid calling the deceased either worm food or
buzzard meat.
Pardon My Dust
Dorothy Parker’s epitaph by
Dorothy Parker
There appears to be a paradoxical
inclination by the bereaved
to insist said worm
food to action when they have
had a mortality experience
(a term popular with the mortuary
profession). Hence, we
hear the dearly departed may
suck grass, grin at daisy
roots, buy the farm, give up
the ghost, pay nature’s debt,
pull a cluck, cash in one’s
chips, fold one’s hand, coil
one’s rope, drop off the hook,
71
slip the cable, sun one’s moccasins, take the long count,
jump the last hurdle, drop the cue, ride off on the last
round-up, or answer the ever-lasting knock.
The report of my death was an exaggeration.
Mark Twain, after reading his own obituary, June 2, 1897
Suicide is our way of telling God,
you can’t fire me—I quit.
72
One of the funniest of Monty Python’s routines involved
the return of a dead parrot, “Maybe he’s just shagged out
after a long squawk—no, he’s bleeding demised, ceased
to be, bereft of life, joined the choir invisible . . .”
One can go wearing the Q (the death face rather coarsely
delineated by comics—tongue lolling out the corner of the
mouth), feet first, toes up, eyes closed, heels foremost,
face turned to the wall, on one’s shield, in a box, or in
repose . . . whence one goes to the bone orchard.
ender specific activity, femininity, manhood,
manliness, masculinity, sexuality, womanhood,
womanliness, intercourse between animate beings,
coition, coitus, copulation,
intimacy, lovemaking, G
magnetism, procreation, relations, reproduction, sensuality, sexuality
fornication, generation,
Bed is the poor
man’s opera.
Italian Proverb
74
A hard man is
good to find.
Mae West
Bewitched, Bothered
and Betwattled
R
Overborne by Desire
Few, if any, still believe that only the male gender suffers
from the pangs of lust. If proof be needed, the phenomena
of Valentino, Elvis and Chippendale’s dancers provide full
support for the theory that sexual appetence is an equal
opportunity employer. Yet, regardless how prevalent its
use, we again point out that the word horny, via horn,
comes from a root word pertaining to the erect penis.
Therefore, for absolute accuracy, a woman may be just as
lustful, dissolute, concupiscent, lascivious, libidinous,
salacious, appetent, licentious, ribald, prurient, wanton,
or humpy as a man, but, unless born a hermaphrodite, or
completed gender reassignment, she will not be horny.
Those terms describing the throes of excess cupidity can
be gender specific and—however we wish they not—the
examples that come to mind for men are pussy simple,
cunt-struck and betwattled. Although a woman may have
75
hot pants or be cocksmitten, we prefer to say either is
confounded by love (more likely confounded by lust, but
it is not our place to proselytize). The unmistakable (and
most conspicuous) concomitant of desire, however, is
borne by the male: Penis in erectus.
F Temporary Priapism
Although it might initially sound like a Viagra high, a
priapism (named after Priapus, a Greek and Roman god
of male generative power) is a medical condition that manifests
itself by an unrelenting erection which is quite painful
76
A stiff prick has
no conscience.
Ancient Proverb
and—here’s the catch—is unrelieved by sexual gratification.
We will remark only upon the temporary kind.
Unlikely as it is to be referenced in one of Martha Stewart’s
fine books, for procreative (or recreational) purposes
everyone will agree that an erection is A Good Thing.
However, if the little devil rears its head when copulation
is merely on the mind but not imminent, it might prompt
some explaining—something we did not find indexed by
Miss Manners either.
If a rise in one’s Levi’s is espied by someone peripheral to
the action, we advise the male in question to adopt an air of
innocence and complain of an involuntary biological reaction.
Genital tumescence, virile reflex, and male arousal
are equally non-accusatory terms. All are preferable to hat
rack, blue-veiner, clothes prop, tent peg, live rabbit,
proud meat, horn colic, bit of a stiff, or sporting some
wood—even if one is ready to dig post holes with it.
VNOTE: Many men consider an inadvertent hard-on
(an expression we do not endorse) as an unwitting condition
and maintain, therefore, that they should not be held
accountable for that over which they hold no control (see
The Unruly Member).
Certainly beyond one’s sway is morning pride, which, for
exonerative purposes, can be identified as matutinal erection.
Indeed, if the male can convey an appropriately
sleepy-headed look, this excuse is good until noon. If one’s
nocturnal erection is inexplicably relieved during the
night, one has shot the bishop.
Gender specific activity, femininity, manhood, manliness, masculinity,
77
78
Another actual affliction is erethism, an abnormal irritability
or responsiveness to stimulation. Erethism (it too comes
from Greek, but we did not find any reference to the god of
crankiness) is an actual disorder, which does give marginal
credibility to the otherwise questionable assertion by some
men that for arousal they need no more inducement than a
stiff breeze. One could propose either of these ailments as
reason for undue . . . excitement, but both are a bit obscure.
We suggest one assert oneself as constitutionally inclined
to passion. It sounds a bit Edwardian, but far better
than randy as a goat.
Beware: If one needs to call upon this explanation
while wearing nothing more than a trench coat, it is
probable the police will look upon one’s suffering
unsympathetically. The docket sheet will read lewd
conduct, however, not weenie-wagger.
Is that a gun in
your pocket—or are you
just glad to see me?
Mae West
F
Humbled in the Act of Love
Alternately, if the male member remains flaccid regardless
of encouragement, he is suffering from orgiastic impotence.
He has not only failed in the furrow, he has no
money in his purse, lead in his pencil, ink in his pen, nor
toothpaste in his tube. When his ability is thus compromised,
he is slack in his matrimonial duty or leaving the
pillow unprest. The culprit is itself deadwood, a dangling
participle, dolphin, flounder, lob-cock, half-mast, flat
tire, hanging Johnny, or Mr. Softy.
femininity, manhood, manliness, masculinity, fornication, generation,
79
The Long Carbine
Whether one is endowed with a howitzer or peashooter,
guns are, and always have been, phallic symbols.
In the 17th century, flintlock guns had a hammer, a flint to
produce a spark, a lockpan that held the priming powder
and a main charge behind the
musket ball. When the hammer
was released, it hit a small flint
rock igniting a spark that lit the
priming powder, and if all went
as planned, then exploded the
main charge. Sometimes this prehigh-
tech procedure backfired
and the priming powder flashed
but did not ignite the main charge.
Hence a flash in the pan, but no
shot was produced. If one had
game (or the enemy) in one’s
sights but needed time to aim,
the hammer could be partially
cocked. If the gun fired while
in this position, it went off halfcocked—
no doubt a quite vexing
and dangerous occurrence.
80
81
manhood, manliness, masculinity, fornication, generation, sexuality,
We recount all of this seeming arcane information only to
provide background to fully understand the following:
If one achieves an erection but one’s intention is thwarted by
a premature ejaculation, one has gone off half-cocked, fired
in the air, shot in the bush, misfired, or has experienced
a flash in the pan. Hanging fire occurs when the priming
powder initially failed to ignite the main charge. This term
has come to be synonymous with indecision, not as some insist,
a lengthy orgasm. These expressions have been bandied
about for both sexual and non-sexual purposes for centuries.
When we study their origins, they do make perfect sense.
Sometimes a cigar is just a cigar.
Attributed to Sigmund Freud
As much as it sounds as if it should be, we all know to
peter-out is not necessarily a sexual innuendo. In fact, the
dictionary definitions for peter are as follows: (1) to diminish,
(2) to become exhausted, (3) a vulgar name for one’s
penis, and (4) one of the twelve apostles. (Insomuch as
one’s penis (3) diminishes (1) when it becomes exhausted
(2), we will conclude that other than that the Apostle Peter
(4) must have had one, he is irrelevant to this discussion).
The French word pete means to explode weakly (also an
expulsion of intestinal gas). Peter dans la main means literally,
to come to nothing. The Dictionary of Word Origins says
that peter-out originated with miners in the mid-1800’s (an
explanation of which, trust us, is even less relevant than
the Apostle Peter). Regardless, what we do know is that to
peter-out means to give out—be spent—and usually not
with a bang (so to speak). Lest one’s lover be unconsoled,
we suggest it is time to explore The French Arts.
If one can get it up, but is sterile—firing blanks, or engaging
in a dry bob, one is improcreant.
82
Agriculturalsidebar
For those unaware, when a horse
and a donkey mate, their offspring
is a mule, a hybrid. This hybrid cannot
reproduce; hence, one occasionally
hears an improcreant male
referred to as a mule.
Dallying,
Firkytoodling, and
Finkdiddling
If one has the Jones for another, as a rule, one dares not
jump their bones without first introducing oneself. Under
the right circumstances, small talk can be dispensed with,
but it is reasonable to
insist that if copulation
is the goal, at least
a little foreplay is in
order. This is known
as canoodling.
Engaging in tonsil
hockey, chewing face,
and cow kissing means,
in baseball analogy,
one has arrived at first
base. Copping a feel
above the waist is a
double. The digital
investigation of a
83
female’s privates (otherwise known as down there) is a bit
for the finger and progression to third base (a hand down
a man’s pants is probably third base also, but one seldom
hears women using sports analogies). Reasonably, physical
congress is going all the way, because one has scored
( yes, scored) a home run.
If one ruts on another without penetration or relocation of
one’s clothing, it is called a dry hump and runners do not
advance to home. Male students of an Ivy League school
perform the Princeton rub and, we are told, no one scores
(bats, if not balls, being optional).
r
Making the Beast with Two Backs
To have a legion of suitable euphemisms for doing the big
nasty at one’s fingertips, there is a very simple formula.
Merely select any combination
of an adjective
from the first column
and noun from the
second. Voila!
84
Adjective Noun
Physical congress
Carnal knowledge
Intimate necessities
Capital embrace
Amorous favors
Connubial attention
Passionate connection
Fulfilling arrangements
Horizontal relief
Illicit affections
Nocturnal pleasures
Conjugal union
Voluptuous combat
Loving consummation
Secret deed
Lewd rites
Naughty conflict
Night association
Nuptial coupling
When in want of euphemistic eloquence, however, we often
rely on Latin as in in coitu or actus coitus. Shakespeare’s
words, however, are positively rhapsodic: The very lists of
love, to make one’s heaven in a lady’s lap, dance on one’s
heels, lay it to one’s heart, or behind door work. Indeed,
having an enseamed or fortunate bed sounds a skosh classier
than parking the pink mustang up a side street. Granted,
Shakespeare was capable of circumlocutory stretch. Groping
for trout in a peculiar river means fishing in a private
stream, which means knowing someone in the Biblical
sense. Knowing someone in the Biblical sense means
manliness, masculinity, fornication, generation, sexuality, womanhood 85
86
that one is engaging in an act of generation, androgyny
(no, not Boy George—at least not necessarily), original
sin, shame, darkness, or, in the words of Delbert McClinton,
plain ol’ makin’ love.
R We Are Not Amused
In Victorian times, illicit love referred to sexual shenanigans
between anyone outside of holy wedlock. So heinous
was such activity, it was called criminal conversation (a
more stupefyingly cryptic euphemism we have yet to behold).
If it became necessary to allude to such foul doings
in print, even so ambiguous a term as criminal conversation
was far too explicit. It was abbreviated as crim. con.
or even c.c., a ruse that was not particularly effective.
Sociologists explain
that sexual shenanigans
inside holy wedlock
were frowned on as
well, but we understand
that authorities
were seldom able to
actually make arrests
for it.
H Wreak One’s Passion
If our ancestors’ sensibilities in print were euphemized into
total obscurity, private conversations were not held to the
same standard. We note that the designation for the act of
c.c. was then, as now, often preceded
by an acquiring verb: getting,
having, copping, grabbing, nabbing,
snatching, fetching, wanting,
and occasionally, begging for any
noun used to specify the female pudendum.
Typical of these nouns are
cookie, nooky, ass, beaver, bird,
jam, pork, bob, snug, box, bull’s
eye, buns, booty, cat, pussy, crack,
cranny, crotch, down, flesh, hole,
fur-pie, honey-pot, kitty, lap, milk,
monkey, mouse, muff, naughty,
oyster, poontang, rump, squirrel,
twat, twittle, and you-know-what
(often times further identified adjectively
as a bit of). All are, even
now, substituted for the detested
“c” word.*
masculinity, fornication, generat ion, sexuality, womanhood, womanliness,
87
*Although it has been in use since the
14th century, we offer less sensibility stunning
euphemisms for the monosyllable
under the heading The Little Man and
His Boat.
On occasion, this vast array of monosyllabic choices causes
an exceedingly aroused male to abandon assigning what he
wants a specific name. He may eliminate the use of a noun
altogether and just says some (get some, want some, etc).
However brainless this sounds, we have to give credit here.
Although man is an unwitting party, Webster’s does define
the word some as “being an unknown, undetermined, or unspecified
unit or thing.” This suggests that when in rut, the
male of the species does not like to restrict his possibilities.
p To Spend
If referring to the male orgasm, shot his wad, blown off,
spooged, or popped his nuts are unnecessary. He has experienced
sexual reflex. He can also have eventuation,
emissio seminis, or effect emission
(although we admit the latter does
sound a bit like a NASA instruction).
For women, making the chimney
smoke seems not overly graphic, but
again, we prefer the French, petit
mort meaning little death. Come,
spend, and get off do refer to orgasm
for both sexes, but then, so does
climax.
88
O
The Loving Spoonful
That which is ejaculated, is just that, ejaculate. Baby batter,
duck butter, man oil, bull gravy, gism, guma glue, buttermilk,
and love juice are a bit too . . . icky.We prefer it called
semen, reproductive fluid, or even sexual discharge.
Leukorrhea is the whitish viscid discharge from the vagina
known more delicately as fleurs blanches or white flowers.
(In the 19th century, wags said this condition was occasioned
by young ladies who read overly explicit French
novels.) Those less discerning call it the twitters. Only the
outright crude would pronounce her dripping for it. Under
the heading of untoward
euphemisms,
we find snail trails
(the resultant traces
of vaginal secretion
on a woman’s leg) and
pecker tracks (dribbles
of semen on trousers—
or, occasionally, a blue
dress).
fornication, generation, sexuality, womanhood, womanliness, intercourse
Prithee, summon
ye laundress.
The cheesy sebaceous matter that collects around the
glans penis and the foreskin or the clitoris and labia minora
is neither gnat bread nor crotch cheese. It is smegma
(granted, a word that sounds only marginally better than
crotch cheese). In the fairer sex, this is understood to be
too much cheese on the taco and an indication of better
attention to personal hygiene.
90
Between two evils,
I always pick the one
I never tried before.
Mae West
r
Bow at the Altar of Eros
One should try every experience once,
excepting incest and folk-dancing.
Arnold Bax
In some societies, anything other than the missionary position
( figura veneris primi) between a churched couple is
not only considered kinky but illegal. The ancient legend
of The Dragon and St. George notwithstanding, doing it
dorsally (coitus a la vache) or doggy-style will get you jail
time. But, if the woman is on her back (star-gazing) and
the man on top (beating the bunny), evidently the kitchen
table is just fine so long as the kids are asleep.
Chinese fashion denotes a sexual arrangement found in
ancient woodcuts revealing the male and the female facing
each other in an entwined X configuration. This reference
is tainted by its association with some rather tasteless jokes
proposing it necessary to copulate with an Asian woman in
such a fashion because her pudendal crevice is supposedly
horizontal.
Q. Why don’t Frenchmen like to eat flies?
A. Because they can’t get their little legs apart.
generation, sexuality, womanhood, womanliness, intercoursebetween
91
R
Different Strokes
There once was a girl from Vancouver
Whose mouth had the strength of a Hoover;
When she turned it on high,
A week would pass by,
Before anyone could remove her.
Q. What is the definition of trust?
A. Two cannibals giving each other a blow job.
In popular vernacular, a blow job, talking to mike, polishing
the helmet, giving head, or lipstick on the dipstick
denotes achieving sexual satisfaction through oral stimulation.
Either sex can be a goot-gobbler, piccolo player,
peter-eater, lick spigot, dicky-licker, or mouth-whore
(one particularly well-practiced is said to be able to suck
the chrome off a trailer hitch). Introduce the use of Latin
and a modicum of class drapes any non-orthogenital sex
act no matter how indelicate: penilingus, fellatio, or irrumatio
(root word, to suck). We add that a female who fellates
is a fellatrice.
Talking to the canoe driver, nose painting,
sneezing in the basket, yodeling in
the canyon, gamahuching, whistling in
the dark, eating at the Y, having a hairburger,
mouth music, lickety-split, and
muff-diving mean cunnilingus. (As this activity is not new,
it is not surprising that those mutton-chop sideburns were
92
called depth markers long before the sixties.) Loop de
loop, 69, or soixante neuf is mutual oral-genital stimulation.
Two women involved in such activity are engaged in
bird-washing.
Granted, shooting the beaver (or, squirrel, or moon)
could pertain to a night out with one’s favorite scent
hound, but most likely it references female exhibitionism.
Alternately, a female-attired male prostitute hides his
candy (and occasionally said candy is discovered by an unamused
client). Goober-grabber usually means a forward
woman. We note however, that goobers can be grabbed by
either sex. Sadly, we can offer no socially acceptable euphemisms
for these last entries.
sexuality, womanhood, womanliness, intercourse between animate 93
Are you a lesbian?
Are you my alternative?
O
The Love That Durst Not Speak Its Name
or the Unmentionable Vice
Paedictcato, crimen innomentatum, coitus in ano—gay
sex is still called buggery in some circles. Also, concubitus
cum persona ejus dem sexus, alternative proclivities, or
same-sex-oriented. (Although the receiver in this act has
been called a pillow-biter, we find that distinction to be
pan-sexual.)
94
95
womanho od, womanliness, intercourse between animate beings, coiti In the unlikelihood one finds it necessary to note the sexual
inclinations of another, we hold that it is best to cleanse one’s
euphemistic repertoire of the terms Nancy-boy, friend of
Dorothy’s (referencing Parker, not the Wizard of Oz—
although one’s confusion on this point might be understandable),
confirmed bachelor, light in the loafers, playing
for the pink team, pouff, and no bull-fighter. Sissybritches
and pantywaist, along with manhole inspector,
or rump ranger should also be avoided. Gunsel (derivation
of the German for gosling) is passably PC if the person
to whom one refers is also a jerk.
Daisy (along with daffodil and pansy) has long been used
as an insulting term for a gay man. That notwithstanding,
participation in a daisy chain would involve a group of
like-minded individuals engaging in simultaneous, er, sexual
acts.
VFYI: It was not until the second decade of the
twentieth century that the word faggot came to be
a disparaging term for a male homosexual. In the
middle ages, faggot meant a bundle of sticks, and,
understandably, fag eventually became slang for
cigarette. In the 1800’s, it also meant to work hard
or become exhausted (by having to cut all that fire
wood?). Its contemporary connotation possibly stems
from its use to describe the tradition of young English
boarding school boys who act as a servant to older
students. The rumors about what actually occurs in
single-sex schools when hormones rage undoubtedly
fanned the fires of this particular vulgarism.
96
As for her, she may be somewhat butch, but rug-muncher,
lesbyterian, and bull-dyke are understandably offensive.
(For further aspersions on masculine women, see Dog City.)
o
Switch Hitting
If one has taken a position of non-discrimination about the
sexual orientation of one’s romantic encounters, one is referred
to on the street as ambisextrous, double-gaited,
batting and bowling, AC-DC, trolling both sides of the
stream, driving a two-way street, or buttering both sides
of the bread.We, however, insist said person is influenced
by an amphi-genous inversion.
w Fetishes and General Freakiness
A widow whose singular vice
was to keep her late husband on ice.
Said “It’s been hard since I lost him-
But, Ill never defrost him!
Cold comfort, but cheap at the price.”
A hundred years ago the learned did not call pornography
smut, but facetiae or curiosa. By the twentieth century, an
abnormal interest in obscene material was known as coprophilia,
although more technically that is the use of feces
for sexual excitement. Those who revel in such activity are
indelicately referred to as fecal-freaks, kitchen cleaners,
and felch queens. To felch means to perform anilingus*
(tookus-lingus to our less somber fellow beings). A pound
cake-queen enjoys being defecated on. Anyone engaged in
this base pastime (beyond the age of two) is taking part in
a Boston Tea Party. An associated activity, golden showers,
is more formally termed micturation. (Our informawomanliness,
intercourse between animate beings, coition, coitus,
97
*If one needs it spelled out, anilingus is to the anus what cunnilingus is
to the cun . . . er, vagina.
tion notwithstanding, polite society prefers that if these . . .
entertainments exist, they pass unremarked.)
Apparently, there is a “queen” designation to fit every
manner of sexual diversion. For instance, one who is
under the spell of a foot fetish (equus eroticism)
is known as a shrimp-queen (referring to the
shape of the toes), enjoys sex in the great
outdoors: a green-queen, or in public:
a tea room-queen.
Once: a philosopher, twice: a pervert!”
Voltaire
(turning down his second invitation to an orgy)
If generously lubricated by cooking oil, your run-of-themill
orgy is a Mazola party. Team cream, gang bang, round
pound, and bunch punch appear self-explanatory—this
poetry occurring, undoubtedly, when one trots out one’s
pussy to pull a train, or choo-choo.
98
Said the masochist to the sadist: “Beat me, beat me!”
Replied the sadist: “No.”
Originally, ascetics employed ritual floggings to induce discipline
through strict self-denial by means of mortification
of the flesh. Evidently, somewhere in this mix of flogging
and religious frenzy, sexual titillation reared its ubiquitous
head. Pious monks in robes metamorphosed into leatherclad
dominatrices wielding wicked-looking whips. Indeed,
if one has not actually experienced it, one has certainly
heard of being whipped into a frenzy.
intercourse between animate beings, coition, coitus, copulation,fornication,
99
My problem is reconciling my gross habits
to my net income.
Attributed to Errol Flynn
There is bondage and sado-masochism. In bondage one
finds erotic stimulation either in exacting or under subjugation.
One who finds sexual satisfaction by inflicting pain on
another is a sadist—not slap artist nor fire-queen (not to
be confused with a flaming-queen who is a relatively benign,
if outrageously flamboyant, male homosexual).
Flagellation (not to be confused with flatulence—although
it is noted that foundered horses used to be flogged in order
to force them to pass gas) can be inflicted either by a
willing compatriot or one’s own hand. Indeed, a masochist
gets off by being beaten literally or figuratively. And while
most people know that sadism found its name by way of
the infamous Marquis DeSade, few realize masochism
100
was named for the German novelist, Leopold von Sacher-
Masoch who evidently also wrote of what he lived.
VNOTE: There is the term Zooerasty, but we decided
that there is some information we just do not want to have.
Clearly, however, limerick writers have been much amused
with the notion.
The Right Reverend Dean of St. Just
Was consumed with erotical lust
He buggered three men
Two mice and a hen
And a little green lizard that bust.
between animate beings, coition, coitus, copulation, fornication, generation,
101
Sex “Sain et Sauf ”
One may hear of peek-freaks and peer-queers, but they
are voyeurs. If one gets one’s jollies by listening, one is an
ecouteur.
102
103
animate beings, coition, coitus, copulation, fornication, generation, w
Dishearten
If a man is sexually aroused and then brought to satisfaction
by manual manipulation of his member by another, he
has been brought down by hand, a more circumspect term
for a hand-job, spitting white, or upshot. (The person
who supplied the hand is a peter-beater who caught an
oyster otherwise known as “sweetheart.”)
No Glove—No Love
It has been some time since one would hear of a condom
wearer as fighting in armor. So ancient is the term, it very
nearly predates the invention of circular protection itself.
It may surprise some to know that condoms were in use as
early as Elizabethan times although they did not become
common (and then only in the large cities) until two centuries
later.
These early sheaths were often made of material such as
sheepskin (ouch), fish bladder (yuck) and eventually, rubber.
Indeed, Casanova is said to have bragged of owning a
pretty little linen ditty (un petit linges) with a delicate drawstring
ribbon—the upper class does have its privileges.
Some of these early protectors were said to bear the portraits
of famous persons—the significance of which does
not immediately leap to mind. Perhaps a modern equivalent
is a lunch box bearing the image of Mighty Morphin’
Power Rangers—but we suppose we digress.
Once these handy little devices were perfected, euphemisms
did abound—keeping down the census, taking a dry run,
or wearing a cheater.Wearing a fearnought, lace curtain,
diving suit, head gasket, nightcap, catcher’s mitt, rubber
cookie, overcoat, raincoat, saddle, shower cap, lifepreserver,
washer, party hat, Dutch cap, phallic thimble,
or, less preferably, a cum drum could reference either a
condom, or a diaphragm. A pussy butterfly is an intrauterine
device or IUD.
Most commonly, however, a protective sheath has been
called, variously, a French, Italian, Spanish, or American
letter. (We did not find indication that it has been called an
English letter. Hmmm.)
Malthus was an English curate. The Maltusian
Theory posits that population tends to increase
geometrically and resources or means of subsistence,
arithmetically. Hard-liners believed that
unless procreation was checked by moral restraint
or even disaster (pestilence, famine, or
war) unrelenting poverty and its resultant human
degradation would inevitably result. This doctrine
is often cited as an excuse for the use of
birth control, hence, labeling any non-procreant
sex, Malthusian.
A vasectomy means not ever having
to say you’re sorry.
w
Sex a Cappella
The rhythm method may indeed be chancy, but it is still a
bit graceless to refer to it as playing Vatican roulette. Riding
bareback means intercourse without protection. On
occasion, condom-less sex is called making faces in that it
often leads to producing babies.
VFYI: As the male anatomy is limited in its ability
to produce semen rapidly, when a man is fortunate
enough as to enjoy what is called a triple header, a
fourth round might result in his coming air.
NOTE: In England if someone tells you to keep your
pecker up, it is indeed an expression of encouragement.
However, not necessarily what one imagines. In Great
Britain, pecker can mean chin.
NOTE, PART 2: There was a big hoo-haw over there when
the American movie Free Willy premiered because willy
does not mean chin in England.
NOTE, PART 3: If fortune has smiled upon one to the extent
that one has somehow missed seeing any of the Austin
Powers’ movies, please understand, in Great Britain, a
shag is not a 70’s haircut.
beings, coition, coitus, copulation, fornication, generation, intimacy, 105
My wife doesn’t understand me.We’re only staying
together for the kids. I’ve never done this before. I only
shoot blanks. If you get pregnant, honey, I’ll take care of you.
I’ll respect you in the morning. I promise it won’t come in your mouth.
107
The check’s in the mail. I’ll call you.
Men Behaving
Badly
Although snatch can be either verb or noun, in either
sense it usually refers to rapid copulation. Irish foreplay,
sometimes known as brace yourself Bridget, is essentially
a Wham Bam, Thank You Ma’am with a lilt. Jewish foreplay,
we have been told, involves only extended pleading.
A flyer can be either prone or vertical. If an upright quicky,
it is also known as a knee-trembler. Occasionally this excursion
doesn’t even entail intercourse. To cop a feel is either
with or without permission and is usually performed
nose open (for those unfamiliar with animal husbandry—
it describes an eager bull) possibly with it in his hand
(which clearly does not).
If it need be said, we
endorse neither the
above behavior nor
the euphemisms, we
only offer them for
elucidative purposes.
108
He would fuck a snake if someone
would hold its head.
109
One need not call a gestating woman knocked up*. She
is enceinte, fecund, or expecting. While we do not find
preggers offensive, round-wombed, about to find pups,
apron-up, one is up the spout, a lap full, or a bun in the
oven do not offer the proper respect as does experiencing
a blessed event.
*Again, this is a phrase to use judiciously in England. For there if a
woman has been knocked up it means she has merely experienced someone
rapping on her door. Whether or not that leads to a shotgun wedding
is beyond our polite speculation.
A Pea in the Pod
110
Misbegotten
The word bastard was once solely a comment on one’s
happenstance of birth. Some say the word came from the
French, fils de bast or packsaddle child.Born on the wrong
side of the blanket (the foul event producing said infant
taking place other than within the marriage bed) or born
in the vestry (left on the church steps) were other roundabout
ways of disparaging one’s heritage. Also, a counterfeit,
wood colt, stall whimper, nullius filius, or side-slip
(as in oops). Producing a child out of wedlock to Shakespeare
was to tender a fool. More specifically in the chain
of generations, a child born out of wedlock to a mother
Only if its
done right.
Is sex dirty?
who was of illegitimate birth herself, was said to be a bellbastard.
Today, the word bastard, as used in this sense is so out of
fashion as to be irrelevant. Indeed, most probably all these
euphemisms have been overridden by “DNA test” and
“child support payments.”
beings, coition, coitus, copulation, fornication, generation, intimacy,
111
112
Unknown to Man
In historical romance novels, a virgin (depending on the
genre) was either picked, plucked, ruined, trimmed,
deflowered, or devirginated. When a sweet young thing
succumbed to seduction, she was persuaded to venery.
Until then she was a chaste treasure, virgin patent, or
rosebud, possibly remaining that way by wearing iron
knickers. Digital investigation of the vagina comes under
the heading of heavy petting. For
anyone in need of exact terminology—
in legalese, carnal knowledge
is the slightest penetration of the
vulva. If one is messing with jailbait,
one is looking at jail time.
Even if she spends more time on
her knees than a priest, she is still
technically a virgin—a demi-vierge—
never having gone all the way.
Indeed, if the results of recently
published, if somewhat unscientific, polls are to believed,
by today’s standards, a fellatio generating enough suction
to suck a golf ball through a garden hose would not be
considered a sex act. We believe this is now known as
The Clinton Exculpation.
In scholarly tomes,
one may come across
the term, claustrum
virginale. This reference
is a bit of a
stretch in that claustrum
is one of the
four basal ganglia in
each cerebral hemisphere
that consists
of a thin lamina of
gray matter separated
from the lenticular
nucleus by a layer of
white matter (whew).
However, we believe
it is more likely that
it is derived from
claustral—cloister.
coition, coitus, copulation , fornication, generation, intimacy, lovemaking,
113
114
Trafficking
with Oneself
95% of people masturbate.
The other 5% just lie about it.
If there are no obliging friends about and one does not care
to avail oneself of commercial outlets (brothels), genital
stimulation via phallengetic motion may be the only alternative
for one’s sexual . . . disposal. Still condemned by
many, the solitary vice has managed not only to survive,
but flourish, even under the threat of blindness, insanity,
hairy palms and your mother’s (and the Church’s) wrath.
There was a young fellow from Yale
Whose face was exceedingly pale.
He spent his vacation
In self-masturbation
Because of the high price of tail.
Many believe masturbation is synonymous with Onanism.
In the Bible, however, Onan’s sin, scholars insist, was not
coitus, copulation, fornication, generation, intimacy, lovemaking,magn
115
masturbation at all but coitus interruptus. (Regardless, he
was slain by God for this heinous sin and let that be a lesson
to us all.) This misunderstanding may well have been a
deliberate Victorian manipulation of the scriptures laying
the groundwork for generations of adolescent guilt-trips.
Additionally, as Onan was a guy, these guilt-trips were
taken primarily by young men. The Victorian rationale
was that all womankind (wives, daughters, mothers—i.e.
any female a Victorian man was not trying to seduce) were
chaste of mind and body. Indeed, most doctors of the era
persisted with the fallacy that ladies were devoid of sexual
desire; therefore, the possibility of these women having an
orgasm with or without the aid of a penis did not exist.
Hence, it is the masculine population whose indiscriminate
nocturnal . . . twiddling came under intense scrutiny and
abject condemnation. So strong was the shame, we still
hear self-pleasuring condemned as genital pollution, selfabuse,
and the sin of youth.
Wanking may be the more oft-used term, but for those
formal occasions, digitally oscillating one’s penis or selfinduced
penile regurgitation would be preferable. In
French, se branler, se crosser, se faire les cinq doigts de la
main, se passer un poignet, or la veuve poignet (branler—to
shake, crosser—to club, faire—perform, cinq doigts—five
fingers, passer—happen, poignet—wrist, veuve—widow—
you do the math.) If merely fiddling with the equipment,
one is playing pocket pool.
As the dogma surrounding this abhorrent act is so intense
and the deed is fraught with such euphemistic eloquence, it
is an absolute necessity to be generous in recounting them.
w
Take Herman to the Circus
The chasm separating proper Victorian sensibility and that
of the unwashed masses about who was and was not the
116
117
Master of One’s Domain (a term “Seinfeld” did not invent)
can be succinctly defined. For every shaming euphemism,
we find dozens that are unrepentant (and if not actually
poetry, one can appreciate the rhyme): bleed the weed,
bang your wang, shake the snake, ram the ham, rope the
Pope, spank the frank, squeeze the cheese, stroke the
bloke, crank one’s whank, flog the log, lube the tube,
wanker the anchor, hone the cone, strain the vein, pump
the stump, torque the fork, thump the pump, tickle the
pickle, jerkin’ the gherkin, yank your plank, tease the
weasel, fist your mister, punchin’ the munchkin and
make the scene with a magazine.
Some are, if not poetic, at least alliterative: burp the baby,
cuddle the Kielbasa, fondle the fig, punish Percy in the
palm, smash the stake, hug the hog, stir one’s stew,
strangle the stogie, slap pappy, bash, beat, or bop the
bishop, pummel the priest, wave the wand, whip one’s
copulation, fornication, generat ion, intimacy, lovemaking, magnetis m,
wire, paddle the pickle, bang the banjo, dash one’s
doodle, grip the gorilla, and prime one’s pump.
There are dated euphemisms—get the German soldiers
marching, have a date with Rosy Palms, polish one’s
helmet, phone the czar, take Herman to the circus, feed
the ducks, clean one’s rifle, give a one gun salute, and
choke the sheriff and wait for the posse to come; and
contemporary—adjust your set, go on Pee Wee’s little
adventure, boot up the hard drive, paint a small Jackson
Pollock, stretch the turtleneck, play the single-string
air guitar, feed the Kleenex, tweak your twinkie, do
the Han Solo, romance the bone, choke Kojak, play
Uno, R2 your D2, upgrade your hardware, test fire
the love-rocket.
There once was a man named McGill,
Whose acts grew exceedingly ill,
He insisted on habits,
involving white rabbits,
and a bird with a flexible bill.
The animal kingdom is not only not exempt, it is well represented:
gag the maggot, lope the mule, wax the dolphin,
burp the worm, look for ticks, and corral the tadpoles.
Granted the prominent feature of euphemisms for
this activity is hardly political correctness, a few examples
would outright enrage PETA: club the baby seal, violate
the hedge-hog, flog the dolphin, suffocate the trout,
pound the pup, pump the python, or choke the chicken.
118
119
I swear, you could ask your
class if they’d had sex with goats
and the next thing you’d hear
is somebody asking,
‘Define sex.’
Overheard from a College Professor
Everything else falls into the category of one-night stands:
come to grips with oneself, climb Mount Baldy, audition
the finger puppets, beat the bald-headed bandit, do the
five-finger solo, iron the wrinkles, make the bald man
puke, shake hands with the unemployed, rough up the
suspect, summon the genie, butter the corn, kill the
snake, seed the rug, paint the ceiling, dig for change,
fire the flesh musket, frost the pastries, mangle the
midget, unload the gun, varnish the flagpole, and cane
the vandal.
While amusing, none of the above makes quite the statement
as does address Congress.
120
w
Searching for Spock
Nymphomaniacal Jill
Tried a dynamite stick for a thrill
They found her vagina
Way over in China
And bits of her tits in Brazil
Contrary to popular opinion, self-pleasuring (even, experts
say, obsessive self-pleasuring) is not unique to the male of
the species. We note that masculine verbal images involve
all manner of phallic symbols such as guns and beasts.
Those feminine (aside from allusions to small furry creatures)
are quite dissimilar. (These are offered, we swear,
only for their sociological enlightenment value.)
There are culinary references—preheat the oven, baste
the beaver, grease the skillet, butter the muffin, skim the
cream, sort the oysters, stir the cauldron, roll the dough,
and stuff the taco. In the aforementioned animal category—
caress the kitty, dunk the beaver, feed the bearded clam,
fan the fur, make the kitty purr, roll the mink, floss the
cat, and check the foxhole. Specific to the feminine sex
too are dig for one’s keys, candle bashing, apply lip gloss,
dust the end table, air the orchid, do one’s nails (also
soak in Palmolive), get a stain out of the carpet, part the
petals, polish the pearl, do something for chapped lips,
gusset typing, unclog the drain, ride side-saddle, paddle
the pink canoe, wake the butterfly, and work in the garden.
fornication, generation, intimacy, lovemaking, magne tism, procreatio 121
122
Fare un ditolino (Italian: to do a little finger)
Specific to self-digitation is to drown the man in the boat,
circle the knoll, flip through the pages, grope the grotto,
leglock the pillow, null the void, read Braille, stroke the
furnace, surf the channel, play the silent trombone, do
the two-finger slot rumba, play solitaire, check one’s oil,
tickle one’s fancy, tiptoe through the two-lips, check the
status of the I /O port, bury the knuckle, and search for
Spock. Oh yes, females can rhyme too: scratch the patch,
scuff the muff, itch the ditch, and rubbin’ the nubbin.
Not specific to women, but peculiar to them is the polymorphously
perverse orgasm where the entire body, not
123
generation, intimacy, lovemaking, magne t ism, procreation, relations, just its genitals, is a source of erotic pleasure. Without
direct clitoral stimulation, some women say psychic
orgasm can be achieved. All one needs is either a risqué
novel or a full bladder. No doubt, euphemisms have been
coined just for such occurrences, but we are still digesting
the information.
w
Pokin’ the Pucker
The word dildo has been in use since the 16th century, but
there is evidence the item was created not long after Eve
herself. The ancient Greeks called them godemiches, the
French, bijoux indiscrets. Made of glass or velvet, they
were also known as paprilla, cazzi, consolateurs, and bienfaiteurs.
Whatever one’s position on the use of mechanical
devices to achieve sexual fulfillment, we can agree that
today’s battery-powered vibrators are
a vast improvement over 18th century
ladies’ penchant for turkey necks
(headless, carcass-less turkey necks
we pray, else it’s a whole other story).
Today, if a woman employs an indiscreet toy too . . . enthusiastically
upon herself, the medical community refers
to it as a picket fence injury. Masturbatory mishaps by the
male of the species seem to revolve around where, shall we
say, the evidence might happen to land and is rarely lethal
(notwithstanding Portnoy’s worry about the bathroom light
bulb). As far as we have evidence, that story about the
young man whose investigation of the erotic effects of a
vacuum cleaner hose left him with a severely elongated
penis, is only an urban legend.
Although many believe that inflatable dolls are a recent
phenomenon, we understand that there was such a thing for
horny sailors called a Dutch sea wife. How anatomically
correct these dolls were remains unascertained, but a
Dutch husband was a bed bolster.
124
Were it not for imagination, a man
would be as happy in the arms of a
chambermaid as of a Duchess.
Samuel Johnson
w
Fleshly Treason
What men call gallantry and the gods, adultery
Is much more common where the climate’s sultry
Lord Byron
Let us make this abundantly clear: adultery is what others
commit. However, if one’s own shoes find themselves under
another’s bed, one has suffered an error of the blood.
A spouse’s infidelity is grounds for divorce (if not an
exchange of gunfire). If one wanders a bit (“I was thinking
of you the whole time, honey”), it begs forgiveness.
In the 1900s, those guilty of facile morals committed a
marriage breach by engaging in illicit embraces. Now, if
intimacy, lovemaking, magne t ism, procreation, relations, reproduction,
125
one’s affections stray, it is called offshore drilling, parallel
parking, or extra curricular activity. In any case, one
is likely to be the recipient of a folded piece of paper announcing
one’s imminent matchruptcy, dewife-ing, or
splitting of the sheets spelled D-I-V-O-R-C-E and the
lawyers won’t be kind. To remain faithful to one’s vows,
however, is to keep league and truce.
If one is not in a committed relationship and simply
screwing around, it is an affaire d’ amour (which is French
for one night stand). Hence, one is in an irregular situation,
breaking the pale, and indecorously familiar.
126
French does offer us very specific nuggets of circumlocutory
gold. Cinq-a-sept refers to a customary afternoon period
for quick assignations, hence the slang, un petit cinqa-
sept—a matinee; Le demon de midi—demon at noon:
mid-life crisis or middle-aged men or women with
eighteen-year-olds. (Ever notice that you never see someone
living solely on social security sporting arm candy?)
Skin off old dead horses is to marry one’s mistress.
Q. How are a redneck divorce and a tornado alike?
A. Somebody’s gonna lose a trailer.
Of a man who was very unhappy in marriage and remarried
immediately after his wife died, Samuel Johnson observed
that it was “triumph of hope over experience.”
w
Family Jewels
Baubles, Bangles and Beads
In polite company, they are privates, urogenital concern,
apparatus, loins, or vitals. Indeed, sports announcers tend
to identify this area as the groin or lower abdomen, apparently
believing that to broadcast a more accurate “Ouch,
that shot to the nuts had to hurt!” is not FCC-becoming.
In less discreet society, the virilia are called doodads, marriage
tackle, peculiar members, nads (short for gonads),
lovemaking, magne t ism, procreatio n, relations, reproduction, sensuality,
127
wares, Adam’s arsenal, stick and bangers, lingam (from
the Kama Sutra), credentials, testimonials, pencil and
tassels, and master of ceremonies.
Whirleygigs, baubles, jinglebangers, whenneymegs,
clangers, clappers, and bangers betray a great deal of fascination
by the male of the species with their own apparatus.
Regardless how noisy they all sound, none have, as far as is
known, ever actually made any audible noise (not counting
that poor man with testicular cancer whose doctor experimented
by replacing his with ball-bearings—they didn’t
rust but his scrotum drooped abysmally).
Not unexpectedly, there is a specific term for the relaxation
of scrotum—the whiffles. There is an equally curious name
for the foreskin—whickerbill.
w
The Unruly Member (My Body’s Captain)
There was a man from Ghent
Who had a penis so long it bent
It was so much trouble
That he kept it double
And instead of coming he went.
Only the female pudendum rivals the membrum virile for
euphemistic grandiloquence such as purple helmeted warrior
of love. Are anthenaeum, Aaron’s rod, carnal stump,
128
Q. What did Adam say to Eve?
A. Stand back, I don’t know how big this thing gets!
husbandman of nature, lance of love, man-root, torch of
cupid, or dribbling dart of love too overwrought? May
we suggest swaydangle, larydoodle, tallywacker, flapperprick,
or bean-tosser (a term of which one dares not guess
a derivation). The basis for bald-headed hermit and oneeyed
trouser snake seem far less obscure.
The ancient term for the male appendage was yard. This,
mercifully, was when this word meant a stick or rod, not
thirty-six inches. Almost as ancient is man’s inclination to
give his appendage a pet name. This selection often reflects
the esteem (or lack thereof ) in which said member is held
by its owner—Big Steve, Pile-Driver, merry-maker,
General Custer, He Who Must Be Obeyed, Old Faithless,
or Sleeping Beauty. For pomposity, one cannot top
plenipo, an abbreviation of plenipotentiary, which means
“a diplomatic agent invested with full power to transact
business.” We also hear Tommy, Dick, Harry, Willie,
Giorgio, Percy, and Peter (which reminds us that it was
Groucho Marx who observed that actor Peter O’Toole’s
name was a penis euphemism times two).
The terms bayonet, bazooka, blade,
brachmard, dagger, dirk, gun, sword, and
weapon reflect the already acknowledged
phallic/weapon imagery.
One can, of course, always call it a penis.
130
131
w
The Upright Wink
or The little man and his boat
Once described by some wit as the factotum (that which
controls all things), “it” is also demesnes (female domain).
If one cannot bring oneself to say the word vagina, try
hypogastric cranny (which will serve the purpose, no
doubt, of completely bewildering one’s audience). More
easily interpreted, if not particularly concise, we have yoni,
love’s sweet quiver, delta of Venus, tufted love mound,
Alter of Hymen, Adam’s own (beg pardon?), nonnynonny,
the Ace of Spades (clearly presaged the advent of
Nair) and the cabbage garden (which does explain that
inane story about where babies came from).
There once was a woman from China,
Who went to sea on a liner,
She slipped on the deck,
And twisted her neck,
And now can see up her vagina.
The mons veneris has generated a whole school of hirsute
appellations, namely: bearded clam, bearskin, brush
brakes, bush, belly whiskers, thicket, down, nature’s
veil, fleece, fluff, motte, and beaver (which alone accounts
for the derivations beaver-den, beaver-flick, beaver hunt,
beaver pose, beaver-retriever, and the ever-popular
beaver-shot). The entire range of pussy metaphors would
require corresponding redundancy and offers no particular
revelations apart from the clearly onomapoetic term kweef
which can most briefly be described as a pussy-fart.
There was a young lady named Brent
With a cunt of enormous extent
And so deep and so wide
The acoustics inside
Were so good you could hear when you spent
132
There is a rather indiscreet tale of a man who claims to have encountered
a vaginal cleft so commodious that upon intromission he
discovered “another bugger looking for his hat.”
w
Nature’s fonts
I once knew two sisters whose breasts
They exposed to their thunderstruck guests
A policeman was called
And the young chap, enthralled
Ogled, but made no arrests.
The female breasts. Or appurtenances,
bosom, bust, front, mammary
glands, mammilla, teats,
balcony, big brown eyes, headlights,
lung warts, love bubbles, baloobas,
bazookas, bazoongies, garbonzas,
gazongas, kajoobies, toraborahs,
maracas, lollies, diddies, or bodacious
ta-tas often encased in an
over-the-shoulder-boulder-holder
or flopper stopper.
Wasn’t it one of the “friends” on TV
who observed that it was God’s plan
magnetism, procreation, relat ions, reproduction, sensuality, sexualit 133
that men didn’t have boobs because, if they did, “they’d
never get anything done”?
I knew a young lady named Claire,
Who possessed a magnificent pair,
Or that’s what I thought,
Till I saw one caught,
On a thorn and begin losing air.
134
beings, coition, coitus, copulation, forni cation, generation, intimacy, lovemaking, 135
136
aft, mentally strange, barmy, unzipped,
batt y, berserk, insane, bonkers, cracked,
loony, crazed, cuckoo, demented, deranged, peculiar,
erratic, flaky, fruity, idiotic, insane, lunatic, mad, maniacal, nuts, potty,
psycho, touched, unbalanced, unglued, unhinged, wacky
D135-
We are all born mad.
Some remain so.
Samuel Becket
The Gazelles are
in the Garden
w
Cerebrally Challenged
As spoken by a Texan, it is not “ig-nor-ant,” but “ig-nernt,”
thus altering the meaning from “unlearned” to “too stupid
to live.” Our exploration of euphemisms for ignorance will
be specific to the second definition.
Stupidity is the deliberate cultivation of ignorance.
William Gaddis
Obtuse, dull, imperceptive, opaque, stolid, unintelligent
—in other words, a peckerhead, one whose intellect is rivaled
only by garden tools, got off the Clue Bus a couple
137
They say that the difference between
genius and stupidity is that
genius has its limits.
of stops early, is dumber than a box of hair and couldn’t
pour water out of a boot with instructions on the heel.
The cerebrally challenged can occasionally be identified
by physical characteristics: narrow between the eyes,
green as a gourd, room temperature IQ, wanting in the
upper story, more nostril hair than sense, numb nuts,
mouth-breather, and flat Peter (trampled penis syndrome).
138
Statistics say that one out
of every four Americans suffers from
some form of mental illness. If your
three best friends look okay,
then it’s you.
If one has too much yardage between the goal posts, one
is likely smart as bait, not the sharpest tool in the shed,
the brightest crayon in the box, nor shiniest bulb on the
Christmas tree. If one is as fat as a hen in the forehead
one is in want of understanding, thick as a plank, dense
as a post, dumber than a bag of hammers, experiencing a
leak in the think-tank, or suffering a crop failure and is
definitely officer material.
w
Whiff of the March Hare
Insanity doesn’t only run in my family, it actually gallops.
Anon.
w
Non Compos Mentis
Although lunatic is a perfectly good word to describe
someone who is absolutely crackers, this loss of reason is
rarely addressed head-on. Circumlocution often implies the
139
Daft, mentally strange, barmy, unzipped, batty,berserk,
afflicted is not properly wound, as in wandered, unhinged,
or unglued.
When someone is dotty, it is not unusual to allude to something
lacking; therefore they are—
A few bricks shy of a load.
Couple of bubbles off plumb.
Several fries short of a happy meal.
One midget shy of a Fellini movie.
Two clowns short of a circus.
One Fruit Loop shy of a full bowl.
One taco short of a combination plate.
A few feathers short of a whole duck.
A few beers shy of a six-pack.
A few peas short of a casserole.
Or:
Both oars aren’t in the water.
Only 50 cards in the deck.
Wants for some pence in the shilling.
Cheese slid off one’s cracker.
All one’s dogs aren’t barking.
One’s elevator doesn’t go all the way to the top.
Lights are on but no one is at home.
No seeds in the pumpkin.
Doesn’t have all his cornflakes in one box.
The wheel’s spinning, but the hamster’s dead.
Her sewing machine’s out of thread.
His antenna doesn’t pick up all the channels.
His belt doesn’t go through all the loops.
Missing a few buttons on the remote control.
140
No grain in the silo.
Receiver is off the hook.
All foam, no root beer.
w
A Stranger to Reason
If absolutely nuts, one is playing
with the squirrels, has walnut
storage disease, or is several nuts
short of a full pouch. In the pinball
game of life, if one’s flippers
are a little further apart than
most, one is dicked in the nob,
blinky (milk about to sour), off
one’s napper, has a slate loose, is
damp in the attic, one’s slinky is
in a kink, one’s skylight leaks, or
one’s drawers are left open.
Of course, if one is wealthy anything
queer one says or does is
merely eccentric.
141
insane, bonkers, cracked, loony, crazed, cuckoo,
142
VCautionv
Even a fool can be right
once in a while.
’N What?
2 Or 5
Ode to Dan Rather
Lower’n the rent on a burning building
Jumpier’n virgin at a prison rodeo
Emptier’n a eunuch’s underpants
Colder’n a well digger’s ass
Stiffer’n a preacher’s prick at a wedding
Tighter’n the bark on a tree
Smoother’n snot on a doorknob
Happier’n a coon on an ear of corn
Awkward’n a cow on skates
Lower’n a snake’s belly
Slicker’n owl shit
Clumsier’n a pig on ice
Smaller’n a bar of soap after a hard day’s wash
Colder’n a copper toilet seat in the Klondike
Deafer’n an adder
Fuller’n a tick
Hotter’n a fresh fucked fox in a forest fire
Happier’n a man who spent the day sorting
out his concubine collection
Sicker’n a pizzened pup
Finer’n bee’s wings
Noisier’n skeletons fucking on a tin roof
Uselesser’n pantyhose on a pig
Panickier’n a pig in a packing plant
Jittery’n than a long-tailed cat in a room
full of rockers
Grinnin’ like a possum eatin’ cactus
Happier’n a baby in barrel of tits
Busier’n a dildo in a harem
Icier’n the shady side of a banker’s heart
Madder’n a wet hen
Slowr’n a wet week
Happier’n clams in high water
Happier’n a puppy with two peters
6
143
demented, deranged, peculiar, erratic, flaky, fruity
The author acknowledges the Dover Publications Pictorial
Archive series, and the satirical graphics of Cruikshank,
Gillray, Hogarth, and Rowlandson.
problems. They use the past to reflect the present in hopes of
resolving its crises. Their novels explore how political history
is shaped by individuals or how it shapes them in turn.
Related Historical Events: Like the American Revolution,
the French Revolution was launched in the spirit of rational
thought and political liberty. But these ideals of the 18thcentury
Enlightenment period were soon compromised when
the French Revolution devolved into the “Terror”—a violent
period of beheadings by the very citizens who overthrew the
tyrannous French monarchy. The French Revolution cast a
long shadow into 19th-century Britain, as industrialization
seemed to divide the English population into the rich and
poor. Many people feared the oppressed working class
would start an English Revolution, but a series of political
compromises and wake-up calls like Dickens’s A Tale of Two
Cities helped to avert the potential crisis.
Extra Credit
Serial fiction: Like many of Dickens’s novels, A Tale of Two
Cities was first published in installments in his magazine All
the Year Round. Many Victorian novels were first published in
serial parts and then later collected into books.
American favorite: Since its publication, A Tale of Two Cities
has always been Dickens’s most popular work in America.
Background Info
Key Facts
Full Title: A Tale of Two Cities
Genre: Historical novel
Setting: London and Paris
Climax: Sydney Carton’s rescue of Charles Darnay from
prison
Protagonist: Charles Darnay
Antagonist: French revolutionaries; Madame Defarge
Point of View: Third person omniscient
Historical and Literary Context
When Written: 1859
Where Written: Rochester and London
When Published: 1859
Literary Period: Victorian era
Related Literary Works: Sir Walter Scott pioneered the
genre of historical fiction. In novels like Waverley, Scott
places fictionalized characters against a war-time historical
tableau. Scott also uses a narrator who alternately explains,
editorializes, preaches, and jokes, like Dickens’s own
characteristic narrative voice. Historical fiction evolved with
works like George Eliot’s Middlemarch with its multiple
plot lines and realistic psychological detail. Scott, Dickens,
and Eliot all use historical fiction to examine contemporary
Author Bio
Full Name: Charles Dickens
Date of Birth: 1812
Place of Birth: Portsmouth, Hampshire, England
Date of Death: 1870
Brief Life Story: Born to a naval clerk, Dickens moved
with his family to London at age 10. When his father was
briefly imprisoned for debt, Charles worked long days at a
warehouse. He left school at age 15, but read voraciously
and acquired extensive knowledge through jobs as a law
clerk, court reporter, and journalist. As a novelist, Dickens
was successful from the start and quickly became the most
famous writer in Victorian England for his unforgettable
characters, comic ingenuity, and biting social critique. He also
enjoyed huge popularity in America where he made several
reading tours. He worked tirelessly, producing a magazine
Household Words (later All the Year Round) and cranking out
still-famous novels including Oliver Twist, Bleak House, Great
Expectations, and David Copperfield. Dickens had ten children
with his wife Catherine Hogarth, but their marriage was never
happy and Catherine left him after Dickens had an affair with
the actress Ellen Ternan. Dickens died in 1870 and is buried in
Poets’ Corner of Westminster Abbey.
The year is 1775. On a mission for his employer, Tellson’s Bank,
Mr. Jarvis Lorry travels to Dover to meet Lucie Manette. On
his way, Mr. Lorry receives a mysterious message and replies
with the words “Recalled to life.” When they meet, Mr. Lorry
reveals to Lucie that her father, Dr. Alexandre Manette, who
she thought was dead, is still alive. Dr. Manette had been secretly
imprisoned for 18 years in the Bastille, but his former servant
Monsieur Defarge, who now owns a wine shop in Paris that
is a center of revolutionary activities, has smuggled Dr. Manette
out of prison and hidden him in the store’s attic. Meanwhile, Defarge’s
wife, Madame Defarge, secretly encodes the names of
the Revolution’s enemies into her knitting. Mr. Lorry and Lucie
arrive in Paris to find Manette compulsively making shoes in a
dark corner—prison has left him insane. Lucie lovingly restores
him to himself and they return to London.
The year is 1780. In London, Charles Darnay stands trial for
treason as a spy. Lucie and Dr. Manette attend, having met Darnay
during their return from France. The defense lawyer is Mr.
Stryver, but it is his bored-looking associate, Sydney Carton,
who wins the case. Carton points out how much he himself resembles
Darnay in order to ruin the main witness’s credibility.
In France, the wealthy aristocracy wallows in luxury and
ignores the suffering poor. Marquis St. Evrémonde recklessly
runs over and kills a child with his carriage. At his castle,
he meets his nephew Charles Evrémonde (a.k.a. Darnay) who
has returned to France to renounce his family. That night, the
Marquis is murdered in his sleep.
Back in England, Charles, Stryver, and Sydney Carton all
frequently visit Dr. Manette and Lucie. Mr. Stryver plans to
propose to Lucie, but Mr. Lorry warns him that his proposal is
unlikely to be accepted. Carton also admires Lucie; he tells her
how she makes him believe that, despite his ruined past, he still
has a shred of goodness deep within him. Charles obtains Dr.
Manette’s permission to marry Lucie, but Manette refuses to
learn Charles’s real name until the wedding day. On the wedding
day, Dr. Manette relapses into his shoe-making madness after
discovering that Charles is an Evrémonde. Mr. Lorry helps him
recover. Charles and Lucie soon have a daughter of their own.
The year is 1789. Defarge leads the peasants in destroying
the Bastille. He searches Dr. Manette’s old cell and finds a letter
hidden in the chimney. The new Republic is declared, but its
citizens grow extremely violent, imprisoning and killing aristocrats.
Charles’s former servant, Gabelle, writes a letter from
prison asking for help. Charles secretly leaves for Paris and is
immediately taken prisoner. Mr. Lorry travels to Paris on bank
business and is soon joined by Lucie and Dr. Manette. Because
of his imprisonment, Dr. Manette is a local hero. He uses his
influence to get Charles a trial, but it takes over a year. Every day
Lucie walks near the prison hoping Charles will see her. Charles
is finally freed after Dr. Manette testifies. But that very night, he
is arrested again on charges brought by Monsieur and Madame
Defarge.
Miss Pross and Jerry Cruncher have come to Paris to
help. On the street, they run into Miss Pross’s brother, Solomon
Pross, whom Jerry recognizes from Charles’s English trial as
John Barsad. Sydney Carton also shows up and, threatening to
reveal Barsad as a spy, forces his cooperation to help Charles.
At Charles’s second trial, Defarge produces Dr. Manette’s
letter from the Bastille, which explains how the twin Evrémonde
brothers—Charles’s father and uncle—brutalized a peasant
girl and her brother, then imprisoned Manette to protect themselves.
Charles is sentenced to death and sent back to prison.
Realizing his letter has doomed Charles, Dr. Manette loses his
mind. That night, Carton overhears Madame Defarge at her wine
shop plotting against Lucie and her daughter in order to exterminate
the Evrémonde line. It is revealed that Madame Defarge
was the sister of the peasants the Evrémondes killed.
Carton conspires with Mr. Lorry to get everyone in a carriage
ready to flee for England. With Barsad’s help, Carton gets into
Charles’s prison cell, drugs him, and swaps clothes with him.
Barsad drags the disguised Charles back to Mr. Lorry’s carriage,
which bolts for England. Madame Defarge shows up at Lucie’s
apartment, but Miss Pross blocks her way. The two scuffle.
When Madame Defarge tries to draw her pistol, she accidentally
shoots herself. The blast deafens Miss Pross for life.
On his way to the guillotine in place of Charles, Carton
promises to hold hands with a young seamstress, who has
been wrongly accused. He dies knowing that his sacrifice was
the greatest thing he’s ever done.
Plot Summary
Characters
Charles Darnay (a.k.a. Charles Evrémonde) – Renouncing
the terrible sins of his family, the Evrémondes, Charles
abandons his position in the French aristocracy to make his own
way in England. Charles believes in the revolutionary ideal of
liberty, but is not a radical revolutionary. Instead, he represents
a rational middle ground between the self-satisfied exploitation
practiced by the old aristocracy and the murderous rage
exhibited by the revolutionaries. Charles has a heroic sense of
justice and obligation, as shown when he arranges to provide for
the oppressed French peasantry, and later endangers himself
in coming to Gabelle’s aid. However, Charles is also deluded
in thinking he can divert the force of history and change the
Revolution for the better. Similarly, Charles constantly overlooks
Sydney Carton’s potential and must learn from his wife, Lucie,
to have faith in Carton. Charles represents an imperfect but
virtuous humanity in whose future we must trust.
Dr. Alexandre Manette – An accomplished French physician
who gets imprisoned in the Bastille, and loses his mind. In his
madness, Manette embodies the terrible psychological trauma of
persecution from tyranny. Manette is eventually “resurrected”—
saved from his madness—by the love of his daughter, Lucie.
Manette also shows how suffering can become strength when
he returns to Paris and gains a position of authority within the
Revolution. Manette tries to return the favor of resurrection when
he saves Charles Evrémonde at his trial. However, Manette
is ultimately a tragic figure: his old letter from the Bastille seals
Charles’s fate. Falling once more into madness, Manette’s story
implies that individuals cannot escape the fateful pull of history.
Lucie Manette – The daughter of Dr. Manette, and
Charles’s wife. With her qualities of innocence, devotion, and
abiding love, Lucie has the power to resurrect, or recall her
father back to life, after his long imprisonment. Lucie is the
novel’s central figure of goodness and, against the forces of
history and politics, she weaves a “golden thread” that knits together
the core group of characters. Lucie represents religious
faith: when no one else believes in Sydney Carton, she does.
Her pity inspires his greatest deed.
L I T C H A R T S GET LIT www. L i tChar t s . com
TM
TM
A Tale of Two Cities
Sydney Carton – In his youth, Sydney Carton wasted his
great potential and mysteriously lost a woman he loved. Now
he’s a drunk and a lawyer who takes no credit for his work.
Carton has no hope for his life. Only Lucie understands his
potential for goodness. In his selfless dedication to her and her
family, Carton represents the transformative power of love. His
self-sacrifice at the end of the novel makes him a Christ figure.
By saving Lucie’s family, Carton redeems himself from sin and
lives on in their grateful memory.
Monsieur Defarge – The former servant of Dr. Manette,
Defarge uses his Paris wine shop as a place to organize French
revolutionaries. Like his wife, Madame Defarge, Defarge
is fiercely committed to overthrowing tyranny and avenging
injustice. Yet Defarge always retains a shred of mercy, and
does not participate in his wife’s plot to kill Lucie. This quality
of mercy makes Defarge a symbol for the failed Revolution,
which ultimately loses sight of its ideals and revels in the
violence it causes.
Madame Defarge – The wife of Monsieur Defarge, Madame
Defarge assists the revolutionaries by stitching the names of
their enemies into her knitting. Madame Defarge wants political
liberty for the French people, but she is even more powerfully
motivated by a bloodthirsty desire for revenge, hoping to
exterminate anyone related to the Evrémondes. Where Lucie
Manette is the embodiment of pity and goodness, Madame
Defarge is her opposite, a figure of unforgiving rage. Over the
course of the novel she emerges as a kind of anti-Christ, completely
devoid of mercy, and as such comes to symbolize the
French Revolution itself, which soon spun out of control and
descended into extreme violence.
Marquis St. Evrémonde – Charles’s uncle and a cruel
French aristocrat committed to preserving the power of the
French nobility. He and his twin brother exemplify the tyrannical
and uncaring aristocracy. When the Marquis is murdered,
his corpse is a symbol of the people’s murderous rage.
Mr. Jarvis Lorry – An older gentleman who works for
Tellson’s bank, Lorry is a model of loyalty and discretion. Lorry
hides his emotions under the cover of “business,” but he works
hard to save the Manettes and to encourage Charles to become
Lucie’s husband.
Mr. Stryver – A lawyer who defends Charles Darnay.
Stryver, as his name implies, only cares about climbing the
professional ladder.
Jerry Cruncher – By day, an odd-job man for Mr. Lorry.
By night, a “resurrection man”—robbing graves to sell body
parts to sketchy doctors. He complains about his wife’s praying
because it makes him feel guilty about his secret activities,
but by the end of the novel he decides to give up his secret job
and endorses praying, a sign that he hopes to be resurrected
himself through the power of Christ.
John Barsad (a.k.a Solomon Pross) – Barsad was born
Solomon Pross, brother to Miss Pross, but then became a
spy, first for the English, then later for the French government.
He is an amoral opportunist. In England, he accuses Charles
Darnay of treason.
Jacques Three – “Jacques” is the code name for every male
revolutionary; they identify themselves by number. Jacques
Three is a cruel, bloodthirsty man who represents the corruption
of the Revolution’s ideals. He controls the jury at the
prison tribunals.
The Vengeance – A peasant woman from Paris and Madame
Defarge’s ultraviolent sidekick. Like Madame Defarge
and Jacques Three, The Vengeance enjoys killing for its own
sake, not for any reasonable political purpose.
The mender of roads (the wood-sawyer) – A French
working man who represents how average people become seduced
by the worst, most violent qualities of the Revolution.
Gabelle – A servant of Charles Evrémonde who carries
out Charles’s secret charities. Gabelle is jailed simply by association
with the aristocracy, showing how justice flies out the
window during the Revolution.
Roger Cly – A spy and colleague of John Barsad who faked
his death to escape prosecution.
Miss Pross – The long-time, devoted servant of Lucie Manette.
She is Solomon Pross’s sister, and hates the French.
Monseigneur – A powerful French aristocrat.
Tyranny and Revolution
Much of the action of A Tale of Two Cities takes place in Paris
during the French Revolution, which began in 1789. In A Tale
of Two Cities, Dickens shows how the tyranny of the French
aristocracy—high taxes, unjust laws, and a complete disregard
for the well-being of the poor—fed a rage among the commoners
that eventually erupted in revolution. Dickens depicts this
process most clearly through his portrayal of the decadent
Marquis St. Evrémonde and the Marquis’ cruel treatment of
the commoners who live in the region under his control.
However, while the French commoners’ reasons for revolting
were entirely understandable, and the French Revolution was
widely praised for its stated ideals of “Liberty, Equality, and Fraternity,“
Dickens takes a more pessimistic view. By showing how
the revolutionaries use oppression and violence to further their
own selfish and bloodthirsty ends, in A Tale of Two Cities Dickens
suggests that whoever is in power, nobles or commoners, will
fall prey to the temptation to exercise their full power. In other
words, Dickens shows that while tyranny will inevitably lead to
revolution, revolution will lead just as inevitably to tyranny. The
only way to break this cycle is through the application of justice
and mercy.
Secrecy and Surveillance
Everybody in A Tale of Two Cities seems to have secrets:
Dr. Manette’s forgotten history detailed in his secret letter;
Charles’s secret past as an Evrémonde; Mr. Lorry’s tightlipped
attitude about the “business” of Tellson’s Bank; Jerry
Cruncher’s secret profession; and Monsieur and Madame
Defarge’s underground activities in organizing the Revolution.
In part, all this secrecy results from political instability. In the
clash between the French aristocracy and revolutionaries, both
sides employ spies to find out their enemies’ secrets and deal
out harsh punishments to anyone suspected of being an enemy.
In such an atmosphere, everyone suspects everyone else, and
everyone feels that they must keep secrets in order to survive.
Through the secrets kept by different characters, A Tale of
Two Cities also explores a more general question about the
human condition: what can we really know about other people,
including those we’re closest to? Even Lucie cannot fathom
the depths of Dr. Manette’s tortured mind, while Sydney
Carton remains a mystery to everybody. Ultimately, through
Lucie’s example, the novel shows that, in fact, you can’t ever
know everything about other people. Instead, it suggests that
love and faith are the only things that can bridge the gap between
two individuals.
Fate and History
Madame Defarge with her knitting and Lucie Manette
weaving her “golden thread” both resemble the Fates, goddesses
from Greek mythology who literally controlled the
“threads” of human lives. As the presence of these two Fate
figures suggests, A Tale of Two Cities is deeply concerned with
human destiny. In particular, the novel explores how the fates
of individuals are shaped by their personal histories and the
broader forces of political history. For instance, both Charles
and Dr. Manette try to shape and change history. Charles
seeks to escape from his family’s cruel aristocratic history
and make his own way in London, but is inevitably drawn “like
a magnet” back to France where he must face his family’s
past. Later in the novel, Dr. Manette seeks to use his influence
within the Revolution to try to save Charles’s life from the
revolutionaries, but Dr. Manette’s own forgotten past resurfaces
in the form of an old letter that dooms Charles. Through
these failures of characters to change the flow of history or to
escape their own pasts, A Tale of Two Cities suggests that the
force of history can be broken not by earthly appeals to justice
or political influence, but only through Christian self-sacrifice,
such as Carton’s self-sacrifice that saves Charles at the end
of the novel.
Sacrifice
A Tale of Two Cities is full of examples of sacrifice, on both a
personal and national level. Dr. Manette sacrifices his freedom
in order to preserve his integrity. Charles sacrifices his family
wealth and heritage in order to live a life free of guilt for his family’s
awful behavior. The French people are willing to sacrifice
their own lives to free themselves from tyranny. In each case,
Dickens suggests that, while painful in the short term, sacrifice
leads to future strength and happiness. Dr. Manette is reunited
with his daughter and gains a position of power in the French
Revolution because of his earlier incarceration in the Bastille.
Charles wins the love of Lucie. And France, Dickens suggests
at the end of the novel, will emerge from its terrible and bloody
revolution to a future of peace and prosperity.
Yet none of these sacrifices can match the most important
sacrifice in the novel—Sydney Carton’s decision to sacrifice his
life in order to save the lives of Lucie, Charles, and their family.
The other characters’ actions fit into the secular definition of
“sacrifice,” in which a person gives something up for noble reasons.
Carton’s sacrifice fits the Christian definition of the word.
In Christianity, God sacrifices his son Jesus in order to redeem
mankind from sin. Carton’s sacrifice breaks the grip of fate and
history that holds Charles, Lucie, Dr. Manette, and even, as the
novel suggests, the revolutionaries.
Resurrection
Closely connected to the theme of sacrifice is the promise of
resurrection. Christianity teaches that Christ was resurrected
into eternal life for making the ultimate sacrifice (his death)
for mankind. Near the end of A Tale of Two Cities, Carton
remembers a Christian prayer: “I am the resurrection and the
life.” As he goes to the guillotine to sacrifice himself, Carton
has a vision of his own resurrection, both in heaven and on
earth through Lucie and Charles’s child, named Sydney Carton,
whose life fulfills the original Carton’s lost potential. Yet
Carton’s is not the only resurrection in the novel. After having
been imprisoned for years, Dr. Manette is “recalled to life” by
Lucie’s love. Jerry Cruncher, meanwhile, works as a “resurrection
man” stealing body parts from buried corpses, but by
the end of the novel he gives it up in favor of praying for a holier
resurrection of his own.
Imprisonment
In the novel, the Bastille symbolizes the nobility’s abuse of
power, exemplified by the unjust imprisonment of Dr. Manette
by Marquis St. Evrémonde. Yet the Bastille is not the
only prison in A Tale of Two Cities. The revolutionaries also
unjustly imprison Charles in La Force prison. Through this parallel,
Dickens suggests that the French revolutionaries come to
abuse their power just as much as the nobility did.
The theme of imprisonment also links to the theme of history
and fate. For instance, when Charles is drawn back to
Paris because of his own past actions, each checkpoint he
passes seems to him like a prison door shutting behind him.
Themes
In LitCharts, each theme gets its own corresponding color,
which you can use to track where the themes occur in the
work. There are two ways to track themes:
Refer to the color-coded bars next • to each plot point
throughout the Summary and Analysis sections.
• Use the ThemeTracker section to get a quick overview of
where the themes appear throughout the entire work.
www. L i t C h a r t s . c om 2 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Summary and Analysis
Book 1, Chapter 1: The Period
“It was the best of times, it was the worst of
times…” The year is 1775, a time that the narrator
describes through a set of contradictions: wisdom
and foolishness, belief and disbelief, optimism and
doubt, light and darkness, hope and despair. The
narrator compares this historical era to his own
present moment in Victorian England.
The contradictions listed in the
opening of the novel portray
1775 as an age of profound
transition, full of promise and
threat. The comparison to Dickens’s
Victorian times establishes
the novel’s use of the past to
comment on the present.
In France, the government spends wildly and hands
out harsh sentences to anyone connected with a
crime, no matter how minor. In England, burglars
infest the cities—even the Mayor of London gets
robbed—and not even frequent hangings can stop
the wave of crime.
The narrator extends the
potential similarities between
revolutionary France and England.
Because of their injustices,
both governments are sowing
the seeds of discontent and
political radicalism.
The narrator tells an allegory of the Woodman and
the Farmer—figures of the coming revolution who are
silently at work. But the royalty in both England and
France believe in their divine right to rule and don’t
notice the gathering storm.
The Woodman stands for Death
and the Farmer for Fate. Both,
the narrator implies, will harvest
the awful products of the monarchy’s
political mistakes.
Book 1, Chapter 2: The Mail
On a stormy night in late November 1775, the mail
coach from London slogs its way toward Dover. Three
passengers sit in the carriage. Everyone is suspicious
of each other. When he hears an approaching horse,
the coach driver stops the carriage: it’s a messenger
seeking one of the passengers, Mr. Jarvis Lorry of
Tellson’s Bank. Mr. Lorry recognizes the man as Jerry
Cruncher, who works odd-jobs for Tellson’s.
The port city of Dover was the
main port for passage between
England and France. The road
from London to Dover, battered
by storm and fraught with
suspicion and highwaymen, represents
the worsening political
conditions in both countries.
Jerry gives Mr. Lorry a note that reads “Wait at
Dover for Mam’selle.” In reply, Mr. Lorry tells Jerry
to return to Tellson’s with the message: “Recalled
to life.” The coach drivers overhear the mysterious
message but can make nothing of it. Neither can
Jerry, though he worries that “recalling to life” would
be bad for his other work.
“Recalled to life” sets up the
theme of resurrection. At both
ends of the book, someone
liberates another person from
prison and saves them from
the grave. Jerry’s odd thought
establishes the mystery of what
his other work might be.
Book 1, Chapter 3: The Night Shadows
The narrator reflects on the strangeness of the human
condition: how we are all mysteries to each other. No
matter how close, we always remain alienated from
each other by our unique individualities.
One of the main themes in all
of Dickens’s work is the search
for mutual understanding and
human sympathy.
Half asleep in the mail coach, Mr. Lorry dreams of
wandering through the inner vaults of Tellson’s Bank
and finding everything safe. He also dreams that he
“was on his way to dig someone out of a grave.” In
his dream, he sees a cadaverous man who has been
buried alive for 18 years. Mr. Lorry asks the man if he
cares to live, then also asks over and over if the man
will “come and see her?” Sometimes the man cries
out that seeing “her” would kill him, at other times
that he must see her immediately.
Mr. Lorry’s dream foreshadows
Dr. Manette’s situation. Lorry’s
questions about whether the
man “cares to live” and whether
he wants to see “her,” link the
idea of Manette’s potential return
to life with a woman, suggesting
that it is love that will
return him to life. The dream
of digging up someone from a
grave also foreshadows Jerry’s
other job as a grave robber.
Book 1, Chapter 4: The Preparation
In Dover, Mr. Lorry takes a room at the Royal George
Hotel. The 17-year-old Lucie Manette arrives that
same afternoon, having received vague instructions
to meet a Tellson’s Bank employee at the Royal
George Hotel regarding some business of her “long
dead” father. Though he describes his news as just
a “business matter,” Mr. Lorry struggles with his
emotions as he explains the “story of one of our
customers”—Lucie’s father, Dr. Manette.
Mr. Lorry works like a secret
agent for Tellson’s Bank. He
uses the cover of “business” to
assist in political activities (like
freeing Dr. Manette). But he
also uses “business” rhetoric
to hide his feelings and protect
others’ emotions, even when
explaining a father’s history to
his daughter.
20 years ago, Dr. Manette, a renowned doctor,
married an English woman and trusted his affairs
to Tellson’s Bank. One day, Manette disappeared,
having been jailed by the authorities and taken to a
secret prison. Rather than tell Lucie the truth, Lucie’s
mother told her that her father was dead. Lucie’s
mother herself died soon afterwards, and Mr. Lorry
took Lucie from Paris to London.
Lucie learns her own and her
father’s real history—her father
suffered imprisonment at the
hand of a tyrannical government.
Lucie’s history makes her a
figure who connects the “two cities”
of Paris and London, and in
A Tale of Two Cities, characters
cannot escape their histories.
Mr. Lorry braces Lucie for a shock: her father is not
dead. He has been found, though he’s a shell of his
former self. Manette is now in the care of a former
servant in Paris, and Mr. Lorry tells the astonished
Lucie that he and she are going to go to Paris so that
she can “restore [her father] to life.” Lucie’s servant,
the loud and red-haired Miss Pross, rushes in and
shouts at Mr. Lorry for upsetting Lucie. Mr. Lorry
asks her to travel with them to France.
Though freed from jail, Manette
is still imprisoned by his traumatic
history. It now becomes
clear that Lucie is the woman
whom Lorry in his dream hoped
could save Manette. Miss Pross
is a stereotypical British servant,
brash, devoted to her mistress.
Book 1, Chapter 5: The Wine-shop
Outside a wine shop in the poor Parisian suburb of
Saint Antoine, a cask of wine accidentally falls and
breaks in the street. Everyone in the area scrambles
to drink the runoff: cupping their hands, slurping it out
of gutters, licking it off the fragments of the broken
cask. It turns into a game with dancing and singing in
the streets. The wine has stained the ground, stained
people’s skin and clothes. Someone jokingly uses the
spilled wine to scrawl the word “Blood” on a wall.
This scene is an extended
metaphor for how people
transform into a frenzied mob.
It foreshadows the blood to be
spilled in the Revolution. The
writing on the wall alludes to
the Biblical story (in Daniel)
of Belshazzar’s feast where a
disembodied hand prophesied
the fall of his empire.
The color-coded bars in Summary and Analysis make it easy to track the themes through the
work. Each color corresponds to one of the themes explained in the Themes section. For instance,
a bar of indicates that all six themes apply to that part of the summary.
Wine
Defarge’s wine shop lies at the center of revolutionary Paris,
and throughout the novel wine symbolizes the Revolution’s intoxicating
power. Drunk on power, the revolutionaries change
from freedom fighters into wild savages dancing in the streets
and murdering at will. The deep red color of wine suggests that
wine also symbolizes blood. When the Revolution gets out of
control, blood is everywhere; everyone seems soaked in its
color. This symbolizes the moral stains on the hands of revolutionaries.
The transformation of wine to blood traditionally
alludes to the Christian Eucharist (in which wine symbolizes
the blood of Christ), but Dickens twists this symbolism: he uses
wine-to-blood to symbolize brutality rather than purification,
implying that the French Revolution has become unholy.
Knitting and the Golden Thread
In classical mythology, three sister gods called the Fates controlled
the threads of human lives. A Tale of Two Cities adapts
the classical Fates in two ways. As she knits the names of her
enemies, Madame Defarge is effectively condemning people
to a deadly fate. On the other hand, as Lucie weaves her
“golden thread” through people’s lives, she binds them into
a better destiny: a tightly-knit community of family and close
friends. In each case, Dickens suggests that human destinies
are either predetermined by the force of history or they are tied
into a larger pattern than we as individuals realize.
Guillotine
The guillotine, a machine designed to behead its victims, is
one of the enduring symbols of the French Revolution. In Tale
of Two Cities, the guillotine symbolizes how revolutionary chaos
gets institutionalized. With the guillotine, killing becomes
emotionless and automatic, and human life becomes cheap.
The guillotine as a symbol expresses exactly what Dickens
meant by adding the two final words (“or Death”) to the end
of the French national motto: “Liberty, Equality, Fraternity, or
Death.”
Shoes and Footsteps
At her London home, Lucie hears the echoes of all the footsteps
coming into their lives. These footsteps symbolize fate.
Dr. Manette makes shoes in his madness. Notably, he always
makes shoes in response to traumatic memories of tyranny, as
when he learns Charles’s real name is Evrémonde. For this
reason, shoes come to symbolize the inescapable past.
Symbols
Symbols are shown in red text whenever they appear in the Plot
Summary and Summary and Analysis sections of this LitChart.
www. L i t C h a r t s . c om 3 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
The jubilation fades and the street returns to its sad,
dirty, impoverished condition. The people are sick
and aged, beaten down by hunger.
Hunger and want are the
conditions that fuel the revolutionary
fire.
Monsieur Defarge, the owner of the wine shop,
enters his store. From her position behind the
counter, his wife, Madame Defarge, silently alerts
him to the presence of Mr. Lorry and Lucie. Defarge
ignores them, instead lamenting the condition of the
people with three men, all of whom go by the name
“Jacques” (a code name used by revolutionaries in
France).
The code name “Jacques” does
double service: because it is a
common name, it both hides
identity and also implies that
this revolution is of the people.
Lucie and Lorry’s presence in
Defarge’s wine shop indicates
that Defarge is Manette’s
former servant.
Once the “Jacques” have left, Mr. Lorry speaks with
Monsieur Defarge. Defarge leads Mr. Lorry and
Lucie up to his attic. The room is dark and kept locked
for the sake of the inhabitant, Monsieur Defarge
explains. Lucie leans on Mr. Lorry for support. Defarge
opens the door and they see a white-haired man in
the corner stooped over a bench and making shoes.
Because his mind was unoccupied
in prison, Dr. Manette
compensated by making shoes
to occupy his hands. Now, even
though he is free, he can’t
escape the prison of his own
mind, so he continues to make
shoes.
Book 1, Chapter 6: The Shoemaker
The shoemaker is dressed in tatters. When Defarge
asks him his name, he replies “One Hundred and Five,
North Tower.” Mr. Lorry then asks the shoemaker if
he recognizes anyone. The shoemaker seems as if he
does for a moment, but his face quickly clouds over.
Dr. Manette suffered so greatly
in prison that his identity was
virtually erased. He knows himself
only by the room number in
the Bastille, the prison in which
he was held.
Lucie approaches, with tears in her eyes. The
shoemaker asks who she is. Noticing her blonde hair,
he removes a necklace he wears and reveals a scrap
of paper containing some golden threads of hair—
stray hairs from his wife, which he has kept all these
years as a spiritual escape from his imprisonment.
Overcome by emotion, Manette struggles to
recognize his daughter. Lucie rocks Manette’s head
on her chest like a child. She promises him that his
agony has ended, and gives thanks to God.
Lucie’s golden hair reminds Manette
of his wife’s golden hair.
These hairs, from before and
after Manette’s incarceration,
form a kind of bridge over his
years in prison. These are the
“golden threads” with which Lucie
weaves a better fate for her
family. Cradling Manette, Lucie
is like a mother and Manette
her child—a metaphor for
Manette’s new life ahead.
Mr. Lorry and Defarge arrange for their immediate
departure. Before he leaves, Manette asks to bring
along his shoemaking tools. With Defarge escorting
them, the group is able to get past the barricades
in the street and reach a carriage. Mr. Lorry asks Dr.
Manette if he wants to be recalled to life. Dr. Manette
replies, “I can’t say.”
Dr. Manette’s desire to keep his
tools close at hand indicates
that his emotional trauma still
lies close to the surface. Dr.
Manette’s statement, “I can’t
say,” indicates that he doesn’t
yet totally believe in the possibility
that he could escape his
traumatic past.
Book 2, Chapter 1: Five Years Later
The year is 1780. The narrator describes Tellson’s
Bank in London as an old, cramped building with
ancient clerks. The bank has business interests
connecting England and France. Encrusted by
tradition and unwilling to change, the bank seems
much like England itself.
The bank is a symbol of
England and France. Like the
tradition-encrusted bank, each
of these countries has problems
with the institutions they’ve
inherited, such as the monarchy.
In his cramped apartment in a poor London
neighborhood, Jerry Cruncher yells at his wife for
“praying against” him, which he insists is interfering
with his work as an “honest tradesman.”
Jerry’s dislike of praying and
insistence that it interferes with
his business, implies that his
work as an “honest tradesmen”
makes him feel guilty.
Jerry and his son then go to work—they sit outside
Tellson’s waiting for odd jobs from the bank. On this
day, word emerges from the bank that a porter is
needed. Jerry hurries inside. Jerry’s young son, left
alone outside, wonders why his father’s boots are
muddy and his fingers stained by rust.
The stains of guilt on Jerry’s
conscience are represented
by the mud and rust from his
nocturnal work, which is as of
yet still unrevealed.
Book 2, Chapter 2: A Sight
One day, Jerry Cruncher is sent to await Mr.
Lorry’s orders at the Old Bailey Courthouse, where a
handsome young gentleman named Charles Darnay
stands accused of treason. Jerry enters the court and
pushes through the crowd gathered to see the trial.
The spectators stare at Darnay, and one onlooker
excitedly predicts that the accused will be convicted
and then brutally drawn-and-quartered.
The sadistic appetites of this
English crowd are similar to
those of the French mob in
Book 1, chapter 5. The title of
the chapter, “A Sight,” indicates
that these people come to the
trial for the fun of it, hoping not
for justice but for the spectacle
of violence.
Charles, who stands accused of being a French
spy, is defended by two lawyers: Mr. Stryver and
the insolent and bored-looking Mr. Carton. When
Darnay glances at a young woman and her father
sitting nearby (Lucie and Dr. Manette), word flashes
through the crowd that these two are witnesses
against Darnay. Nonetheless, Lucie’s face radiates a
compassion that awes the spectators.
The compassion in Lucie’s face
indicates that she does not
want to condemn Charles, even
though she is a witness for the
prosecution. This foreshadows
Charles’s final trial in Paris,
when Dr. Manette, contrary to
his intentions, dooms Charles.
Book 2, Chapter 3: A Disappointment
The Attorney General prosecuting the case demands
that the jury sentence Charles to death. He calls
a witness, the “unimpeachable patriot” John
Barsad, whose testimony implicates Charles as a
spy. However, on cross-examination Stryver reveals
Barsad to be a gambler and brawler and a generally
untrustworthy witness. Stryver similarly is able to
raise questions about the motivations of another
witness, Roger Cly, Charles’s former servant.
The prosecuting attorney
foreshadows the later prosecutors
in France who will bend
the truth to seek an execution.
Ironically, Charles is accused
of spying while John Barsad
and Roger Cly (who are later
revealed to be actual English
spies) are presented as “unimpeachable”
witnesses.
Mr. Lorry, Lucie, and Dr. Manette are each called to
testify: they had all met Charles aboard ship on their
way back from Paris five years earlier. Lucie explains
how Charles helped her care for her father, swaying
the jury in Charles’s favor. But she then accidentally
turns the court against Darnay. How? First she admits
that Charles was traveling with other Frenchmen and
carrying lists. Second she mentions Charles’s joking
comment that George Washington’s place in history
might one day match that of England’s King George III.
Another irony: as will be
revealed later, Charles’s “suspicious”
activities are actually his
humanitarian efforts to help his
impoverished tenants in France.
He is putting himself in danger
to help others. His comment
about George Washington
(who was leading the American
Revolution at the time) indicates
that he has revolutionary
sympathies.
Later, while Mr. Stryver is unsuccessfully crossexamining
a witness who has been called to identify
Charles, Carton hands Stryver a note. After reading
from the note, Stryver forces the court to notice the
striking resemblance between Charles and Carton,
shattering the witness’s credibility.
Besides serving an important
role in the plot, the uncanny
resemblance between Carton
and Charles links them and
sets them up as doubles to be
compared and contrasted.
The jury goes to deliberate. Carton continues to look
bored, stirring only to order help when he notices
Lucie start to faint. Finally, the jury returns from its
deliberations with a verdict of not guilty.
Carton’s boredom identifies him
as uninterested in the world
and empty. Only Lucie seems to
interest him.
Book 2, Chapter 4: Congratulatory
After the trial, Charles kisses Lucie’s hands in gratitude
and thanks Stryver for his help. Dr. Manette is now
a distinguished citizen of London. He can still become
gloomy, but this occurs only occasionally because
Lucie serves as a “golden thread” linking him to his
life before and after his imprisonment. Stryver, Dr.
Manette, and Lucie depart in a carriage.
Though Lucie’s love and compassion,
her “golden thread,”
have returned Dr. Manette’s
to life, his grip on sanity is still
tenuous, only as strong as a
thread of hair.
A drunk Sydney Carton emerges from the shadows.
His shabby clothes and impertinent manners offend
Mr. Lorry, who departs. Carton and Charles go out
to dinner at a tavern, where Carton slyly asks Charles
whether being tried for his life is worth the sympathy
and compassion he now gets from Lucie. Annoyed,
Charles comments on Carton’s drinking. In response,
Carton says, “I am a disappointed drudge, sir. I care for
no man on earth, and no man on earth cares for me.”
After Charles leaves, Carton curses his own reflection
in a mirror and then curses Charles, who reminds him
of what he might have been.
Carton’s lack of manners and
shabby looks show that he
doesn’t care much about life.
His bitter comments about the
compassion Charles receives
from Lucie show that Carton
craves Lucie’s pity. His words
also suggest that Carton only
saved Charles because he
wanted to help Lucie. Carton
curses Charles because their
resemblance forces Carton to
consider his own life, which was
ruined by some past experience.
www. L i t C h a r t s . c om 4 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 5: The Jackal
The narrator describes Mr. Stryver as an ambitious
man starting to climb the professional ladder. Due to
his problem distilling information, he partnered with
Sydney Carton, who now secretly does all the work
for Stryver to win his cases. If Stryver is a lion in court,
Carton is a cunning jackal behind the scenes.
As his name implies, Stryver
“strives” to get ahead in the
world. He is uninterested in
sacrifice because he is only out
for himself.
After leaving the tavern where he dined with Charles,
Carton joins Stryver in his apartment. To stay awake,
he wraps a wet towel around his head and works
through a pile of legal documents. Stryver watches.
Carton willingly makes himself
a slave to Stryver’s legal work.
He is sacrificing his potential
for no reason, which is a kind
of suicide.
Afterwards, Stryver and Carton drink and talk.
Stryver comments on Carton’s moodiness and lack
of direction, which have been evident since their days
at university. Carton responds that he lacks Stryver’s
ambition, and must live in “rust and repose.” Stryver
changes the subject to Lucie’s beauty. Carton mocks
her as a “golden-haired doll,” but Stryver senses
Carton’s true feelings might be different.
This exchange reveals an
important part of Carton’s
character and history. He is
always working for others, never
seeking the credit, as Stryver
would. Carton’s denials about
his interest in Lucie don’t even
convince Stryver.
After leaving Stryver, Carton stumbles home
through the grey dawn, imagining for a moment a city
of hope, full of love and grace. But it passes and he
cries into his pillow, resigned to his miserable life.
Carton’s vision is of a celestial
city in heaven. But in his current
state of empty self-pity, he can
only glimpse it for a moment.
Book 2, Chapter 6: Hundreds of People
Four months pass. Mr. Lorry visits Dr. Manette and
Lucie at their home. Lucie has decorated the house
beautifully, but Mr. Lorry notices that Manette’s
shoe-making workbench is still in the house.
The beautiful house symbolizes
the Manettes’ return to life, but
the presence of the workbench
indicates that Manette is not
yet completely free of his past.
Dr. Manette and Lucie are out, though. Mr. Lorry
speaks with Miss Pross, who comments on and
dismisses all the suitors who constantly call on Lucie.
She adds that her brother, Solomon Pross, is the
only man good enough for Lucie. Lorry remains silent,
though he knows Solomon is a cheat and scoundrel.
Mr. Lorry then asks if Dr. Manette ever uses his
workbench or speaks about his imprisonment. Miss
Pross responds that Dr. Manette does not think about
his traumatic years of imprisonment.
Miss Pross’s comments
introduce her brother, while
Lorry’s skepticism establishes
that Solomon is not all that he
seems—he’s really a spy. Dr.
Manette’s silence about his
imprisonment and insistence
on keeping his shoe-making
workbench show that he has
not resolved his traumatic past:
he’s still hiding from it.
Lucie and Manette return. Charles arrives to visit
moments later. Charles tells them of his recent trip
to the Tower of London, where a workman recently
realized that what he had thought were someone’s
initials carved into a wall (“D.I.G.”) were actually
instructions: beneath the floor, they found the ashes
of a letter. Dr. Manette nearly faints at this story.
Charles’s story foreshadows
what will be discovered in Dr.
Manette’s old cell: his carved
initials and a letter telling his
story. Dr. Manette almost faints
because he can’t face his past
and senses the letter’s danger,
whether consciously or not.
Sydney Carton also visits. Sitting out on the veranda
as a storm approaches, Lucie tells him that she
sometimes imagines that the echoes of the footsteps
from the pedestrians below belong to people who will
soon come into their lives. Carton says it must be a
great crowd to make such a sound, and says that he
will welcome these people into his life.
The storm and footsteps
symbolize the oncoming French
Revolution. Carton’s comment
is prophetic: in the end, he
welcomes the Revolution into
his life and sacrifices himself to
the Revolution to save Lucie.
Book 2, Chapter 7: Monseigneur in Town
The scene cuts to Paris and the inner sanctum of
Monseigneur, a powerful French lord. He drinks
some hot chocolate with four richly dressed servants
to help him. Monseigneur is surrounded by luxury, by
state officials who know nothing of state business but
everything about dressing well. Every aristocrat there
seems disfigured by the “leprosy of unreality.”
The hot chocolate exemplifies
the nobility’s self-indulgent and
foolish focus on personal comforts.
They are so out of touch
with the hard realities of the
common people in France that
the narrator compares their
disconnection to a disease.
One sinister lord with a pinched nose, the Marquis
Evrémonde, leaves in a huff that the Monseigneur
did not treat him a bit more warmly. He takes out
his anger by having his carriage speed through the
streets, scattering the commoners in the way.
The Marquis cares only about
power. Feeling snubbed by the
Monseigneur, he makes himself
feel powerful again by taking it
out on the commoners, whom
he clearly cares nothing about.
The carriage runs over and kills a little girl. As a tall man
wails over his dead daughter, the Marquis scolds the
people for not taking care of their children and tosses
the man a gold coin. As his carriage pulls away, the
coin sails back in: Monsieur Defarge threw it back.
Furious, the Marquis screams that he will “exterminate
[the commoners] from the earth.” He drives away
while Madame Defarge looks on, knitting.
The girl’s death is a metaphor
for the brutality of tyranny.
Defarge throwing the coin back
shows how tyranny inspires
revolution, creating a situation
where both sides want to
destroy the other. For his actions
against the commoners, the
Marquis gets his name knitted
into Defarge’s register of death.
Book 2, Chapter 8: Monseigneur in the Country
Returning through the village he rules and has taxed
nearly to death, Marquis Evrémonde stops to
question a mender of roads who the Marquis had
noticed staring at his passing carriage. The man
explains that he saw someone hanging on beneath
the carriage who then ran off into the fields.
The stowaway represents how
the Marquis is bringing his own
troubles home to roost. The
trouble is spreading from the
cities through the country.
The Marquis drives on, passing a shoddy graveyard.
A woman approaches the carriage and petitions the
Marquis for help for her husband who has recently
died of hunger, like so many others. The Marquis
dismissively asks the women if she expects him to
be able to restore the dead man to life or to feed
everyone? The woman responds that all she wants is
a simple grave marker for her husband, so he won’t
be forgotten. The Marquis drives away.
The Marquis fails to realize
that he does have the power
to feed the people. But it would
require sympathizing with them
or even sacrificing some of
his prosperity and power. The
Marquis’s lack of pity contrasts
with Lucie’s compassion. Unlike
the Marquis, she has the power
to restore someone to life.
Book 2, Chapter 9: The Gorgon’s Head
At his luxurious castle, the Marquis Evrémonde waits
for the arrival of his nephew, Charles Evrémonde
(a.k.a. Charles Darnay) from London. Charles explains
he has been questing for a “sacred object,” but that
he’s run into trouble. The Marquis dismisses him, but
complains that the power of the French aristocracy
has waned. They used to hold the right of life and
death, and ruled by fear and repression.
The object of Charles’s sacred
quest is Lucie. Charles’ “trouble”
in winning her love is his
aristocratic background. Notice
also the contrast between Lucie
and the aristocracy: she has the
power to restore life, while the
French nobility rule through the
power of taking life away.
Charles responds that the Evrémondes have lost
their family honor by injuring anyone who stood
between them and pleasure. He adds that when his
mother died, she commanded him to have mercy
on the people. He renounces his family name and
property, which he says is cursed, and explains that
he will work for a living in England. The Marquis
scoffs at his nephew’s “new philosophy,” tells him to
accept his “natural destiny,” and goes to bed.
The “new philosophy” of the
Enlightenment, which inspired
both the American and French
Revolutions, held that all people
are born equal, that no one
has a natural right to rule. Yet
rather than facing his past,
Charles tries to run from it by
renouncing his family and living
and working in England.
As the morning dawns, the expressions on the
castle’s stone faces seem to have changed to shock.
Bells ring and villagers gather to share urgent news:
the Marquis has been found dead with a knife in his
chest and a note signed “Jacques.”
The stone faces represent the
old institution of the nobility,
shocked at the unthinkable: a
challenge to their power. Yet the
murder also shows that despite
their ideals, the revolutionaries
are as bloodthirsty and revengedriven
as the nobles.
Book 2, Chapter 10: Two Promises
A year passes. Charles now makes a passable living
in London as a French teacher. Charles visits Dr.
Manette. During the visit, Charles tells Dr. Manette
of his deep love for Lucie. Dr. Manette at first seems
frightened by the news, but relaxes when Charles
promises that he intends not to separate them, but to
share the Manettes’ home and bind Lucie closer to her
father. Dr. Manette suspects that Stryver and Carton
are also interested in Lucie, but promises to vouch for
Charles’s love for Lucie should Lucie ever ask.
Charles has sacrificed his
wealth and aristocratic heritage
to try to win Lucie’s love. Since
only Lucie’s love keeps Dr. Manette
sane, any threat to their
bond makes him worry. Charles
understands this and promises
that his relationship to Lucie
won’t interfere with Lucie’s
relationship with Manette.
Charles thanks Dr. Manette for his confidence in
him, and wants to return the favor by sharing a secret
of his own: his real name. But Manette suddenly
stops him. He asks Charles to tell him on the morning
of his wedding, not before. That night, Lucie returns
and finds her father again making shoes.
Dr. Manette must have a hunch
that Charles is an Evrémonde.
By stopping Charles from
revealing the truth, he continues
to try to repress his pain. But he
is not entirely successful, as his
return to shoemaking shows.
www. L i t C h a r t s . c om 5 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 11: A Companion Picture
That same night, as Sydney Carton plows through
heaps of legal papers, Mr. Stryver announces
that he intends to get married. He chastises Carton
for acting strangely around people, especially the
Manettes. Stryver explains how he works to get along
with people, which gets him ahead in the world.
Stryver is not an evil character,
but he is selfish. All his actions
are focused on getting ahead.
He would never consider sacrificing
any of his hard-earned
success for any reason.
Because Carton had previously (though insincerely)
insulted Lucie, Stryver breaks the news to him
carefully: he plans to marry her. Stryver thinks she’s
a “charming creature” and will improve his home and
professional standing; besides, she would be lucky to
marry a man of such rising distinction. Carton drinks
harder and says almost nothing. Stryver worries
about Carton and tells him to get married, to settle
down with some wealthy woman.
Stryver wants Lucie for all
the wrong reasons: she’ll be a
trophy wife who will help him
professionally. This contrasts
with the feelings of profound
love that both Charles and
Carton feel for Lucie. Stryver
thinks that Carton can find
redemption on an earthly path,
like getting married for money.
Book 2, Chapter 12: The Fellow of Delicacy
On his way to Lucie Manette’s house to propose,
Mr. Stryver passes Tellson’s Bank and decides
to drop in on Mr. Lorry. When Stryver tells him of
his plans, Mr. Lorry stiffens and advises him not to
proceed. Stryver is stunned and insulted. Mr. Lorry
clarifies that he knows Lucie’s likely answer. But
Stryver cannot believe that any girl could refuse him.
Stryver thinks the world
revolves around him, that
everyone must believe in the
virtue of pursuing earthly
rewards, at which he excels. But
Mr. Lorry has a sense that Lucie
has different goals and a more
profound destiny.
Mr. Lorry asks Stryver to wait while he visits the
Manettes to see about Stryver’s chances. Stryver
agrees and returns home to think it over. When Mr.
Lorry arrives with the expected bad news, Stryver
has already decided to drop it. He explains that Lucie
shares the “vanities and giddiness of empty-headed
girls” and that he’s better off without her.
Stryver convinces himself he
never wanted Lucie. But his
insult about Lucie is so far off
that it shows his foolishness. A
selfish materialist like Stryver
will never deserve or receive the
rewards of love and restored life
that Lucie can provide.
Book 2, Chapter 13: The Fellow of No Delicacy
Although his awkward social skills obscure it, Sydney
Carton loves to visit the Manette house. After
Mr. Stryver informs him that he’s given up his
plans to propose, Carton visits Lucie for a private
conversation.
Carton’s earlier insults of Lucie
were just a show. From his visits,
it is clear that he loves and
admires her for her compassion
and goodness.
Lucie is astonished when Carton breaks into tears
over his wasted life during the visit. She asks if she
can help him, if she can persuade him to live a better
life. Carton says no, that his life was over long ago. But
Lucie responds that she believes he has it in him to live
a much worthier life, and that she can help him.
Carton’s past is a mystery. Not
unlike Dr. Manette, Carton has
been imprisoned in his own
depression since some trauma
in his youth. Even he does not
anticipate the great deeds he is
capable of, but Lucie does.
Carton tells Lucie he loves her, that she is “the last
dream of [his] soul.” But that even if she loved him
back, he would probably just make her miserable.
Carton asks only one thing: for Lucie to confirm that
there is still something in him to pity, some shred of
humanity to sympathize with. She does and Carton
tells Lucie he would do anything, even give his own
life, for her and the family she loves.
Prophetic words. Carton’s soul
dreams of Lucie’s pity, of being
forgiven and welcomed by her
boundless compassion. Carton
sees this compassion as the
most important thing in the
world, and with the strength he
derives from Lucie’s faith, he
would do anything to protect it.
Book 2, Chapter 14: The Honest Tradesman
Outside of Tellson’s Bank, Jerry Cruncher sees an
approaching funeral procession. An angry crowd
harasses the drivers of the hearse with shouts of
“Spies!” Cruncher learns the hearse carries the body
of Roger Cly, a convicted spy against the English.
The English crowd threatening
the spies foreshadows the
French mob that, in later
chapters, will actually lynch its
enemies in public.
Jerry follows the mob, which roughs up the drivers
and takes over the procession. They drive into the
country and bury Roger Cly with mock ceremony.
Then they start carousing, busting up local pubs until
the police intervene.
The mobs’ anger at the spy
Roger Cly escalates into a
general zest for mayhem,
foreshadowing the French
revolutionaries who lose sight
of their ideals in their thirst
for blood.
Back at home, Jerry once again complain’s about his
wife’s praying. His son, Young Jerry, asks his father
about where he goes at night. Jerry tells his son that
he goes fishing, as Mrs. Cruncher knows.
Mrs. Cruncher knows Jerry’s
secret, which is why she prays:
she feels guilty about Jerry’s
secret occupation.
That night, Young Jerry sneaks out after his father,
whose “fishing gear” includes a crowbar and ropes.
He follows his father to the grave of Roger Cly, and
watches his father start digging, then runs in terror,
with visions of Cly’s coffin chasing after him.
Jerry is a grave robber! Jerry,
who “fishes” for dead bodies,
represents a perversion of
Jesus, who was described as a
fisher of men. .
The next morning, frustrated that Cly’s body had
been missing, Jerry Cruncher furiously rebukes
his wife for her praying and intervening in the work
of an “honest tradesman.” Later, Young Jerry asks
his father what a “resurrection man” is and says
he would like to be one when he grows up. Jerry is
worried, but also a little proud.
Cly’s missing body will play an
important part in the plot in
later chapters. A “resurrection
man” (grave robber) perverts
the idea of resurrection. Rather
than bringing the dead back to
life, resurrection men sell stolen
body parts to doctors.
Book 2, Chapter 15: Knitting
One day, Monsieur Defarge enters his shop with
the mender of roads and takes him to the attic
with the three “Jacques.” The mender of roads
tells his story: he had watched a man clinging to the
underside of Marquis Evrémonde’s carriage, and
about a year later saw soldiers escort the same man,
who was accused of killing the Marquis, to prison. A
petition to save the man’s life was presented to the
King and Queen, but to no avail. The man was hung
on a gallows above the village fountain. The mender
of roads explains how the corpse cast a long and
frightening shadow.
In presenting a petition, the commoners
are working within the
established political structure:
accepting the nobles as rulers
and making an appeal to their
mercy. But the nobles squander
their chance to show mercy, and
hang the murderer as a warning.
The effect is the opposite: the
dead man’s shadow represents
the commoner’s desire for revenge
and revolution. By showing
no mercy the nobles give up any
chance of receiving any mercy.
Defarge sends the mender of roads outside and
consults with the Jacques. Jacques Three, hungry
for blood, agrees with Defarge that the Marquis’s
castle and the entire Evrémonde race should be
exterminated. Another Jacques points to Madame
Defarge’s knitting, which lists in its stitching the
names of everyone the revolutionaries mean to kill.
Just as the Marquis would
exterminate the people, those
people would exterminate him.
In other words, the revolutionaries
are just as blood-minded as
the corrupt and brutal aristocracy
they seek to overthrow.
Several days later, Monsieur and Madame Defarge
take the mender of roads to Versailles to see a
procession of the King and Queen. The mender of
roads, overwhelmed with excitement, shouts “Long
live the King!” Defarge thanks the man for helping to
keep the aristocrats unaware of the people’s rage.
The mender of roads exemplifies
the fickle mob, who crave spectacle
above all else. One minute
he’s working for the Revolution,
the next he’s overcome with joy
at seeing the king. The Defarges
exploit people like him.
Book 2, Chapter 16: Still Knitting
When the Defarges return home that evening, they
receive information that an Englishman named John
Barsad has been sent to spy on them. Madame
Defarge promises to add his name to her knitting.
Defarge admits to his wife that he’s tired and
doubts the Revolution will come during their lives.
Madame Defarge counters that the Revolution is like
an earthquake: it builds slowly, but when it comes
it releases catastrophic damage. She says she is
content to wait, and will act when necessary.
For all his revolutionary zeal,
Monsieur Defarge also has
some sympathetic human
attributes. Madame Defarge, on
the other hand, is tireless and
merciless. Her comment suggests
just what the Revolution
will be like when it comes: not
a controlled political action with
rational goals defined by political
ideals, but a vengeful riot.
John Barsad enters the shop the next day. In
conversation with the Defarges, Barsad comments
on the plight of the people, trying to get the Defarges
to reveal their revolutionary sympathies. Wise to his
scheme, the Defarges reveal nothing.
John Barsad the spy has already
been spied upon. Suspicion and
surveillance are in full swing.
Barsad changes tactics. Knowing that Defarge was
once Dr. Manette’s servant, he mentions that Lucie
is now married to Charles Darnay—who is in reality
the nephew of the Marquis Evrémonde. After
watching the impact of this news, Barsad leaves.
Because Charles and Lucie bring
together opposite sides of the
French political divide—nobility
and daughter of a revolutionary
hero—their marriage provokes
anger on both sides.
Defarge is in disbelief. He feels a deep anxiety when
Madame Defarge adds Charles’s name to her
knitting.
To Defarge, human connections
still mean something. To Madame
Defarge, all aristocrats
must die, no matter what.
www. L i t C h a r t s . c om 6 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 17: One Night
Lucie spends the last night before her wedding to
Charles with her father. She asks Dr. Manette if he
believes that her marriage will bring them closer. Dr.
Manette assures her that he wants to see her fulfilled,
and couldn’t live with himself otherwise.
Dr. Manette clings to Lucie for
his emotional security. But he
does the noble thing and risks
his mental health in order to
ensure her happiness.
For the first time, Dr. Manette talks to Lucie about
his imprisonment in the Bastille. He tells her that
while there, he passed the time by imagining how his
unborn daughter would grow up. Would she know
nothing about him, or think about her lost father and
weave his memory into the family of her own?
Manette’s thoughts about living
on in his daughter’s memory
after death hint at Carton’s reward
for his sacrifice at the end
of the novel: a legacy carried on
by Lucie’s future family.
Late that night, Lucie sneaks downstairs to check
on her sleeping father. Dr. Manette’s face is deeply
worn from his trials, but he is peacefully asleep.
Manette’s peaceful face is
“imprisoned” in a worn body,
hinting that he won’t be able to
escape his past quite so easily.
Book 2, Chapter 18: Nine Days
On the day of the wedding, Charles Darnay and Dr.
Manette speak privately. When they emerge, Mr.
Lorry notices that Manette looks deathly pale, though
he had looked perfectly normal before the meeting.
Charles has just revealed his
secret to Dr. Manette: he is
an Evrémonde. Somehow this
means something to Manette.
After the wedding, Charles and Lucie leave for their
honeymoon in Wales. The plan is for Dr. Manette to
join the newlyweds after nine days. But after Lucie
leaves, Mr. Lorry notices that Dr. Manette seems
absent-minded. By that evening, Manette is lost and
incoherent, making shoes again in his room. Mr. Lorry
and Miss Pross keep an anxious watch over him, and
decide not to tell Lucie in hopes that Dr. Manette will
improve. He doesn’t improve for nine days.
The discovery that Lucie has
married an Evrémonde pushes
Manette back into his old shoemaking
mania. These events link
the Evrémondes to Manette’s
years in prison, though just
what role the Evrémondes
played in Manette’s imprisonment
remains unclear.
Book 2, Chapter 19: An Opinion
On the tenth day, Mr. Lorry wakes to find Dr. Manette
reading as if nothing has happened. Discovering that
Dr. Manette has no memory of the past nine days,
Mr. Lorry carefully tries to figure out what caused the
relapse by asking Dr. Manette’s opinion about the
medical case of a “friend” who’s daughter Mr. Lorry
cares about. Nonetheless, Manette quickly seems to
suspect what’s going on.
Ever a model of discretion, Mr.
Lorry avoids mentioning anything
that happened directly. Dr.
Manette is still hiding from his
past, even when discussing it.
Mr. Lorry very discreetly describes Dr. Manette’s
situation, never using Manette’s name. He asks what
might have caused the relapse and how he might help
to prevent another one. Dr. Manette replies that it
would be far too painful for the “patient” to tell anyone
his secrets, but surmises that something must have
recently reminded the patient of his past trauma. He
then assures Mr. Lorry that the worst should be over,
and that only something extraordinary could upset
the patient’s mind again.
Dr. Manette represses his
traumas, which remain hidden
until they violently erupt. This
is a metaphor for the French
Revolution itself—the nobles
suppressed the commoners until
a revolt erupted. Dr. Manette
now knows the truth about
Charles’s past, but doesn’t
entirely remember his own.
Mr. Lorry then explains that this “friend” has a
hobby, “blacksmith work,” that may be associated
with the trauma. He wonders if the blacksmith tools
should be removed. Looking worried, Dr. Manette
answers that if manual labor helped the man get
through the trauma, he should be allowed to keep
the tools. Eventually Dr. Manette agrees that the
tools should be removed, but only if these tools are
removed while the patient is elsewhere at the time.
Dr. Manette needs these tools
like a child needs a security
blanket. His inability to face
losing the tools, or even to be
present when they are taken
away, is another example of
Manette’s persistent avoidance
of his traumatic past, whether
conscious or not. But he is still
willing to sacrifice the tools.
That night, after Manette has left to join Lucie and
Charles, Mr. Lorry and Miss Pross remove the
shoemaker’s tools and destroy the bench. Feeling as
guilty as murderers, they burn or bury everything.
Notice how the burying of Dr.
Manette’s work bench parallels
and contrasts with Jerry’s digging
up of dead bodies.
Book 2, Chapter 20: A Plea
The first person to visit Lucie and Charles after they
return from their honeymoon is Sydney Carton.
Carton apologizes for his drunkenness during past
encounters, and asks for Charles’ friendship. Carton
declares himself a worthless man, but says he has a
favor to ask: would Charles mind if he occasionally
visited his house? Of course not, Charles replies.
The novel foreshadows that
Carton, as the first to meet
the married couple, will be
especially important to Charles
and Lucie’s life as a family. For
his part, Charles is just being
polite, humoring Carton out of
his sense of obligation to him.
At dinner that night, Charles comments to Lucie,
Manette, Mr. Lorry, and Miss Pross about Carton‘s
careless and reckless behavior. Later that night in
their room, Lucie suggests that Charles was too
judgmental toward Carton. She asks Charles to have
faith in Carton, who she believes has a wounded heart
but is nevertheless capable of doing tremendous
good. Charles blesses Lucie for her compassion and
promises to have more sympathy for Carton.
Unlike Charles, Lucie has a
deep sympathy and compassion
for Carton’s pitiful soul. Even
though she hardly understands
his behavior, Lucie has faith.
Her prediction about Carton
foreshadows the incredible
sacrifice that Carton will make
for the Manette family.
Book 2, Chapter 21: Echoing Footsteps
Years pass. Lucie weaves her “golden thread”
of positive influence through the family. She often
sits by the parlor window and ponders the echoing
footsteps rising from the street below. She gives birth
to a daughter, Lucie, who particularly likes Sydney
Carton. Her second child, a son, dies in childhood.
As the political situation starts
to unravel in France, Lucie
weaves her domestic community
more tightly together in
London. Her daughter, like her,
has an innocent belief that
Carton is a good man.
In the year 1789, distressing “echoes” arrive from
France. Mr. Lorry confides in Charles that the Paris
office of Tellson’s Bank has been flooded with anxious
aristocrats trying to save their property.
Charles sacrificed his property
to try to escape his family’s
past. Aristocrats who hung on to
their wealth have now lost it.
The scene cuts to Defarge’s wine shop, now the
center of a revolutionary maelstrom. The streets are
thronged with dingy, angry people, armed with guns,
knives, or any weapon they can get their hands on.
The dirty angry revolutionaries
show that the Revolution will
be more about revenge than
Enlightenment ideals.
Defarge leads this army to the Bastille. Madame
Defarge rallies the women, swearing they can kill
as well as the men. After fierce fighting, the Bastille
surrenders and the people swarm inside to free the
prisoners. Defarge and Jacques Three demand
that an older officer show them “One hundred and
five, North Tower.” There, they find Dr. Alexandre
Manette’s initials “A.M.” and search the room.
The taking of the Bastille was
one of the major early events of
the French Revolution. It’s anniversary
is still celebrated as the
French Independence Day. Note
Madame Defarge’s bloodthirstiness.
Manette’s initials on the
wall recall Charles’s story about
the Tower of London.
Returning to the Bastille courtyard, the crowd swarms
the old officer and stabs him to death. Madame
Defarge takes her long knife and slices off his head.
Seven prisoners are freed. Seven prison guards are
killed and their heads are stuck on pikes.
The exchange of the seven
prisoners with seven guards
suggests that power may have
switched sides, but that nothing
has really changed. Madame
Defarge’s beheading of the
guard foreshadows the guillotine.
Book 2, Chapter 22: The Sea Still Rises
Madame Defarge, now the leader of the female
revolutionaries, sits in the wine shop with her
second-in-command, a stocky woman whose violent
acts have earned her the name The Vengeance. No
spies dare come into this neighborhood anymore.
If Madame Defarge represents
Fate, her assistant reveals
exactly 0what kind of Fate
is in store: angry and violent
vengeance in response to years
of tyranny and oppression.
Monsieur Defarge returns with news that an old
aristocrat, who once said that starving people should
just eat grass, tried to fake his own death but has
now been caught. Anger swells—a revolutionary mob
rushes from the neighborhood to the courts building.
The mob overwhelms the officials, captures the old
aristocrat, then drags, beats, and stuffs his mouth with
straw. Finally, they hang him from a lamppost.
The story of the murdered
aristocrat alludes to the famous
story of Queen Marie Antoinette
who, when told that the
starving people had no bread,
replied “Let them eat cake.”
The statement exemplifies cruel
snobbery, but the response is
out of proportion to the offense.
Afterwards, the commoners return home, eat their
“scanty suppers,” play with their kids, and make
love. Back at the shop, Defarge tells his wife that he
is happy the Revolution has finally come. “Almost,”
Madame Defarge replies.
The scenes of the commoners
at home highlights that the
vicious mob is made up of ordinary
people. Madame Defarge’s
comment shows her insatiable
appetite for revenge.
www. L i t C h a r t s . c om 7 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 23: The Fire Rises
While at work in the ruined countryside of France, the
mender of roads encounters a shaggy but powerful
man. Addressing each other as “Jacques,” they
confirm that something will happen “tonight.”
“Jacques” keeps cropping up
everywhere, suggesting how the
revolutionary cause is taken up
again and again by new people.
In the dark courtyard of the castle of Marquis
Evrémonde, four torch-bearing figures appear.
Soon, fire rages through the castle—its stone faces
look tormented and are lost in flame. The inferno
becomes a pillar of fire surging high into the sky.
The stone faces symbolize the
ancient French nobility, which
gets decimated by the Revolution.
The burning castle is a
symbol of the failing aristocracy
and the commoners’ revenge.
A man from the castle rushes into the village
screaming for help to put out the fire and salvage the
valuables in the castle. The crowd of villagers refuses
to budge.
Now the nobility is asking the
people for help, when for so
long they refused to listen to
the people’s appeals for aid.
Later, the villagers surround the house of Monsieur
Gabelle, the government “functionary” in charge of
the area. He is forced to hide on his roof, but is able
to come down in the morning. The narrator explains
that other functionaries in other areas aren’t so lucky,
and that fires are burning all over France.
Though Gabelle is not an
aristocrat himself, he works for
the government. His association
with the aristocrats is enough
for the revolutionaries to distrust
and want to harm him.
Book 2, Chapter 24: Drawn to the Loadstone Rock
It’s now 1792. In the three years that have passed,
there have been battles and bloodshed. The French
nobility has scattered. Many French aristocrats have
become emigrants, fleeing France for London where
they gather at Tellson’s Bank for news.
Though both London and
Paris teetered on the edge of
revolt at the beginning of the
novel, only France has fallen
into revolution.
Inside the bank, Charles is trying to talk Mr. Lorry
out of his latest mission: going to the Paris branch of
the bank to protect whatever bank documents he can.
The aged Mr. Lorry is apparently the youngest clerk at
the bank, and he plans to take Jerry Cruncher for
protection. He will leave that night.
Charles may have democratic
sympathies, but Tellson’s Bank is
invested in old money and aims
to preserve it. This makes Mr.
Lorry’s political and moral positions
in the book ambiguous.
Just then, Mr. Lorry is given a letter addressed to
the “Marquis St. Evrémonde.” Not knowing such a
person, he asks the assembled French nobles. They
declare the man a coward who betrayed his noble
family. Though insulted, Charles does not respond.
Instead, he tells Mr. Lorry that he is an acquaintance
of the Marquis and will deliver the letter.
Although the nobles are wrong
about him, Charles has not
demonstrated to France what
kind of man he is. Because he
ran from his past rather than
confronting it, the nobles and
the commoners despise him.
The letter is from Gabelle. He was arrested, brought
to Paris, and charged with treason for helping an
emigrant, Charles Evrémonde. Gabelle writes that
the peasants neither know nor care that he in fact
was trying to help them, working on Charles’s orders.
He begs Charles to come save his life.
Gabelle was trying to help the
commoners on Charles’s behalf.
But the revolutionaries no longer
care about the truth. They just
want to kill aristocrats. Charles
now gets an opportunity to
restore Gabelle to life.
Charles realizes that he must go to Paris. His sense
of justice obliges him to help Gabelle. He also thinks
he can do something to stop the Revolution’s terrible
violence and urge the people toward mercy. The
narrator describes Charles as being drawn to Paris as
to a Loadstone Rock (a naturally magnetic rock).
Charles wrongly thinks one man
can influence history, or sway
the mob. In fact, the reference
to the magnetic “loadstone”
suggests that even the choice
to return is not really Charles’s
own, that his past has fated him
to go back.
Charles gives Mr. Lorry a reply to send to Gabelle:
Evrémonde will come. Charles packs secretly, writes
a letter each to Lucie and Dr. Manette, and without
telling them leaves for France the following night.
Charles thinks he can do this
on his own, not realizing that
he will also magnetically pull
Lucie and Dr. Manette back to
Paris as well.
Book 3, Chapter 1: In Secret
Charles arrives in France and finds things very
different from when he left. At each village and
checkpoint, he is subjected to the sneering of
revolutionaries dedicated to what the narrator calls
the new republic of “Liberty, Equality, Fraternity, or
Death.” Charles feels each gate close behind him like
a prison door.
Themes of imprisonment
and fate merge as Charles is
gradually locked into his journey
to Paris. The narrator’s addition
of the words “or death” to the
motto of the Revolution shows
its ideals have been perverted.
Three soldiers accompany Charles to Paris as
his “escort.” Upon arriving in Paris, they deliver
Charles—whom they now call their “prisoner”—to
Monsieur Defarge. Charles demands to know under
what charges he is held, and is told that new laws
against emigrants have been passed. Defarge quietly
asks him why he ever returned to France in this, the
age of “La Guillotine.“ Charles asks Defarge to help
him. Defarge refuses.
As he gets closer to Paris,
Charles goes from free man to
escorted suspect to prisoner,
though he has done nothing.
Defarge refuses to help Charles,
but he shows some sympathy.
The revolutionaries invoke the
guillotine as if it’s a saint: bloodthirsty
violence has replaced
religious compassion.
Defarge conducts Charles to the prison of La Force
with a note for the jailor saying “In secret.” The jail is
full to bursting with aristocrats who welcome Charles
with incredible politeness and sympathize with his
fate. Charles is jailed in a solitary cell in a tower. He
realizes he has been virtually left for dead. Charles
paces off the dimensions of the room again and
again: “five paces by four and a half.”
Defarge helped free Dr.
Manette from his secret
imprisonment, but now Defarge
secretly jails Manette’s son-inlaw.
The Revolution has become
a tyranny. Charles paces to
deal with the isolation of imprisonment,
just as Dr. Manette
turned to making shoes.
Book 3, Chapter 2: The Grindstone
Mr. Lorry arrives at the Paris branch of Tellson’s
Bank. It sits next to the former house of a grand
French noble that has been converted into an armory
for the revolutionaries. In the courtyard there’s a
large grindstone.
The house’s transformation
symbolizes the Revolution:
formerly representing the
excesses of the nobility, now the
house represents the revenge
that excess inspired.
Mr. Lorry is stunned when Lucie and Dr. Manette
rush in. They left London immediately after reading
Charles’s letters. Dr. Manette’s fame as a Bastille
prisoner has granted him access and information,
and he has learned that Charles has been imprisoned
at La Force.
In his return to Paris, Dr.
Manette represents redemption
through suffering. He’s been
restored to his former life, and
suffering has earned him political
power within the Revolution.
Noises outside draw them to the window. Half-naked
men covered in blood are turning the grindstone to
sharpen swords. Frenzied, blood-smeared women
pour wine into the men’s mouths. The mob runs
howling into the streets with their weapons.
The revolutionaries are
described as uncivilized savages,
engaged in some terrible ritual.
Note the wine-blood connection
and the intoxication of violence.
Mr. Lorry whispers to Dr. Manette that the mob
has gone to kill the prisoners at La Force. Horrified,
Manette runs out to the mob. Manette and the
remaining revolutionaries rush to La Force as the mob
cries out, “Help for the Bastille prisoner’s kindred in
La Force!”
It is not enough for the
revolutionaries to imprison their
enemies. They must kill them.
Manette, though, uses the political
power he gained from his
sacrifice to save Charles.
Book 3, Chapter 3: The Shadow
Feeling it necessary to separate Tellson’s Bank from
his own personal business, Mr. Lorry finds an
apartment for Lucie and her family, and leaves Jerry
Cruncher with them to act as guard. On the way
back to Tellson’s Mr. Lorry is stopped by Monsieur
Defarge, who brings news that Charles is safe, a
note for Lucie from Dr. Manette, and instructions for
Lorry to let Defarge in to see Lucie.
Mr. Lorry keeps his two worlds
as separate as possible, but
is deeply committed to both.
It is unclear if Defarge has
tampered with this letter, but
certainly at this moment he is
acting as a secret agent for the
Revolution.
On their way to the apartment, Mr. Lorry and
Defarge are joined by Madame Defarge, who is
knitting, and The Vengeance. Defarge tells Lorry
that, in order to be able to protect Lucie, Madame
Defarge must see and remember Lucie’s face.
That Madame Defarge is
knitting shows that she’s
planning to add Lucie’s name
to her list of victims. “Safety”
and “security” are words the
power-hungry use to mask their
real intentions.
In the apartment, Lucie reads the note from Charles:
he is fine, and under Dr. Manette’s protection.
She gratefully kisses one of Madame Defarge’s
hands, but Madame Defarge coldly withdraws to
her knitting. Lucie pleads for Madame Defarge
to help Charles, to use her influence as a “sisterwoman.”
Madame responds that she has seen so
many women suffering for imprisoned husbands that
Lucie’s predicament doesn’t mean much. After they
leave, Lucie tells Mr. Lorry that Madame Defarge
seems to throw a shadow over all her hopes.
This crucial meeting between
the two key female characters
reveals a lot about each: Lucie
has compassion even for this
terrible woman and asks for
her pity; Madame Defarge
shows she is no “sister-woman”
but is a cold messenger of
death. Madame Defarge is
meant to be a frightening
perversion of femininity, while
Lucie, with her goodness and
compassion, is the model of it.
www. L i t C h a r t s . c om 8 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 4: Calm in the Storm
After four days, Dr. Manette returns. He tells Lorry
that 1100 defenseless prisoners have been murdered,
convicted by a self-appointed Tribunal. The Tribunal
also nearly condemned Charles to death, but Dr.
Manette was able to sway the crowd and Charles was
returned to his cell.
Although Charles’s trial in
England was unfair, the French
Tribunal is depicted as even
more monstrous, a total sham
of justice. Dr. Manette seems
to have brought Charles back
to life.
Dr. Manette has been invigorated by his newfound
authority. He believes his suffering has become
strength and power, capable of breaking Charles
out of prison. Having earned the respect of the
revolutionaries, he has been made the inspecting
physician of a number of prisons, including La Force. In
this new role, he can protect Charles. However, as time
passes, he cannot seem to get Charles freed.
The novel implies that through
suffering comes redemption,
and that faith can empower
people to break the pull of
fate of history. Yet even Dr.
Manette’s political power is not
enough to free Charles.
A year goes by. The Revolution gains in force. The
King and Queen of France are beheaded. As the
revolutionaries grow stronger, their courts zealously
prosecute people, guilty or not. Suspicion reigns. Civil
freedoms disappear.
After the Republic was declared
in France in 1792, the “Reign
of Terror” began: a period of
spying, fear, and escalating
numbers of executions.
The guillotine becomes an institution, and guillotines
can now be found in the streets all over Paris. The
narrator says that in Paris the guillotine has come to
replace the Cross as an idol for worship.
The guillotine, a tool to make
it easier to execute people by
beheading, has become a sacrilegious
idol in place of Christ.
This signals that compassion, in
France, is dead.
Book 3, Chapter 5: The Wood-sawyer
Through it all, Lucie tries to keep a normal English
household to relieve her mind. Dr. Manette reassures
her that he can save Charles. He suggests that she
walk near the prison at a place where Charles might
see her from the window of his cell in order to boost
Charles’s spirits. Lucie does just that, everyday, rain,
shine, or snow.
Lucie fits the classic Victorian
stereotype of female strength
through domesticity (“the angel
in the house”) and selfless
dedication to her husband. Just
as Dr. Manette will unwittingly
doom Charles later, he dooms
Lucie with his advice here.
As Lucie stands at her spot on the street each day,
a wood-sawyer—formerly a mender of roads—
who works nearby always says hello. As he cuts his
wood, the wood-sawyer jokes that he is guillotining a
little family. Though the wood-sawyer unnerves her,
Lucie is always polite and friendly to him.
The mender of roads has
transformed into a man drunk
on the violence of the Revolution.
His sawing represents the
potential executions of Charles,
Lucie, and their daughter.
One snowy day, as Lucie stands outside the prison,
she sees a crowd of people dancing to a popular
revolutionary song. Lucie is horrified by their savage
movements and screams.
Another intense depiction of
revolutionaries as crazed savages
who worship the violence
of the Revolution.
Moments later, Dr. Manette appears. He tells Lucie
that Charles’s trial will be held tomorrow, and
promises her that all will work out well. Lucie kisses
her hand in farewell to Charles as she departs, just
as Madame Defarge comes around the corner.
Manette and Madame Defarge salute each other.
For Lucie, her kiss is a gesture
of love toward her husband.
For Madame Defarge, it’s a
crime of commiserating with an
enemy of the state. But Defarge
is not yet ready to make her
play against Dr. Manette.
Book 3, Chapter 6: Triumph
A rowdy, bloodthirsty crowd gathers for the trial of
“Charles Evrémonde, called Darnay.” Defarge and
Madame Defarge sit in the front row. Madame
Defarge is knitting away. Charles is sentenced
to death as an emigrant, despite the fact that the
law was passed after his imprisonment. The crowd
screams to cut off his head.
This is a court not of justice but
of unchecked political passions.
Charles’s sentence is, in fact,
a travesty of justice—the law
shouldn’t even apply to him.
The crowd does not care about
justice, though. It just wants the
spectacle of his execution.
In his testimony, Charles explains that he actually
isn’t an emigrant: he gave up his aristocratic title and
property, then worked as a French tutor and married
a French woman: Lucie Manette. He says that he
returned to France to save the life of a citizen of the
Republic: Gabelle.
Charles finally explains who
he is to the French people.
By swearing that he is still
a Frenchman, Charles offers
himself as a positive, non-violent
role model for change.
Gabelle, who had been forgotten in prison before the
trial, takes the witness stand and confirms Charles’s
story. Then Dr. Manette testifies, praising Charles’s
character and republican ideals.
Gabelle was left for dead.
Imprisonment is like the grave.
Dr. Manette once again tries
to use political tactics to free
Charles.
The jury votes to acquit Charles. The boisterous
crowd now celebrates Charles as a patriot and carries
him through the streets in celebration.
Charles goes from death row to
a public parade, floating on the
fickle allegiance of the mob.
When she sees Charles, Lucie faints with joy. In their
apartment, she thanks God, then her father, who
declares, “I have saved him.”
Dr. Manette’s political influence
seems to be enough to save
Charles after all.
Book 3, Chapter 7: A Knock at the Door
The next day, Manette remains confident and proud
at having saved Charles, but Lucie continues to fear
for her husband’s safety because so many other
innocent people have been imprisoned and killed. For
safety’s sake, they keep no outside servants, using
only Jerry and Miss Pross. Miss Pross vehemently
and regularly voices her distaste for the French.
Lucie’s worries counter Dr.
Manette’s confidence in his political
power. As Lucie suspects,
everyone in France succumbs to
the Reign of Terror. Miss Pross
embodies the inherent English
distrust of the French.
That afternoon, as Miss Pross and Jerry are out on
errands, Lucie hears footsteps on the stairs outside
the apartment. Then there is a knock at the door. Four
armed revolutionaries enter and declare that Charles
Evrémonde is again the prisoner of the Republic.
In the revolutionary Republic,
laws can change in an instant
as the new people in power
begin to abuse it. The footsteps
in the hall echo the footsteps
Lucie used to hear in England.
Dr. Manette tries to intervene, but the soldiers tell
him that he must make sacrifices if the Revolution
demands it. Still, out of respect for Manette, the men
explain that evidence for the charge comes from
three people: Monsieur and Madame Defarge, and
one other, whom they refuse to name.
The Revolution demands that
the revolutionaries be willing
to sacrifice the lives of others,
even family members, without
question. Manette’s political
power can’t stand up to the
pull of fate and history or to
the Revolution’s all-consuming
desire for blood.
Book 3, Chapter 8: A Hand at Cards
While they’re out on their errands, Miss Pross
screams when she recognizes her brother, Solomon
Pross, disguised as a French republican. Solomon
tells her to be quiet, or else she’ll get him killed. Jerry,
meanwhile, also thinks he recognizes this man, but
can’t quite remember his name.
As an unthinking English patriot,
Miss Pross has never questioned
her brother’s integrity,
but as this chapter will show,
he’s a traitorous opportunist in
an ugly political world.
Sydney Carton, appearing out of nowhere, tells
Jerry the name he is trying to remember: John
Barsad. Having arrived in Paris a day earlier, Carton
explains, Carton chanced upon and recognized
Barsad from Charles Darnay’s English trial. Carton
also learned that Barsad was serving as a French
government spy working in the prisons.
Dickens’s novels are often filled
with extreme coincidences, such
as Carton and Barsad’s sudden
appearances. Though one can
guess that Carton came to
Paris out of concern for Lucie.
Carton and Jerry escort John Barsad to Tellson’s
Bank, where Mr. Lorry also recognizes him. Carton
says he has a plan to help Charles. He then blackmails
Barsad, threatening to reveal him as a spy of the
French government and as a former English spy, both
of which would enrage the revolutionaries. Carton
then reveals that he has seen Barsad associating with
another known English spy: Roger Cly.
Because Carton has nothing to
lose, he can play the dangerous
game of counter-intelligence.
Carton wants to save Charles
in part for Charles’s sake, but
to a larger extent because of
his feelings for Lucie. Recall his
promise to Lucie at the end of
Book 2, Chapter 13.
Barsad grins: Cly is dead, he says. He then takes
out a certificate of burial and says he buried Cly
himself. To everyone’s surprise, Jerry angrily objects
that Barsad had placed “shameful impositions upon
tradesmen,” and then reveals that Cly’s body wasn’t
in his coffin. Barsad realizes he’s caught and agrees
to help. Carton takes him into an adjoining room
to talk.
Jerry’s secret job as a “resurrection
man” saves the day!
But note that it takes being
caught in a lie to get Barsad to
help Charles. There is no honor
among spies. And Barsad has
no concept of sacrificing himself
to a higher cause.
www. L i t C h a r t s . c om 9 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 9: The Game Made
After a while, Barsad leaves and Carton explains
to Mr. Lorry that if Charles is convicted, Barsad
will smuggle Carton into Charles’s cell. Refusing to
explain anything more, Carton asks that Lucie be
told nothing about the plan. He then asks if Mr. Lorry
is satisfied with his long life. Mr. Lorry replies that,
nearing the end, he feels closer again to his life’s
beginning. Carton says he knows the feeling. Mr.
Lorry gains a new respect for Carton.
Carton’s exchange with Lorry
suggests that Carton plans to
sacrifice himself and expects
to die. As always, he works for
other people without taking
credit, but this time he works
for a greater cause. Mr. Lorry’s
sense of returning to the beginning
takes on a religious tone
with Carton: he will be reborn
in heaven.
Carton visits a pharmacy and buys a mysterious
packet of drugs that the chemist warns are very
potent. All night, Carton wanders the streets of Paris.
As he walks, he remembers a prayer the priest spoke
at his father’s funeral: “I am the resurrection and the
life, saith the Lord: he that believeth in me, though he
were dead, yet shall he live: and whosoever liveth and
believeth in me, shall never die.”
The prayer Carton remembers
comes from the story of Jesus
and Lazarus, whom Jesus
resurrects in the Bible (John
11:25). The line says that Jesus
will resurrect and give eternal
life not only to Lazarus, but to
anyone who believes in him.
As he continues to walk, he encounters a young girl,
whom he helps across the street. She kisses him, and
once more Carton remembers the prayer.
Carton is showing compassion
to others, and receiving blessings
(the kiss) in return.
Carton arrives at the courthouse the next morning
for Charles’s trial, where Jacques Three is the
head of the jury. As the trial begins, the prosecutor
announces who brought the charges: Defarge,
Madame Defarge, and Dr. Alexandre Manette.
Like the wood-sawyer, Jacques
Three enjoys political executions.
As in Charles’s first trial,
Manette is again forced by
fate and history to serve as a
witness for the prosecution.
The court erupts in chaos. Manette objects that he
never denounced Charles. The judge silences him.
Defarge then takes the stand and explains how,
during the storming of the Bastille, he searched
Manette’s old cell and found a letter hidden in the
chimney. The judge asks that it be read aloud.
Manette’s hidden letter recalls
Charles’s story about the Tower
of London. It represents all
the trauma and revenge that
Dr. Manette has repressed,
consciously or unconsciously.
Book 3, Chapter 10: The Substance of the Shadow
Defarge explains that Dr. Manette wrote the letter
while in the Bastille to explain how he ended up in
prison. He then reads the letter. Walking home one
night in 1757, Dr. Manette was taken into a carriage
by two men, identical twins. From their coat of arms,
he learned that they were Evrémondes: Charles’s
father (who was then the Marquis) and his uncle (who
became the Marquis after Charles’s father died, and
was murdered in Book 2, Chapter 9).
The letter tells the story of
Manette’s imprisonment. The
twin Evrémonde brothers
epitomize the selfishness and
cruelty of aristocratic power.
They take what they want,
when they want, by whatever
means necessary.
The men took the doctor to see two patients: one,
a beautiful young woman deliriously calling out for
her family, and the other, a peasant boy with a stab
wound in his chest. As Manette treated the boy,
the boy told him that the young woman was his
sister. After she married, the two aristocrats decided
they wanted her for themselves. So they forced her
husband to endure impossibly hard work until he
died. Then they took her away and raped her.
The Evrémondes don’t
recognize the individual rights
of peasants, the sovereignty of
marriage, or the sacredness
of female sexuality, which was
a huge deal in Dickens’s time.
They are the worst example
of aristocratic tyranny, and, as
such, they embody many of
the reasons the commoners
revolted.
The peasant boy and young woman’s father died
upon hearing the news. The boy then sent his
younger sister to a distant, secret place, and, seeking
revenge, snuck into the Evrémondes’ castle. He
confronted one of the Evrémondes, who stabbed
him. The boy soon died, but before he did he cursed
the Evrémondes by marking the air with a cross of his
own blood. The young woman died within a week.
The nobles then offered Dr. Manette some gold in
return for his silence, but he declined and returned
home, disgusted with all he had seen.
The curse seals the fate of the
Evrémonde brothers. While
Charles did not know this story,
he sensed his family’s dark
past when he renounced it in
Book 2, Chapter 9. Dr. Manette
refused to be bought off by the
Evrémondes, despite the danger
of such an action. He sacrificed
his freedom to preserve his
integrity.
The next day, the wife of the Marquis (and Charles’s
mother) visited Dr. Manette. Hearing what had
happened, she hoped to find and help the surviving
sister of the abused peasant family. She told her little
boy Charles that he must someday repay this injured
girl. Unfortunately, Manette didn’t know where the
girl was.
That surviving sister, as future
events in the novel will show,
is Madame Defarge. Ironically,
Charles has pledged himself to
help this girl, while she blindly
seeks revenge and does everything
in her power to kill him.
Dr. Manette soon sent a letter to the authorities
detailing the crimes of the Evrémonde brothers.
But the Marquis intercepted and burned Manette’s
letter. He then sent Manette in secret to the Bastille.
Manette ends his letter from prison with a curse on
the Evrémondes.
Manette tried to condemn
the Evrémondes officially and
failed—just as he does now,
having tried to use his political
influence to save Charles. Both
governments are corrupt. His
curse seals Charles’s fate.
Incensed at the actions of the Evrémondes, the jury
sentences Charles to death. The crowd goes wild.
Just days before the crowd
cheered Charles as a patriot.
Book 3, Chapter 11: Dusk
As the crowd celebrates Charles’s conviction in the
streets, John Barsad, who is escorting Charles back
to his cell, lets Lucie her embrace her husband for
the last time. Charles says farewell and asks her to
kiss their daughter. Lucie tells him she feels that they
will not be long separated and will meet in heaven.
Lucie has some kind of serene
connection to the next world.
If their love isn’t possible in
the world, it will be renewed
in heaven. Note how well
positioned Barsad is to smuggle
Carton into Charles’s cell.
Devastated, Dr. Manette tries to apologize to
Charles. But Charles stops him, and instead thanks
him, acknowledging all that Dr. Manette must have
suffered to offer his own daughter back into the
Evrémonde family he justifiably hates.
Like Dr. Manette, Charles also
had a horrific secret past, of
which he was unaware, come
back to haunt him. He cannot
escape the curse on his family.
Lucie faints. Carton carries her to a carriage and
escorts her home. There, he instructs Dr. Manette
to use any remaining influence to try to save Charles.
Dr. Manette hurries away. However, once he’s gone,
Carton and Mr. Lorry confess they have no hope.
Carton is just distracting Dr.
Manette; he knows that politics
are no longer of any use.
Something stronger is necessary
to break the grip of fate, history,
and the Revolution.
Lucie’s daughter begs Carton to help. Carton
embraces her and, before he leaves, kisses the
unconscious Lucie and whispers, “A life you love.”
As his farewell implies, Carton’s
goal is to give Lucie and her
family a happy life. He is willing
to sacrifice himself for that.
Book 3, Chapter 12: Darkness
Sydney Carton decides to make sure he is seen
around Paris. He eventually wanders into a wine
shop—Defarge’s wine shop. Defarge and Madame
Defarge marvel at his physical resemblance to
Charles, but have no idea who he is.
Carton wants to make sure
that it is known that there is
someone who looks just like
Darnay walking free on the
streets of Paris.
Carton eavesdrops on a conversation between
Defarge, Madame Defarge, The Vengeance, and
Jacques Three, in which Madame Defarge plots to
exterminate the Evrémonde line—including Lucie and
Lucie’s daughter. She says that she and the woodsawyer
will testify against Lucie for sympathizing
with a prisoner. Jacques Three promises a conviction.
Monsieur Defarge, however, hesitates, and suggests
that poor Dr. Manette has suffered enough.
The bloodthirsty juries of
the Revolution need only the
slightest suspicion to convict
someone. Jacques Three’s promise
indicates that there is no
justice, and that the trials are
shams. Monsieur Defarge’s pity
for Manette makes Madame
Defarge’s utter mercilessness
stand out even more starkly.
Madame Defarge responds by revealing her history
with the Evrémondes: she is the missing sister of
the peasant family whom the Evrémonde brothers
abused and killed. She vows to carry out her brother’s
dying curse. She barks at Defarge that he can tell
wind and fire where to stop, “but don’t tell me.”
Jacques Three and The Vengeance are thrilled.
Madame Defarge exceeds the
forces of nature. She is a the
terrifying product of tyrannous
cruelty. She symbolizes all
of the people abused by the
aristocrats, and her vengeance
is the embodiment of the
Revolution.
Carton hurries home. Soon, Dr. Manette returns
too, begging for his shoemaker’s bench. Shocked,
Carton and Mr. Lorry realize that Dr. Manette has
lost his mind. Carton instructs Mr. Lorry to gather
everyone’s passports, including Carton’s, and leave
the next day before Madame Defarge’s accusations
make it impossible for them to leave France. Then
Carton says farewell, blesses Lucie, and leaves.
A key tipping point: the curse
against Charles cannot be
stopped, and Dr. Manette’s
insanity is now permanent. After
failing to save Charles, Manette
reverts to his own fate as a
traumatized prisoner. Carton
takes control of things, setting
up his final plan.
www. L i t C h a r t s . c om 10 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 13: Fifty-two
In the prison, 52 people, including Charles, await
execution that day. Charles writes a final letter to
Lucie, in which he says that he did not know about
her father’s history and that he believes Dr. Manette
was unaware of his damning letter. Charles writes
much the same to Dr. Manette. He also writes to Mr.
Lorry, but never thinks to write to Carton.
Neither Charles nor Dr.
Manette were aware of their
real legacies. The don’t control
their own destinies. Charles
has underestimated Carton
before. The fact that Carton is
under no obligation to make
his sacrifice only increases its
symbolic power.
Suddenly John Barsad opens the cell door and lets in
Carton. Carton tells Charles to start changing clothes
with him. Then Carton dictates a letter for Charles to
write, in which he asks “someone” to remember him
and is grateful to have the chance to prove himself.
In Book 2, Chapter 4 Carton
envied Charles. Now he
becomes Charles by literally
sacrificing his identity to save
Charles’s life. The “someone” in
the letter is Lucie.
As Charles writes, Carton waves the packet of drugs
under his nose. Charles passes out. Carton finishes
swapping their clothes and Barsad carries Charles,
now disguised as Sydney Carton, back to Mr. Lorry.
Charles has been helpless to
stop history, and is not just passive,
but actually unconscious,
during his escape.
Soon the guards arrive and take Carton, whom they
think is Charles Evrémonde, out to join the other
condemned prisoners. A young woman, who was
wrongly accused and convicted, asks him if she can
hold his hand. Suddenly, the women realizes that
he is not Evrémonde. “Are you dying for him?” she
asks. “And his wife and child,” Carton replies. Carton
promises to hold the woman’s hand until the end.
The young girl reveals how
corrupt and merciless the
republic’s tribunals are. Her innocence
also lets her recognize
Carton for who he is: a figure
of Christ, giving his life to save
others. Holding Carton’s hand
suggests how the girl’s faith will
sustain her.
At the Paris barricade, guards check the papers of the
passengers in a carriage: Mr. Lorry, Dr. Manette,
Lucie, and “Sydney Carton,” who is unconscious.
They wave the carriage through.
Just as Mr. Lorry smuggled the
infant Lucie out of Paris, he now
transports these mostly helpless
passengers to safety.
Book 3, Chapter 14: The Knitting Done
At the shop of the wood-sawyer, Madame Defarge
holds a secret conference with Jacques Three and The
Vengeance. Madame says that she no longer trusts
Monsieur Defarge, and that they must exterminate
the Evrémondes themselves. Jacques Three swears
that his jury will condemn Lucie, and fantasizes about
the blond hair and blue eyes of Lucie’s beheaded child
at the guillotine. The wood-sawyer and Madame
Defarge promise to testify against Lucie.
Lucie kissed her hand to
the prison as a gesture of
loyalty and compassion. But the
revolutionaries see it as an act
of treason. The revolutionaries
have given up all human feeling
and mercy, as is shockingly
apparent in Jacques Three’s sick
fantasy about murdering an
innocent girl.
Madame Defarge strides through the streets like
a tigress, a woman without pity, armed with a knife
and loaded pistol. She heads to Lucie’s apartment,
hoping to strengthen her case by catching Lucie
insulting the Revolution in her grief.
Madame Defarge combines the
figures of Fate and Death. She
is terrifying and inhuman. She
represents death as opposed to
resurrection, murder as opposed
to sacrifice.
At the apartment, Jerry Cruncher and Miss Pross
get ready to leave in their own carriage. Jerry swears
that he will give up grave robbing, and states that his
opinions about praying have changed. He adds that
he hopes Mrs. Cruncher is praying right then.
Jerry gives up his work as a
“resurrection man” because
that job belongs to Christ. With
death (Madame Defarge) on
the move, Jerry turns to religion
to save him.
Jerry leaves to make arrangements. Soon after,
Madame Defarge arrives at the apartment and
demands that Miss Pross let her see Lucie. Miss
Pross refuses to budge from Lucie’s bedroom door.
Madame Defarge tries to shove her aside, but
Miss Pross grabs her. During the ensuing struggle,
Madame Defarge grabs for her pistol. But as she
grabs the weapon it accidentally goes off, killing her.
Miss Pross flees the apartment in terror. She meets
up with Jerry and discovers that she has permanently
lost her hearing.
Lucie kissed Madame Defarge’s
hands and asked for mercy.
That failed. Now, the faithful
English servant Miss Pross
wrestles with a faithless French
former servant turned revolutionary.
Madame Defarge’s accidental
suicide shows how the
revolutionaries sow the seeds of
their own destruction. In fact, as
the Reign of Terror progressed,
many French revolutionaries
died under their own guillotines.
Book 3, Chapter 15: The Footstep Die Out Forever
Three carts rumble through the Paris streets carrying
the condemned prisoners to the guillotine. Some
onlookers, used to the spectacle, are bored. Others
gather to see Charles Evrémonde and insult him.
This alludes to Christ’s journey
to the crucifixion, during which
Christ was also harassed and
insulted by spectators.
The Vengeance is in the crowd. She has been saving
a front-row seat for Madame Defarge and holding
her knitting. She bitterly regrets that her friend will
miss the festivities.
Madame Defarge is separated
from her knitting: the grip of
fate has been broken.
The young woman is scheduled to be beheaded
by the guillotine just before Carton. She thanks
Carton for helping her stay composed, and says he
must have been sent to her from Heaven. Carton
tells her to focus only on him and to have no fear.
When her time comes, they kiss, and she calmly
goes to the guillotine. Carton is next. He says “I am
the resurrection and the life.” Carton ascends the
platform, his face looking serene and prophetic, and
the guillotine crashes down on his head.
As Christ comforted his fellow
prisoners on the cross, Carton
also comforts the girl, urging
her to look past the suffering of
politics toward a heavenly future.
With such faith, the condemned
have no fear. Carton’s prayer
suggests that they will live
forever. His serene face implies
the certainty of his salvation
and resurrection, brought about
through faith.
The narrator describes Carton’s final thoughts. He
recognizes that Barsad, The Vengeance, and all
the “new oppressors” will die by the guillotine they
now celebrate. Yet he is also sure that Paris will rise
up from its ashes, struggling to be free. He sees a
vision of Lucie with a new son, named after him,
who will live a successful and prosperous life. He also
sees Dr. Manette restored to health, and Mr. Lorry
leaving all his considerable wealth to the Manette’s
and then passing tranquilly away. And Carton knows
he is blessed and treasured by all these people. The
novel ends with Carton’s final thoughts, “It is a far, far
better thing that I do, than I have ever done; it is a far,
far better rest that I go to than I have ever known.”
In Carton’s vision, the revolutionaries
who showed no mercy
will not receive any, just like the
aristocracy before them. The
novel makes the case for mercy,
in particular Christian mercy, as
a vital force to counteract the
tendency of the powerful toward
tyranny, and suggests that
France will eventually find this
balance. For his selfless sacrifice,
which alone could break the
grip of fate and history, Carton
is resurrected not just in heaven
but also through Lucie’s son,
who lives out Lucie’s hope that
Carton would live a better life.
Important Quotes
Book 1, Chapter 1 Quotes
It was the best of times, it was the worst of times, it was the
age of wisdom, it was the age of foolishness, it was the epoch
of belief, it was the epoch of incredulity, it was the season
of Light, it was the season of Darkness, it was the spring of
hope, it was the winter of despair, we had everything before
us, we had nothing before us, we were all going direct to
Heaven, we were all going direct the other way.
Book 1, Chapter 3 Quotes
A wonderful fact to reflect upon, that every human creature is
constituted to be that profound secret and mystery to every
other. A solemn consideration, when I enter a great city by
night, that every one of those darkly clustered houses encloses
its own secret; that every room in every one of them
encloses its own secret; that every beating heart in the hundreds
of thousands of breasts there, is, in some of its imaginings,
a secret to the heart nearest it!
Book 1, Chapter 5 Quotes
The children had ancient faces and grave voices; and upon
them, and upon the grown faces, and ploughed into every
furrow of age and coming up afresh, was the sign, Hunger. It
was prevalent everywhere. Hunger was pushed out of the tall
houses, in the wretched clothing that hung upon poles and
lines; Hunger was patched into them with straw and rag and
wood and paper; Hunger was repeated in every fragment of
the small modicum of firewood that the man sawed off; Hunger
stared down from the smokeless chimneys, and started
up from the filthy street that had no offal, among its refuse,
of anything to eat. Hunger was the inscription on the baker’s
shelves, written in every small loaf of his scanty stock of bad
bread; at the sausage-shop, in every dead-dog preparation
that was offered for sale.
Book 1, Chapter 6 Quotes
If you hear in my voice … any resemblance to a voice that
once was sweet music in your ears, weep for it, weep for it! If
you touch, in touching my hair, anything that recalls a beloved
head that lay on your breast when you were young and free,
weep for it, weep for it! If, when I hint to you of a Home that
is before us, where I will be true to you with all my duty and
with all my faithful service, I bring back the remembrance of a
Home long desolate, while your poor heart pined away, weep
for it, weep for it! — Lucie Manette
Book 2, Chapter 2 Quotes
The sort of interest with which this man was stared and
breathed at, was not a sort that elevated humanity … The
form that was to be doomed to be so shamefully mangled,
was the sight; the immortal creature that was to be so butchered
and torn asunder, yielded the sensation. Whatever gloss
the various spectators put upon the interest, according to
their several arts and powers of self-deceit, the interest was,
at the root of it, Ogreish.
www. L i t C h a r t s . c om 11 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 4 Quotes
Only his daughter had the power of charming this black
brooding from his mind. She was the golden thread that united
him to a Past beyond his misery, and to a Present beyond
his misery: and the sound of her voice, the light of her face,
the touch of her hand, had a strong beneficial influence with
him almost always.
Book 2, Chapter 5 Quotes
Waste forces within him, and a desert all around, this man
stood still on his way across a silent terrace, and saw for
a moment, lying in the wilderness before him, a mirage of
honourable ambition, self-denial, and perseverance. In the
fair city of this vision, there were airy galleries from which the
loves and graces looked upon him, gardens in which the fruits
of life hung ripening, waters of Hope that sparkled in his sight.
A moment, and it was gone. Climbing to a high chamber in a
well of houses, he threw himself down in his clothes on a neglected
bed, and its pillow was wet with wasted tears.
Book 2, Chapter 7 Quotes
But, the comfort was, that all the company at the grand hotel
of Monseigneur were perfectly dressed. If the Day of Judgment
had only been ascertained to be a dress day, everybody there
would have been eternally correct. Such frizzling and powdering
and sticking up of hair, such delicate complexions artificially
preserved and mended, such gallant swords to look at, and
such delicate honour to the sense of smell, would surely keep
anything going, for ever and ever. … with the rustle of silk and
brocade and fine linen, there was a flutter in the air that fanned
Saint Antoine and his devouring hunger far away.
Book 2, Chapter 8 Quotes
Expressive signs of what made them poor, were not wanting;
the tax for the state, the tax for the church, the tax for the lord,
tax local and tax general, were to be paid here and to be paid
there, according to solemn inscription in the little village, until
the wonder was, that there was any village left unswallowed.
Book 2, Chapter 9 Quotes
“Repression is the only lasting philosophy. The dark deference
of fear and slavery, my friend,” observed the Marquis,
“will keep the dogs obedient to the whip, as long as this roof,”
looking up to it, “shuts out the sky.”
Book 2, Chapter 10 Quotes
He had loved Lucie Manette from the hour of his danger. He
had never heard a sound so sweet and dear as the sound of
her compassionate voice; he had never seen a face so tenderly
beautiful, as hers when it was confronted with his own
on the edge of the grave that had been dug for him.
Book 2, Chapter 13 Quotes
For you, and for any dear to you, I would do anything. If my
career were of that better kind that there was any opportunity
or capacity of sacrifice in it, I would embrace any sacrifice for
you and for those dear to you. Try to hold me in your mind, at
some quiet times, as ardent and sincere in this one thing. The
time will come, the time will not be long in coming, when new
ties will be formed about you […] O Miss Manette, […] when
you see your own bright beauty springing up anew at your feet,
think now and then that there is a man who would give his life,
to keep a life you love beside you! — Sydney Carton
Book 2, Chapter 16 Quotes
Another darkness was closing in as surely, when the church
bells, then ringing pleasantly in many an airy steeple over
France, should be melted into thundering cannon; when the
military drums should be beating to drown a wretched voice,
that night all potent as the voice of Power and Plenty, Freedom
and Life. So much was closing in about the women who
sat knitting, knitting, that they their very selves were closing
in around a structure yet unbuilt, where they were to sit knitting,
knitting, counting dropping heads.
Book 2, Chapter 18 Quotes
Nothing would induce him to speak more. He looked up, for
an instant at a time, when he was requested to do so; but, no
persuasion would extract a word from him. He worked, and
worked, and worked, in silence, and words fell on him as they
would have fallen on an echoless wall, or on the air.
Book 2, Chapter 20 Quotes
My husband, it is so. I fear he is not to be reclaimed; there
is scarcely a hope that anything in his character or fortunes
is reparable now. But, I am sure that he is capable of good
things, gentle things, even magnanimous things. – Lucie
Book 2, Chapter 21 Quotes
The sea of black and threatening waters, and of destructive
upheaving of wave against wave, whose depths were yet unfathomed
and whose forces were yet unknown. The remorseless
sea of turbulently swaying shapes, voices of vengeance,
and faces hardened in the furnaces of suffering until the
touch of pity could make no mark on them.
Book 2, Chapter 22 Quotes
The raggedest nightcap, awry on the wretchedest head, had
this crooked significance in it: “I know how hard it has grown
for me, the wearer of this, to support life in myself; but do
you know how easy it has grown for me, the wearer of this, to
destroy life in you?” Every lean bare arm, that had been without
work before, had this work always ready for it now, that it
could strike. The fingers of the knitting women were vicious,
with the experience that they could tear.
Book 2, Chapter 23 Quotes
With the rising and falling of the blaze, the stone faces
showed as if they were in torment. When great masses of
stone and timber fell, the face with the two dints in the nose
became obscured: anon struggled out of the smoke again, as
if it were the face of the cruel Marquis, burning at the stake
and contending with the fire.
Book 2, Chapter 24 Quotes
Like the mariner in the old story, the winds and streams had
driven him within the influence of the Loadstone Rock, and
it was drawing him to itself, and he must go. Everything that
arose before his mind drifted him on, faster and faster, more
and more steadily, to the terrible attraction. His latent uneasiness
had been … that he who could not fail to know that he
was better than they, was not there, trying to do something to
stay bloodshed, and assert the claims of mercy and humanity.
Book 3, Chapter 1 Quotes
Not a mean village closed upon him, not a common barrier
dropped across the road behind him, but he knew it to be
another iron door in the series that was barred between him
and England. The universal watchfulness so encompassed
him, that if he had been taken in a net, or were being forwarded
to his destination in a cage, he could not have felt his
freedom more completely gone.
Book 3, Chapter 2 Quotes
As these ruffians turned and turned, their matted locks now
flung forward over their eyes, now flung backward over their
necks, some women held wine to their mouths that they
might drink; and what with dropping blood, and what with
dropping wine, and what with the stream of sparks struck out
of the stone, all their wicked atmosphere seemed gore and
fire. The eye could not detect one creature in the group free
from the smear of blood.
Book 3, Chapter 4 Quotes
Above all, one hideous figure grew … the figure of the sharp
female called La Guillotine. It was the popular theme for jests; it
was the best cure for headache, it infallibly prevented the hair
from turning grey, it imparted a peculiar delicacy to the complexion,
it was the National Razor which shaved close: who kissed
La Guillotine, looked through the little window and sneezed into
the sack. It was the sign of the regeneration of the human race.
It superseded the Cross. Models of it were worn on breasts from
which the Cross was discarded, and it was bowed down to and
believed in where the Cross was denied.
Book 3, Chapter 5 Quotes
No fight could have been half so terrible as this dance. It was
so emphatically a fallen sport—a something, once innocent,
delivered over to all devilry—a healthy pastime changed into
a means of angering the blood, bewildering the senses, and
steeling the heart. Such grace as was visible in it, made it the
uglier, showing how warped and perverted all things good by
nature were become.
Book 3, Chapter 6 Quotes
Looking at the Jury and the turbulent audience, he might have
thought that the usual order of things was reversed, and that
the felons were trying the honest men.
Book 3, Chapter 8 Quotes
Miss Pross recalled soon afterwards, and to the end of her
life remembered, that as she pressed her hands on Sydney’s
arm and looked up in his face, imploring him to do no hurt to
Solomon, there was a braced purpose in the arm and a kind
of inspiration in the eyes, which not only contradicted his light
manner, but changed and raised the man.
Book 3, Chapter 9 Quotes
“I am the resurrection and the life, saith the Lord: he that
believeth in me, though he were dead, yet shall he live: and
whosoever liveth and believeth in me, shall never die.”
Before that unjust Tribunal, there was little or no order of procedure,
ensuring to any accused person any reasonable hearing.
There could have been no such Revolution, if all laws,
forms, and ceremonies, had not first been so monstrously
abused, that the suicidal vengeance of the Revolution was to
scatter them all to the winds.
Book 3, Chapter 10 Quotes
The boy’s eyes, which had been fixed on mine, slowly turned
to the looker-on, and I saw in the two faces that all he said
was true. The two opposing kinds of pride confronting one
another, I can see, even in this Bastille; the gentleman’s, all
negligent indifference; the peasants, all trodden-down sentiment,
and passionate revenge. — Dr. Manette
Book 3, Chapter 14 Quotes
There were many women at that time, upon whom the time
laid a dreadfully disfiguring hand; but, there was not one
among them more to be dreaded than this ruthless woman,
now taking her way along the streets … imbued from her
childhood with a brooding sense of wrong, and an inveterate
hatred of a class, opportunity had developed her into a
tigress. She was absolutely without pity.
Book 3, Chapter 15 Quotes
Along the Paris streets, the death-carts rumble, hollow and
harsh. Six tumbrils carry the day’s wine to La Guillotine. All the
devouring and insatiate Monsters imagined since imagination
could record itself, are fused in the one realisation, Guillotine.
… Crush humanity out of shape once more, under similar hammers,
and it will twist itself into the same tortured forms. Sow
the same seed of rapacious license and oppression over again,
and it will surely yield the same fruit according to its kind.
www. L i t C h a r t s . c om 12 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book,
Chapter
Themes
Backstory
–– Dr. Alexandre Manette is secretly imprisoned in the Bastille by Marquis St. Evrémonde and his twin brother.
–– Mr. Jarvis Lorry smuggles Dr. Manette’s daughter, the infant Lucie, to safety in London.
1,1
–– It’s 1775 and the King and Queen of France govern their country harshly. The allegorical figures of Fate and Death
stalk the land.
1,2 –– Mr. Lorry travels to Dover in a mail coach. Jerry Cruncher delivers a note to him, and Mr. Lorry replies with the
words “Recalled to life.”
1,3 –– Mr. Lorry has a dream about digging someone up from the grave.
1,4 –– In Dover, Mr. Lorry meets Lucie Manette and explains that her father, Dr. Manette, has been found.
1,5 –– Mr. Lorry and Lucie meet Monsieur Defarge at his wine shop. He takes them to the attic where Dr. Manette is making shoes in a corner.
1,6 –– Dr. Manette gradually recognizes Lucie. Mr. Lorry helps them all leave Paris.
2,1 –– Jerry Cruncher waits for jobs outside Tellson’s Bank. He complains about his wife’s praying.
2,2 –– In London, Charles Darnay stands trial for treason. His defense lawyers are Mr. Stryver and Sydney Carton. Lucie and Dr. Manette are witnesses for the
prosecution.
2,3 –– Carton undercuts an accusing witness when Carton points out how much he himself looks like Charles. The jury acquits Charles.
2,4 –– Carton takes Charles to a tavern and drunkenly, insultingly prods him about Lucie Manette.
2,5 –– Carton works through the night to do all of Stryver’s legal work.
2,6 –– Dr. Manette and Lucie move into a house. They receive frequent visits from Mr. Lorry, Charles, and Carton.
2,7 –– In Paris, Marquis Evrémonde runs over a little girl with his carriage as the Defarges look on.
2,8 –– A mender of roads tells the Marquis he saw a man clinging onto his carriage. Driving on, the Marquis spurns the petition of a woman wanting a gravestone
for her husband.
2,9
–– Charles Evrémonde visits the Marquis and renounces his family’s name and property.
–– The sleeping Marquis is murdered in his bed; a note on the knife reads “Jacques.”
2,10 –– A year passes. Charles obtains Dr. Manette’s permission to marry Lucie, but Dr. Manette refuses to learn Charles’s real name.
2,11 –– Stryver tells Carton that he will propose marriage to Lucie Manette.
2,12 –– Mr. Lorry strongly advises Stryver not to propose to Lucie, and Stryver changes his mind.
2,13 –– Carton visits Lucie to express his love and his hope that she might pity him. Carton promises his life to her.
2,14 –– Jerry Cruncher watches a funeral procession for the spy Roger Cly. That night, Jerry digs up the coffin to steal the body, but the body is missing.
2,15 –– The mender of roads tells the Defarges about Marquis Evrémonde’s murder, and how the criminal was later caught and hanged.
2,16 –– John Barsad tries to spy on Monsieur and Madame Defarge. The Defarges are shocked to learn that Lucie Manette has married an Evrémonde.
2,17 –– Lucie spends the evening before her wedding with Dr. Manette and promises to become closer to him.
Theme Key
Tyranny and Revolution
Secrecy and Surveillance
Fate and History
Sacrifice
Resurrection
Imprisonment
ThemeTrackerTM
The LitCharts ThemeTracker is a mini-version of the entire LitChart. The ThemeTracker provides a quick timeline-style rundown of all the important plot points and allows you to track the themes throughout
the work at a glance.
www. L i t C h a r t s . c om 13 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
2,18 –– Charles privately tells Dr. Manette his real name. When the couple leaves for a honeymoon, Dr. Manette relapses into making shoes.
2,19 –– Dr. Manette snaps out of it. That night, Mr. Lorry and Miss Pross destroy the shoemaker’s bench and bury the tools.
2,20 –– Carton meets Charles and Lucie on their return. Later, Charles notes that Carton is awkward and strange, but Lucie demands that her husband give Carton
a chance.
2,21
–– Time passes. Lucie gives birth to a daughter (“Lucie”) and a son who doesn’t survive.
–– In 1789, Defarge leads the people to destroy the Bastille. He searches Dr. Manette’s old cell. Madame Defarge beheads the prison warden with a knife.
2,22 –– The Defarges lead another mob to seize and execute an aristocrat who insulted the people.
2,23
–– The castle of Marquis Evrémonde is burned down. No one helps. More arson and fighting occur across the nation.
–– A mob pursues Gabelle who hides on his roof.
2,24 –– The year is 1792. Mr. Lorry receives a letter for Evrémonde from Gabelle who has been imprisoned and asks for help. Charles leaves for Paris the next night.
3,1 –– After arriving in France, Charles is escorted by soldiers to Paris, taken prisoner, and jailed in secret.
3,2 –– Mr. Lorry, Lucie, and Dr. Manette arrive in Paris. Bloodthirsty revolutionaries are on a rampage killing their prisoners. Dr. Manette rushes out and saves
Charles.
3,3 –– Mr. Lorry gets Lucie an apartment. The Defarges bring a note from Dr. Manette. The cold Madame Defarge terrifies Lucie.
3,4 –– Dr. Manette’s reputation in Paris grows, but Charles is not freed. A year passes and the guillotine becomes a popular form of capital punishment.
3,5 –– Every day, Lucie walks near the prison where Charles might see her. She is watched by the wood-sawyer and Madame Defarge.
3,6 –– At Charles’s trial, Gabelle and Dr. Manette testify in his favor. Charles is freed and celebrated as a patriot.
3,7 –– Charles is reunited with Lucie, but that night he is arrested again on charges brought by Monsieur and Madame Defarge, as well as one other unnamed
person.
3,8 –– Jerry Cruncher and Miss Pross run into Solomon Pross in the streets. Sydney Carton shows up and identifies him as John Barsad. Carton threatens to
reveal him as a spy unless he cooperates to help Charles.
3,9
–– Carton wanders the Paris streets and buys a packet of drugs. A prayer runs through his head: “I am the resurrection and the life.”
–– At Charles’s trial, the prosecutor brings charges from the Defarges and Dr. Alexandre Manette.
3,10
–– Monsieur Defarge reads from Dr. Manette’s letter he discovered in the Bastille. The Marquis Evrémonde and his twin brother secretly imprisoned Manette
to hide their crimes against a peasant family.
–– Charles is convicted and sentenced to be killed within 24 hours.
3,11 –– Lucie and Charles say their goodbyes. Dr. Manette freaks out. Carton and Mr. Lorry quietly confess they have no hope for a political solution, but Carton
implies that he has a plan.
3,12
–– At the wine shop, Carton overhears Madame Defarge plotting to convict Lucie. Madame Defarge is the missing daughter of the peasant family persecuted
by the Evrémondes.
–– Carton instructs Mr. Lorry to get a carriage ready to leave for England.
3,13
–– John Barsad lets Carton into Charles’s prison cell. Carton drugs Charles, swaps clothes with him, and makes Barsad carry Charles back to Mr. Lorry, who
immediately leaves with everyone.
–– As Carton is taken to the guillotine, another prisoner, an innocent girl, asks to hold Carton’s hand until the end.
3,14 –– Madame Defarge goes to Lucie’s apartment. She scuffles with Miss Pross and accidentally shoots herself.
3,15 –– Carton is executed by the guillotine. Before he dies, he realizes that his sacrifice is the greatest thing he’s ever done and has a vision of his resurrection
through Lucie’s son, who will one day be born and named after him.
www. L i t C h a r t s . c om 14 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Installation Guide
Sun™ ONE Application Server
Version 7, Enterprise Edition
817-2146-10
September 2003
Sun Microsystems, Inc.
4150 Network Circle
Santa Clara, CA 95054 U.S.A.
Copyright © 2003 Sun Microsystems, Inc. All rights reserved.
THIS SOFTWARE CONTAINS CONFIDENTIAL INFORMATION AND TRADE SECRETS OF SUN MICROSYSTEMS, INC. USE,
DISCLOSURE OR REPRODUCTION IS PROHIBITED WITHOUT THE PRIOR EXPRESS WRITTEN PERMISSION OF SUN
MICROSYSTEMS, INC. U.S. Government Rights - Commercial software. Government users are subject to the Sun Microsystems, Inc.
standard license agreement and applicable provisions of the FAR and its supplements. Use is subject to license terms.
This distribution may include materials developed by third parties.
Sun, Sun Microsystems, the Sun logo, Java, Sun™ ONE, the Java Coffee Cup logo and the Sun™ ONE logo are trademarks or
registered trademarks of Sun Microsystems, Inc. in the U.S. and other countries.
UNIX is a registered trademark in the U.S. and other countries, exclusively licensed through X/Open Company, Ltd.
This product is covered and controlled by U.S. Export Control laws and may be subject to the export or import laws in other
countries. Nuclear, missile, chemical biological weapons or nuclear maritime end uses or end users, whether direct or indirect, are
strictly prohibited. Export or reexport to countries subject to U.S. embargo or to entities identified on U.S. export exclusion lists,
including, but not limited to, the denied persons and specially designated nationals lists is strictly prohibited.
________________________________________________________________________________________
Copyright © 2003 Sun Microsystems, Inc., 4150 Network Circle, Santa Clara, California 95054, Etats-Unis. Tous droits réservés.
CE LOGICIEL CONTIENT DES INFORMATIONS CONFIDENTIELLES ET DES SECRETS COMMERCIAUX DE SUN
MICROSYSTEMS, INC. SON UTILISATION, SA DIVULGATION ET SA REPRODUCTION SONT INTERDITES SANS
L’AUTORISATION EXPRESSE, ÉCRITE ET PRÉALABLE DE SUN MICROSYSTEMS, INC. Droits du gouvernement américain,
utlisateurs gouvernmentaux - logiciel commercial. Les utilisateurs gouvernmentaux sont soumis au contrat de licence standard de
Sun Microsystems, Inc., ainsi qu aux dispositions en vigueur de la FAR (Federal Acquisition Regulations) et des suppléments à
celles-ci. L’utilisation est soumise aux termes de la Licence.
Cette distribution peut comprendre des composants développés pardes tierces parties.
Sun, Sun Microsystems, le logo Sun, Java, Sun™ ONE, le logo Java Coffee Cup et le logo Sun™ ONE sont des marques de fabrique ou
des marques déposées de Sun Microsystems, Inc. aux Etats-Unis et dans d’autres pays.
UNIX est une marque déposée aux Etats-Unis et dans d’autres pays et licenciée exlusivement par X/Open Company, Ltd.
Ce produit est soumis à la législation américaine en matière de contrôle des exportations et peut être soumis à la règlementation en
vigueur dans d’autres pays dans le domaine des exportations et importations. Les utilisations, ou utilisateurs finaux, pour des armes
nucléaires, des missiles, des armes biologiques et chimiques ou du nucléaire maritime, directement ou indirectement, sont
strictement interdites. Les exportations ou réexportations vers les pays sous embargo américain, ou vers des entités figurant sur les
listes d’exclusion d’exportation américaines, y compris, mais de manière non exhaustive, la liste de personnes qui font objet d’un
ordre de ne pas participer, d’une façon directe ou indirecte, aux exportations des produits ou des services qui sont régis par la
législation américaine en matière de contrôle des exportations et la liste de ressortissants spécifiquement désignés, sont
rigoureusement interdites.
3
Contents
About This Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Who Should Use This Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
How This Guide is Organized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Using the Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Documentation Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
General Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Conventions Referring to Directories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Product Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
For More Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Chapter 1 Preparing to Install . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Installation Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Installation Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Application Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Administration Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Java 2 Software Development Kit (J2SE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Sun ONE Message Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Sample Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Always-On Technology Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
High-Availability Database (HADB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
HADB Management Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Load Balancer Plug-in . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Installation Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Graphical Interface Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Command-Line Interface Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Silent Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Distribution of the Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Installation Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Platform Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Configuration 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Configuration 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Configuration 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Solaris Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Solaris 8 Patch Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Shared Message Queue Broker Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Hardened Solaris Operating Environment Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
General Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
High-Availability Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Topology Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Space Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Web Server Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Shared Memory Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Remote Access Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Accessing the Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Chapter 2 Installing Enterprise Edition Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
About Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Installation Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Installation Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
Installation Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Installing Application Server Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Installing the Load Balancer Plug-in . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Installing in Silent Mode (Non-Interactive) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Creating the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Syntax for Creating the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Example Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Modifying the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Installing in Silent Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Chapter 3 Preparing for HADB Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Configuring Shared Memory and Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Setting Up Host Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Setting up RSH for HADB Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Setting Up SSH for HADB Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
SSH Requirements and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Installing SSH for Solaris 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Configuring SSH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Setting Up the User Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Setting Up Administration for Non-Root . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Using the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
How the clsetup Command Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5
How the Input Files Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
What the clsetup Command Accomplishes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Commands Used by the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
clsetup Requirements and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
Editing the clsetup Input Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
The clinstance.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
The clpassword.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
The clresource.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Running the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Cleanup Procedures for the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Chapter 4 Post-installation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Starting and Stopping the Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Using the Command-line Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Using start-domain and stop-domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
Using start-instance and stop-instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Getting Helpful Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Using the Administration Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Creating Domains and Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Web Services Client Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Stopping and Starting the HADB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Stopping the HADB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Starting the HADB After Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Chapter 5 Uninstalling the Enterprise Edition Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
About Uninstalling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstallation Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstalling the Application Server Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstalling in Silent Mode (non-interactive) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Chapter 6 Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
About Logs and Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
J2SE Installation/Upgrade Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Incompatible J2SE version---cannot upgrade. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Failure to install J2SE reported through install log file. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Forgotten User Name or Password . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Forgotten Admin Server Port Number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Connection Refused for Administration Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Server Won’t Start: CGI Error Occurs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Set Limits on File Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
On Solaris: Change Kernel Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
6 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Uninstallation Failure Cleanup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
Appendix A Installation Cheatsheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
1. Fulfill the installation requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
2. Install the software components. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3. Complete the high-availability installation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4. Complete the post-installation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7
About This Guide
This Installation Guide provides instructions for installing the Sun™ Open Net
Environment (Sun ONE) Application Server 7, Enterprise Edition product.
The following topics are addressed here:
• Who Should Use This Guide
• How This Guide is Organized
• Using the Documentation
• Documentation Conventions
• Product Support
• For More Information
Who Should Use This Guide
This manual is intended for system administrators, network administrators,
evaluators, application server administrators, and developers who want to install
the Sun ONE Application Server software.
This guide assumes you are familiar with the following:
• Installation of enterprise-level software products
• UNIX® operating system
• Client/server programming model
• Internet and World Wide Web
• High-availability and clustering concepts
How This Guide is Organized
8 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
How This Guide is Organized
This guide contains the following documentation components:
• Chapter 1, “Preparing to Install” on page 15—Provides information on the
installation components, installation methods, and requirements for installing
Sun ONE Application Server 7, Enterprise Edition software
• Chapter 2, “Installing Enterprise Edition Software” on page 31—Provides
instructions for installing the Sun ONE Application Server 7, Enterprise
Edition software components. Includes instructions for performing a
non-interactive silent installation.
• Chapter 3, “Preparing for HADB Setup” on page 53—Provides instructions for
configuring shared memory, and setting up host communications and the user
environment for the high-availability configuration.
• Chapter 4, “Post-installation Tasks” on page 81—Describes additional tasks
you may need to perform during or after installing the Sun ONE Application
Server software.
• Chapter 5, “Uninstalling the Enterprise Edition Software” on
page 87—Provides instructions for uninstalling the Sun ONE Application
Server 7 software. Includes instructions for performing a non-interactive silent
uninstallation.
• Chapter 6, “Troubleshooting” on page 93—Provides information on logging as
well as solutions to problems you may encounter during or after installation or
uninstallation.
• Appendix A, “Installation Cheatsheet” on page 101—Provides a checklist of
the summarized tasks of installing the Sun ONE Application Server Version 7,
Enterprise Edition software.
Using the Documentation
The Sun ONE Application Server 7, Enterprise Edition manuals are available in
Portable Document Format (PDF) and Hypertext Markup Language (HTML) on
the documentation CD that is distributed with the product.
The following table lists tasks and concepts described in the Sun ONE Application
Server manuals. The left column lists the tasks and concepts, and the right column
lists the corresponding manuals.
Using the Documentation
About This Guide 9
Application Server Documentation Roadmap
For information about See the following
Late-breaking information about the software and the documentation Release Notes
Comprehensive, table-based summary of supported hardware, operating system, JDK,
and JDBC/RDBMS.
Platform Summary
Sun ONE Application Server 7 overview, features available with each
product edition
Product Overview
Diagrams and descriptions of server architecture, benefits of the Sun ONE
Application Server architectural approach
Server Architecture
New enterprise, developer, and operational features of Sun ONE
Application Server 7
What’s New
How to get started with the Sun ONE Application Server 7 product. Includes
new features, architectural overview, and sample application tutorial.
Getting Started Guide
Installing the Sun ONE Application Server software and its components, such as
sample applications, the Administration interface, and the high-availability
components. Instructions for implementing a basic high-availability configuration are
included.
Installation Guide
Evaluating your system needs and enterprise to ensure that you deploy Sun
ONE Application Server in a manner that best suits your site. General issues
and concerns that you must be aware of when deploying an application
server are also discussed.
System Deployment Guide
Best practices for HTTP session availability that application architects and
developers can use
Application Design Guidelines
for Storing Session State
Creating and implementing J2EE applications intended to run on the
Application Server 7 that follow the open Java standards model for servlets,
Enterprise JavaBeans™ (EJBs™), JavaServer Pages (JSPs), and other J2EE
components. Includes general information about application design,
developer tools, security, assembly, deployment, debugging, and creating
lifecycle modules. A comprehensive Application Server glossary is included.
Developer’s Guide
Creating and implementing J2EE web applications that follow the Java™
Servlet and JavaServer Pages™ (JSP™) specifications on the Application
Server 7. Discusses web application programming concepts and tasks, and
provides sample code, implementation tips, and reference material. Topics
include results caching, JSP precompilation, session management, security,
deployment, SHTML, and CGI.
Developer’s Guide to Web
Applications
Using the Documentation
10 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Creating and implementing J2EE applications that follow the open Java
standards model for enterprise beans on the Sun ONE Application Server 7.
Discusses Enterprise JavaBeans™ (EJB™) programming concepts and tasks,
and provides sample code, implementation tips, and reference material.
Topics include container-managed persistence, read-only beans, and the XML
and DTD files associated with enterprise beans.
Developer’s Guide to
Enterprise JavaBeans
Technology
Creating Application Client Container (ACC) clients that access J2EE
applications on the Application Server 7
Developer’s Guide to Clients
Creating web services in the Sun ONE Application Server environment Developer’s Guide to Web
Services
Java™ Database Connectivity (JDBC™), transaction, Java Naming and
Directory Interface™ (JNDI), Java™ Message Service (JMS), and JavaMail™
APIs
Developer’s Guide to J2EE
Services and APIs
Creating custom NSAPI plugins Developer’s Guide to NSAPI
Information and instructions on the configuration, management, and deployment of
the Sun ONE Application Server subsystems and components, from both the
Administration interface and the command-line interface. Topics include cluster
management, the high-availability database, load balancing, and session persistence.
A comprehensive Application Server glossary is included.
Administrator’s Guide
Editing Sun ONE Application Server configuration files, such as the server.xml
file
Administrator’s Configuration
File Reference
Configuring and administering security for the Sun ONE Application Server
operational environment. Includes information on general security,
certificates, and SSL/TLS encryption. HTTP server-based security is also
addressed.
Administrator’s Guide to
Security
Configuring and administering service provider implementation for J2EE™
Connector Architecture (CA) connectors for the Sun ONE Application
Server 7. Topics include the Administration Tool, Pooling Monitor, deploying
a JCA connector, and sample connectors and sample applications.
J2EE CA Service Provider
Implementation
Administrator’s Guide
Migrating your applications to the new Sun ONE Application Server 7
programming model, specifically from iPlanet Application Server 6.x and from
Netscape Application Server 4.0. Includes a sample migration.
Migrating and Redeploying
Server Applications Guide
How and why to tune your Sun ONE Application Server to improve
performance
Performance Tuning Guide
Information on solving Sun ONE Application Server problems Troubleshooting Guide
Application Server Documentation Roadmap (Continued)
For information about See the following
Documentation Conventions
About This Guide 11
Documentation Conventions
This section describes the types of conventions used throughout this guide:
• General Conventions
• Conventions Referring to Directories
General Conventions
The following general conventions are used in this guide:
• File and directory paths are given in UNIX® format (with forward slashes
separating directory names).
• URLs are given in the format:
http://server.domain/path/file.html
In these URLs, server is the server name where applications are run; domain is
your Internet domain name; path is the server’s directory structure; and file is
an individual filename. Italic items in URLs are placeholders.
• Font conventions include:
? The monospace font is used for sample code and code listings, API and
language elements (such as function names and class names), file names,
path names, directory names, and HTML tags.
? Italic type is used for code variables.
Messages that you may encounter while running Sun ONE Application
Server 7. Includes a description of the likely cause and guidelines on how to
address the condition that caused the message to be generated.
Error Message Reference
Utility commands available with the Sun ONE Application Server; written in
manpage style
Utility Reference Manual
Using the Sun ONE Message Queue software. The Sun ONE Message
Queue documentation at:
http://docs.sun.com/db?p
=prod/s1.s1msgqu
Application Server Documentation Roadmap (Continued)
For information about See the following
Product Support
12 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? Italic type is also used for book titles, emphasis, variables and placeholders,
and words used in the literal sense.
? Bold type is used as either a paragraph lead-in or to indicate words used in
the literal sense.
• Installation root directories for most platforms are indicated by install_dir in
this document.
• Instance root directories are indicated by instance_dir in this document, which
is an abbreviation for the following:
default_config_dir/domains/domain/instance
Conventions Referring to Directories
By default, when using the Solaris 8 and 9 package-based installation, the
application server files are spread across several root directories. These directories
are described in this section.
• install_dir refers to /opt/SUNWappserver7, which contains the static portion of
the installation image. All utilities, executable files, and libraries that make up
the application server reside in this location.
• default_config_dir refers to /var/opt/SUNWappserver7/domainswhich is the
default location for any domains that are created.
• install_config_dir refers to /etc/opt/SUNWappserver7/, which contains
installation-wide configuration information such as licenses and the master list
of administrative domains configured for this installation.
Product Support
Use your early access support process for any product or documentation issues
and for submitting defects.
If you have general feedback on the product or documentation, please send this to
appserver-feedback@sun.com.
If you have problems with your system, contact customer support using one of the
following mechanisms:
• The online support web site at:
http://www.sun.com/supportraining/
For More Information
About This Guide 13
• The telephone dispatch number associated with your maintenance contract
Please have the following information available prior to contacting support. This
helps to ensure that our support staff can best assist you in resolving problems:
• Description of the problem, including the situation where the problem occurs
and its impact on your operation
• Machine type, operating system version, and product version, including any
patches and other software that might be affecting the problem
• Detailed steps on the methods you have used to reproduce the problem
• Any error logs or core dumps
For More Information
Useful information can be found at the following Internet locations:
• Sun ONE products and services information
http://www.sun.com/service/sunps/sunone/index.html
• Sun ONE developer information
http://wwws.sun.com/software/product_categories/application_development.html
• Sun ONE learning solutions
http://wwws.sun.com/software/training/
• Sun ONE product data sheets
http://wwws.sun.com/software/
• Sun Microsystems product documentation
http://docs.sun.com/
For More Information
14 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
15
Chapter 1
Preparing to Install
This chapter explains the Sun Open Net Environment (Sun ONE) Application
Server 7, Enterprise Edition software components, the scope and limitations of
your installation choices, and the system requirements for the Application Server
environment.
The following topics are addressed here:
• Installation Roadmap
• Installation Components
• Installation Methodology
• Distribution of the Product
• Installation Requirements
• Accessing the Documentation
Read the Sun ONE Application Server Release Notes for any late-breaking installation
information.
For more information about configuring the Sun ONE Application Server software
after installation, refer to the Sun ONE Application Server Administrator’s Guide.
The following location contains helpful information, including Technical Notes,
Forum discussions, tools and utilities, and product downloads:
http://wwws.sun.com/software/products/appsrvr/home_appsrvr.html
Installation Roadmap
16 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Installation Roadmap
Implementing the functionality of the Enterprise Edition of Sun ONE Application
Server 7 is not a simple process. The complexities of the high-availability database
(HADB), clustering, failover, and load balancing are different for each possible
scenario and for each installation.
The roadmap in Table 1-1describes the high-level tasks that are required to fully
implement the Sun ONE Application Server 7, Enterprise Edition software. The
right column provides the location of instructions for the task.
In addition to this high-level roadmap, the summarized installation steps are
presented in a checklist format in Appendix A, “Installation Cheatsheet.”
Table 1-1 Installation Roadmap
Step Description of Task Location of Instructions
1 Decide on your high-availability
configuration and set up your systems.
System Deployment Guide
2 Verify that Enterprise Edition requirements
are met.
“Installation Requirements” on page 23
Platform Summary
3 Install the software components. “Installing Enterprise Edition Software” on page 31
4 Set up shared memory for the HADB hosts. “Configuring Shared Memory and Semaphores” on
page 53
5 Set up communication for the HADB
management client using SSH or RSH.
“Setting Up Host Communication” on page 55
6 Set the environment variables for the HADB
management client.
“Setting Up the User Environment” on page 63
7 Set up a basic cluster. “Using the clsetup Command” on page 65
8 Start the application server instances. “Starting and Stopping the Server” on page 81
9 Install the load balancer plug-in. “Installing the Load Balancer Plug-in” on page 44
10 Set up the loadbalancer.xml file. Administrator’s Guide, Configuring Load Balancing
11 Tailor your high-availability setup. Administrator’s Guide, HADB Configuration
Administrator’s Guide, Session Persistence
12 Administer the installed cluster. Administrator’s Guide, Cluster Management
Installation Components
Chapter 1 Preparing to Install 17
Sun ONE Application Server 7, Enterprise Edition documentation is located on the
documentation CD that accompanies the product.
Installation Components
The Sun ONE Application Server Version 7, Enterprise Edition product is made up
of the following software components that work together to create the Application
Server platform:
• Application Server
• Administration Client
• Sun ONE Message Queue
• Java 2 Software Development Kit (J2SE)
• Sample Applications
• Always-On Technology Components
Application Server
This component includes the core components of the Sun ONE Application Server
software and is dependent on the J2SE component. Refer to What’s New and the
Product Overview documents for a more in-depth explanation of the features of Sun
ONE Application Server 7, Enterprise Edition.
Administration
The Administration interface and the command-line interface are automatically
installed when you install the Application Server component. When the
Administration interface has been started, the initial page of the Application Server
graphical interface is displayed.
• Admin Server—Provides administration facilities (one Admin Server per
domain).
• Administration interface—Graphical interface used for performing server
administration tasks. Also called the Admin Console.
Installation Components
18 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Command-line interface—Performs the same tasks as the Administration
interface. A number of high-availability commands are available with this
release. Refer to the Application Server Administrator’s Guide for instructions on
using these commands.
• Multiple administrative domains—This mechanism allows different
administrators to create and manage their own sets of application server
instances.
Both the graphical and command-line administration clients allow you to manage
and configure your servers and the applications hosted on them, as well as help
you deploy your applications.
Full instructions for using the administration tools are contained in the Sun ONE
Application Server Administrator’s Guide, the Administration interface online help,
and the asadmin and hadbm man pages.
Administration Client
The administration client is the separate command-line component of the
Application Server. It is installed automatically when the Sun ONE Application
Server component is installed and is dependent on the J2SE component.
You can choose to install the command-line version of this client separately on a
machine where the Application Server is not installed. Do this by selecting the Sun
ONE Administration Client component instead of the Sun ONE Application Server
component during installation.
Java 2 Software Development Kit (J2SE)
The Sun ONE Application Server product requires the J2SE 1.4.1_03 and leverages
the performance and feature improvements that are part of the 1.4 platform.
During an installation, you can choose to reuse a J2SE component that is already
installed on your system as long as the J2SE version is correct.
NOTE The Sun ONE Application Server 7 product is only certified to work
with J2SE 1.4.1_03 from Sun Microsystems. Third-party J2SE
development kits, even with appropriate version numbers, are not
supported.
Installation Components
Chapter 1 Preparing to Install 19
The J2SE is installed here by default: /usr/j2se
Sun ONE Message Queue
The Sun ONE Message Queue, Platform Edition software is a production
implementation of the Java Messaging Service (JMS) 1.0.2 specification. It is
automatically installed when you install the Application Server software.
The Platform Edition of Sun ONE Message Queue differs from the Enterprise
Edition in that Platform Edition does not have the following Message Queue
features:
• Support for multi-broker message services
• HTTP/HTTPS connections
• Secure connection services
• Scalable connection capability
• Multiple queue delivery policies
For further information, the Sun ONE Message Queue has its own documentation
set that can be found at the following location:
http://docs.sun.com/db?p=prod/s1.s1msgqu
Sample Applications
The Sun ONE Application Server Version 7, Enterprise Edition product includes
over sixty sample applications that are available when you install the Application
Server software. This component is dependent on the Application Server
component.
All samples come with the source, schema, Ant build scripts, and EAR files. These
sample applications are categorized as follows:
• Technology samples—Introduce you to various technical aspects of the Java™
2 Platform, Enterprise Edition (J2EE™) specification as well as the value added
features of the Sun ONE platform. High-availability samples are included.
• Interoperability samples—Provide more detailed views on how these
technologies come together on the Application Server platform.
The sample applications are installed here:
Installation Components
20 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
install_dir/samples
More information about the samples can be found here:
install_dir/samples/index.html
Always-On Technology Components
The Sun ONE Application Server 7, Enterprise Edition includes the Always-On
Technology, which supports multi-tiered, multi-machine, clustered application
server deployments. In Enterprise Edition, the web tier supports load balancing
and application traffic partitioning using a web server plug-in.
Various topologies for the Always On Technology are discussed in the Sun ONE
Application Server System Deployment Guide. For instructions on configuring and
administering high availability for the Application Server, refer to the Sun ONE
Application Server Administrator’s Guide.
The following installation components provide the basis for the Always-On
Technology:
• High-Availability Database (HADB)
• HADB Management Client
• Load Balancer Plug-in
High-Availability Database (HADB)
The Application Server provides a transactional, highly-available and
highly-scalable session state persistence infrastructure. Application Server uses the
HADB to store session information.
For additional information on this component, refer to the HADB Configuration
chapter in the Sun ONE Application Server Administrator’s Guide.
HADB Management Client
The HADB management client is the command-line interface for the HADB. A full
set of utilities is available for performing HADB configuration, runtime
management, and monitoring.
Instructions for using the utilities are contained in the Sun ONE Application Server
Administrator’s Guide, the hadbm man pages, and the asadmin session persistence
man pages.
Installation Methodology
Chapter 1 Preparing to Install 21
Load Balancer Plug-in
The load balancer is responsible for taking incoming HTTP requests and
distributing them across the instances in the cluster. The load balancer also makes
it possible for sessions to fail over to new instances when an instance becomes
unavailable, and for a user to quiesce an instance prior to taking it offline.
The Application Server high-availability load balancer plug-in is an enhanced
version of the HTTP reverse proxy plug-in. In addition, third-party load balancers
can be used. This component is dependent on a pre-installed web server.
Supported web servers are listed in the Sun ONE Application Server Platform
Summary.
For additional information on this component, refer to “Installing the Load
Balancer Plug-in” on page 44 and the Configuring Load Balancing in the Sun ONE
Application Server Administrator’s Guide.
Installation Methodology
The Sun ONE Application Server can be installed or uninstalled using the
command-line interface or the graphical interface. You can install interactively
using either the graphical or command-line interfaces, or you can use silent mode
to replicate an installation scenario on one or multiple machines.
Partial and incremental (subsequent) installations are supported. Using either of
the interactive methods, you can do a partial installation which can be followed by
any number of incremental installations. For silent mode, you can do a partial
initial installation, but any subsequent installations must be done using an
interactive method.
The installation program or uninstallation program checks for component
dependencies and does not allow you to install or uninstall components without
their dependent components.
The following sections explain the various installation methods:
• Graphical Interface Method
• Command-Line Interface Method
• Silent Mode
Installation Methodology
22 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Graphical Interface Method
If you choose to use the graphical interface for installation, you are provided with a
set of interactive graphical dialogs.
To invoke the installation program using the graphical (default) method:
./setup
To invoke the uninstallation program using the graphical (default) method:
./uninstall
Command-Line Interface Method
If you choose to the use the command-line interface, the installation steps are the
same as for the graphical-interface installation, but a graphics-capable display is
not provided.
To invoke the installation program using the command-line method:
./setup -console
To invoke the uninstallation program using the command-line method:
./uninstall -console
If you are using Telnet to access a remote server, you can use the command-line
interface to install the product in an interactive fashion.
Silent Mode
You can use silent mode to perform a scripted installation based on the presence of
a parameter file that was created during an interactive installation. In silent mode,
the Application Server software is installed or uninstalled without any interaction
with you. By referring to the installation configuration file, the components that
were installed or uninstalled in the interactive model are automatically installed or
uninstalled on one or multiple servers.
NOTE For a hardened Solaris operating environment, you must use the
command-line method. To start the installation program in a
hardened environment, you will need to perform the steps in
“Hardened Solaris Operating Environment Requirement” on
page 26.
Distribution of the Product
Chapter 1 Preparing to Install 23
Instructions for using silent mode are contained in “Installing in Silent Mode
(Non-Interactive)” on page 48 and “Uninstalling in Silent Mode (non-interactive)”
on page 90.
Distribution of the Product
The Sun ONE Application Server 7, Enterprise Edition software is available on a
CD-ROM. The Enterprise Edition license is automatically installed with the
product and doesn’t expire. No other licenses can be transitioned to the Enterprise
Edition license.
The package-based model installs the components as packages. By default, the
installation locations are spread across three directory roots:
• /opt/SUNWappserver7 contains the static portion of the installation image. All
utilities, executables and libraries of the Application Server software reside in
this location.
• /etc/opt/SUNWappserver7 contains installation-wide configuration
information such as licenses and the master list of administrative domains
configured for this installation.
• /var/opt/SUNWappserver7/domains is the default area under which
administrative domains are created.
Installation Requirements
This section lists the requirements that must be met before installing the Sun ONE
Application Server 7, Enterprise Edition product.
• Platform Requirements
• Solaris Requirements
• General Requirements
• High-Availability Requirements
NOTE Only product patches and upgrades affect /opt/SUNWappserver7.
Installation Requirements
24 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Platform Requirements
Table 1-2 through Table 1-4 summarize the Sun ONE Application Server 7,
Enterprise Edition requirements for the various high-availability configurations.
Configuration 1
Table 1-2 describes a three-machine type of configuration:
• * Machine 1—Web Server
• ** Machine 2—Application Server instance 1, HADB Node 1
• *** Machine 3—Application Server instance 2, HADB Node 2
Configuration 2
Table 1-3 describes a two-machine type of configuration:
• * Machine 1—Web Server/Application Server (1 Admin Server instance, 1
Application Server instance)
• ** Machine 2—Application Server instance 2 (1 Admin Server instance, 1
Application Server instance), 2 HADB Nodes
Table 1-2 Platform Requirements for Configuration 1
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
96 MB 128 MB 250 MB 500 MB
2 ** Solaris 8, 9
for SPARC
32 and
64 bit
512 MB
(256 MB for
AppServ; 256
MB HADB)
768 MB 500 MB
(250 MB for
AppServ;
250 MB for
HADB)
750 MB
3 *** Solaris 8, 9
for SPARC
32 and
64 bit
768 MB 500 MB
(250 MB for
AppServ;
250 MB for
HADB)
750 MB
Installation Requirements
Chapter 1 Preparing to Install 25
Configuration 3
Table 1-4 describes a single-machine type of configuration:
• * Machine 1—Web Server/Application Server (1 Admin Server instance, 2
Application Server instances), 2 HADB Nodes
You can check your operating system version using the uname or showrev
command. Disk space can be checked using the df -k command. RAM can be
checked using the prtconf or top commands.
For the latest information about supported directory servers, web servers, web
browsers, and so on, refer to the Sun ONE Application Server Platform Summary.
Solaris Requirements
The following Solaris-specific requirements must be met:
• Solaris 8 Patch Requirements
• Shared Message Queue Broker Requirement
• Hardened Solaris Operating Environment Requirement
Table 1-3 Platform Requirements for Configuration 2
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
352 MB 640 MB 500 MB 1 GB
2 ** Solaris 8, 9
for SPARC
32 and
64 bit
768 MB 1 GB 750 MB 1 GB
Table 1-4 Platform Requirements for Configuration 3
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
992 MB 1.5 GB 1.128 GB 1.75 GB
Installation Requirements
26 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Solaris 8 Patch Requirements
For Solaris 8 systems, the following Solaris patches must be installed:
• 109326-06
• 108827-26
• 110934-02
These patches are available individually from the patch finder page here:
http://sunsolve.sun.com/pub-cgi/show.pl?target=patches/patch-access
Shared Message Queue Broker Requirement
If your machine has an active installation of the Solaris 9 bundled version of the
Application Server software, and you install the unbundled version of the server,
the Message Queue broker for these application server installations will be shared.
Therefore, if you fail to uniquely name your domains and instances, you may
receive the following errors when starting up the second instance with the same
domain or instance name:
SEVERE: JMS5024: JMS service startup failed
SEVERE: CORE5071: An error occured during initialization
To avoid these errors, see JMS Support in the Sun ONE Application Server
Administrator’s Guide.
Hardened Solaris Operating Environment Requirement
Hardening means customizing existing services or functions so as to improve the
overall security of the platform. The hardening process generally includes tasks
such as disabling unnecessary services, strengthening ownership and permissions
on objects, and enabling miscellaneous security functions such as non-default
logging and auditing. A hardened operating system usually doesn't allow
GUI-based applications to be run in the environment.
NOTE Solaris 8 systems should have the “Sun recommended patch cluster”
installed. The patch cluster includes the three required patches listed
in this section and is available under “Recommended and Security
Patches” here:
http://sunsolve.sun.com/
Installation Requirements
Chapter 1 Preparing to Install 27
The following two libraries are required to install and use Sun ONE Application
Server 7, Enterprise Edition in a hardened Solaris operating environment:
• libC.so.5
• libCrun.so.1
These libraries can be obtained by installing the SUNWlibC (Sun Workshop
Compilers Bundled libC) package which is part of the Solaris distribution in the
end-user package cluster (not in the core).
General Requirements
The following additional requirements should be met before installing the Sun
ONE Application Server 7, Enterprise Edition product:
• Removing previously-installed Sun ONE Application Server 7 software—If
there is previously-installed Sun ONE Application Server 7 software on the
target machine, you must remove it using the uninstallation program before
starting installation.
• Available ports
? You’ll assign one for the Admin Server and another for the HTTP server
default instance during installation.
? The installation program will detect used ports and assign two others for
you: Sun ONE Message Queue (by default, 7676), and IIOP (by default,
3700). If either of these default port numbers are in use, the installation
program will assign the next available port (for example, 7677 or 7678, and
so on).
? Additional ports will be needed for the HADB servers. Refer to the HADB
configuration chapter in the Sun ONE Application Server Administrator’s
Guide for guidelines.
• Root privileges—You must have root privileges on your target machine.
NOTE Solaris 9 bundled installations or non-package-based evaluation
installations do not affect the Enterprise Edition installation
program, so they do not need to be removed from your system.
However, port conflicts must be resolved.
Installation Requirements
28 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Single installation—You can have only one installation per machine, however,
you can have multiple instances running within the same installation.
High-Availability Requirements
The following requirements are key to setting up your high-availability
environment:
• Topology Planning
• Space Considerations
• Web Server Installation
• Shared Memory Setup
• Remote Access Setup
Topology Planning
Before you install the Sun ONE Application Server 7, Enterprise Edition software,
you will need to decide on product topology, that is, which component will be
hosted on which available system. The Sun ONE Application Server and the HADB
server can generally be hosted in two ways:
• Application Server and HADB server node hosted on the same system
• Application Server and HADB server node hosted on separate systems
In both cases, at least two systems per component are needed to achieve high
availability.
The installation program enforces explicit component dependencies, but will not
otherwise limit combinations of product components that can be installed on a
particular machine. As a result, the number of possible product topologies is quite
large.
Details on the various topologies that can be implemented for the Always On
Technology are discussed in the Enterprise Edition of the Sun ONE Application
Server Operational Deployment Guide.
Installation Requirements
Chapter 1 Preparing to Install 29
Space Considerations
Data devices should not be filled beyond 50% of capacity because additional space
is needed to refragment the HADB. If refragmentation fails, it might be because
devices are too full and there is not enough space. If devices are running at 80% or
90% of capacity and refragmentation fails, the HADB will need to be cleared,
meaning that all data removed from the database and the session schema.
It is important to monitor the space on the devices using the hadbm deviceinfo
command. When device capacity exceeds 50%, additional nodes should be added.
Refer to the Sun ONE Application Server Administrator’s Guide and the Sun ONE
Performance Tuning Guide for information and instructions.
Web Server Installation
Before you start the installation process, your web server must be installed on any
machine where you are going to install the load balancer plug-in.
Currently-supported versions include the following:
• Sun ONE Web Server 6.0 SP6
• Apache Web Server 1.3.27
For installing the Sun ONE Web Server, refer to the iPlanet WebServer Installation
Guide at this location:
http://docs.sun.com/db/prod/s1websrv
Instructions for installing the plug-in are contained in “Installing the Load Balancer
Plug-in” on page 44.
Shared Memory Setup
You will need to configure shared memory on the HADB hosts before you can set
up the HADB. This can be done before or after installing the high-availability
components. Refer to “Configuring Shared Memory and Semaphores” on page 53
for instructions.
Remote Access Setup
Before you can set up the HADB, you will need to configure remote access on the
HADB hosts to enable the high-availability management client to communicate
among HADB nodes. This can be done before or after installing the
high-availability components. Refer to “Setting Up Host Communication” on
page 55 for instructions on configuring OpenSSH/SSH or RSH.
Accessing the Documentation
30 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Accessing the Documentation
The Sun ONE Application Server documentation is provided in a number of ways:
• Manuals—The Sun ONE Application Server 7, Enterprise Edition manuals and
release notes, in HTML and in printable PDF, are available on the
documentation CD-ROM that comes with the product.
• Online help—Click the Help button in the graphical interface to launch a
context-sensitive help window.
• Man pages—To view man pages at the command line, you must first add
install_dir/man to your MANPATH environment variable (Solaris) and add the
HADB /bin directory to PATH. After setting the variable, you can access man
pages for the Sun ONE Application Server commands by typing man
command_name on the command line. For example:
man asadmin
man hadbm
31
Chapter 2
Installing Enterprise Edition Software
This chapter provides instructions for installing the Sun ONE Application Server 7,
Enterprise Edition product. You can install this version of the product interactively
or you can use silent mode to replicate an installation scenario on multiple
machines. Refer to “Installation Roadmap” on page 16 to see the full sequence of
events for implementing the Sun ONE Application Server 7, Enterprise Edition
product.
The following topics are addressed here:
• About Installation
• Installing Application Server Software
• Installing the Load Balancer Plug-in
• Installing in Silent Mode (Non-Interactive)
You should be familiar with the information in “Preparing to Install” on page 15
before beginning the tasks in this chapter.
For any late-breaking updates to these instructions, check the Sun ONE Application
Server Release Notes. For more information about configuring your application
server after installation, refer to the Sun ONE Application Server Administrator’s
Guide.
The following location contains product downloads in addition to other useful
information:
http://www.sun.com/software/products/appsrvr/home_appsrvr.html
About Installation
32 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
About Installation
Only one Sun ONE Application Server 7 installation can reside on a single
machine. If an installation of Application Server 7 already exists on your system,
the installation program will not overwrite it, but the pre-existing installation will
be detected and you will not be allowed to proceed with the installation until you
have removed the existing Application Server 7 software using the uninstallation
program.
The following topics are addressed in this section:
• Installation Components
• Installation Options
Installation Components
In general, you are installing the basic components that provide the functionality of
the Sun ONE Application Server Version 7, Enterprise Edition product. You can
choose not to install some of the components. Later, if you want to add a
component that you initially chose not to install, you can do an incremental
installation of that component, providing dependencies are met.
Since only one installation of the same component package on the same system is
allowed, the installation program tries to detect components that are already
installed. If a component is already installed, installation of that component is
disabled.
NOTE Solaris 9 bundled installations and non-package-based evaluation
installations do not affect the Enterprise Edition installation
program, so they do not need to be removed from your system.
NOTE Using either of the interactive methods, you can do a partial
installation which can be followed by any number of incremental
(subsequent) installations. For silent mode, you can do a partial
initial installation, but any subsequent installations must be done
using an interactive method.
About Installation
Chapter 2 Installing Enterprise Edition Software 33
The installation program enforces component dependencies as specified for each
component. Once component dependencies are satisfied, component life cycles are
independent. A particular component can be installed or uninstalled dynamically
through incremental installation and partial uninstallation mechanisms without
corrupting other components.
The following installation components are included with the Sun ONE Application
Server 7, Enterprise Edition product:
• Sun ONE Application Server—all of Sun ONE Appserver 7, including its
graphical and command-line administrative tools, the asadmin command, and
Sun ONE Message Queue 3.0.1
• Sun ONE Application Server Administration Client—only the asadmin
command
• Java 2 Software Development Kit (J2SE), Standard Edition 1.4.0_03
• Sample applications (Optional)
• High-Availability Database (HADB)—all of HADB, including the hadbm
command
• HADB Management Client—only the hadbm command
• Load balancer plug-in for web servers
NOTE If you want to install the Application Server and an HADB server
node on the same system, select both components. Otherwise select
only one of them.
The clsetup command must be run from a machine where the
asadmin and the hadbm utilities are available. Instructions for using
the clsetup command to create a basic cluster can be found in
“Using the clsetup Command” on page 65.
NOTE You can choose to install the administration client command-line
version separately on a machine where the Application Server is not
installed. Do this by selecting only the Administration Client
component during incremental installation.
About Installation
34 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
The load balancer plug-in is usually installed in a separate process on a
separate machine. Refer to “Installing the Load Balancer Plug-in” on page 44
for guidelines.
See “Installation Components” on page 17 for further description of the Sun ONE
Application Server components.
Installation Options
There are three ways you can perform the installation:
• Graphical method (interactive)—The installation program prompts you using
a sequence of graphical screens. This is the default method.
• Command-line method (interactive)—The installation program prompts you
using a sequence of command-line prompts and messages.
• Silent mode—The installation program reads installation parameters from a
supplied configuration file and logs all output into a log file.
The setup command allows you to specify the method you want to use for
installation, and allows you to create a configuration file for silent installation.
Use the following syntax when running the setup command:
setup [-console] [-silent config_file] [-savestate]
Table 2-1 describes the setup command options.
NOTE The default installation mode is the graphical method, so if you
don’t specify an option when you run setup, the installation
program presents the graphical screens.
Table 2-1 Options for the setup Command
Option Description
-console Runs the installation using the command-line method.
About Installation
Chapter 2 Installing Enterprise Edition Software 35
Installation Syntax
• To run the installation using the graphical interface, type the following at the
command prompt (no options; this is the default method):
./setup
• To run the installation using the command-line interface, type:
./setup -console
• To run the installation using the graphical interface and create an installation
configuration file for silent mode installation:
./setup -savestate
The file called statefile will be created in install_dir.
• To run the installation using the command-line interface and create an
installation configuration file for silent mode installation:
./setup -console -savestate
The file called statefile will be created in install_dir.
• To run a silent mode installation based on an existing installation configuration
file:
./setup -silent config_file
Refer to “Installing in Silent Mode (Non-Interactive)” on page 48 for further
specifics on silent mode installation and the installation configuration file.
-silent config_file Runs the installation in silent mode. Installation parameters are read from
an existing installation configuration file. This option is mutually exclusive
with the savestate option.
The installation configuration file path must be explicitly provided; there
is no default file path. Refer to “Installing in Silent Mode
(Non-Interactive)” on page 48 for further specifics on silent mode
installation and the installation configuration file.
-savestate Runs the installation using either the graphical or command-line method
and creates an installation configuration file based on this installation. This
option is mutually exclusive with the silent option. If you do not specify
this option, no installation configuration file will be created.
The file will be called statefile and located in install_dir.
Table 2-1 Options for the setup Command (Continued)
Option Description
Installing Application Server Software
36 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• To display the available command-line arguments for the setup command:
./setup -help
or
./setup -h
Installing Application Server Software
This section provides instructions for installing the Sun ONE Application Server
software using either the graphical-interface or command-line interface. The steps
are identical for both methods. However, for the command-line interface,
text-based screens are displayed instead of graphical screens.
After you have planned the topology, run the installation program on each system,
selecting and installing the appropriate components based on your topology.
1. Uninstall any previous versions of the Sun ONE Application Server 7 software
on the machines where you are going to install the Enterprise Edition of the
Application Server.
2. Verify that all requirements that apply to your installation have been met. See
“Installation Requirements” on page 23 for information on requirements.
3. Log in as root and create a temporary directory for the product distribution
file.
TIP If you are familiar with high availability concepts and installation of
enterprise-level products, you may want to use the summary
checklists in Appendix A, “Installation Cheatsheet.”
NOTE If the previously-installed packages are bundled in the Solaris
operating environment, they need not be removed. However, port
conflicts must be resolved.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 37
4. Start all the processes on your system that use ports and are expected to run at
the same time as the Application Server software. This allows the installation
program to detect what ports are in use and avoid assigning them for other
purposes.
5. For a download, unzip the .gz file as follows:
gunzip sun-appserver7-sol.tar.gz
6. For a download, untar the unzipped file as follows:
tar -xvf sun-appserver7-sol.tar
This process may take a little time. When the files are unpacked, you will see
the sun-appserver7 directory, which contains the setup file and the pkg
directory.
7. Navigate to the sun-appserver7 directory.
8. Select your installation method.
Refer to “Installation Options” on page 34 for guidelines on selecting the
correct options to use with the setup command.
When the installation starts, the Welcome page of the installation program is
displayed.
9. Read the Welcome page and click Next.
The License Agreement page is displayed.
10. Read the License Agreement and click Yes to agree to the terms of the license
(or type Yes at the command line), then click Next.
NOTE If you are installing the load balancer plug-in, your web server
must already be installed on the machines where you are going
to install the load balancer plug-in before you start the
installation process. Refer to “High-Availability Requirements”
on page 28.
NOTE Click the Help button to display context-sensitive information for a
page.
Installing Application Server Software
38 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
After you accept the License Agreement, the Select Installation Directory page
is displayed.
11. Specify the path to your Sun ONE Application Server installation directory
(default is /opt/SUNWappserver7).
If you are installing only the HADB Server component, you can choose /opt as
the installation directory. This will install the HADB packages into their
default location, which is /opt/SUNWhadb.
? Click Browse to browse for a directory (or press Enter at the command line
to accept the default installation directory).
? If you enter a directory name that does not already exist, the Create New
Directory? dialog is displayed.
• Click Create Directory (or type 1 at the command line) to create a new
directory.
• Click Choose New (or type 2 at the command line) to return to the
Select Installation Directory page.
The Component Selection page displays the available components.
12. Choose from the components listed on the Component Selection page (or type
Yes or press Enter to accept a component from the command line).
NOTE You must accept the license agreement to continue with the
installation.
NOTE You must select identical installation directories on all systems
hosting HADB Server nodes.
NOTE When installing the Sun ONE Application Server together with
HADB, if you do not want to use the default installation folder,
you can create alternate directories, then create symlinks (ln -s)
to these directories from the /var/opt and /etc/opt directories.
The standards for packaging Solaris packages require that the
licenses and configuration files are located in the /var/opt and
/etc/opt directories.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 39
? Sun ONE Application Server, with graphical and command-line
interfaces (J2SE and Sun ONE Message Queue are installed along with this
component)
? (Optional) Sample Applications
? Sun ONE Application Server Administration Client (select only this
component to install standalone command-line)
? High-Availability Database
? High-Availability Database Administration Client
? Load Balancer Plug-in
Refer to “Installing the Load Balancer Plug-in” on page 44 for instructions
on installing this component separately.
13. Sun ONE Message Queue—If the installation program detects a version of the
Sun ONE Message Queue preinstalled in your system, you are presented with
one of the following actions:
? If the correct version of the package-based Sun ONE Message Queue is
installed, it will be reused. You can choose to exit at this point. If you don’t
exit, the installation program will use the installed version and proceed to
the next step.
NOTE If some components are disabled on the Component Selection
page (or if a command-line mode installation did not offer them
for installation), this means the disabled component has been
detected as already installed on your system.
NOTE If you want to install Sun ONE Application Server and an
HADB server node on the same system, select them both.
Otherwise, select only one of them.
NOTE If you do not already have your web server installed on the machine
where you are installing the load balancer plug-in, you cannot
continue to install the load-balancer plug-in.
Installing Application Server Software
40 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? If there is no package-based Sun ONE Message Queue installed, you can
choose to let the installation program install the Sun ONE Message Queue
packages automatically.
? If an incorrect version of the package-based Sun ONE Message Queue is
found, a message is displayed, asking if you want to upgrade your current
version or cancel. Choose one of the following:
• To have the installation program update your current Sun ONE
Message Queue, click Upgrade (or type 1 at the command line).
• To exit the installation program, click Cancel (or type 2 at the
command line).
14. For J2SE—The installation program looks in the /usr/j2se default location to
detect if you have the correct version of the J2SE preinstalled in your machine.
You are presented with one of the following actions (if you have any problems
in this step, refer to “J2SE Installation/Upgrade Issues” on page 94):
? If the correct version of the package-based J2SE is installed, it will be
reused or you can enter the path to another correct version. The installation
program proceeds to the next step.
? If there is no package-based J2SE installed, you can choose to let the
installation program install the J2SE package automatically or reuse an
existing J2SE installation.
? If an incorrect version of the package-based J2SE is found, a message is
displayed asking if you want to upgrade your current version or cancel.
Choose one of the following options:
• To have the installation program update your current J2SE version,
click Upgrade (or type 1 at the command line).
• To exit the installation program, click Cancel (or type 2 at the
command line).
Before continuing with the installation, you must uninstall the J2SE
currently located in /usr/j2se or upgrade it to J2SE 1.4.1_03. Then
restart the Application Server installation.
NOTE Because other applications might be running and using this J2SE
installation, upgrading J2SE is a potentially disruptive process. You
may prefer to cancel the current installation and take care of all
dependencies (such as gracefully shutting down processes).
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 41
15. Specify your product configuration directory.
Accept the default (/etc/opt/SUNWappserver7) or enter the path to your Sun
ONE Application Server product configuration directory.
? Click the ellipsis (...) to browse for a directory (or press Enter at the
command line to accept the default installation directory).
? If the directory does not already exist, the Create New Directory? dialog is
displayed.
? Click Create Directory (or type 1 at the command line). You can also click
Choose New (or type 2 at the command line) to select an existing directory.
16. Specify your server configuration directory.
Accept the default (/var/opt/SUNWappserver7) or enter the path to your Sun
ONE Application Server Version 7, Enterprise Edition domains installation
directory.
? Click the ellipsis (...) to browse for a directory (or press Enter at the
command line to accept the default installation directory).
? If the directory does not already exist, the Create New Directory? dialog is
displayed.
? Click Create Directory (or type 1 at the command line). You can also click
Choose New (or type 2 at the command line) to select an existing directory.
If you selected Application Server for installation, the Server Configuration
Information page is displayed. Skip to Step 18.
If you selected the load balancer plug-in, the Web Server Directory page is
displayed. Proceed to Step 17
17. If you selected the load balancer plug-in, identify your web server as follows:
NOTE If your J2SE requires an upgrade, you will need to reboot your
machine after completing the Application Server installation.
NOTE This Sun ONE Application Server 7 software is certified to work
with J2SE 1.4.1_03 from Sun Microsystems. Third-party J2SE
development kits, even with appropriate version number, are not
supported.
Installing Application Server Software
42 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? Choose which web server you are going to install (Sun ONE Web Server or
Apache Web Server).
? Enter the web server instance path.
Default values will be offered based on web server type. The installation
program checks to see if appropriate configuration files can be found at the
provided location.
Refer to “Installing the Load Balancer Plug-in” on page 44 if you are installing
the load balancer plug-in separately.
18. If you selected the Sun ONE Application Server component, enter the
following:
? Admin User—Name of the user who administers the server (for example,
admin).
? Admin User’s Password—Password to access the Admin Server.
Minimum number of characters is 8. For example, adminadmin. Re-enter
the password to confirm your choice.
? Admin Server Port—Port number to access the Admin Server.
A default port number is displayed (for example 4848, if that port is not in
use on your machine). Change the default number if necessary. The
installation program will check port numbers for validity and availability
when you click Next.
? HTTP Server Port—Port number to access the default server instance.
A default port number is displayed (for example 80, if that port is not in use on
your machine). Change the default number if necessary. The installation
program will check port numbers for validity and availability when you click
Next.
NOTE The installation program automatically detects ports in use and
suggests currently unused ports for the default settings. By default,
the initial default ports are 80 for the HTTP server and 4848 for the
Admin Server.
If these initial default ports are being actively used on your system,
the installation program will suggest alternative port numbers.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 43
19. Click Next.
The installation program proceeds to verify that you have enough disk space
based on the components you selected. The Checking Disk Space progress
indicator bar is displayed.
? If you do not have enough disk space, an error message is displayed.
In this case, you need to exit the installation program, create enough space,
and restart the installation. Information on space requirements is
contained in “Platform Requirements” on page 24.
? If you have enough disk space, the Ready to Install page is displayed.
20. On the Ready to Install page, you have the following choices:
? Click Back if you want to return to the previous page. Disk space is
rechecked if you do this.
? Click Install Now (or type 1 at the command line) to start the installation
process.
? Click Cancel to exit the installation program.
An Installation progress indicator bar is displayed.
When installation finishes, the Installation Summary page is displayed.
21. Check the installation outcome on the Installation Summary page. If
installation failure has occurred, review the following log file:
? /var/sadm/install/logs/Sun_ONE_Application_Server_install.log
Refer to “About Logs and Messages” on page 93 for additional information.
22. Click Finish (or type Finish at the command line) to complete the installation.
The installation components are now installed on your systems.
23. Start the server.
You can start the Sun ONE Application Server software by using the
instructions on “Starting and Stopping the Server” on page 81.
When the Admin Console has been started, the initial page of the Application
Server graphical interface is displayed.
Installing the Load Balancer Plug-in
44 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
24. If you have not already done so, add the HADB bin directory to the PATH
environment variable as described in “Setting Up the User Environment” on
page 63.
25. If you selected the HADB components, verify that you have successfully
installed the HADB software by doing the following on each host:
hadbm --help
The result of this command should be a list of all commands available using
the hadbm command-line utility.
You are now ready to configure your system for high availability. Proceed to
“Preparing for HADB Setup” on page 53 to begin this process.
Installing the Load Balancer Plug-in
This section provides instructions for installing the load balancer plug-in
component separately.
To install the load balancer plug-in component, perform these steps:
1. Check the system that will be hosting the web server and load balancer plug-in
to see if a previously-installed load balancer plug-in or reverse proxy plug-in is
present. If it is, you will need to remove it using the unistallation program.
As root, run the following command:
pkginfo SUNWaspx
On a clean system, the following message will be displayed:
ERROR: information for "SUNWaspx" was not found.
2. Verify that the correct web server is present on the machines were you are
going to install the load balancer plug-in. Currently supported versions
include the following:
? Sun ONE Web Server 6.0 SP6
NOTE The installation program creates an initial domain called domain1
with a single instance called server1. Refer to “Creating Domains
and Instances” on page 84 for instructions on creating additional
domains and instances.
Installing the Load Balancer Plug-in
Chapter 2 Installing Enterprise Edition Software 45
? Apache Web Server 1.3.27
3. Log in as root and create a temporary directory for the product distribution
file.
4. For a download, unzip the .gz file as follows:
gunzip sun-appserver7-sol.tar.gz
5. For a download, untar the unzipped file as follows:
tar -xvf sun-appserver7-sol.tar
This process may take a little time. When the files are unpacked, you will see
the sun-appserver7 directory, which contains the setup file and the pkg
directory.
6. Navigate to the sun-appserver7 installation directory.
7. Select your installation method.
Refer to “Installation Options” on page 34 for guidelines on selecting the
correct options to use with the setup command.
When the installation starts, the Welcome page of the installation program is
displayed.
8. Read the License Agreement and click Yes to agree to the terms of the license
(or type Yes at the command line), then click Next.
After you accept the License Agreement, the Select Installation Directory page
is displayed.
9. Specify the path to your Sun ONE Application Server installation directory
(default is /opt/SUNWappsrver7).
? Click Browse to browse for a directory (or press Enter at the command line
to accept the default installation directory).
NOTE Make a note of the web server installation directory. This
information will be needed during installation.
NOTE You must accept the license agreement to continue with the
installation.
Installing the Load Balancer Plug-in
46 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? If you enter a directory name that does not already exist, the Create New
Directory? dialog is displayed.
• Click Create Directory (or type 1 at the command line) to create a new
directory.
• Click Choose New (or type 2 at the command line) to return to the
Select Installation Directory page.
The Component Selection page displays the available components.
10. Choose the load balancer plug-in component on the Component Selection page
(or type Yes or press Enter to accept the component from the command line).
If you selected the load balancer plug-in component, the Web Server Directory
page displays.
11. Identify your web server.
? Choose the web server you have installed (Sun ONE Web Server or
Apache Web Server).
? Enter the web server instance path.
Default values will be offered based on server type. The installation
program checks to see if appropriate configuration files can be found at the
specified location.
12. Click Next.
The installation program proceeds to verify that you have enough disk space
based on the components you selected. The Checking Disk Space progress
indicator bar is displayed.
? If you do not have enough disk space, an error message is displayed.
In this case, you need to exit the installation program, create enough space,
and restart the installation. Information on space requirements is
contained in “Platform Requirements” on page 24.
? If you have enough disk space, the Ready to Install page is displayed.
NOTE If some components are disabled on the Component Selection page
(or if a command-line mode installation did not offer them for
installation), this means that disabled component has been detected
as already installed on your system.
Installing the Load Balancer Plug-in
Chapter 2 Installing Enterprise Edition Software 47
13. On the Ready to Install page, you have the following choices:
? Click Back if you want to return to the previous page. Disk space is
rechecked if you do this.
? Click Install Now (or type 1 at the command line) to start the installation
process.
? Click Cancel to exit the installation program.
An Installation progress indicator bar is displayed.
When installation finishes, the Installation Summary page is displayed.
14. Check installation outcome on the Installation Summary page. If installation
failure has occurred, review the following log file:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
Refer to “About Logs and Messages” on page 93 for additional information.
15. Click Finish (or type Finish at the command line) to complete the installation.
16. Edit the supplied loadbalancer.xml.example file to include references to
actual application server instances. This file is located in the following location:
For Sun ONE Web Server:
webserver_instance_dir/config/loadbalancer.xml.example
For Apache Web Server:
webserver_instance_dir/conf/loadbalancer.xml.example
17. After you have made your modifications, save the
loadbalancer.xml.example file as loadbalancer.xml in the same directory.
NOTE If you want to configure more than one web server instance, or want
to add additional instances at a later time, you will need to manually
configure them. Instructions for doing this are contained in the
Configuring Load Balancer Plug-in section in the Sun ONE
Application Server Administrator’s Guide.
Refer to the Apache documentation for information on the Apache
Web Server.
Installing in Silent Mode (Non-Interactive)
48 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Installing in Silent Mode (Non-Interactive)
If you choose to install the Sun ONE Application Server Version 7, Enterprise
Edition software in silent mode, the installation program runs without any user
input. This is made possible when the installation program accesses a text file that
provides the installation program with the configuration information it needs.
The following topics are discussed in this section:
• Creating the Installation Configuration File
• Installing in Silent Mode
Creating the Installation Configuration File
The installation configuration file is created when you use the savestate option
with the setup command to start a interactive installation. During the interactive
installation, your input is collected and stored in the configuration file you
specified. This forms the template for silent installation, which you can use later to
install the product on one or more machines.
If needed, you can modify the installation configuration file.
The following topics are addressed in this section:
• Syntax for Creating the Installation Configuration File
• Example Installation Configuration File
• Modifying the Installation Configuration File
Syntax for Creating the Installation Configuration File
The syntax for creating an installation configuration file is as follows:
For graphical method:
./setup -savestate
For command-line method:
./setup -console -savestate
Refer to “Installation Options” on page 34 for more detailed information.
Example Installation Configuration File
An installation configuration file looks similar to the following:
Installing in Silent Mode (Non-Interactive)
Chapter 2 Installing Enterprise Edition Software 49
# Wizard Statefile created: Mon Jan 27 16:25:26 PST 2003
# Wizard path: /tmp/herc/sun-appserver7/./appserv.class
# Install Wizard Statefile section for Sun ONE Application Server
#
[STATE_BEGIN Sun ONE Application Server 108a4222b3a6a8ed98832d45238c7e8bb16c67a5]
defaultInstallDirectory = /opt/SUNWappserver7
currentInstallDirectory = /opt/SUNWappserver7
SELECTED_COMPONENTS = Java 2 SDK, Standard Edition 1.4.1_03#Application
Server#Sun ONE Message Queue 3.0.1#Sample Applications#Load Balancing
Plugin#Uninstall#Startup
USE_BUNDLED_JDK = FALSE
JDK_LOCATION = /usr/j2se
JDK_INSTALLTYPE = PREINSTALLED
AS_INSTALL_DEFAULT_CONFIG_DIR = /etc/opt/SUNWappserver7
AS_INSTALL_CONFIG_DIR = /etc/opt/SUNWappserver7
AS_INSTALL_DEFAULT_VAR_DIR = /var/opt/SUNWappserver7
AS_INSTALL_VAR_DIR = /var/opt/SUNWappserver7
DOMAINS_DIR = /var/opt/SUNWappserver7/domains
WEBSERVER_INSTALL_DEFAULT_DIR = /usr/iplanet/servers
WEBSERVER_INSTALL_DIR = /opt/iplanet/servers/https-tesla.red.iplanet.com
INST_ASADMIN_USERNAME = admin
INST_ASADMIN_PASSWORD = adminadmin
INST_ASADMIN_PORT = 4848
INST_ASWEB_PORT = 81
INSTALL_STATUS = SUCCESS
[STATE_DONE Sun ONE Application Server 108a4222b3a6a8ed98832d45238c7e8bb16c67a5]
Modifying the Installation Configuration File
You can modify the installation configuration file by editing the variables and
values described in Table 2-2.
Table 2-2 Installation Configuration File Variables
Variable Name Valid values (if
applicable)
Content Comments
defaultInstallDirectory Default installation
directory path
Value not actively used by
installation program.
currentInstallDirectory Selected installation
directory path
Installing in Silent Mode (Non-Interactive)
50 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
SELECTED_COMPONEN
TS
List of product
components
selected for
installation
Pound (#') character is used as
list delimiter.
USE_BUNDLED_JDK TRUE
FALSE
Whether to install
J2SE bundled with
the product
JDK_LOCATION J2SE path Preinstalled J2SE path if
USE_BUNDLED_J2SE is set to
false; otherwise installation
location for bundled J2SE.
JDK_INSTALLTYPE PREINSTALLED
CANNOTUPGRADE
UPGRADABLE
CLEANINSTALL
How to handle
existing J2SE
installation
Only PREINSTALLED and
CLEANINSTALL are valid
values for silent installation
configuration file.
AS_INSTALL_DEFAULT_
CONFIG_DIR
Default
configuration files
directory path
Value not actively used by
installation program.
AS_INSTALL_CONFIG_DI
R
Selected
configuration file
directory path
AS_INSTALL_DEFAULT_
VAR_DIR
Default domains
configuration files
directory path
Value not actively used by
installation program.
AS_INSTALL_VAR_DIR Selected domains
configuration file
directory path
DOMAINS_DIR Selected domains
configuration file
directory path, plus
domains
subdirectory
AS_INSTALL_VAR_DIR and
DOMAINS_DIR are generally
redundant. However, both
entries are needed by legacy
installation program code.
WEBSERVER_INSTALL_D
EFAULT_DIR
Default web server
instance directory
path
Value not actively used by
installation program.
WEBSERVER_INSTALL_D
IR
Selected web server
instance directory
path
Table 2-2 Installation Configuration File Variables (Continued)
Variable Name Valid values (if
applicable)
Content Comments
Installing in Silent Mode (Non-Interactive)
Chapter 2 Installing Enterprise Edition Software 51
Installing in Silent Mode
To install the Sun ONE Application Server software in non-interactive silent mode,
perform these steps:
1. With a text editor, examine the current installation configuration file and verify
that it contains what you want to use for your silent installation.
2. Save your config_file with any name. For example:
cp statefile my_silent_config
3. Copy your installation configuration file to each machine where you plan to
install the Sun ONE Application Server Version 7, Enterprise Edition software.
4. Copy the Sun ONE Application Server installation files to each machine where
you plan to install the Application Server software.
INST_ASADMIN_USERN
AME
Administrator
username for initial
domain
INST_ASADMIN_PASSW
ORD
Administrator
password for initial
domain
INST_ASADMIN_PORT 0 - 65535 Administration
server port number
for initial domain
INST_ASWEB_PORT 0 - 65535 Server port number
for initial server
instance
INSTALL_STATUS SUCCESS
FAILURE
Installation
outcome
Mandated by installer
implementation. Value not
actively used by installation
program.
NOTE For silent mode, you can do a partial initial installation, but any
incremental (subsequent) installations must be done using an
interactive method.
Table 2-2 Installation Configuration File Variables (Continued)
Variable Name Valid values (if
applicable)
Content Comments
Installing in Silent Mode (Non-Interactive)
52 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
5. If you are not in the directory already, navigate to the directory where you
copied the installation files and your installation configuration file.
6. As superuser, start silent installation at the command line using the following
command format:
./setup -silent config_file
The installation program reads the specified config_file, checks for adequate disk
space, then installs the product based on the data in config_file.
When the prompt is returned, the silent installation is complete and the
installation components are now installed on your systems.
7. You can start the Application Server software by using the instructions on
“Starting and Stopping the Server” on page 81.
When the Admin Console has been started, the initial page of the Application
Server graphical interface is displayed.
You are now ready to configure your system for high availability. Proceed to
“Preparing for HADB Setup” on page 53 to begin this process.
53
Chapter 3
Preparing for HADB Setup
After the high-availability components have been installed on the servers that will
be part of an cluster, perform the tasks in this chapter to prepare for setting up high
availability. Refer to “Installation Roadmap” on page 16 to see the full sequence of
events for implementing the Sun ONE Application Server 7, Enterprise Edition
product.
The following topics are addressed here:
• Configuring Shared Memory and Semaphores
• Setting Up Host Communication
• Setting Up the User Environment
• Setting Up Administration for Non-Root
• Using the clsetup Command
After you have done the tasks here, proceed to the Sun ONE Application Server
Administrator’s Guide for comprehensive instructions on configuring and
managing the cluster, the load balancer plug-in, and the high-availability database
(HADB).
Information on high-availability topologies is available in the Sun ONE Application
Server System Deployment Guide.
Configuring Shared Memory and Semaphores
You will need to configure shared memory for the HADB host machines before
beginning to work with the HADB.
1. Log in as root.
Configuring Shared Memory and Semaphores
54 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Add lines similar to the following to the /etc/system file if they do not
already exist:
For shared memory:
set shmsys:shminfo_shmmax=0x80000000
set shmsys:shminfo_shmseg=20
This example sets maximum shared memory shmmax to 2GB (hexadecimal
0x80000000) which is sufficient for most configurations.
The shmsys:shminfo_shmmax setting is calculated as 10,000,0000 per 256 MB
and should set to be identical to the memory size for the host. To determine
your host’s memory, run this command:
prtconf | grep Memory
Then plug the value into the following formula:
((<host> MB / 256 MB) * 10,000,000)
For semaphores:
Your /etc/system file may already contain semmni, semmns, and semmnu
entries. For example:
set semsys:seminfo_semmni=10
set semsys:seminfo_semmns=60
set semsys:seminfo_semmnu=30
NOTE If necessary, make sure permissions are set correctly to
administer the HADB as non-root user. See “Setting up RSH for
HADB Administration” on page 56, Step 5.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 55
If the entries are present, increment the values by adding 16, 128, and 1000
respectively, as follows:
set semsys:seminfo_semmni=26
set semsys:seminfo_semmns=188
set semsys:seminfo_semmnu=1030
If your /etc/system file does not contain the above mentioned entries, add the
following entries at the end of the file:
set semsys:seminfo_semmni=16
set semsys:seminfo_semmns=128
set semsys:seminfo_semmnu=1000
This is sufficient to run up to 16 HADB nodes on the computer.
3. Reboot the machine for changes to take effect.
For an explanation of HADB nodes, see Configuring the High Availability
Database in the Sun ONE Application Server Administrator’s Guide.
Setting Up Host Communication
To implement remote access for HADB administration, all machines that will be
used for running HADB servers and the HADB management client must be
configured for Remote Shell (RSH) or Secured Shell (OpenSSH/SSH).
RSH is a simple remote shell command and does not have any security features.
The SSH communication channel provides a level of security by encrypting the
data that passes between the HADB nodes.
NOTE Your original /etc/system file may or may not contain all of these
entries.
Setting Up Host Communication
56 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
This section contains instructions for the following tasks:
• Setting up RSH for HADB Administration
• Setting Up SSH for HADB Administration
Setting up RSH for HADB Administration
If you want to use RSH instead of SSH, you must explicitly specify RSH using the
set managementProtocol option. Refer to Table 3-3 on page 72 for guidelines on
setting this parameter in the clresource.conf file.
To implement RSH, perform these steps:
1. Log in as root.
2. Edit the /etc/hosts file to contain entries for all the selected HADB hosts,
including the host name of the local host. Use localhost format. For example:
computer1.xbay.company.com
computer99.zmtn.company.com
3. Append this file to the /etc/hosts file of all selected installation hosts.
NOTE For Solaris 9, it is recommended that you use the default installation
of SSH. However, you can use RSH if preferred by following the
instructions in “Setting up RSH for HADB Administration” on
page 56 and then editing the clresource.conf file to specific RSH as
described in “Running the clsetup Command” on page 77.
On Solaris 8, by default SSH is not installed. Follow the instructions
in “Installing SSH for Solaris 8” on page 59 if SSH is not on your
Solaris 8 system.
If you want to use SSH, but it is not configured or not available, you
will not be able to use the hadbm command. Refer to “SSH
Requirements and Limitations” on page 58 to verify that SSH is
recognized.
NOTE SSH is the strongly recommended default for the hadbm create
command because SSH is more secure than RSH.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 57
4. Create a .rhosts file in the $HOME directory of the HADB user, if one does
not already exist.
vi .rhosts
5. Verify that permissions are set to Read Only for group and other. For example:
rw-r--r--
6. Add the host name of each HADB host, including the name of your local host,
followed by the name of your database user. For example, if the database user
is Jon:
computer1.xbay.company.com Jon
computer99.zmtn.company.com Jon
mine456.red.mycompany.com Jon
7. Append this file to the .rhosts file of each HADB host.
8. Check host communication for each host. For example:
rsh computer99.zmtn.company.com uname -a
If all is well, the identity will be returned from the other host.
Setting Up SSH for HADB Administration
SSH is strongly recommended for using the hadbm create command because SSH
is more secure than RSH.
This section contains the following sections:
• SSH Requirements and Limitations
• Installing SSH for Solaris 8
• Configuring SSH
NOTE From a security perspective, the DSA-based version 2 protocol is
recommended instead of the RSA-based version 1 protocol. The
version you select depends on the SSH client software in use at your
site.
Setting Up Host Communication
58 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
SSH Requirements and Limitations
You may need to take action on any or all of the following requirements during
your SSH setup:
• Location of the SSH binaries—The high-availability management client expects
to find the ssh and scp binaries in the following location on each HADB host:
/usr/bin
? If the binaries are on your system but this location is not correct, you will
need to make a symbolic link from /usr/bin to the correct location.
? If you are on a Solaris 8 system, the SSH binaries are not installed by
default and so may not be present. If this is the case, follow the instructions
in “Installing SSH for Solaris 8” on page 59.
• Support—The only tested support is for SunSSH and OpenSSH. If you are
using another version of SSH, it is best to refer to the setup instructions in that
product’s documentation to ensure that your SSH communications work
correctly.
• OpenSSH clients and daemons—If you are running in an environment with
OpenSSH clients and daemons, you should name the key file as follows:
~/.ssh/authorized_keys2 or ~/.ssh/authorized_keys.
• Running as root—If you are running the HADB admin clients as root, make
sure that the sshd configuration (/etc/ssh/sshd_config) on all machines has
the PermitRootLogin parameter set to yes.
NOTE Although SSH is installed by default on Solaris 9 systems, on Solaris
8, by default SSH is not installed. Instructions for installing SSH for
Solaris 8 are contained in “Installing SSH for Solaris 8” on page 59.
NOTE By default, Sun SSH does not permit root login; it is set to no. If the
sshd configuration is changed, sshd must be restarted. Type the
following to restart the service:
/etc/init.d/sshd stop/start
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 59
• No SSH protocol version 2 support—If your SSH clients and daemons do not
support SSH protocol version 2, you will need to run ssh-keygen without
options. The key file will then be named identity.pub instead of id_dsa.pub.
This file must be appended to ~/.ssh/authorized_keys.
• Mixed SSH environment—If you are operating in a mixed SSH environment,
you will need to create both files ~/.ssh/authorized_keys2 and
~/.ssh/authorized_keys; the latter may contain both version 1 and version 2
keys.
• Co-location—If the Sun ONE Application Server and the HADB are co-located
on the same machine, you will need to create a known_hosts file under the
.ssh directory by running one of the following commands:
ssh localhost
or
ssh hostname
Installing SSH for Solaris 8
The ssh and scp binaries are not installed by default on Solaris 8 systems. If the
binaries are not on your Solaris 8 system, perform these steps:
1. Go to the following site:
http://www.sunfreeware.com/openssh8.html
On this site, you may receive a message similar to the following:
===PLEASE NOTE!!!............ make a note of some of the mirror
sites so that if the servers are down, you can still download
from a mirror site.
If you receive such a message, try one of the many mirror sites listed in the
FTP/Mirror Sites link. For example:
http://sunfreeware.secsup.org/
2. On this site, follow the instructions in the Installation Steps to download and
install all the necessary OpenSSH packages and patches.
3. After you have installed OpenSSH, proceed to the next section on Configuring
SSH.
Configuring SSH
To set up SSH on a system where the ssh and scp binaries are already installed,
perform the steps in one of the following sections:
Setting Up Host Communication
60 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• SSH for Non-Mounted Home Directories
• SSH for Mounted Home Directories
SSH for Non-Mounted Home Directories
To implement SSH in systems with home directories that are not mounted,
perform these steps:
1. Verify that SSH requirements have been understood and met as specified in
“SSH Requirements and Limitations” on page 58.
2. Log in to the host as the HADB user.
3. Generate your keys by running the following:
ssh-keygen -t dsa
For SSH1 and OpenSSH/1, you normally do not need to give any parameters
to the ssh-keygen command.
4. For the next three prompts, accept the default options by pressing Enter.
5. Repeat steps 1, 2, and 3 for all machines in your cluster.
A file called identity.pub or id_dsa.pub (depends on whether you are using
SSH version 1 or version 2) located in your ~/.ssh directory holds the public key.
To connect to a machine without being asked for a password, the content of this file
must be appended to a file called authorized_keys on all the machines.
6. To set up login identity, go to your user directory:
~/.ssh.
For SSH1, OpenSSH/1:
a. Copy the identity.pub file and name it authorized_keys.
b. For each of the other machines in the cluster, copy the content of the
identity.pub file and append it to the local authorized_keys file.
OpenSSH/2:
a. Copy the id_dsa.pub file and name it authorized_keys2.
b. For each of the other machines in the cluster, copy the content of the
id_dsa.pub file and append it to the local authorized_keys2 file.
7. Copy the authorized_keys file to the ~/.ssh directory on all the HADB
machines.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 61
8. Verify that the .ssh directory, HADB user’s home directory, and the
.ssh/authorized_keys file do not have write permissions for group and
other.
If needed, disable these group/other write permissions as follows:
chmod og-w ~/.ssh
chmod og-w ~/.ssh/authorized_keys
chmod og-w $HOME
Replace $HOME with the home directory of the HADB user. For example:
chmod og-w ~/johnsmith
9. To enable login without any user input, at initial SSH usage (after the SSH
environment is set up) you need to add the node machine name to the
known_hosts file under the /.ssh directory as follows:
a. Type the following:
ssh machine_name
You will be prompted with a Yes/No question whether to add
machine_name to the known_hosts file.
b. Answer Yes.
You will now be able to log in without any input.
10. To verify that SSH is set up correctly, SSH to each host in the cluster before
trying to run the management tool for HADB.
You are automatically logged in without a password requirement.
SSH for Mounted Home Directories
To implement SSH in systems with mounted home directories, perform these
steps:
1. Verify that SSH requirements have been understood and met as specified in
“SSH Requirements and Limitations” on page 58
NOTE If the files under the ~/.ssh directory have even read permission
given to group/other, you cannot set up an automatic SSH login
identity. In this case, if you try ssh machine_name, the system
complains about the incorrect permissions and asks for a password.
In other words, it is best not to give any permissions at all for
group/other if you want to enable automatic login.
Setting Up Host Communication
62 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Log in to host as the HADB user.
3. Generate your keys by running the following:
ssh-keygen -t dsa
For SSH1 and OpenSSH/1, you normally do not need to give any parameters
to the ssh-keygen command.
4. For the next three prompts, accept the default options by pressing Enter.
A file called identity.pub or id_dsa.pub (depends on whether you are using
SSH version 1 or version 2) located in your ~/.ssh directory holds the public
key. To connect to a machine without being asked for a password, the content
of this file must be appended to a file called authorized_keys2 on all the
machines. This can be done as follows:
5. To set up login identity, go to your user directory:
~/.ssh.
For SSH1, OpenSSH/1—Copy the identity.pub file and name it
authorized_keys.
For OpenSSH/2—Copy the id_dsa.pub file and name it authorized_keys.
6. Verify that the .ssh directory and the .ssh/authorized_keys file do not have
write permissions for group and other.
If necessary, disable these group/other write permissions as follows:
chmod og-w ~/.ssh
chmod og-w ~/.ssh/authorized_keys
chmod og-w /$HOME
Replace HOME with the home directory of the HADB user. For example:
chmod og-w ~/johnsmith.
NOTE If the files under the ~/.ssh directory have even read permission
given to group/other, you cannot set up an automatic SSH login
identity. In this case, if you try to run ssh machine_name, the system
complains about =incorrect permissions and asks for a password. In
other words, it is best not to give any permissions for group/other if
you want to enable automatic login.
Setting Up the User Environment
Chapter 3 Preparing for HADB Setup 63
7. To enable login without any user input, at initial SSH usage (after the SSH
environment is set up) you need to add the node machine name to the
known_hosts file under the /.ssh directory
a. Type the following:
ssh machine_name
You will be queried about whether or not to add machine_name to the
known_hosts file.
b. Answer Yes.
You will now be able to log in without any input.
8. To verify that SSH is set up correctly, SSH to each host in the cluster before
trying to run the management tool for HADB.
You are automatically logged in without a password requirement.
Setting Up the User Environment
After you have set up host communication, you can run the hadbm command from
the install_dir/SUNWhadb/4/bin directory location as follows:
./hadbm
However, it is much more convenient to set up your local environment to use the
high-availability management client commands from anywhere. To set this up,
perform the following steps.
1. Set the PATH variable as follows.
setenv PATH ${PATH}:install_dir/bin:install_dir/SUNWhadb/4/bin
2. Verify that the PATH settings are correct by running the following commands:
which asadmin
which hadbm
NOTE The examples in this section apply to using csh. If you are using
another shell, refer to the man page for your shell for instructions on
setting variables.
Setting Up Administration for Non-Root
64 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
3. If multiple Java versions are installed on your system, you must ensure that the
JAVA_HOME environment variable points to the correct Java version (1.4.1_03
for Enterprise Edition).
setenv JAVA_HOME java_install_dir
setenv PATH ${PATH}:${JAVA_HOME}/bin
Setting Up Administration for Non-Root
By default, during the initial installation or setup of the Sun ONE Application
Server, write permissions of the files and paths created for Sun ONE Application
Server are given to root only. For a user other than root to create or manage the Sun
ONE Application Server, write permissions on the associated files must be given to
that specific user, or to a group to which the user belongs. The files that are affected
are the following (with their default locations):
• Sun ONE Application Server configuration files—install_config_dir/cl*.conf
• Sun ONE Application Servers setup and administration scripts—
install_dir/bin/cl*
• HADB binaries—install_dir/SUNWhadb
• HADB configuration—/etc/opt/SUNWhadb
You can create a user group for managing the Sun ONE Application Server as
described in the following procedure. (An alternate approach is to set permissions
and ownership for the specific user.)
To create a Sun ONE Application Server user group and set permissions on the
installation root directory, repeat the following process for each affected file:
1. Log in as root.
2. From the command prompt, create the Sun ONE Application Server user
group. For example:
# groupadd s1asuser
You can type groupadd at the command line to see appropriate usage.
3. Change the group ownership for each affected file to the newly-created group.
For example:
chgrp -R s1asuser install_config_dir/cl*.conf
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 65
4. Set the write permission for the newly-created group:
chmod -R g+rw install_config_dir/cl*.conf
5. Repeat steps 3 and 4 for each affected file.
6. Make the clsetup and cladmin commands executable by the newly-created
group. For example:
chmod -R g+x install_dir/bin/cl*
7. Delete and recreate the default domain, domain1, using the --sysuser option.
The sysuser must also belong to the newly-created group. For example:
asadmin delete-domain domain1
asadmin create-domain --sysuser bleonard --adminport 4848
--adminuser admin --adminpassword password domain1
Using the clsetup Command
The purpose of the clsetup command is to automate the process of setting up a
basic cluster in a typical configuration. The clsetup command is located in
install_dir/bin, where install_dir is the directory where the Sun ONE Application
Server software is installed.
The clsetup command is bundled with the Sun ONE Application Server software
along with the cladmin command.
The following topics are addressed in this section:
• How the clsetup Command Works
• clsetup Requirements and Limitations
• Editing the clsetup Input Files
NOTE The cladmin command is used to streamline the process of
configuring and administering the cluster after all installation and
configuration tasks are complete, and is not documented here.
When you have completed the tasks in this Installation Guide, refer
to the Sun ONE Application Server Administrator’s Guide for
instructions on creating the HADB and on using on the cladmin
command.
Using the clsetup Command
66 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Running the clsetup Command
• Cleanup Procedures for the clsetup Command
How the clsetup Command Works
The clsetup command is a set of Sun ONE Application Server commands that are
gathered together in a script that allows a cluster to be configured automatically,
based on prepopulated input files. As part of cluster setup, an HADB is created,
but you will still need to set up your working cluster using the hadbm commands
as described in the Sun ONE Application Server Administrator’s Guide.
The following topics are addressed in this section:
• How the Input Files Work
• What the clsetup Command Accomplishes
• Commands Used by the clsetup Command
How the Input Files Work
Three input files are used by the clsetup command to configure the cluster:
• clinstance.conf—This file is pre-populated with information about
application server instances server1 and server2. Refer to “The clinstance.conf
File” on page 70 for information on the contents of this file.
• clpassword.conf—This file is pre-populated with the Admin Server
password for domain1, which you provided when you installed the Sun ONE
Application Server 7, Enterprise Edition software. Refer to “The
clpassword.conf File” on page 71 for information on the contents of this file.
• clresource.conf—This file is pre-populated with information about the
cluster resources: HADB, JDBC connection pool, JDBC resource, and session
store and persistence. Refer to “The clresource.conf File” on page 72 for
information on the contents of this file.
NOTE The clsetup command interface is unstable. An unstable interface
may be experimental or transitional, and may therefore change
incompatibly, be removed, or be replaced by a more stable interface
in the next release.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 67
You can use the clsetup configuration parameters as they are preconfigured to set
up a typical cluster configuration. To support a different configuration, you can
make edits to any or all of the configuration files.
What the clsetup Command Accomplishes
Using the pre-populated values in the clsetup input files, the clsetup command
accomplishes the following:
• Creates a new server instance named server2 in the default domain named
domain1. The HTTP port number for server2 is the next sequential number
after the HTTP port number specified for server1 during installation (for
example, if port number 80 is provided for server1 during installation, the port
number for server2 is 81).
• Creates the HADB named hadb with two nodes on the local machine. The port
base is 15200, and the database password is password.
• Creates the HADB tables required to store session information in the HADB.
• Creates a connection pool named appservCPL in all the instances listed in the
clinstance.conf file (server1, server2).
• Creates a JDBC resource named jdbc/hastore in all the instances listed in the
clinstance.conf file (server1, server2).
• Configures the session persistence information in all the instances listed in the
clinstance.conf file (server1, server2).
• Enables high availability in all the instances listed in the clinstance.conf file
(server1, server2).
NOTE The configuration parameters required to set up the cluster are
always read from the input files, and cannot be supplied through
the command line.
NOTE Because the clresource.conf and clpassword.conf input files
store passwords, they are access-protected with 0600 permissions.
Using the clsetup Command
68 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Commands Used by the clsetup Command
The clsetup command uses a number of hadbm and asadmin commands to
perform the steps for setting up the cluster. In Table 3-1, the clsetup task is
described in the left column and the command used to accomplish the task is listed
in the right column.
clsetup Requirements and Limitations
The following requirements and limitations apply to the clsetup command:
• The install paths, device paths, configuration paths, and so on must be the
same on all machines that are of the cluster.
• Before you can use the clsetup command, the asadmin and hadbm commands
must be available on the local machine. Therefore, this command can only be
run on a machine where the following are installed:
? The Sun ONE Application Server component or the Sun ONE Application
Server Administration Client component
? The HADB component or the HADB Management Client component
• Before you can use the clsetup command, you must have configured shared
memory as described in “Configuring Shared Memory and Semaphores” on
page 53. The clsetup command does not set any shared memory values.
Table 3-1 hadbm and asadmin Commands Used by the clsetup Command
Task Performed by clsetup Command
Checks to see if database exists. hadbm status
Creates and starts the HADB. hadbm create
Gets the JDBC URL. hadbm get jdbcURL
Creates the session store. asadmin create-session-store
Checks the instance status. asadmin show-instance-status
Creates the instance. asadmin create-instance
Creates the JDBC connection pool. asadmin create-jdbc-connection-pool
Registers the data source. asadmin create-jdbc-resource
Configures the persistence type asadmin configure-session-persistence
Reconfigures the instance. asadmin reconfig -u admin
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 69
• Before you can use the clsetup command, you must have set up the HADB
cluster host communication for SSH or RSH as described in “Setting Up Host
Communication” on page 55.
• If you are using RSH (which is not the default), you will need to uncomment
the following line in the clresource.conf file (remove the # sign):
#set managementProtocol
• If you are co-locating the Application Server and the HADB on the same
machine using SSH, a known_hosts file must exist under the .ssh directory. If
it does not, run either the ssh localhostor the ssh hostname command before using
the clsetup command.
• Before running the clsetup command, you must start the Admin Servers of all
the Sun ONE Application Server instances that are part of the cluster.
• The administrator password must be the same for all domains that are part of
the cluster.
• If the entities to be handled (HADB nodes and Application Server instances)
already exist, the clsetup command does not delete or reconfigure them, and
the respective configuration steps are skipped.
• The values specified in the input files will be the same for all the instances in a
cluster. The clsetup command is not designed to set up instances with
different values. For example, this command cannot create a JDBC connection
pool with different settings for each instance.
• The clsetup command does not perform any inetd configuration; the HADB
is created with no inetd settings. Instructions for performing inetd
configuration are contained in the Sun ONE Application Server Administrator’s
Guide.
• Host names in the shell initialization files—If prompts are included with host
names in your .cshrc or .login files, the clsetup command may appear to
hang. You will need to remove any prompts and excess output in any remote
command invocations. For example, running the hostname command on hostB
should print hostB without a prompt.
• To run the clsetup command as a user other than root, you'll need to make the
changes as described in “Setting Up Administration for Non-Root” on page 64.
Using the clsetup Command
70 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Editing the clsetup Input Files
The input files that are needed for the clsetup command are installed under the
configuration installation directory, default /etc/opt/SUNWappserver7, as part of
the installation procedure. The installation program pre-populates these files with
the values to set up a typical configuration, but you can edit any or all of them as
needed using a text editor.
This section addresses the following topics:
• The clinstance.conf File
• The clpassword.conf File
• The clresource.conf File
The clinstance.conf File
For the clsetup command to work properly, all application server instances that
are part of a cluster must be defined in the clinstance.conf file. During
installation, the installation program creates a clinstance.conf file with entries
for two instances. If you add more instances to the cluster, you must add
information about these additional instances.
The format of the clinstance.conf file is as follows:
# Comment
instancename instance_name
user user_name
host localhost
port admin_port_number
domain domain_n
instanceport instance_port_number
One set of entries is required for each instance that is part of the cluster. Any line
that starts with a hash mark (#) is treated as a comment.
NOTE The order in which these entries appear in the clinstance.conf file
is important and must not be changed from the order specified here.
If you add information about more application server instances,
entries for these instances must appear in this order.
Comments can be added anywhere in the file.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 71
Table 3-2 provides information about the entries in the clinstance.conf file. The
left column contains the parameter name, the middle column defines the
parameter, and the right column contains the default value specified by the
installation program.
Example clinstance.conf File
This clinstance.conf file contains information about two instances.
#Instance 1
instancename server1
user admin
host localhost
port 4848
domain domain1
instanceport 80
#Instance 2
instancename server2
user admin
host localhost
port 4848
domain domain1
instanceport 81
The clpassword.conf File
When the clsetup command is run, the asadmin command needs the Admin
Server password, which is specified in the clpassword.conf file during
installation.
The format of the clpassword.conf file is as follows:
Table 3-2 Entries in the clinstance.conf File
Parameter Definition Default Value
instancename Application Server instance name server1, server2
user Admin Server user name admin
host Host name localhost
port Admin Server port number 4848
domain Administrative domain name domain1
instanceport Application Server instance port 80, 81
Using the clsetup Command
72 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
AS_ADMIN_PASSWORD= password
where password is the Admin Server password.
Permissions 0600 are preset on the clpassword.conf file, which can only be
accessed by the root user.
The clresource.conf File
During installation, the installation program creates the clresource.conf file to
set up a typical configuration. The clresource.conf file contains information
about the following resources that are part of the cluster:
• HADB information
• Session store information
• JDBC connection pool information
• JDBC resource information
• Session persistence information
Permissions 0600 are preset on the clresource.conf file, which can only be
accessed by the root user.
The parameters of the clresource.conf file are described in the following tables.
The left column contains the parameter name, the middle column defines the
parameter, and the right column contains the default value specified by the
installation program.
Table 3-3 describes the HADB parameters in the clresource.conf file.
NOTE Before running the clsetup command, the values specified in the
clresource.conf file can be modified for optimization, or for
setting up a different configuration. If you edit the values, make sure
that the order and format of the file is not changed.
Any line that begins with a hash mark (#) is treated as a comment.
Table 3-3 HADB Parameters in the clresource.conf File
Parameter Definition Default Value
historypath Path for the history files. /var/tmp
devicepath Path for the data and log devices. /opt/SUNWappserver7/SUNWhadb/4
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 73
Table 3-4 describes the session store parameters in the clresource.conf file.
datadevices Number of data devices on each node. 1
portbase Port base number used for node 0.
Other nodes are then assigned port
number bases in increments of 20 from
the number specified here (a random
number in the range 10000 - 63000).
15200
spares Number of spare nodes. 0
set Comma-separated list of database
configuration attributes.
For explanations of valid database
configuration attributes, see Sun ONE
Application Server Administrator’s Guide.
For example, to specify the use of RSH
instead of SSH (the default), uncomment
the following line:
#set managementProtocol=rsh
inetd Indicates if HADB runs with the inet
daemon.
false
inetdsetupdir Directory where theinet daemon
setup files will be put.
/tmp
devicesize Size of device in MB. This size is
applicable to all devices.
512
dbpassword Password for the HADB user. password
hosts All hosts used for all data nodes. Values are populated automatically
based on the hosts specified during
installation.
NOTE The database name is specified at the end of the [HADBINFO] section in the
clresource.conf file.
Table 3-3 HADB Parameters in the clresource.conf File (Continued)
Parameter Definition Default Value
Using the clsetup Command
74 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Table 3-5 describes the JDBC connection pool parameters in the clresource.conf
file.
Table 3-4 Session Store Parameters in the clresource.conf File
Parameter Definition Default Value
storeurl URL of the HADB store REPLACEURL
NOTE: Value is replaced by actual URL at
runtime.
storeuser User who has access to the
session store
appservusr
NOTE: Must match the username property in
Table 3-5.
storepassword Password for the storeuser password
NOTE: Must match the password property in
Table 3-5.
dbsystempassword Password for the HADB system
user
password
Table 3-5 JDBC Connection Pool Parameters in the clresource.conf File
Parameter Definition Default Value
steadypoolsize Minimum and initial number of
connections maintained in the pool.
8
maxpoolsize Maximum number of connections that
can be created.
32
datasourceclass
name
Name of the vendor-supplied JDBC
datasource.
Name of the vendor-supplied JDBC
datasources capable datasource class
will implement
javax.sql.XADatasource
interface.
Non-XA or Local transactions only
datasources will implement
javax.sql.Datasource interface.
com.sun.hadb.jdbc.ds.HadbDataS
ource
isolationlevel Specifies the transaction isolation level
on the pooled database connections.
repeatable-read
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 75
Table 3-6 describes the JDBC resource parameters in the clresource.conf file.
Table 3-7 describes the session persistence parameters in the clresource.conf file.
validationmetho
d
Specifies the type of validation method. meta-data
property Property used to specify username,
password, and resource configuration.
username=appservusr:password=p
assword:cacheDataBaseMetaData=
false:eliminateRedundantEndTra
nsaction=true:serverList=REPLA
CEURL
NOTE: Make sure that the username and
password properties use the same values
as shown in the Session Store Parameters
table. REPLACEURL is replaced by the
actual URL at runtime.)
NOTE The connection pool name is specified at the end of the
[JDBC_CONNECTION_POOL] section in the clresource.conf file.
Table 3-6 JDBC Resource Parameters in the clresource.conf File
Parameter Definition Default Value
connectionpoolid Name of the connection pool appservCPL
NOTE: Connection pool name is specified in
Table 3-5.
NOTE The JDBC resource name is defined at the end of the [JDBC_RESOURCE] section
in the clresource.conf file.
Table 3-5 JDBC Connection Pool Parameters in the clresource.conf File (Continued)
Parameter Definition Default Value
Using the clsetup Command
76 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Example clresource.conf File
[HADBINFO]
historypath /var/tmp
devicepath /opt/SUNWappserver7/SUNWhadb/4
datadevices 1
portbase 15200
spares 0
#set managementProtocol=rsh
inetd false
inetdsetupdir /tmp
devicesize 512
dbpassword password
hosts machine1,machine1
hadb
[SESSION_STORE]
storeurl REPLACEURL
storeuser appservusr
storepassword password
dbsystempassword password
[JDBC_CONNECTION_POOL]
steadypoolsize 8
maxpoolsize 32
datasourceclassname com.sun.hadb.jdbc.ds.HadbDataSource
isolationlevel repeatable-read
validationmethod meta-data
property
username=appservusr:password=password:cacheDataBaseMetaData=false:e
liminateRedundantEndTransaction=true:serverList=REPLACEURL
appservCPL
Table 3-7 Session Persistence Parameters in the clresource.conf File
Parameter Definition Default Value
type Session persistence type ha
frequency Session frequency web-method
scope Session scope session
store Session store jdbc/hastore
NOTE: Store name is defined at end of the
[JDBC_RESOURCE] section.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 77
[JDBC_RESOURCE]
connectionpoolid appservCPL
jdbc/hastore
[SESSION_PERSISTENCE]
type ha
frequency web-method
scope session
store jdbc/hastore
Running the clsetup Command
The syntax for running the clsetup command is as follows:
clsetup [--help] [--instancefile instance_file_location] [--resourcefile
resource_file_location] [--passwordfile password_file_location]
If no arguments are specified, the clsetup command assumes the following
defaults:
--instancefile is install_config_dir/clinstance.conf
--resourcefile is install_config_dir/clresource.conf
--passwordfile is install_config_dir/clpassword.conf
You can override these arguments by providing custom input file locations. For
example:
./clsetup --resourcefile /tmp/myappservresource.conf
To run the clsetup command, perform the following steps:
1. Verify that the requirements have been met as described in “clsetup
Requirements and Limitations” on page 68.
NOTE When providing custom input files, follow the required format
found in the input files. For information on doing this, see “Editing
the clsetup Input Files” on page 70.
NOTE If you want to run the clsetup command as a user other than root,
follow the instructions in “Setting Up Administration for Non-Root”
on page 64 to set this up.
Using the clsetup Command
78 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Verify that the input files have the information that is required to set up the
cluster. If necessary, edit the input files following the guidelines in “Editing the
clsetup Input Files” on page 70.
3. If you are using RSH, edit the clresource.conf file to uncomment the
following line (remove the # sign):
#set managementProtocol
4. Go to the Sun ONE Application Server installation /bin directory:
cd install_dir/bin
5. Invoke the clsetup command using the appropriate syntax. For example, to
run the command using the defaults:
./clsetup
The clsetup command displays the welcome message, the prerequisites for
configuring the cluster, and the following message:
Do you want to start configuring your cluster? [Yes/No]
6. To start configuring, type Yes and press Enter.
The clsetup command runs in verbose mode. The various commands are
displayed on the screen as they run, and the output is redirected to the log file,
/var/tmp/clsetup.log.
If a vital error occurs (for example, failure to create a non-existing HADB), the
configuration stops and the error is recorded in the log file. If the log file
already exists, the output is appended to the existing log file.
7. When the clsetup command completes the configuration, you are advised
about the location of the log file. It’s a good idea to scan the log file after each
run.
8. Upon completion, the clsetup command returns the exit codes as described in
Table 3-8:
NOTE If the entities to be handled (HADB nodes and Application
Server instances) already exist, the clsetup command does not
delete or reconfigure them, and the respective configuration
steps are skipped. This type of event is recorded in the log file.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 79
You can obtain a list of the exit codes by running the following command from the
command line immediately after running the clsetup command:
‘echo $?’
Cleanup Procedures for the clsetup Command
After running the clsetup command, errors that have occurred are logged in the
log file /var/tmp/clsetup.log. Examine the log file after every run of the
clsetup command and correct any significant errors that are reported (for
example, failure to create a non-existing instance).
You can undo all or part of the configuration as follows:
• To delete an Application Server instance, use the following command:
asadmin delete-instance instance_name
Table 3-8 Exit Codes for the clsetup Command
Exit Code Description
0 Successful exit
2 Usage error
3 Instance file not found
4 Instance file cannot be read
5 Resource file not found
6 Resource file cannot be read
7 Password file not found
8 Password file cannot be read
10 Script cannot find asadmin
11 Script cannot find hadbm
12 Cannot create temporary file
13 Session store configuration failed
14 Create HADB failed
15 HADB get jdbcURL failed
16 User exits in welcome message
Using the clsetup Command
80 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
For example:
asadmin delete-instance server1
• To delete the HADB, use the following commands:
a. hadbm stop database_name
For example:
hadbm stop hadb
b. hadbm delete database_name
For example:
hadbm delete hadb
• To clear the session store, use the following command:
cladmin clear-session-store --storeurl URL_information
--storeuser storeUsername --storepassword store_user_name
For example:
cladmin clear-session-store --storeurl
jdbc:sun:hadb:localhost:10005,localhost::10025 --storeuser
appservusr --storepassword password
• To delete the JDBC connection pool, use the following command:
asadmin delete-jdbc-connection-pool connectionpool_name
For example:
asadmin delete-jdbc-connection-pool appservCPL
• To delete the JDBC resource, use the following command:
cladmin delete-jdbc-resource JDBCresource_Name
For example:
cladmin delete-jdbc-resource jdbc/hastore
After you have completed the tasks in this chapter (and the post-installation tasks
in the following chapter, if needed), proceed to the Sun ONE Application Server
Administrator’s Guide for instructions on configuring the HADB and managing the
cluster, the load balancer plug-in, and the HADB.
81
Chapter 4
Post-installation Tasks
This chapter discusses some tasks you may need to perform during or after
installing the Sun ONE Application Server 7, Enterprise Edition software.
The following topics are addressed here:
• Starting and Stopping the Server
• Creating Domains and Instances
• Web Services Client Implementation
• Stopping and Starting the HADB
Starting and Stopping the Server
Because the Sun ONE Application Server is not automatically started during
installation, you will need to start the application server environment yourself
using either of the following methods:
• Using the Command-line Interface
• Using the Administration Interface
Using the Command-line Interface
You can use the asadmin command-line interface to start and stop:
• The entire application server
• A specific administrative domain
• An individual application server instance
Starting and Stopping the Server
82 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
The subcommands of asadmin listed in Table 4-1 are relevant to start and stop
operations.
Using start-domain and stop-domain
If the Application Server is running, use the following command to stop both the
Admin Server as well as the application server instance of the initially-configured
domain:
asadmin stop-domain --domain domain1 --local
where domain1 is the default name of the administrative domain defined during
installation of the Application Server.
As the command completes, you should observe the following results:
asadmin stop-domain --domain domain1 --local
Instance domain1:server1 stopped
Domain domain1 Stopped.
Likewise, you can start the initially-configured administrative domain by running
the following command:
asadmin start-domain --domain domain1
As the command completes, you should observe the following results:
Table 4-1 Start/Stop Subcommands of asadmin
Subcommand Description
start-appserv Starts the entire Application Server.
stop-appserv Stops the Application Server.
start-domain Starts the administrative server and application server
instances of the specified administrative domain
stop-domain Stops the administrative server and the application server
instances of the specified administrative domain.
start-instance Starts the specified application server instance. Can be run in
either a local or remote mode. In local mode, running this
subcommand does not require the administrative server to be
running.
stop-instance Stops the specified application server instance. Similar in
operation to start-instance.
Starting and Stopping the Server
Chapter 4 Post-installation Tasks 83
asadmin start-domain --domain domain1
Instance domain1:admin-server started
Instance domain1:server1 started
Domain domain1 Started.
Using start-instance and stop-instance
To stop a specific application server instance without relying on the presence of an
Admin Server, you can use the following command:
asadmin stop-instance --local server1
where server1 is the default name of the application server instance. If your
environment contains more than one administrative domain, then you need to
specify the administrative domain name when invoking the stop-instance
command. For example:
asadmin stop-instance --local --domain domain1 server1
To start a specific application server instance in local mode, you can use the
following command:
asadmin start-instance --local server1
If you want to start or stop an instance on a remote system, you can specify the
target Admin Server and administrative user name and password on the
start-instance and stop-instance commands.
Getting Helpful Information
If you run either of these subcommands without parameters, usage information is
displayed. For example:
asadmin start-instance
Invalid number of operands received
Command 'start-instance' not executed successfully
USAGE: start-instance [--user admin_user] [--password
admin_password] [--host localhost] [--port 4848] [--local=false]
[--domain domain_name] [--debug=false] [--secure | -s]
instancename
Alternatively, you can issue the subcommands followed by the --help option to
obtain complete usage information.
Creating Domains and Instances
84 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Using the Administration Interface
When the Admin Server is running, you can use the web-based Administration
interface to start and stop application server instances.
To start the Administration interface (also called the Admin Console or the
graphical interface):
1. Open a browser window and specify the location of your Admin Server’s
console application.
During installation, the default port number for the Admin Server is set to
4848. If this port was already in use, or you selected another port number,
specify that port number. For example:
http://localhost:4848
2. Sign into the Administration interface using the administrative user name and
password specified during installation.
After you've been successfully authenticated, the initial screen of the
Administration interface is displayed.
3. Select the server1 node to access the start and stop functions.
The application server instance is either in a Running or Not Running state.
4. Depending on the server instance state, click either Start or Stop to start or stop
the application server instance.
Creating Domains and Instances
The installation program creates an initial domain called domain1 with a single
instance called server1. Create any additional domains and server instances using
following commands:
To create a new domain:
asadmin create-domain --adminport port_number --adminuser admin
--adminpassword password domain_name [--path domain_path][--sysuser
sys_user] [--passwordfile file_name]
To create a new instance:
Web Services Client Implementation
Chapter 4 Post-installation Tasks 85
asadmin create-instance --instanceport instanceportinstance_name
[--user admin_user] [-password admin_password] [--host localhost]
[--port 4848] [--sysuser sys_user] [--domain domain_name]
[--local=false] [--passwordfile filename][--secure|-s]
Refer to the asadmin man pages for additional information on these commands.
Web Services Client Implementation
To install and configure the web services client, refer to the Sun ONE Application
Server Developer's Guide to Clients.
Stopping and Starting the HADB
This section addresses the following topics:
• Stopping the HADB
• Starting the HADB After Stopping
Stopping the HADB
If you are uninstalling, you will need to stop the running HADB on the node where
you are working. The hadbm stop command stops all HADM processes on each
node. It also captures the role of each node and saves this information locally to the
/etc/opt/SUNWhadb/dbdef/mydb/stopstate file. The hadbm start command
references this file so it knows what role to give the nodes when it starts the
database.
To stop a running HADB, perform these steps:
1. Log in as root on the system where the HADB is running.
2. Run the hadbm stop command using the following format:
hadbm stop hadb_name
This command stops the database.
3. Type yes or y to confirm, anything else to cancel. When the HADB is stopped,
the following is displayed:
Database successfully stopped
Stopping and Starting the HADB
86 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
4. Verify the HADB is stopped by running the following command:
hadbm status --nodes hadb_name
The state for all nodes should be Stopped.
Starting the HADB After Stopping
You must issue the hadbm start command from the host where you issued the
hadbm stop command because the stopstate file is on that host and the
stopstate file is needed for the hadbm start command.
To start all active nodes of an HADB after it has been stopped using the hadbm stop
command, perform these steps:
1. Log in as root on the system where the HADB resides.
2. Run the hadbm start command from the host where you issued the hadbm
stop command using the following format:
hadbm start hadb_name
After the HADB has started, the following is displayed:
Database successfully started
NOTE If the inetd process was still running, the clu_nsup_srv process
would be running and the state for the nodes (NodeState) would be
Starting.
87
Chapter 5
Uninstalling the Enterprise Edition
Software
This chapter contains instructions for uninstalling the Sun ONE Application Server
7, Enterprise Edition software from your system.
The following topics are addressed here:
• About Uninstalling
• Uninstalling the Application Server Software
• Uninstalling in Silent Mode (non-interactive)
About Uninstalling
The installation program enforces component dependencies as specified for each
component. Once component dependencies are satisfied, component life cycles are
independent. A particular component can be installed or uninstalled dynamically
through incremental installation and partial uninstallation mechanisms without
corrupting other components.
Uninstallation failure will result in a complete rollback of the installation, requiring
you to reinstall the product.
NOTE If an uninstallation fails, you may need to clean up some leftover
files or processes before attempting a new installation. In this case,
perform the tasks in “Uninstallation Failure Cleanup” on page 98.
Uninstalling the Application Server Software
88 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Components
The following components can be uninstalled separately or as a complete package:
• Sun ONE Application Server, including its graphical and command-line
administrative tools and Sun ONE Message Queue 3.0.1
• Java 2 Software Development Kit (J2SE), Standard Edition 1.4.1_03
• Sun ONE Application Server Administration Client (command-line tool only)
• Sample applications
• High-Availability Database (HADB)
• Load balancer plug-in for web servers
Installation files, configuration files, and all log files are removed from local and
remote hosts during installation.
Uninstallation Requirements
The following must be true for uninstallation to succeed:
• All databases are stopped and disabled prior to uninstalling.
For guidelines on stopping the HADB, refer to the “Stopping the HADB” on
page 85.
• All database hosts are reachable by SSH or RSH for the root user.
For instructions on setting this up HADB communications, refer to “Setting Up
Host Communication” on page 55.
• The uninstallation program is run from the original installation host.
Uninstalling the Application Server Software
The uninstallation program detects any running Sun ONE Application Server
processes and stops them before continuing to uninstall.
Uninstalling the Application Server Software
Chapter 5 Uninstalling the Enterprise Edition Software 89
To uninstall the Application Server software, perform the following steps:
1. Verify that you have met the requirements in “Uninstallation Requirements”
on page 88.
2. Log in as root on the machine where you want to uninstall the Sun ONE
Application Server 7, Enterprise Edition software.
3. Navigate to your machine’s Sun ONE Application Server 7 installation
directory.
4. Select your installation method.
? To run uninstallation using the graphical interface, type the following at
the command prompt (no options; this is the default method):
./uninstall
? To run uninstallation using the command-line interface, type:
./uninstall -console
The Welcome page of the uninstallation program is displayed.
5. Read the Welcome page and click Next (or press Enter at the command line) to
continue.
6. You will be queried about whether you want to do an incremental
uninstallation.
? If you answer No, the Ready to Install page is displayed as shown in
Step 7.
? If you answer Yes, the component selection page is displayed showing the
components that are installed on your system.
NOTE If your J2SE is installed in a directory other than /usr/j2se, you
must use the following command:
./uninstall -javahome valid_j2se_directory
where valid_j2se_directory is the path to your J2SE 1.4.1_03
installation.
Uninstalling in Silent Mode (non-interactive)
90 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
7. Select which components you want to uninstall and click Next (or press Enter
at the command line).
The Ready to Uninstall page is displayed showing a list of the Application
Server components you have selected to uninstall.
8. Click Uninstall Now (or press Enter on the command line) to start the
uninstallation process.
The Uninstallation progress indicator bar is displayed.
When uninstallation finishes, the Uninstall Summary page is displayed.
9. Review the details by clicking Details.
A details listing displays the top portion of the log file. Complete information
on the uninstallation can be found in the uninstallation log file specified at the
end of the details listing:
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.log
10. Click Dismiss to close the Details page.
11. Click Close (or press Enter at the command line) to quit the uninstallation
program.
12. Verify that uninstallation succeeded by checking to see that the Application
Server components have been removed from the system.
Uninstalling in Silent Mode (non-interactive)
To uninstall the Sun ONE Application Server software in non-interactive silent
mode, perform these steps:
NOTE If uninstallation is interrupted, or if you have trouble installing the
Application Server software after removing a previous version or a
component, refer to “Uninstallation Failure Cleanup” on page 98.
NOTE The interactive methods allow you to select which components you
want to uninstall; silent mode does not. That is, incremental, or
partial, uninstallation is not available for silent mode.
Uninstalling in Silent Mode (non-interactive)
Chapter 5 Uninstalling the Enterprise Edition Software 91
1. Log in as root on the machine where you want to uninstall the Application
Server 7, Enterprise Edition software.
2. Start silent uninstallation at the command line as follows:
./uninstall -silent
When the prompt is returned, the silent uninstallation is completed.
3. Verify that uninstallation succeeded by checking to see that the Sun ONE
Application Server components have been removed from the system.
4. Repeat this process for each server where you want to uninstall.
Uninstalling in Silent Mode (non-interactive)
92 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
93
Chapter 6
Troubleshooting
This chapter describes how to solve common problems that might occur during
installation of the Sun ONE Application Server 7, Enterprise Edition software.
The following topics are addressed here:
• About Logs and Messages
• J2SE Installation/Upgrade Issues
• Forgotten User Name or Password
• Forgotten Admin Server Port Number
• Connection Refused for Administration Interface
• Server Won’t Start: CGI Error Occurs
• Uninstallation Failure Cleanup
About Logs and Messages
Both the installation and uninstallation programs create log files and log all
installation and uninstallation events to these files. The primary purpose of these
log files is to provide troubleshooting information.
In addition to installation program messages and log files, operating system
utilities such as pkginfo and showrev on Solaris can be used to gather system
information.
Log file entries include information about the attempted action, the outcome of the
action, and, if applicable, the cause of failure. The log files contain the following
types of message entries:
J2SE Installation/Upgrade Issues
94 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• INFO—These messages mark normal completion of a particular installation
tasks.
• WARNING—These messages mark non-critical failures. Warning messages
generally contain information about the cause and the nature of the failure, and
also provide possible remedies.
• ERROR—These messages mark critical failures that cause installation or
uninstallation status to be reported as Failed. Error messages generally provide
detailed information about the nature and the cause of the problem that
occurred.
For a full listing of the Sun ONE Application Server error messages, refer to the
Enterprise Edition of the Sun ONE Application Server Error Messages Reference.
The following Application Server logs can be useful for troubleshooting:
• For problems you may have with the installation or uninstallation process:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.log
• In addition to these log files, low-level installation and uninstallation log files
are created at these locations:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.<timestamp>
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.<timestamp>
• For problems with the clsetup command:
/var/tmp/clsetup.log
• For problems with the cladmin command:
/var/tmp/cladmin.log
J2SE Installation/Upgrade Issues
The installation program can only upgrade your J2SE installation when the
following requirements are met:
1. The following Solaris J2SE packages reside on the machine where you are
performing installation:
? SUNWj3rt
? SUNWj3dev
J2SE Installation/Upgrade Issues
Chapter 6 Troubleshooting 95
? SUNWj3man
? SUNWj3dmo
Verify this by running the pkginfo -i -l command on these packages.
2. The version of the Solaris J2SE packages is greater than or equal to version 1.3
and less than version 1.4.1_03.
3. The /usr/j2se (default) directory is writable by the user performing the
installation.
The following types of errors may occur if you attempt to upgrade your J2SE
during installation:
• Incompatible J2SE version---cannot upgrade.
• Failure to install J2SE reported through install log file.
Incompatible J2SE version---cannot upgrade.
If you receive this type of error, the first or second requirements above have not
been met.
Solution
Resolve your J2SE package or version issues by either fixing the Solaris packages or
completely removing the Solaris packages (only if they are not used by any other
application programs) using the pkgrm command.
If you remove the packages, you can then install the J2SE component using the
installation program by selecting the Install Java 2 SDK (1.4.1_03) option in the Java
Configuration panel.
Failure to install J2SE reported through install
log file.
If you receive this type of error, the third requirement above has not been met.
NOTE The installation program can only upgrade a package-based J2SE
installation, not a file-based J2SE installation.
Forgotten User Name or Password
96 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Solution
Verify that your /usr/j2se directory is not read-only.
Forgotten User Name or Password
If you do not remember the administrative user name that was supplied during
installation, try these solutions in this order:
1. Enter the user name admin. This is the default user name specified in the server
configuration dialog during installation.
2. If this doesn’t work, look in the following file:
domain_config_dir/domain1/admin-server/config/admpw
This file contains the administrator's user name followed by the encrypted
form of the administrative user's password. Seeing the user name may jog your
memory.
3. If this doesn’t work, delete the administrative domain and recreate it with a
new password.
4. As a last resort, uninstall and reinstall the Sun ONE Application Server.
Forgotten Admin Server Port Number
If you do not remember the HTTP server port number of the Admin Server, you
can inspect the Admin Server's configuration file to determine the HTTP server
port number:
1. Navigate to domain_config_dir/domain1/admin-server/config/ and open the
server.xml file in a text editor.
2. Look for the following element:
http-listener id="http-listener-1" address="0.0.0.0"
port="4848"...
In this case, port 4848 is the HTTP port number in use.
Connection Refused for Administration Interface
Chapter 6 Troubleshooting 97
Connection Refused for Administration Interface
If the connection was refused when attempting to invoke the graphical
Administration interface, it is likely that the Admin Server is not running. The
Admin Server log file may be helpful in determining the reason the Admin Server
is not running.
To start the Admin Server, use the command-line instructions in “Starting and
Stopping the Server” on page 81.
Server Won’t Start: CGI Error Occurs
If the Sun ONE Application Server won’t start, you may receive the following
error:
[05/Aug/2002:01:12:12] SEVERE (21770): cgi_init reports:
HTTP4047: could not initialize CGI subsystem
(Cgistub path /export/home/sun/appserver7/appserv/lib/Cgistub),
err fork() failure [Not enough space]
The system may require additional resources. Possible solutions are described in
the following sections:
• Set Limits on File Descriptions
• On Solaris: Change Kernel Parameters
Set Limits on File Descriptions
You can use the ulimit command to determine the number of available file
descriptors or set limits on the system’s available file descriptors. The ulimit
command displays the limits for the current shell and its descendants.
For the sh shell, the ulimit -a command lists all the current resource limits. The
ulimit -n command lists the maximum file descriptors plus 1.
On Solaris: Change Kernel Parameters
On Solaris, increase the system resources by modifying the /etc/system file to
include the following entries:
Uninstallation Failure Cleanup
98 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
set rlim_fd_max=4086
set rlim_fd_cur=1024
The system will require a reboot for the new kernel parameters to take effect.
After you have set the shell resources, the Sun ONE Application Server should
start.
Uninstallation Failure Cleanup
If an uninstallation fails, you may need to clean up some leftover files or processes
before attempting a new installation.
1. Log in as root.
2. Navigate to your installation directory and check the content of the
/var/sadm/install/productregistry file for installed packages. That is,
check for files having the SUNW string.
cat /var/sadm/install/productregistry | grep SUNW
3. Run pkgrm for the SUNW packages that were found in the product registry file.
For example:
pkgrm SUNWasaco
4. Remove the following files, if they are present:
/tmp/setupSDKNative
/tmp/SolarisNativeToolkit_3.0_1
5. After the packages have been removed, manually remove the Sun ONE
Application Server-specific product registry file itself.
rm /var/sadm/install/productregistry
6. At the command line, find and kill all appservd processes that may be running
by typing the following:
ps -ef | grep appservd
kill -9 PID
7. Remove all remaining files under the Sun ONE Application Server installation
directories.
Uninstallation Failure Cleanup
Chapter 6 Troubleshooting 99
8. If present, remove the following log file:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
This is necessary because every iteration of installation appends the log
information to this file if it exists.
Uninstallation Failure Cleanup
100 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
101
Appendix A
Installation Cheatsheet
Sun ONE Application Server 7, Enterprise Edition is a complex product to
implement. However, if you are an experienced installer and are familiar with
configuring high-availability systems, the summarized steps in this appendix may
be useful.
The installation phase of product implementation consists of the following
high-level tasks:
1. Fulfill the installation requirements.
2. Install the software components.
3. Complete the high-availability installation tasks.
4. Complete the post-installation tasks.
When you have finished the tasks listed in this appendix, the installation is
considered complete. You are now ready to proceed to the high-availability
configuration tasks as documented in the Sun ONE Application Server
Administrator’s Guide.
1. Fulfill the installation requirements.
Table A-2 lists the requirements that must be met in order to install the Sun ONE
Application Server Version 7, Enterprise Edition product.
Table A-1 Installation Requirements Tasks
Done Task Location of Full Instructions
Platform and HA configuration—Verify platform
and HA configuration have been met.
“Platform Requirements” on page 24
2. Install the software components.
102 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Install the software components.
Table A-2 lists the tasks for installing the Sun ONE Application Server Version 7,
Enterprise Edition product components.
(if applicable) Solaris 8 patches—Verify the correct
patches are installed.
“Solaris 8 Patch Requirements” on page 26
(if applicable) Solaris 9 bundled, Message Queue
broker—Verify unique naming of domains and
instances.
“Shared Message Queue Broker Requirement”
on page 26
(if applicable) Hardened Solaris operating
system—Verify needed libraries are installed.
“Hardened Solaris Operating Environment
Requirement” on page 26
(if applicable) Existing installations—Remove any
existing Application Servers using uninstallation.
“General Requirements” on page 27
Available ports—Plan your port preferences. “General Requirements” on page 27
Root privileges—Verify that the installation person
has root privileges on target machine.
HA topology—Plan your high-availability topology. “Topology Planning” on page 28
Operational Deployment Guide
HA space—Evaluate your high-availability space
requirements
“Space Considerations” on page 29
“Platform Requirements” on page 24
Web server—Install the Sun ONE Web Server 6.0 SP6 “Web Server Installation” on page 29
iPlanet WebServer Installation Guide
Table A-2 Product Installation Tasks
Done Task Location of Full Instructions
Requirements—Verify that requirements are met. Table A-1 on page 101
Table A-1 Installation Requirements Tasks (Continued)
Done Task Location of Full Instructions
3. Complete the high-availability installation tasks.
Appendix A Installation Cheatsheet 103
3. Complete the high-availability installation
tasks.
Table A-3 lists the high-availability preparation tasks that are part of installing the
Sun ONE Application Server Version 7, Enterprise Edition product.
Start processes—Start processes that use ports and will run at same
time as Application Server.
Procedure starts here:
“Installing Application Server
Software” on page 36
(if applicable) Download the software bundle:
gunzip sun-appserver7-sol.tar.gz
tar -xvf sun-appserver7-sol.tar
Choose your installation method:
To invoke the graphical interface— ./setup
To invoke the command-line interface— ./setup -console
Select installation components (load balancer is usually installed
separately) and respond to all installation program prompts.
Check installation summary and logs.
Set PATH environment variable for HADB /bin.
Start the Application Server.
Verify that asadmin and hadbm commands run.
If a previous load balancer plugin is installed, remove it with the
uninstallation program
Procedure starts here:
“Installing the Load Balancer
Plug-in” on page 44
Verify that the correct web server is installed: Sun ONE 6.0 SP6 or
Apache Web Server 1.3.27
Invoke the Installation program to install the load balancer plugin
and respond to all installation program prompts.
(if applicable) Perform silent installation “Installing in Silent Mode
(Non-Interactive)” on page 48
Table A-3 High-Availability Installation Tasks
Done Task Location of Full Instructions
Configure shared memory for the HADB
hosts.
“Configuring Shared Memory and Semaphores” on
page 53
Table A-2 Product Installation Tasks (Continued)
Done Task Location of Full Instructions
4. Complete the post-installation tasks.
104 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
4. Complete the post-installation tasks.
Table A-4 lists the final tasks required for installing the Sun ONE Application
Server Version 7, Enterprise Edition product.
Set up communication for the HADB
hosts, either using RSH or SSH. (SSH is
Solaris 9 default)
“Setting up RSH for HADB Administration” on page 56
“Setting Up SSH for HADB Administration” on page 57
If needed for Solaris 8, install OpenSSH. “Installing SSH for Solaris 8” on page 59
Set up the user environment for hadbm. “Setting Up the User Environment” on page 63
(If applicable) Set up for running the
clsetup command as non-root.
“Setting Up Administration for Non-Root” on page 64
Verify that clsetup requirements are
met.
“clsetup Requirements and Limitations” on page 68
(If applicable) Edit the clsetup input files
for your configuration.
“Editing the clsetup Input Files” on page 70
Run clsetup to configure a basic cluster. “Running the clsetup Command” on page 77
Scan clsetup logs and correct any errors. “Cleanup Procedures for the clsetup Command” on
page 79
Table A-4 Post-Installation Tasks
Done Task Location of Full Instructions
(If needed) Use asadmin commands to
start or stop the Application Server.
“Creating Domains and Instances” on page 84
(If needed) Create additional domains. “Creating Domains and Instances” on page 84
(If needed) Use hadbm commands to stop
or start the HADB.
“Stopping and Starting the HADB” on page 85
(If needed) Install and configure web
services client.
Developer’s Guide to Clients
Table A-3 High-Availability Installation Tasks (Continued)
Done Task Location of Full Instructions
105
Index
SYMBOLS
.rhosts file 57
/etc/opt/SUNWappserver7/config 23
/etc/ssh/sshd_config 58
/opt/SUNWappserver7 23
/usr/j2se 95
/var/opt/SUNWappserver7/domains 23
A
Admin Console. See Administration interface.
Admin Server, not started 97
administration client 18, 33
Administration interface 17
connection refused 97
starting/stopping 84
administration server port 27, 42
administration tools overview 17
Always-On Technology 20
Apache Web Server 29, 42, 45, 47
AS_ADMIN_PASSWORD 72
asadmin commands 68, 83
asadmin delete-instance 80
asadmin delete-jdbc-connection-pool 80
C
cheatsheet for installation 101
cladmin clear-session-store 80
cladmin command 65
cladmin delete-jdbc-resource 80
cleanup after uninstall failure 98
cleanup after uninstallation failure 98
clinstance.conf file 70, 71
clpassword.conf file 71
clresource.conf file 72–77
clsetup command 65–80
cleanup procedures 79
exit codes 78
input files 70
log 78
non-root setup 64
requirements 68
running 77
syntax 77
command-line command 81
command-line interface method 22
communications setup for HADB 55
configuration directory, specifying 41
configuration file (silent mode) 48
modifying 49
variables 49
console option 34
customer support 12
Section
106 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
D
default port numbers 42
default_config_dir 12
dependencies 32
df -k command 25
directories 12
installation 12
instance root 12
documentation 30
directory conventions 12
font conventions 11
general conventions 11
path formats 11
URL formats 11
domains
creating 84
specifying directory 41
starting and stopping 82
E
Enterprise Edition
license 23
overview 20
environment variables HADB settings 63
error messages 94
exit codes for clsetup 78
F
font conventions 11
freeware 59
G
graphical interface method 22
gunzip 37, 45
H
HADB 69
clsetup parameters 72
communications setup 55
configuring shared memory 53
non-root setup 64
refragmentation 29
setting the environment 63
setting up remote access 55
space considerations 29
starting after stopping 86
stopping 85
HADB management client 20, 29, 63
hadbm 44, 68
hadbm delete 80
hadbm deviceinfo 29
hadbm man pages 20
hadbm start 86
hadbm stopdb 85
hardened operating environment 22, 26
high availability 53–65
commands 18
host communications setup 55
overview 20
requirements 28
HTTP reverse proxy plug-in 21
HTTP server
port 27, 42
I
incremental installation 21, 32, 51
inetd 86
inetd configuration 69
install_config_dir 12
install_dir 12
installation 31–52
cheatsheet 101
incremental 32
J2SE 40
logs 93
methods 21, 35
Section
Index 107
post-install tasks 81
roadmap 16
silent mode configuration file 48
installation root directories 12
instances
creating 84
root directories 12
starting and stopping 83
J
J2SE 40
third-party 18, 41
troubleshooting upgrade 94
Java Messaging Service (JMS) overview 19
JAVA_HOME setting 64
JDBC connection pool 69
JDBC connection pool parameters 74
JMS service startup failure 26
L
licensing 23
limitations on clsetup 68
load balancer plug-in 21, 29, 37
installation 44–47
log files 93
logs
clsetup 78
troubleshooting 94
M
man pages 20, 30
Message Queue broker issue 26
methods of installation 21, 35
N
non-root setup 64, 69
O
OpenSSH 55, 58
options for the setup command 34
P
package-based model 23, 95
parameter-driven installation 48
partial installation 21, 32, 51
password, forgotten 96
patches 23, 26
PATH 44
path formats 11
PATH setting for HADB_ROOT 63
pkginfo 44, 93
platforms, supported 24
plug-ins
HTTP reverse proxy 21
load balancer 21
port number, forgotten 96
ports 42
administration server 27, 42, 96
HTTP server 27, 42
inaccessible 97
requirements 27
ports in use 37
post-installation tasks 81–86
privileges, root 27
prtconf command 25
R
refragmentation of HADB 29
Section
108 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
remote access for HADB 55
Remote Shell (RSH) setup 55
requirements
clsetup 68
high availability 28
ports 27
space 24
system 24
technical 27
roadmap for installation 16
root directories
instance 12
root privileges 27
RSH 55
S
sample applications overview 19
savestate option 35
Secured Shell (SSH) setup 55
semaphores 53
server, starting and stopping 81
session persistence parameters 75
session store parameters 73
set shmsys 54
setenv JAVA_HOME 64
setenv PATH 63
setup command usage 34
setupSDKNative 98
shared memory, configuring for HADB 53
showrev 93
silent installation 22, 48–52
silent mode 90
silent option 35
Solaris 8 56, 58
Solaris 9 26, 56
Solaris J2SE packages 94
space for the HADB 29
space requirements 24
SSH 55
ssh-keygen 62
start-appserv 82
start-domain 82
starting a domain 82
starting an instance 83
starting the server 43, 52, 81
starting/stopping the Administration interface 84
start-instance 83
statefile 35
stop-appserv 82
stop-domain 82
stop-instance 83
stopping a domain 82
stopping an instance 83
stopping the server 81
stopstate file 86
summary of installation tasks 101
Sun ONE Message Queue 27, 39
on Solaris 26
overview 19
Sun ONE Web Server 29, 42, 44
sunfreeware 59
SUNWlibC 27
supported platforms 24
syntax for the setup command 34
system requirements 24
system resources, increasing 97
T
tar 37, 45
tasks summary 101
technical requirements 27
third-party J2SE 18, 41
top command 25
topology requirements for high-availability 28
troubleshooting 93–98
J2SE upgrade 94
logs 93
Sun ONE Message Queue broker 26
Section
Index 109
U
ulimit 97
uname command 25
uninstallation
logs 93
requirements 88
troubleshooting 94
uninstallation failure cleanup 98
uninstalling 87–91
URL formats 11
user name, forgotten 96
W
warning messages 94
web server
requirements for high availability 29, 37, 44
specifying instance path 41
web services client 85
Section
110 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
www.it-ebooks.info
Apache Solr 4
Cookbook
Over 100 recipes to make Apache Solr faster,
more reliable, and return better results
Rafal Kuc
BIRMINGHAM - MUMBAI
www.it-ebooks.info
Apache Solr 4 Cookbook
Copyright © 2013 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system,
or transmitted in any form or by any means, without the prior written permission of the
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the
information presented. However, the information contained in this book is sold without
warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers
and distributors will be held liable for any damages caused or alleged to be caused directly
or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies
and products mentioned in this book by the appropriate use of capitals. However, Packt
Publishing cannot guarantee the accuracy of this information.
First published: July 2011
Second edition: January 2013
Production Reference: 1150113
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B3 2PB, UK.
ISBN 978-1-78216-132-5
www.packtpub.com
Cover Image by J. Blaminsky (milak6@wp.pl)
www.it-ebooks.info
Credits
Author
Rafal Kuc
Reviewers
Ravindra Bharathi
Marcelo Ochoa
Vijayakumar Ramdoss
Acquisition Editor
Andrew Duckworth
Lead Technical Editor
Arun Nadar
Technical Editors
Jalasha D'costa
Charmaine Pereira
Lubna Shaikh
Project Coordinator
Anurag Banerjee
Proofreaders
Maria Gould
Aaron Nash
Indexer
Tejal Soni
Production Coordinators
Manu Joseph
Nitesh Thakur
Cover Work
Nitesh Thakur
www.it-ebooks.info
About the Author
Rafal Kuc is a born team leader and software developer. Currently working as a Consultant
and a Software Engineer at Sematext Inc, where he concentrates on open source technologies
such as Apache Lucene and Solr, ElasticSearch, and Hadoop stack. He has more than
10 years of experience in various software branches, from banking software to e-commerce
products. He is mainly focused on Java, but open to every tool and programming language
that will make the achievement of his goal easier and faster. Rafal is also one of the founders
of the solr.pl site, where he tries to share his knowledge and help people with their
problems with Solr and Lucene. He is also a speaker for various conferences around the
world such as Lucene Eurocon, Berlin Buzzwords, and ApacheCon.
Rafal began his journey with Lucene in 2002 and it wasn't love at first sight. When he
came back to Lucene later in 2003, he revised his thoughts about the framework and saw
the potential in search technologies. Then Solr came and that was it. From then on, Rafal
has concentrated on search technologies and data analysis. Right now Lucene, Solr, and
ElasticSearch are his main points of interest.
www.it-ebooks.info
Acknowledgement
This book is an update to the first cookbook for Solr that was released almost two year ago
now. What was at the beginning an update turned out to be a rewrite of almost all the recipes
in the book, because we wanted to not only bring you an update to the already existing
recipes, but also give you whole new recipes that will help you with common situations
when using Apache Solr 4.0. I hope that the book you are holding in your hands (or reading
on a computer or reader screen) will be useful to you.
Although I would go the same way if I could get back in time, the time of writing this book
was not easy for my family. Among the ones who suffered the most were my wife Agnes
and our two great kids, our son Philip and daughter Susanna. Without their patience and
understanding, the writing of this book wouldn't have been possible. I would also like to
thank my parents and Agnes' parents for their support and help.
I would like to thank all the people involved in creating, developing, and maintaining Lucene
and Solr projects for their work and passion. Without them this book wouldn't have been written.
Once again, thank you.
www.it-ebooks.info
About the Reviewers
Ravindra Bharathi has worked in the software industry for over a decade in
various domains such as education, digital media marketing/advertising, enterprise
search, and energy management systems. He has a keen interest in search-based
applications that involve data visualization, mashups, and dashboards. He blogs at
http://ravindrabharathi.blogspot.com.
Marcelo Ochoa works at the System Laboratory of Facultad de Ciencias Exactas of the
Universidad Nacional del Centro de la Provincia de Buenos Aires, and is the CTO at Scotas.
com, a company specialized in near real time search solutions using Apache Solr and Oracle.
He divides his time between University jobs and external projects related to Oracle, and big
data technologies. He has worked in several Oracle related projects such as translation of
Oracle manuals and multimedia CBTs. His background is in database, network, web, and
Java technologies. In the XML world, he is known as the developer of the DB Generator for
the Apache Cocoon project, the open source projects DBPrism and DBPrism CMS, the
Lucene-Oracle integration by using Oracle JVM Directory implementation, and the Restlet.org
project – the Oracle XDB Restlet Adapter, an alternative to writing native REST web services
inside the database resident JVM.
Since 2006, he has been a part of the Oracle ACE program. Oracle ACEs are known for
their strong credentials as Oracle community enthusiasts and advocates, with candidates
nominated by ACEs in the Oracle Technology and Applications communities.
He is the author of Chapter 17 of the book Oracle Database Programming using Java and
Web Services, Kuassi Mensah, Digital Press and Chapter 21 of the book Professional XML
Databases, Kevin Williams, Wrox Press.
www.it-ebooks.info
www.PacktPub.com
Support files, eBooks, discount offers and more
You might want to visit www.PacktPub.com for support files and downloads related to
your book.
Did you know that Packt offers eBook versions of every book published, with PDF and ePub
files available? You can upgrade to the eBook version at www.PacktPub.com and as a print
book customer, you are entitled to a discount on the eBook copy. Get in touch with us at
service@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up
for a range of free newsletters and receive exclusive discounts and offers on Packt books
and eBooks.
http://PacktLib.PacktPub.com
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book
library. Here, you can access, read and search across Packt's entire library of books.
Why Subscribe?
ff Fully searchable across every book published by Packt
ff Copy and paste, print and bookmark content
ff On demand and accessible via web browser
Free Access for Packt account holders
If you have an account with Packt at www.PacktPub.com, you can use this to access
PacktLib today and view nine entirely free books. Simply use your login credentials for
immediate access.
www.it-ebooks.info
www.it-ebooks.info
Table of Contents
Preface 1
Chapter 1: Apache Solr Configuration 5
Introduction 5
Running Solr on Jetty 6
Running Solr on Apache Tomcat 10
Installing a standalone ZooKeeper 14
Clustering your data 15
Choosing the right directory implementation 17
Configuring spellchecker to not use its own index 19
Solr cache configuration 22
How to fetch and index web pages 27
How to set up the extracting request handler 30
Changing the default similarity implementation 32
Chapter 2: Indexing Your Data 35
Introduction 35
Indexing PDF files 36
Generating unique fields automatically 38
Extracting metadata from binary files 40
How to properly configure Data Import Handler with JDBC 42
Indexing data from a database using Data Import Handler 45
How to import data using Data Import Handler and delta query 48
How to use Data Import Handler with the URL data source 50
How to modify data while importing with Data Import Handler 53
Updating a single field of your document 56
Handling multiple currencies 59
Detecting the document's language 62
Optimizing your primary key field indexing 67
www.it-ebooks.info
ii
Table of Contents
Chapter 3: Analyzing Your Text Data 69
Introduction 70
Storing additional information using payloads 70
Eliminating XML and HTML tags from text 73
Copying the contents of one field to another 75
Changing words to other words 77
Splitting text by CamelCase 80
Splitting text by whitespace only 82
Making plural words singular without stemming 84
Lowercasing the whole string 87
Storing geographical points in the index 88
Stemming your data 91
Preparing text to perform an efficient trailing wildcard search 93
Splitting text by numbers and non-whitespace characters 96
Using Hunspell as a stemmer 99
Using your own stemming dictionary 101
Protecting words from being stemmed 103
Chapter 4: Querying Solr 107
Introduction 108
Asking for a particular field value 108
Sorting results by a field value 109
How to search for a phrase, not a single word 111
Boosting phrases over words 114
Positioning some documents over others in a query 117
Positioning documents with words closer to each other first 122
Sorting results by the distance from a point 125
Getting documents with only a partial match 128
Affecting scoring with functions 130
Nesting queries 134
Modifying returned documents 136
Using parent-child relationships 139
Ignoring typos in terms of performance 142
Detecting and omitting duplicate documents 145
Using field aliases 148
Returning a value of a function in the results 151
Chapter 5: Using the Faceting Mechanism 155
Introduction 155
Getting the number of documents with the same field value 156
Getting the number of documents with the same value range 158
www.it-ebooks.info
iii
Table of Contents
Getting the number of documents matching the query and subquery 161
Removing filters from faceting results 164
Sorting faceting results in alphabetical order 168
Implementing the autosuggest feature using faceting 171
Getting the number of documents that don't have a value in the field 174
Having two different facet limits for two different fields in the same query 177
Using decision tree faceting 180
Calculating faceting for relevant documents in groups 183
Chapter 6: Improving Solr Performance 187
Introduction 187
Paging your results quickly 188
Configuring the document cache 189
Configuring the query result cache 190
Configuring the filter cache 192
Improving Solr performance right after the startup or commit operation 194
Caching whole result pages 197
Improving faceting performance for low cardinality fields 198
What to do when Solr slows down during indexing 200
Analyzing query performance 202
Avoiding filter caching 206
Controlling the order of execution of filter queries 207
Improving the performance of numerical range queries 208
Chapter 7: In the Cloud 211
Introduction 211
Creating a new SolrCloud cluster 211
Setting up two collections inside a single cluster 214
Managing your SolrCloud cluster 216
Understanding the SolrCloud cluster administration GUI 220
Distributed indexing and searching 223
Increasing the number of replicas on an already live cluster 227
Stopping automatic document distribution among shards 230
Chapter 8: Using Additional Solr Functionalities 235
Introduction 235
Getting more documents similar to those returned in the results list 236
Highlighting matched words 238
How to highlight long text fields and get good performance 241
Sorting results by a function value 243
Searching words by how they sound 246
Ignoring defined words 248
www.it-ebooks.info
iv
Table of Contents
Computing statistics for the search results 250
Checking the user's spelling mistakes 253
Using field values to group results 257
Using queries to group results 260
Using function queries to group results 262
Chapter 9: Dealing with Problems 265
Introduction 265
How to deal with too many opened files 265
How to deal with out-of-memory problems 267
How to sort non-English languages properly 268
How to make your index smaller 272
Diagnosing Solr problems 274
How to avoid swapping 280
Appendix: Real-life Situations 283
Introduction 283
How to implement a product's autocomplete functionality 284
How to implement a category's autocomplete functionality 287
How to use different query parsers in a single query 290
How to get documents right after they were sent for indexation 292
How to search your data in a near real-time manner 294
How to get the documents with all the query words to the top
of the results set 296
How to boost documents based on their publishing date 300
Index 305
www.it-ebooks.info
Preface
Welcome to the Solr Cookbook for Apache Solr 4.0. You will be taken on a tour through the
most common problems when dealing with Apache Solr. You will learn how to deal with the
problems in Solr configuration and setup, how to handle common querying problems, how
to fine-tune Solr instances, how to set up and use SolrCloud, how to use faceting and
grouping, fight common problems, and many more things. Every recipe is based on
real-life problems, and each recipe includes solutions along with detailed descriptions
of the configuration and code that was used.
What this book covers
Chapter 1, Apache Solr Configuration, covers Solr configuration recipes, different servlet
container usage with Solr, and setting up Apache ZooKeeper and Apache Nutch.
Chapter 2, Indexing Your Data, explains data indexing such as binary file indexing, using Data
Import Handler, language detection, updating a single field of document, and much more.
Chapter 3, Analyzing Your Text Data, concentrates on common problems when analyzing your
data such as stemming, geographical location indexing, or using synonyms.
Chapter 4, Querying Solr, describes querying Apache Solr such as nesting queries, affecting
scoring of documents, phrase search, or using the parent-child relationship.
Chapter 5, Using the Faceting Mechanism, is dedicated to the faceting mechanism in
which you can find the information needed to overcome some of the situations that you can
encounter during your work with Solr and faceting.
Chapter 6, Improving Solr Performance, is dedicated to improving your Apache Solr cluster
performance with information such as cache configuration, indexing speed up, and much more.
Chapter 7, In the Cloud, covers the new feature in Solr 4.0, the SolrCloud, and the setting up
of collections, replica configuration, distributed indexing and searching, and understanding
Solr administration.
www.it-ebooks.info
Preface
2
Chapter 8, Using Additional Solr Functionalities, explains documents highlighting, sorting
results on the basis of function value, checking user spelling mistakes, and using the
grouping functionality.
Chapter 9, Dealing with Problems, is a small chapter dedicated to the most common
situations such as memory problems, reducing your index size, and similar issues.
Appendix, Real Life Situations, describes how to handle real-life situations such as
implementing different autocomplete functionalities, using near real-time search,
or improving query relevance.
What you need for this book
In order to be able to run most of the examples in the book, you will need the Java Runtime
Environment 1.6 or newer, and of course the 4.0 version of the Apache Solr search server.
A few chapters in this book require additional software such as Apache ZooKeeper 3.4.3,
Apache Nutch 1.5.1, Apache Tomcat, or Jetty.
Who this book is for
This book is for users working with Apache Solr or developers that use Apache Solr to build
their own software that would like to know how to combat common problems. Knowledge of
Apache Lucene would be a bonus, but is not required.
Conventions
In this book, you will find a number of styles of text that distinguish between different kinds of
information. Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "The lib entry in the solrconfig.xml file tells
Solr to look for all the JAR files from the ../../langid directory".
A block of code is set as follows:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text_general" indexed="true" stored="true"/>
<field name="description" type="text_general" indexed="true"
stored="true" />
<field name="langId" type="string" indexed="true" stored="true" />
www.it-ebooks.info
Preface
3
When we wish to draw your attention to a particular part of a code block, the relevant lines
or items are set in bold:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
Any command-line input or output is written as follows:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","file":{"set":"New file name"}}]'
New terms and important words are shown in bold. Words that you see on the screen, in
menus or dialog boxes for example, appear in the text like this: "clicking the Next button
moves you to the next screen".
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this
book—what you liked or may have disliked. Reader feedback is important for us to develop
titles that you really get the most out of.
To send us general feedback, simply send an e-mail to feedback@packtpub.com,
and mention the book title through the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing or
contributing to a book, see our author guide on www.packtpub.com/authors.
www.it-ebooks.info
Preface
4
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you
to get the most from your purchase.
Downloading the example code
You can download the example code files for all Packt books you have purchased from
your account at http://www.packtpub.com. If you purchased this book elsewhere,
you can visit http://www.packtpub.com/support and register to have the files
e-mailed directly to you.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be
grateful if you would report this to us. By doing so, you can save other readers from frustration
and help us improve subsequent versions of this book. If you find any errata, please report them
by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata
submission form link, and entering the details of your errata. Once your errata are verified, your
submission will be accepted and the errata will be uploaded to our website, or added to any list
of existing errata, under the Errata section of that title.
Piracy
Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt,
we take the protection of our copyright and licenses very seriously. If you come across any
illegal copies of our works, in any form, on the Internet, please provide us with the location
address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Questions
You can contact us at questions@packtpub.com if you are having a problem with any aspect
of the book, and we will do our best to address it.
www.it-ebooks.info
1
Apache Solr
Configuration
In this chapter we will cover:
ff Running Solr on Jetty
ff Running Solr on Apache Tomcat
ff Installing a standalone ZooKeeper
ff Clustering your data
ff Choosing the right directory implementation
ff Configuring spellchecker to not use its own index
ff Solr cache configuration
ff How to fetch and index web pages
ff How to set up the extracting request handler
ff Changing the default similarity implementation
Introduction
Setting up an example Solr instance is not a hard task, at least when setting up the simplest
configuration. The simplest way is to run the example provided with the Solr distribution, that
shows how to use the embedded Jetty servlet container.
If you don't have any experience with Apache Solr, please refer to the Apache Solr tutorial
which can be found at: http://lucene.apache.org/solr/tutorial.html before
reading this book.
www.it-ebooks.info
Apache Solr Configuration
6
During the writing of this chapter, I used Solr version 4.0 and Jetty
version 8.1.5, and those versions are covered in the tips of the following
chapter. If another version of Solr is mandatory for a feature to run, then
it will be mentioned.
We have a simple configuration, simple index structure described by the schema.xml file,
and we can run indexing.
In this chapter you'll see how to configure and use the more advanced Solr modules; you'll
see how to run Solr in different containers and how to prepare your configuration to different
requirements. You will also learn how to set up a new SolrCloud cluster and migrate your
current configuration to the one supporting all the features of SolrCloud. Finally, you will
learn how to configure Solr cache to meet your needs and how to pre-sort your Solr indexes
to be able to use early query termination techniques efficiently.
Running Solr on Jetty
The simplest way to run Apache Solr on a Jetty servlet container is to run the provided
example configuration based on embedded Jetty. But it's not the case here. In this recipe,
I would like to show you how to configure and run Solr on a standalone Jetty container.
Getting ready
First of all you need to download the Jetty servlet container for your platform. You can get your
download package from an automatic installer (such as, apt-get), or you can download it
yourself from http://jetty.codehaus.org/jetty/.
How to do it...
The first thing is to install the Jetty servlet container, which is beyond the scope of this book,
so we will assume that you have Jetty installed in the /usr/share/jetty directory or you
copied the Jetty files to that directory.
Let's start by copying the solr.war file to the webapps directory of the Jetty installation
(so the whole path would be /usr/share/jetty/webapps). In addition to that we need
to create a temporary directory in Jetty installation, so let's create the temp directory in the
Jetty installation directory.
Next we need to copy and adjust the solr.xml file from the context directory of the Solr
example distribution to the context directory of the Jetty installation. The final file contents
should look like the following code:
www.it-ebooks.info
Chapter 1
7
<?xml version="1.0"?>
<!DOCTYPE Configure PUBLIC "-//Jetty//Configure//EN" "http://www.
eclipse.org/jetty/configure.dtd">
<Configure class="org.eclipse.jetty.webapp.WebAppContext">
<Set name="contextPath">/solr</Set>
<Set name="war"><SystemProperty name="jetty.home"/>/webapps/solr.
war</Set>
<Set name="defaultsDescriptor"><SystemProperty name="jetty.home"/>/
etc/webdefault.xml</Set>
<Set name="tempDirectory"><Property name="jetty.home" default="."/>/
temp</Set>
</Configure>
Downloading the example code
You can download the example code files for all Packt books you
have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit
http://www.packtpub.com/support and register to have the
files e-mailed directly to you.
Now we need to copy the jetty.xml, webdefault.xml, and logging.properties files
from the etc directory of the Solr distribution to the configuration directory of Jetty, so in our
case to the /usr/share/jetty/etc directory.
The next step is to copy the Solr configuration files to the appropriate directory. I'm talking
about files such as schema.xml, solrconfig.xml, solr.xml, and so on. Those files
should be in the directory specified by the solr.solr.home system variable (in my case
this was the /usr/share/solr directory). Please remember to preserve the directory
structure you'll see in the example deployment, so for example, the /usr/share/solr
directory should contain the solr.xml (and in addition zoo.cfg in case you want to
use SolrCloud) file with the contents like so:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
All the other configuration files should go to the /usr/share/solr/collection1/conf
directory (place the schema.xml and solrconfig.xml files there along with any additional
configuration files your deployment needs). Your cores may have other names than the default
collection1, so please be aware of that.
www.it-ebooks.info
Apache Solr Configuration
8
The last thing about the configuration is to update the /etc/default/jetty file and
add –Dsolr.solr.home=/usr/share/solr to the JAVA_OPTIONS variable of that
file. The whole line with that variable could look like the following:
JAVA_OPTIONS="-Xmx256m -Djava.awt.headless=true -Dsolr.solr.home=/usr/
share/solr/"
If you didn't install Jetty with apt-get or a similar software, you may not have the /etc/
default/jetty file. In that case, add the –Dsolr.solr.home=/usr/share/solr
parameter to the Jetty startup.
We can now run Jetty to see if everything is ok. To start Jetty, that was installed, for example,
using the apt-get command, use the following command:
/etc/init.d/jetty start
You can also run Jetty with a java command. Run the following command in the Jetty
installation directory:
java –Dsolr.solr.home=/usr/share/solr –jar start.jar
If there were no exceptions during the startup, we have a running Jetty with Solr deployed
and configured. To check if Solr is running, try going to the following address with your web
browser: http://localhost:8983/solr/.
You should see the Solr front page with cores, or a single core, mentioned. Congratulations!
You just successfully installed, configured, and ran the Jetty servlet container with Solr deployed.
How it works...
For the purpose of this recipe, I assumed that we needed a single core installation with only
schema.xml and solrconfig.xml configuration files. Multicore installation is very similar
– it differs only in terms of the Solr configuration files.
The first thing we did was copy the solr.war file and create the temp directory. The WAR
file is the actual Solr web application. The temp directory will be used by Jetty to unpack
the WAR file.
The solr.xml file we placed in the context directory enables Jetty to define the context
for the Solr web application. As you can see in its contents, we set the context to be /solr,
so our Solr application will be available under http://localhost:8983/solr/. We
also specified where Jetty should look for the WAR file (the war property), where the web
application descriptor file (the defaultsDescriptor property) is, and finally where the
temporary directory will be located (the tempDirectory property).
www.it-ebooks.info
Chapter 1
9
The next step is to provide configuration files for the Solr web application. Those files should
be in the directory specified by the system solr.solr.home variable. I decided to use the
/usr/share/solr directory to ensure that I'll be able to update Jetty without the need of
overriding or deleting the Solr configuration files. When copying the Solr configuration files,
you should remember to include all the files and the exact directory structure that Solr needs.
So in the directory specified by the solr.solr.home variable, the solr.xml file should be
available – the one that describes the cores of your system.
The solr.xml file is pretty simple – there should be the root element called solr. Inside it
there should be a cores tag (with the adminPath variable set to the address where Solr's
cores administration API is available and the defaultCoreName attribute that says which
is the default core). The cores tag is a parent for cores definition – each core should have
its own cores tag with name attribute specifying the core name and the instanceDir
attribute specifying the directory where the core specific files will be available (such as
the conf directory).
If you installed Jetty with the apt-get command or similar, you will need to update
the /etc/default/jetty file to include the solr.solr.home variable for Solr
to be able to see its configuration directory.
After all those steps we are ready to launch Jetty. If you installed Jetty with apt-get
or a similar software, you can run Jetty with the first command shown in the example.
Otherwise you can run Jetty with a java command from the Jetty installation directory.
After running the example query in your web browser you should see the Solr front page
as a single core. Congratulations! You just successfully configured and ran the Jetty servlet
container with Solr deployed.
There's more...
There are a few tasks you can do to counter some problems when running Solr within the Jetty
servlet container. Here are the most common ones that I encountered during my work.
I want Jetty to run on a different port
Sometimes it's necessary to run Jetty on a different port other than the default one. We have
two ways to achieve that:
ff Adding an additional startup parameter, jetty.port. The startup command would
look like the following command:
java –Djetty.port=9999 –jar start.jar
www.it-ebooks.info
Apache Solr Configuration
10
ff Changing the jetty.xml file – to do that you need to change the following line:
<Set name="port"><SystemProperty name="jetty.port"
default="8983"/></Set>
To:
<Set name="port"><SystemProperty name="jetty.port"
default="9999"/></Set>
Buffer size is too small
Buffer overflow is a common problem when our queries are getting too long and too complex,
– for example, when we use many logical operators or long phrases. When the standard head
buffer is not enough you can resize it to meet your needs. To do that, you add the following
line to the Jetty connector in thejetty.xml file. Of course the value shown in the example
can be changed to the one that you need:
<Set name="headerBufferSize">32768</Set>
After adding the value, the connector definition should look more or less like the
following snippet:
<Call name="addConnector">
<Arg>
<New class="org.mortbay.jetty.bio.SocketConnector">
<Set name="port"><SystemProperty name="jetty.port" default="8080"/></
Set>
<Set name="maxIdleTime">50000</Set>
<Set name="lowResourceMaxIdleTime">1500</Set>
<Set name="headerBufferSize">32768</Set>
</New>
</Arg>
</Call>
Running Solr on Apache Tomcat
Sometimes you need to choose a servlet container other than Jetty. Maybe because your
client has other applications running on another servlet container, maybe because you just
don't like Jetty. Whatever your requirements are that put Jetty out of the scope of your interest,
the first thing that comes to mind is a popular and powerful servlet container – Apache
Tomcat. This recipe will give you an idea of how to properly set up and run Solr
in the Apache Tomcat environment.
www.it-ebooks.info
Chapter 1
11
Getting ready
First of all we need an Apache Tomcat servlet container. It can be found at the Apache Tomcat
website – http://tomcat.apache.org. I concentrated on the Tomcat Version 7.x because
at the time of writing of this book it was mature and stable. The version that I used during the
writing of this recipe was Apache Tomcat 7.0.29, which was the newest one at the time.
How to do it...
To run Solr on Apache Tomcat we need to follow these simple steps:
1. Firstly, you need to install Apache Tomcat. The Tomcat installation is beyond the
scope of this book so we will assume that you have already installed this servlet
container in the directory specified by the $TOMCAT_HOME system variable.
2. The second step is preparing the Apache Tomcat configuration files. To do that we
need to add the following inscription to the connector definition in the server.xml
configuration file:
URIEncoding="UTF-8"
The portion of the modified server.xml file should look like the following
code snippet:
<Connector port="8080" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
3. The third step is to create a proper context file. To do that, create a solr.xml file
in the $TOMCAT_HOME/conf/Catalina/localhost directory. The contents of
the file should look like the following code:
<Context path="/solr" docBase="/usr/share/tomcat/webapps/solr.war"
debug="0" crossContext="true">
<Environment name="solr/home" type="java.lang.String" value="/
usr/share/solr/" override="true"/>
</Context>
4. The next thing is the Solr deployment. To do that we need the apache-solr-
4.0.0.war file that contains the necessary files and libraries to run Solr that
is to be copied to the Tomcat webapps directory and renamed solr.war.
5. The one last thing we need to do is add the Solr configuration files. The files that you
need to copy are files such as schema.xml, solrconfig.xml, and so on. Those
files should be placed in the directory specified by the solr/home variable (in our
case /usr/share/solr/). Please don't forget that you need to ensure the proper
directory structure. If you are not familiar with the Solr directory structure please take
a look at the example deployment that is provided with the standard Solr package.
www.it-ebooks.info
Apache Solr Configuration
12
6. Please remember to preserve the directory structure you'll see in the example
deployment, so for example, the /usr/share/solr directory should contain
the solr.xml (and in addition zoo.cfg in case you want to use SolrCloud)
file with the contents like so:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
7. All the other configuration files should go to the /usr/share/solr/collection1/
conf directory (place the schema.xml and solrconfig.xml files there along with
any additional configuration files your deployment needs). Your cores may have other
names than the default collection1, so please be aware of that.
8. Now we can start the servlet container, by running the following command:
bin/catalina.sh start
9. In the log file you should see a message like this:
Info: Server startup in 3097 ms
10. To ensure that Solr is running properly, you can run a browser and point it to an
address where Solr should be visible, like the following:
http://localhost:8080/solr/
If you see the page with links to administration pages of each of the cores defined, that
means that your Solr is up and running.
How it works...
Let's start from the second step as the installation part is beyond the scope of this book.
As you probably know, Solr uses UTF-8 file encoding. That means that we need to ensure
that Apache Tomcat will be informed that all requests and responses made should use that
encoding. To do that, we modified the server.xml file in the way shown in the example.
The Catalina context file (called solr.xml in our example) says that our Solr application
will be available under the /solr context (the path attribute). We also specified the WAR
file location (the docBase attribute). We also said that we are not using debug (the debug
attribute), and we allowed Solr to access other context manipulation methods. The last thing
is to specify the directory where Solr should look for the configuration files. We do that by
adding the solr/home environment variable with the value attribute set to the path to
the directory where we have put the configuration files.
www.it-ebooks.info
Chapter 1
13
The solr.xml file is pretty simple – there should be the root element called solr. Inside
it there should be the cores tag (with the adminPath variable set to the address where
the Solr cores administration API is available and the defaultCoreName attribute describing
which is the default core). The cores tag is a parent for cores definition – each core should
have its own core tag with a name attribute specifying the core name and the instanceDir
attribute specifying the directory where the core-specific files will be available (such as the
conf directory).
The shell command that is shown starts Apache Tomcat. There are some other options of the
catalina.sh (or catalina.bat) script; the descriptions of these options are as follows:
ff stop: This stops Apache Tomcat
ff restart: This restarts Apache Tomcat
ff debug: This start Apache Tomcat in debug mode
ff run: This runs Apache Tomcat in the current window, so you can see the output on
the console from which you run Tomcat.
After running the example address in the web browser, you should see a Solr front page with
a core (or cores if you have a multicore deployment). Congratulations! You just successfully
configured and ran the Apache Tomcat servlet container with Solr deployed.
There's more...
There are some other tasks that are common problems when running Solr on Apache Tomcat.
Changing the port on which we see Solr running on Tomcat
Sometimes it is necessary to run Apache Tomcat on a different port other than 8080, which is
the default one. To do that, you need to modify the port variable of the connector definition
in the server.xml file located in the $TOMCAT_HOME/conf directory. If you would like your
Tomcat to run on port 9999, this definition should look like the following code snippet:
<Connector port="9999" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
While the original definition looks like the following snippet:
<Connector port="8080" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
www.it-ebooks.info
Apache Solr Configuration
14
Installing a standalone ZooKeeper
You may know that in order to run SolrCloud—the distributed Solr installation—you need to have
Apache ZooKeeper installed. Zookeeper is a centralized service for maintaining configurations,
naming, and provisioning service synchronization. SolrCloud uses ZooKeeper to synchronize
configuration and cluster states (such as elected shard leaders), and that's why it is crucial to
have a highly available and fault tolerant ZooKeeper installation. If you have a single ZooKeeper
instance and it fails then your SolrCloud cluster will crash too. So, this recipe will show you how
to install ZooKeeper so that it's not a single point of failure in your cluster configuration.
Getting ready
The installation instruction in this recipe contains information about installing ZooKeeper
Version 3.4.3, but it should be useable for any minor release changes of Apache ZooKeeper.
To download ZooKeeper please go to http://zookeeper.apache.org/releases.html.
This recipe will show you how to install ZooKeeper in a Linux-based environment. You also
need Java installed.
How to do it...
Let's assume that we decided to install ZooKeeper in the /usr/share/zookeeper
directory of our server and we want to have three servers (with IP addresses 192.168.1.1,
192.168.1.2, and 192.168.1.3) hosting the distributed ZooKeeper installation.
1. After downloading the ZooKeeper installation, we create the necessary directory:
sudo mkdir /usr/share/zookeeper
2. Then we unpack the downloaded archive to the newly created directory. We do that
on three servers.
3. Next we need to change our ZooKeeper configuration file and specify the servers that
will form the ZooKeeper quorum, so we edit the /usr/share/zookeeper/conf/
zoo.cfg file and we add the following entries:
clientPort=2181
dataDir=/usr/share/zookeeper/data
tickTime=2000
initLimit=10
syncLimit=5
server.1=192.168.1.1:2888:3888
server.2=192.168.1.2:2888:3888
server.3=192.168.1.3:2888:3888
www.it-ebooks.info
Chapter 1
15
4. And now, we can start the ZooKeeper servers with the following command:
/usr/share/zookeeper/bin/zkServer.sh start
5. If everything went well you should see something like the following:
JMX enabled by default
Using config: /usr/share/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
And that's all. Of course you can also add the ZooKeeper service to start automatically during
your operating system startup, but that's beyond the scope of the recipe and the book itself.
How it works...
Let's skip the first part, because creating the directory and unpacking the ZooKeeper server
there is quite simple. What I would like to concentrate on are the configuration values of the
ZooKeeper server. The clientPort property specifies the port on which our SolrCloud servers
should connect to ZooKeeper. The dataDir property specifies the directory where ZooKeeper
will hold its data. So far, so good right ? So now, the more advanced properties; the tickTime
property specified in milliseconds is the basic time unit for ZooKeeper. The initLimit
property specifies how many ticks the initial synchronization phase can take. Finally, the
syncLimit property specifies how many ticks can pass between sending the request and
receiving an acknowledgement.
There are also three additional properties present, server.1, server.2, and server.3.
These three properties define the addresses of the ZooKeeper instances that will form the
quorum. However, there are three values separated by a colon character. The first part is the
IP address of the ZooKeeper server, and the second and third parts are the ports used by
ZooKeeper instances to communicate with each other.
Clustering your data
After the release of Apache Solr 4.0, many users will want to leverage SolrCloud distributed
indexing and querying capabilities. It's not hard to upgrade your current cluster to SolrCloud,
but there are some things you need to take care of. With the help of the following recipe you
will be able to easily upgrade your cluster.
Getting ready
Before continuing further it is advised to read the Installing a standalone ZooKeeper
recipe in this chapter. It shows how to set up a ZooKeeper cluster in order to be ready
for production use.
www.it-ebooks.info
Apache Solr Configuration
16
How to do it...
In order to use your old index structure with SolrCloud, you will need to add the following
field to your fields definition (add the following fragment to the schema.xml file, to its
fields section):
<field name="_version_" type="long" indexed="true" stored="true"
multiValued="false"/>
Now let's switch to the solrconfig.xml file – starting with the replication handlers. First,
you need to ensure that you have the replication handler set up. Remember that you shouldn't
add master or slave specific configurations to it. So the replication handlers' configuration
should look like the following code:
<requestHandler name="/replication" class="solr.ReplicationHandler" />
In addition to that, you will need to have the administration panel handlers present, so the
following configuration entry should be present in your solrconfig.xml file:
<requestHandler name="/admin/" class="solr.admin.AdminHandlers" />
The last request handler that should be present is the real-time get handler, which should
be defined as follows (the following should also be added to the solrconfig.xml file):
<requestHandler name="/get" class="solr.RealTimeGetHandler">
<lst name="defaults">
<str name="omitHeader">true</str>
</lst>
</requestHandler>
The next thing SolrCloud needs in order to properly operate is the transaction log
configuration. The following fragment should be added to the solrconfig.xml file:
<updateLog>
<str name="dir">${solr.data.dir:}</str>
</updateLog>
The last thing is the solr.xml file. It should be pointing to the default cores administration
address – the cores tag should have the adminPath property set to the /admin/cores
value. The example solr.xml file could look like the following code:
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1"
host="localhost" hostPort="8983" zkClientTimeout="15000">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
And that's all, your Solr instances configuration files are now ready to be used with SolrCloud.
www.it-ebooks.info
Chapter 1
17
How it works...
So now let's see why all those changes are needed in order to use our old configuration files
with SolrCloud.
The _version_ field is used by Solr to enable documents versioning and optimistic locking,
which ensures that you won't have the newest version of your document overwritten by
mistake. Because of that, SolrCloud requires the _version_ field to be present in your
index structure. Adding that field is simple – you just need to place another field definition
that is stored and indexed, and based on the long type. That's all.
As for the replication handler, you should remember not to add slave or master specific
configuration, only the simple request handler definition, as shown in the previous example.
The same applies to the administration panel handlers: they need to be available under the
default URL address.
The real-time get handler is responsible for getting the updated documents right away,
even if no commit or the softCommit command is executed. This handler allows Solr
(and also you) to retrieve the latest version of the document without the need for re-opening
the searcher, and thus even if the document is not yet visible during usual search operations.
The configuration is very similar to the usual request handler configuration – you need to
add a new handler with the name property set to /get and the class property set to solr.
RealTimeGetHandler. In addition to that, we want the handler to be omitting response
headers (the omitHeader property set to true).
One of the last things that is needed by SolrCloud is the transaction log, which enables realtime
get operations to be functional. The transaction log keeps track of all the uncommitted
changes and enables a real-time get handler to retrieve those. In order to turn on transaction
log usage, one should add the updateLog tag to the solrconfig.xml file and specify the
directory where the transaction log directory should be created (by adding the dir property as
shown in the example). In the configuration previously shown, we tell Solr that we want to use
the Solr data directory as the place to store the transaction log directory.
Finally, Solr needs you to keep the default address for the core administrative interface, so
you should remember to have the adminPath property set to the value shown in the example
(in the solr.xml file). This is needed in order for Solr to be able to manipulate cores.
Choosing the right directory implementation
One of the most crucial properties of Apache Lucene, and thus Solr, is the Lucene directory
implementation. The directory interface provides an abstraction layer for Lucene on all the
I/O operations. Although choosing the right directory implementation seems simple, it can
affect the performance of your Solr setup in a drastic way. This recipe will show you how to
choose the right directory implementation.
www.it-ebooks.info
Apache Solr Configuration
18
How to do it...
In order to use the desired directory, all you need to do is choose the right directory
factory implementation and inform Solr about it. Let's assume that you would like to use
NRTCachingDirectory as your directory implementation. In order to do that, you need to
place (or replace if it is already present) the following fragment in your solrconfig.xml file:
<directoryFactory name="DirectoryFactory" class="solr.
NRTCachingDirectoryFactory" />
And that's all. The setup is quite simple, but what directory factories are available to use?
When this book was written, the following directory factories were available:
ff solr.StandardDirectoryFactory
ff solr.SimpleFSDirectoryFactory
ff solr.NIOFSDirectoryFactory
ff solr.MMapDirectoryFactory
ff solr.NRTCachingDirectoryFactory
ff solr.RAMDirectoryFactory
So now let's see what each of those factories provide.
How it works...
Before we get into the details of each of the presented directory factories, I would like to
comment on the directory factory configuration parameter. All you need to remember is that
the name attribute of the directoryFactory tag should be set to DirectoryFactory
and the class attribute should be set to the directory factory implementation of your choice.
If you want Solr to make the decision for you, you should use solr.
StandardDirectoryFactory. This is a filesystem-based directory factory that tries
to choose the best implementation based on your current operating system and Java
virtual machine used. If you are implementing a small application, which won't use many
threads, you can use solr.SimpleFSDirectoryFactory which stores the index file
on your local filesystem, but it doesn't scale well with a high number of threads. solr.
NIOFSDirectoryFactory scales well with many threads, but it doesn't work well on
Microsoft Windows platforms (it's much slower), because of the JVM bug, so you should
remember that.
solr.MMapDirectoryFactory was the default directory factory for Solr for the 64-bit Linux
systems from Solr 3.1 till 4.0. This directory implementation uses virtual memory and a kernel
feature called mmap to access index files stored on disk. This allows Lucene (and thus Solr) to
directly access the I/O cache. This is desirable and you should stick to that directory if near
real-time searching is not needed.
www.it-ebooks.info
Chapter 1
19
If you need near real-time indexing and searching, you should use solr.
NRTCachingDirectoryFactory. It is designed to store some parts of the index
in memory (small chunks) and thus speed up some near real-time operations greatly.
The last directory factory, solr.RAMDirectoryFactory, is the only one that is not
persistent. The whole index is stored in the RAM memory and thus you'll lose your index after
restart or server crash. Also you should remember that replication won't work when using
solr.RAMDirectoryFactory. One would ask, why should I use that factory? Imagine a
volatile index for an autocomplete functionality or for unit tests of your queries' relevancy.
Just anything you can think of, when you don't need to have persistent and replicated data.
However, please remember that this directory is not designed to hold large amounts of data.
Configuring spellchecker to not use its own
index
If you are used to the way spellchecker worked in the previous Solr versions, you may
remember that it required its own index to give you spelling corrections. That approach
had some disadvantages, such as the need for rebuilding the index, and replication between
master and slave servers. With the Solr Version 4.0, a new spellchecker implementation was
introduced – solr.DirectSolrSpellchecker. It allowed you to use your main index to
provide spelling suggestions and didn't need to be rebuilt after every commit. So now, let's
see how to use that new spellchecker implementation in Solr.
How to do it...
First of all, let's assume we have a field in the index called title, in which we hold titles
of our documents. What's more, we don't want the spellchecker to have its own index and
we would like to use that title field to provide spelling suggestions. In addition to that, we
would like to decide when we want a spelling suggestion. In order to do that, we need to do
two things:
1. First, we need to edit our solrconfig.xml file and add the spellchecking
component, whose definition may look like the following code:
<searchComponent name="spellcheck" class="solr.
SpellCheckComponent">
<str name="queryAnalyzerFieldType">title</str>
<lst name="spellchecker">
<str name="name">direct</str>
<str name="field">title</str>
<str name="classname">solr.DirectSolrSpellChecker</str>
<str name="distanceMeasure">internal</str>
<float name="accuracy">0.8</float>
<int name="maxEdits">1</int>
www.it-ebooks.info
Apache Solr Configuration
20
<int name="minPrefix">1</int>
<int name="maxInspections">5</int>
<int name="minQueryLength">3</int>
<float name="maxQueryFrequency">0.01</float>
</lst>
</searchComponent>
2. Now we need to add a proper request handler configuration that will use the
previously mentioned search component. To do that, we need to add the following
section to the solrconfig.xml file:
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">title</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck">on</str>
<str name="spellcheck.extendedResults">true</str>
<str name="spellcheck.count">5</str>
<str name="spellcheck.collate">true</str>
<str name="spellcheck.collateExtendedResults">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
3. And that's all. In order to get spelling suggestions, we need to run the following query:
/spell?q=disa
4. In response we will get something like the following code:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">5</int>
</lst>
<result name="response" numFound="0" start="0">
</result>
<lst name="spellcheck">
<lst name="suggestions">
<lst name="disa">
<int name="numFound">1</int>
<int name="startOffset">0</int>
<int name="endOffset">4</int>
<int name="origFreq">0</int>
<arr name="suggestion">
<lst>
<str name="word">data</str>
<int name="freq">1</int>
www.it-ebooks.info
Chapter 1
21
</lst>
</arr>
</lst>
<bool name="correctlySpelled">false</bool>
<lst name="collation">
<str name="collationQuery">data</str>
<int name="hits">1</int>
<lst name="misspellingsAndCorrections">
<str name="disa">data</str>
</lst>
</lst>
</lst>
</lst>
</response>
If you check your data folder you will see that there is not a single directory responsible
for holding the spellchecker index. So, now let's see how that works.
How it works...
Now let's get into some specifics about how the previous configuration works, starting
from the search component configuration. The queryAnalyzerFieldType property
tells Solr which field configuration should be used to analyze the query passed to the
spellchecker. The name property sets the name of the spellchecker which will be used in
the handler configuration later. The field property specifies which field should be used
as the source for the data used to build spelling suggestions. As you probably figured out,
the classname property specifies the implementation class, which in our case is solr.
DirectSolrSpellChecker, enabling us to omit having a separate spellchecker index.
The next parameters visible in the configuration specify how the Solr spellchecker should
behave and that is beyond the scope of this recipe (however, if you would like to read more
about them, please go to the following URL address: http://wiki.apache.org/solr/
SpellCheckComponent).
The last thing is the request handler configuration. Let's concentrate on all the properties
that start with the spellcheck prefix. First we have spellcheck.dictionary, which
in our case specifies the name of the spellchecking component we want to use (please
note that the value of the property matches the value of the name property in the search
component configuration). We tell Solr that we want the spellchecking results to be present
(the spellcheck property with the value set to on), and we also tell Solr that we want to see
the extended results format (spellcheck.extendedResults set to true). In addition to
the mentioned configuration properties, we also said that we want to have a maximum of five
suggestions (the spellcheck.count property), and we want to see the collation and its
extended results (spellcheck.collate and spellcheck.collateExtendedResults
both set to true).
www.it-ebooks.info
Apache Solr Configuration
22
There's more...
Let's see one more thing – the ability to have more than one spellchecker defined in a
request handler.
More than one spellchecker
If you would like to have more than one spellchecker handling your spelling suggestions you
can configure your handler to use multiple search components. For example, if you would like
to use search components (spellchecking ones) named word and better (you have to have
them configured), you could add multiple spellcheck.dictionary parameters to your
request handler. This is how your request handler configuration would look:
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">title</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck.dictionary">word</str>
<str name="spellcheck.dictionary">better</str>
<str name="spellcheck">on</str>
<str name="spellcheck.extendedResults">true</str>
<str name="spellcheck.count">5</str>
<str name="spellcheck.collate">true</str>
<str name="spellcheck.collateExtendedResults">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
Solr cache configuration
As you may already know, caches play a major role in a Solr deployment. And I'm not talking
about some exterior cache – I'm talking about the three Solr caches:
ff Filter cache: This is used for storing filter (query parameter fq) results and mainly
enum type facets
ff Document cache: This is used for storing Lucene documents which hold stored fields
ff Query result cache: This is used for storing results of queries
www.it-ebooks.info
Chapter 1
23
There is a fourth cache – Lucene's internal cache – which is a field cache, but you can't
control its behavior. It is managed by Lucene and created when it is first used by the
Searcher object.
With the help of these caches we can tune the behavior of the Solr searcher instance. In this
task we will focus on how to configure your Solr caches to suit most needs. There is one thing
to remember – Solr cache sizes should be tuned to the number of documents in the index,
the queries, and the number of results you usually get from Solr.
Getting ready
Before you start tuning Solr caches you should get some information about your Solr instance.
That information is as follows:
ff Number of documents in your index
ff Number of queries per second made to that index
ff Number of unique filter (the fq parameter) values in your queries
ff Maximum number of documents returned in a single query
ff Number of different queries and different sorts
All these numbers can be derived from Solr logs.
How to do it...
For the purpose of this task I assumed the following numbers:
ff Number of documents in the index: 1.000.000
ff Number of queries per second: 100
ff Number of unique filters: 200
ff Maximum number of documents returned in a single query: 100
ff Number of different queries and different sorts: 500
Let's open the solrconfig.xml file and tune our caches. All the changes should be made in
the query section of the file (the section between <query> and </query> XML tags).
1. First goes the filter cache:
<filterCache
class="solr.FastLRUCache"
size="200"
initialSize="200"
autowarmCount="100"/>
www.it-ebooks.info
Apache Solr Configuration
24
2. Second goes the query result cache:
<queryResultCache
class="solr.FastLRUCache"
size="500"
initialSize="500"
autowarmCount="250"/>
3. Third we have the document cache:
<documentCache
class="solr.FastLRUCache"
size="11000"
initialSize="11000" />
Of course the above configuration is based on the example values.
4. Further let's set our result window to match our needs – we sometimes need to
get 20–30 more results than we need during query execution. So we change the
appropriate value in the solrconfig.xml file to something like this:
<queryResultWindowSize>200</queryResultWindowSize>
And that's all!
How it works...
Let's start with a little bit of explanation. First of all we use the solr.FastLRUCache
implementation instead of solr.LRUCache. So the called FastLRUCache tends to be faster
when Solr puts less into caches and gets more. This is the opposite to LRUCache which tends
to be more efficient when there are more puts than gets operations. That's why we use it.
This colud be the first time you see cache configuration, so I'll explain what cache configuration
parameters mean:
ff class: You probably figured that out by now. Yes, this is the class implementing the
cache.
ff size: This is the maximum size that the cache can have.
ff initialSize: This is the initial size that the cache will have.
ff autowarmCount: This is the number of cache entries that will be copied to the
new instance of the same cache when Solr invalidates the Searcher object – for
example, during a commit operation.
As you can see, I tend to use the same number of entries for size and initialSize, and
half of those values for autowarmCount. The size and initialSize properties can be
set to the same size in order to avoid the underlying Java object resizing, which consumes
additional processing time.
www.it-ebooks.info
Chapter 1
25
There is one thing you should be aware of. Some of the Solr caches (documentCache
actually) operate on internal identifiers called docid. Those caches cannot be automatically
warmed. That's because docid is changing after every commit operation and thus copying
docid is useless.
Please keep in mind that the settings for the size of the caches is usually good for the
moment you set them. But during the life cycle of your application your data may change,
your queries may change, and your user's behavior may, and probably will change. That's why
you should keep track of the cache usage with the use of Solr administration pages, JMX, or
a specialized software such as Scalable Performance Monitoring from Sematext (see more
at http://sematext.com/spm/index.html), and see how the utilization of each of the
caches changes in time and makes proper changes to the configuration.
There's more...
There are a few additional things that you should know when configuring your caches.
Using a filter cache with faceting
If you use the term enumeration faceting method (parameter facet.method=enum)
Solr will use the filter cache to check each term. Remember that if you use this method,
your filter cache size should have at least the size of the number of unique facet values
in all your faceted fields. This is crucial and you may experience performance loss if this
cache is not configured the right way.
When we have no cache hits
When your Solr instance has a low cache hit ratio you should consider not using caches at all
(to see the hit ratio you can use the administration pages of Solr). Cache insertion is not free
– it costs CPU time and resources. So if you see that you have a very low cache hit ratio, you
should consider turning your caches off – it may speed up your Solr instance. Before you turn
off the caches please ensure that you have the right cache setup – a small hit ratio can be a
result of bad cache configuration.
When we have more "puts" than "gets"
When your Solr instance uses put operations more than get operations you should consider
using the solr.LRUCache implementation. It's confirmed that this implementation behaves
better when there are more insertions into the cache than lookups.
Filter cache
This cache is responsible for holding information about the filters and the documents that
match the filter. Actually this cache holds an unordered set of document IDs that match the
filter. If you don't use the faceting mechanism with a filter cache, you should at least set its
size to the number of unique filters that are present in your queries. This way it will be possible
for Solr to store all the unique filters with their matching document IDs and this will speed up
the queries that use filters.
www.it-ebooks.info
Apache Solr Configuration
26
Query result cache
The query result cache holds the ordered set of internal IDs of documents that match the given
query and the sort specified. That's why if you use caches you should add as many filters as you
can and keep your query (the q parameter) as clean as possible. For example, pass only the
search box content of your search application to the query parameter. If the same query will be
run more than once and the cache has enough capacity to hold the entry, it will be used to give
the IDs of the documents that match the query, thus a no Lucene (Solr uses Lucene to index
and query data that is indexed) query will be made saving the precious I/O operation for the
queries that are not in the cache – this will boost up your Solr instance performance.
The maximum size of this cache that I tend to set is the number of unique queries and their
sorts that are handled by my Solr in the time between the Searcher object's invalidation.
This tends to be enough in most cases.
Document cache
The document cache holds the Lucene documents that were fetched from the index. Basically,
this cache holds the stored fields of all the documents that are gathered from the Solr index.
The size of this cache should always be greater than the number of concurrent queries multiplied
by the maximum results you get from Solr. This cache can't be automatically warmed – that is
because every commit is changing the internal IDs of the documents. Remember that the cache
can be memory consuming in case you have many stored fields, so there will be times when you
just have to live with evictions.
Query result window
The last thing is the query result window. This parameter tells Solr how many documents
to fetch from the index in a single Lucene query. This is a kind of super set of documents
fetched. In our example, we tell Solr that we want the maximum of one hundred documents
as a result of a single query. Our query result window tells Solr to always gather two hundred
documents. Then when we need some more documents that follow the first hundred they
will be fetched from the cache, and therefore we will be saving our resources. The size of the
query result window is mostly dependent on the application and how it is using Solr. If you
tend to do a lot of paging, you should consider using a higher query result window value.
You should remember that the size of caches shown in this task is not
final, and you should adapt them to your application needs. The values and
the method of their calculation should only be taken as a starting point to
further observation and optimization of the process. Also, please remember
to monitor your Solr instance memory usage as using caches will affect the
memory that is used by the JVM.
www.it-ebooks.info
Chapter 1
27
See also
There is another way to warm your caches if you know the most common queries that are sent
to your Solr instance – auto-warming queries. Please refer to the Improving Solr performance
right after a startup or commit operation recipe in Chapter 6, Improving Solr Performance.
For information on how to cache whole pages of results please refer to the Caching whole
result pages recipe in Chapter 6, Improving Solr Performance.
How to fetch and index web pages
There are many ways to index web pages. We could download them, parse them, and index
them with the use of Lucene and Solr. The indexing part is not a problem, at least in most
cases. But there is another problem – how to fetch them? We could possibly create our own
software to do that, but that takes time and resources. That's why this recipe will cover how
to fetch and index web pages using Apache Nutch.
Getting ready
For the purpose of this task we will be using Version 1.5.1 of Apache Nutch. To download the
binary package of Apache Nutch, please go to the download section of http://nutch.
apache.org.
How to do it...
Let's assume that the website we want to fetch and index is http://lucene.apache.org.
1. First of all we need to install Apache Nutch. To do that we just need to extract the
downloaded archive to the directory of our choice; for example, I installed it in the
directory /usr/share/nutch. Of course this is a single server installation and it
doesn't include the Hadoop filesystem, but for the purpose of the recipe it will be
enough. This directory will be referred to as $NUTCH_HOME.
2. Then we'll open the file $NUTCH_HOME/conf/nutch-default.xml and set
the value http.agent.name to the desired name of your crawler (we've taken
SolrCookbookCrawler as a name). It should look like the following code:
<property>
<name>http.agent.name</name>
<value>SolrCookbookCrawler</value>
<description>HTTP 'User-Agent' request header.</description>
</property>
www.it-ebooks.info
Apache Solr Configuration
28
3. Now let's create empty directories called crawl and urls in the $NUTCH_HOME
directory. After that we need to create the seed.txt file inside the created urls
directory with the following contents:
http://lucene.apache.org
4. Now we need to edit the $NUTCH_HOME/conf/crawl-urlfilter.txt file.
Replace the +.at the bottom of the file with +^http://([a-z0-9]*\.)*lucene.
apache.org/. So the appropriate entry should look like the following code:
+^http://([a-z0-9]*\.)*lucene.apache.org/
One last thing before fetching the data is Solr configuration.
5. We start with copying the index structure definition file (called schema-solr4.
xml) from the $NUTCH_HOME/conf/ directory to your Solr installation configuration
directory (which in my case was /usr/share/solr/collection1/conf/).
We also rename the copied file to schema.xml.
We also create an empty stopwords_en.txt file or we use the one provided with Solr
if you want stop words removal.
Now we need to make two corrections to the schema.xml file we've copied:
ff The first one is the correction of the version attribute in the schema tag. We need
to change its value from 1.5.1 to 1.5, so the final schema tag would look like this:
<schema name="nutch" version="1.5.1">
ff Then we change the boost field type (in the same schema.xml file) from string
to float, so the boost field definition would look like this:
<field name="boost" type="float" stored="true" indexed="false"/>
Now we can start crawling and indexing by running the following command from the $NUTCH_
HOME directory:
bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 50
Depending on your Internet connection and your machine configuration you should finally see
a message similar to the following one:
crawl finished: crawl-20120830171434
This means that the crawl is completed and the data was indexed to Solr.
www.it-ebooks.info
Chapter 1
29
How it works...
After installing Nutch and Solr, the first thing we did was set our crawler name. Nutch does
not allow empty names so we must choose one. The file nutch-default.xml defines more
properties than the mentioned ones, but at this time we only need to know about that one.
In the next step, we created two directories; one (crawl) which will hold the crawl data and
the second one (urls) to store the addresses we want to crawl. The contents of the seed.
txt file we created contains addresses we want to crawl, one address per line.
The crawl-urlfilter.txt file contains information about the filters that will be used to
check the URLs that Nutch will crawl. In the example, we told Nutch to accept every URL that
begins with http://lucene.apache.org.
The schema.xml file we copied from the Nutch configuration directory is prepared to be used
when Solr is used for indexing. But the one for Solr 4.0 is a bit buggy, at least in Nutch 1.5.1
distribution, and that's why we needed to make the changes previously mentioned.
We finally came to the point where we ran the Nutch command. We specified that we wanted
to store the crawled data in the crawl directory (first parameter), and the addresses to crawl
data from are in the urls directory (second parameter). The –solr switch lets you specify
the address of the Solr server that will be responsible for the indexing crawled data and is
mandatory if you want to get the data indexed with Solr. We decided to index the data to Solr
installed at the same server. The –depth parameter specifies how deep to go after the links
defined. In our example, we defined that we want a maximum of three links from the main
page. The –topN parameter specifies how many documents will be retrieved from each level,
which we defined as 50.
There's more...
There is one more thing worth knowing when you start a journey in the land of Apache Nutch.
Multiple thread crawling
The crawl command of the Nutch command-line utility has another option – it can
be configured to run crawling with multiple threads. To achieve that you add the
following parameter:
-threads N
So if you would like to crawl with 20 threads you should run the crawl command like sot:
bin/nutch crawl crawl/nutch/site -dir crawl -depth 3 -topN 50 –threads 20
www.it-ebooks.info
Apache Solr Configuration
30
See also
If you seek more information about Apache Nutch please refer to the http://nutch.
apache.org and go to the Wiki section.
How to set up the extracting request
handler
Sometimes indexing prepared text files (such as XML, CSV, JSON, and so on) is not enough.
There are numerous situations where you need to extract data from binary files. For example,
one of my clients wanted to index PDF files – actually their contents. To do that, we either
need to parse the data in some external application or set up Solr to use Apache Tika. This
task will guide you through the process of setting up Apache Tika with Solr.
How to do it...
In order to set up the extracting request handler, we need to follow these simple steps:
1. First let's edit our Solr instance solrconfig.xml and add the following
configuration:
<requestHandler name="/update/extract" class="solr.extraction.
ExtractingRequestHandler" >
<lst name="defaults">
<str name="fmap.content">text</str>
<str name="lowernames">true</str>
<str name="uprefix">attr_</str>
<str name="captureAttr">true</str>
</lst>
</requestHandler>
2. Next create the extract folder anywhere on your system (I created that folder in the
directory where Solr is installed), and place the apache-solr-cell-4.0.0.jar
from the dist directory (you can find it in the Solr distribution archive). After that you
have to copy all the libraries from the contrib/extraction/lib/ directory to the
extract directory you created before.
3. In addition to that, we need the following entries added to the solrconfig.xml file:
<lib dir="../../extract" regex=".*\.jar" />
www.it-ebooks.info
Chapter 1
31
And that's actually all that you need to do in terms of configuration.
To simplify the example, I decided to choose the following index structure (place it in the
fields section in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="text" type="text_general" indexed="true" stored="true"/>
<dynamicField name="attr_*" type="text_general" indexed="true"
stored="true" multiValued="true"/>
To test the indexing process, I've created a PDF file book.pdf using PDFCreator which
contained the following text only: This is a Solr cookbook. To index that file, I've
used the following command:
curl "http://localhost:8983/solr/update/extract?literal.id=1&commit=true"
-F "myfile=@book.pdf"
You should see the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">578</int>
</lst>
</response>
How it works...
Binary file parsing is implemented using the Apache Tika framework. Tika is a toolkit for
detecting and extracting metadata and structured text from various types of documents,
not only binary files but also HTML and XML files. To add a handler that uses Apache Tika,
we need to add a handler based on the solr.extraction.ExtractingRequestHandler
class to our solrconfig.xml file as shown in the example.
In addition to the handler definition, we need to specify where Solr should look for the
additional libraries we placed in the extract directory that we created. The dir attribute
of the lib tag should be pointing to the path of the created directory. The regex attribute
is the regular expression telling Solr which files to load.
www.it-ebooks.info
Apache Solr Configuration
32
Let's now discuss the default configuration parameters. The fmap.content parameter tells
Solr what field content of the parsed document should be extracted. In our case, the parsed
content will go to the field named text. The next parameter lowernames is set to true;
this tells Solr to lower all names that come from Tika and have them lowercased. The next
parameter, uprefix, is very important. It tells Solr how to handle fields that are not defined
in the schema.xml file. The name of the field returned from Tika will be added to the value of
the parameter and sent to Solr. For example, if Tika returned a field named creator, and we
don't have such a field in our index, then Solr would try to index it under a field named attrcreator
which is a dynamic field. The last parameter tells Solr to index Tika XHTML elements
into separate fields named after those elements.
Next we have a command that sends a PDF file to Solr. We are sending a file to the /update/
extract handler with two parameters. First we define a unique identifier. It's useful to
be able to do that during document sending because most of the binary document won't
have an identifier in its contents. To pass the identifier we use the literal.id parameter.
The second parameter we send to Solr is the information to perform the commit right after
document processing.
See also
To see how to index binary files please refer to the Indexing PDF files and Extracting metadata
from binary files recipes in Chapter 2, Indexing Your Data.
Changing the default similarity
implementation
Most of the time, the default way of calculating the score of your documents is what you need.
But sometimes you need more from Solr; that's just the standard behavior. Let's assume that
you would like to change the default behavior and use a different score calculation algorithm
for the description field of your index. The current version of Solr allows you to do that and
this recipe will show you how to leverage this functionality.
Getting ready
Before choosing one of the score calculation algorithms available in Solr, it's good to read
a bit about them. The description of all the algorithms is beyond the scope of the recipe and
the book, but I would suggest going to the Solr Wiki pages (or look at Javadocs) and read the
basic information about available implementations.
www.it-ebooks.info
Chapter 1
33
How to do it...
For the purpose of the recipe let's assume we have the following index structure (just add the
following entries to your schema.xml file to the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_general" indexed="true" stored="true"/>
<field name="description" type="text_general_dfr" indexed="true"
stored="true" />
The string and text_general types are available in the default schema.xml file provided
with the example Solr distribution. But we want DFRSimilarity to be used to calculate the
score for the description field. In order to do that, we introduce a new type, which is defined
as follows (just add the following entries to your schema.xml file to the types section):
<fieldType name="text_general_dfr" class="solr.TextField"
positionIncrementGap="100">
<analyzer type="index">
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="stopwords.txt" enablePositionIncrements="true" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<analyzer type="query">
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="stopwords.txt" enablePositionIncrements="true" />
<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt"
ignoreCase="true" expand="true"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<similarity class="solr.DFRSimilarityFactory">
<str name="basicModel">P</str>
<str name="afterEffect">L</str>
<str name="normalization">H2</str>
<float name="c">7</float>
</similarity>
</fieldType>
Also, to use per-field similarity we have to add the following entry to your schema.xml file:
<similarity class="solr.SchemaSimilarityFactory"/>
And that's all. Now let's have a look and see how that works.
www.it-ebooks.info
Apache Solr Configuration
34
How it works...
The index structure presented in this recipe is pretty simple as there are only three fields.
The one thing we are interested in is that the description field uses our own custom
field type called text_general_dfr.
The thing we are mostly interested in is the new field type definition called text_general_
dfr. As you can see, apart from the index and query analyzer there is an additional section
– similarity. It is responsible for specifying which similarity implementation to use to
calculate the score for a given field. You are probably used to defining field types, filters,
and other things in Solr, so you probably know that the class attribute is responsible for
specifying the class implementing the desired similarity implementation which in our case
is solr.DFRSimilarityFactory. Also, if there is a need, you can specify additional
parameters that configure the behavior of your chosen similarity class. In the previous
example, we've specified four additional parameters: basicModel, afterEffect,
normalization, and c, which all define the DFRSimilarity behavior.
solr.SchemaSimilarityFactory is required to be able to specify the similarity
for each field.
There's more...
In addition to per-field similarity definition, you can also configure the global similarity:
Changing the global similarity
Apart from specifying the similarity class on a per-field basis, you can choose any other
similarity than the default one in a global way. For example, if you would like to use
BM25Similarity as the default one, you should add the following entry to your
schema.xml file:
<similarity class="solr.BM25SimilarityFactory"/>
As well as with the per-field similarity, you need to provide the name of the factory class
that is responsible for creating the appropriate similarity class.
www.it-ebooks.info
2
Indexing Your Data
In this chapter, we will cover:
ff Indexing PDF files
ff Generating unique fields automatically
ff Extracting metadata from binary files
ff How to properly configure Data Import Handler with JDBC
ff Indexing data from a database using Data Import Handler
ff How to import data using Data Import Handler and delta query
ff How to use Data Import Handler with the URL data source
ff How to modify data while importing with Data Import Handler
ff Updating a single field of your document
ff Handling multiple currencies
ff Detecting the document language
ff Optimizing your primary key field indexing
Introduction
Indexing data is one of the most crucial things in every Lucene and Solr deployment. When
your data is not indexed properly your search results will be poor. When the search results
are poor, it's almost certain the users will not be satisfied with the application that uses Solr.
That's why we need our data to be prepared and indexed as well as possible.
www.it-ebooks.info
Indexing Your Data
36
On the other hand, preparing data is not an easy task. Nowadays we have more and more
data floating around. We need to index multiple formats of data from multiple sources.
Do we need to parse the data manually and prepare the data in XML format? The answer is
no – we can let Solr do that for us. This chapter will concentrate on the indexing process and
data preparation beginning from how to index data that is a binary PDF file, teaching how to
use the Data Import Handler to fetch data from database and index it with Apache Solr, and
finally describing how we can detect the document's language during indexing.
Indexing PDF files
Imagine that the library on the corner that we used to go to wants to expand its collection and
make it available for the wider public though the World Wide Web. It asked its book suppliers
to provide sample chapters of all the books in PDF format so they can share it with the online
users. With all the samples provided by the supplier came a problem – how to extract data for
the search box from more than 900 thousand PDF files. Solr can do it with the use of Apache
Tika. This recipe will show you how to handle such a task.
Getting ready
Before you start getting deeper into the task, please refer to the How to set up the extracting
request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through
the process of configuring Solr to use Apache Tika. We will use the same index structure
and Solr configuration presented in that recipe, and I assume you already have Solr properly
configured (according to the mentioned recipe) and ready to work.
How to do it...
To test the indexing process I've created a PDF file book.pdf using PDFCreator
(http://sourceforge.net/projects/pdfcreator/) which contained the
following text only: This is a Solr cookbook.. To index that file I've used the
following command:
curl "http://localhost:8983/solr/update/extract?literal.id=1&commit=true"
-F "myfile=@cookbook.pdf"
You should then see the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">578</int>
</lst>
</response>
www.it-ebooks.info
Chapter 2
37
To see what was indexed I've run the following within a web browser:
http://localhost:8983/solr/select/?q=text:solr
In return I've got:
<?xml version="1.0" encoding="UTF-8"?>
<response>
…
<result name="response" numFound="1" start="0">
<doc>
<arr name="attr_created"><str>Thu Oct 21 16:11:51 CEST 2010</
str></arr>
<arr name="attr_creator"><str>PDFCreator Version 1.0.1</str></arr>
<arr name="attr_producer"><str>GPL Ghostscript 8.71</str></arr>
<arr name="attr_stream_content_type"><str>application/octetstream</
str></arr>
<arr name="attr_stream_name"><str>cookbook.pdf</str></arr>
<arr name="attr_stream_size"><str>3209</str></arr>
<arr name="attr_stream_source_info"><str>myfile</str></arr>
<str name="author">Gr0</str>
<arr name="content_type"><str>application/pdf</str></arr>
<str name="id">1</str>
<str name="keywords"/>
<date name="last_modified">2010-10-21T14:11:51Z</date>
<str name="subject"/>
<arr name="title"><str>cookbook</str></arr>
</doc>
</result>
</response>
How it works...
The curl command we used sends a PDF file to Solr. We are sending a file to the /update/
extract handler along with two parameters. It's useful to be able to do that during document
sending because most of the binary documents won't have an identifier in its contents. To
pass the identifier we use the literal.id parameter. The second parameter we send asks
Solr to perform the commit operation right after document processing.
The test file I've created, for the purpose of the recipe, contained a simple sentence:
"This is a Solr cookbook".
Remember the contents of the PDF file I created? It contained the word "Solr". That's why
I asked Solr to give me documents which contain the word "Solr" in a field named text.
www.it-ebooks.info
Indexing Your Data
38
In response, I got one document which matched the given query. To simplify the example,
I removed the response header part. As you can see in the response there were a few fields
that were indexed dynamically – their names start with attr_. Those fields contained
information about the file such as the size, the application that created it, and so on.
As we can see, we have our identifier indexed as we wished, and some other fields that
were present in the schema.xmlfile that Apache Tika could parse and return to Solr.
Generating unique fields automatically
Imagine you have an application that crawls the web and index documents found during
that crawl. The problem is that for some particular reason you can't set the document
identifier during indexing, and you would like Solr to generate one for you. This recipe
will help you, if you faced a similar problem.
How to do it...
The following steps will help you to generate unique fields automatically:
1. First let's create our index structure by adding the following entries to the schema.
xmlfields section:
<field name="id" type="uuid" indexed="true" stored="true"
default="NEW" multiValued="false"/>
<field name="name" type="text_general" indexed="true"
stored="true"/>
<field name="text" type="text_general" indexed="true"
stored="true"/>
2. In addition to that, we need to define the uuid field type by adding the following entry
to the types section of our schema.xml file:
<fieldType name="uuid" class="solr.UUIDField" indexed="true" />
3. In addition to that, we must remove the unique field definition, because Solr doesn't
allow using a unique field with the default="NEW" configuration, so the following
needs to be removed:
<uniqueKey>id</uniqueKey>
4. And now, let's try to index a document without an id field, for example one like this:
<add>
<doc>
<field name="name">Test name</field>
<field name="text">Test text contents</field>
</doc>
</add>
www.it-ebooks.info
Chapter 2
39
In order to see if Solr generated an identifier for the document, let's run the following query:
http://localhost:8983/solr/select?q=*:*&indent=true
The response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="name">Test name</str>
<str name="text">Test text contents</str>
<str name="id">b6f17c35-e5ad-4a09-b799-71580ca6be8a</str>
</doc>
</result>
</response>
As you can see in the response, our document had one additional field we didn't add manually
– the id field, which is what we wanted to have.
How it works...
The idea is quite simple – we let Solr generate the id field for us. To do that, we defined
the id field to be based on the uuid field type, and to have a default value of new
(default="NEW"). By doing this we tell Solr that we want that kind of behavior. If you
look at the uuid field type, you can see that it is a simple type definition based on solr.
UUIDField. Nothing complicated.
Having your document's identifiers generated automatically is handy in some cases, but it
also comes with some restrictions from Solr and its components. One of the issues is that
you can't have the unique field defined, and because of that, the elevation component won't
work. Of course that's only an example. But if your application doesn't know the identifiers of
the documents and can't generate them, then using solr.UUIDField is one of the ways of
having document identifiers for your indexed documents.
www.it-ebooks.info
Indexing Your Data
40
Extracting metadata from binary files
Suppose that our current client has a video and music store. Not the e-commerce one,
just the regular one – just around the corner. And now he wants to expand his business
to e-commerce. He wants to sell the products online. But his IT department said that
this will be tricky – because they need to hire someone to fill up the database with the
product names and their metadata. And that is the place where you come in and tell
them that you can extract titles and authors from the MP3 files that are available as
samples.Now let's see how that can be achieved.
Getting ready
Before you start getting deeper into the task, please refer to the How to set up the extracting
request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through
the process of configuring Solr to use Apache Tika.
How to do it...
1. Let's start by defining an index structure in the file schema.xml. The field definition
section should look like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="author" type="string" indexed="true" stored="true"
multiValued="true"/>
<field name="title" type="text" indexed="true" stored="true"/>
<dynamicField name="ignored_*" type="string" indexed="false"
stored="false" multiValued="true"/>
2. Now let's get the solrconfig.xml file ready:
<requestHandler name="/update/extract" class="solr.extraction.
ExtractingRequestHandler">
<lst name="defaults">
<str name="lowernames">true</str>
<str name="uprefix">ignored_</str>
<str name="captureAttr">true</str>
</lst>
</requestHandler>
3. Now we can start sending the documents to Solr. To do that, let's run the
following command:
curl "http://localhost:8983/solr/update/extract?literal.
id=1&commit=true" -F "myfile=@sample.mp3"
www.it-ebooks.info
Chapter 2
41
4. Let's check how the document was indexed. To do that type a query like the following
to your web browser:
http://localhost:8983/solr/select/?q=title:207
As a result I've got the following document:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">title:207</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="author">Armin Van Buuren</str>
<str name="id">1</str>
<str name="title">Desiderium 207 (Feat Susana)</str>
</doc>
</result>
</response>
So it seems that everything went well.
How it works...
First we define an index structure that will suit our needs. I decided that besides the unique
ID, I need to store the title and author name. We also defined a dynamic field called ignored
to handle the data we don't want to index (not indexed and not stored).
The next step is to define a new request handler to handle our updates, as you already
know. We also added a few default parameters to define our handler behavior. In our case
the parameter uprefix tells Solr to index all unknown fields to the dynamic field whose
name begins with ignored_, thus the additional data will not be visible in the index.
The last parameter tells Solr to index Tika XHTML elements into separate fields named
after those elements.
Next we have a command that sends an MP3 file to Solr. We are sending a file to the /
update/extract handler with two parameters. First we define a unique identifier and
pass that identifier to Solr using the literal.id parameter. The second parameter we
send to Solr is information to perform a commit right after document processing.
www.it-ebooks.info
Indexing Your Data
42
The query is a simple one, so I'll skip commenting on this part.
The last listing is an XML with Solr response. As you can see, there are only fields that are
explicitly defined in schema.xml – no dynamic fields. Solr and Tika managed to extract the
name and author of the file.
See also
ff If you want to index other types of binary files please refer to the Indexing PDF files
recipe in this chapter.
How to properly configure Data Import
Handler with JDBC
One of our clients is having a problem. His database of users grew to such size that even
the simple SQL select statement is taking too much time, and he seeks how to improve the
search time. Of course he heard about Solr but he doesn't want to generate XML or any other
data format and push it to Solr; he would like the data to be fetched. What can we do about
it? Well there is one thing – we can use one of the contribute modules of Solr, Data Import
Handler. This task will show you how to configure the basic setup of Data Import Handler and
how to use it.
How to do it...
1. First of all, copy the appropriate libraries that are required to use Data Import
Handler. So, let's create the dih folder anywhere on your system (I created it in the
directory where Solr is installed), and place apache-solr-dataimporthandler-
4.0.0.jar and apache-solr-dataimporthandler-extras-4.0.0.jar from
the Solr distribution dist directory in the folder. In addition to that, we need the
following entry to be added to the solrconfig.xml file:
<lib dir="../../dih" regex=".*\.jar" />
2. Next we need to modify the solrconfig.xml file. You should add an entry like the
following code:
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</requestHandler>
www.it-ebooks.info
Chapter 2
43
3. Now we will create the db-data-config.xml file that is responsible for the Data
Import Handler configuration. It should have contents like the following example:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<document>
<entity name="user" query="SELECT user_id, user_name from
users">
<field column="user_id" name="id" />
<field column="user_name" name="name" />
<entity name="user_desc" query="select desc from users_
description where user_id=${user.user_id}">
<field column="description" name="description" />
</entity>
</entity>
</document>
</dataConfig></dataConfig>
If you want to use other database engines, please change the driver, url,
and user and password attributes.
4. Now, let's create a sample index structure. To do that we need to modify the fields
section of the schema.xml file to something like the following snippet:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="text" indexed="true" stored="true" />
<field name="user_desc" type="text" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true"
stored="true"/>
5. One more thing before the indexing – you should copy an appropriate JDBC driver
to the lib directory of your Solr installation or the dih directory we created before.
You can get the library for PostgreSQL here http://jdbc.postgresql.org/
download.html.
6. Now we can start indexing. Run the following query to Solr:
http://localhost:8983/solr/dataimport?command=full-import
As you may know, the HTTP protocol is asynchronous, and thus you won't be updated on how
the process of indexing is going. To check the status of the indexing process, you can run the
command once again.
And that's how we configure Data Import Handler.
www.it-ebooks.info
Indexing Your Data
44
How it works...
First we have a solrconfig.xml part which actually defines a new request handler, Data
Import Handler, to be used by Solr. The <str name="config"> XML tag specifies the name
of the Data Import Handler configuration file.
The second listing is the actual configuration of Data Import Handler. I used the JDBC
source connection sample to illustrate how to configure Data Import Handler. The contents
of this configuration file start with the root tag named dataConfig which is followed by
a second tag defining a data source and named dataSource. In the example, I used the
PostgreSQL database and thus the JDBC driver is org.postgresql.Driver. We also
define the database connection URL (attribute named url), and the database credentials
(attributes user and password).
Next we have a document definition – a tag named document. This is the section containing
information about the document that will be sent to Solr. The document definition is made of
database queries – the entities.
The entity is defined by a name (the name attribute) and a SQL query (the query attribute).
The entity name can be used to reference values in sub-queries – you can see an example
of such a behavior in the second entity named user_desc. As you may already have noticed,
entities can be nested to handle sub-queries. The SQL query is there to fetch the data from
the database and use it to fill the entity variables which will be indexed.
After the entity comes the mapping definition. There is a single field tag for every column
returned by a query, but that is not a must – Data Import Handler can guess what the
mapping is (for example, where the entity field name matches the column name), but I
tend to use mappings because I find it easier to maintain. But let's get back to fields. The
field tag is defined by two attributes: column which is the column name returned by a
query, and name which is the field to which the data will be written.
Next we have a Solr query to start the indexing process. There are actually five commands
that can be run:
ff /dataimport: This will return the actual status.
ff /dataimport?command=full-import: This command will start the full import
process. Remember that the default behavior is to delete the index contents at
the beginning.
ff /dataimport?command=delta-import: This command will start the
incremental indexing process.
ff /dataimport?command=reload-config: This command will force
a configuration reload.
ff /dataimport?command=abort: This command will stop the indexing process.
www.it-ebooks.info
Chapter 2
45
There's more...
If you don't want to delete the index contents at the start of the full indexing using Data
Import Handler, add the clean=false parameter to your query. An example query should
look like this:
http://localhost:8983/solr/data?command=full-import&clean=false
Indexing data from a database using Data
Import Handler
Let's assume that we want to index the Wikipedia data, and we don't want to parse the whole
Wikipedia data and make another XML file. Instead we asked our DB expert to import the
data dump information from the PostgreSQL database, so we could fetch that data. Did I say
fetch? Yes it is possible – with the use of Data Import Handler and JDBC data source. This
task will guide you through how to do it.
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter
to get to know the basics about how Data Import Handler is configured. I'll assume that
you already have Solr set up according to the instructions available in the mentioned recipe.
How to do it...
The Wikipedia data I used in this example is available under the Wikipedia downloads
page at http://download.wikimedia.org/.
1. First let's add a sample index structure. To do that we need to modify the fields
section of the schema.xml file so it looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="string" indexed="true" stored="true"/>
<field name="revision_id" type="string" indexed="true"
stored="true"/>
<field name="contents" type="text" indexed="true" stored="true"/>
www.it-ebooks.info
Indexing Your Data
46
2. The next step is to add the request handler definition to the solrconfig.xml file,
like so:
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</requestHandler>
3. Now we have to add a db-data-config.xml file to the conf directory of your Solr
instance (or core):
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/wikipedia" user="wikipedia"
password="secret" />
<document>
<entity name="page" query="SELECT page_id, page_title from
page">
<field column="page_id" name="id" />
<field column="page_title" name="name" />
<entity name="revision" query="select rev_id from revision
where rev_page=${page.page_id}">
<field column="rev_id" name="revision_id" />
<entity name="pagecontent" query="select old_text from
pagecontent where old_id=${revision.rev_id}">
<field column="old_text" name="contents" />
</entity>
</entity>
</entity>
</document>
</dataConfig>
4. Now let's start indexing. Type the following URL into your browser:
http://localhost:8983/solr/dataimport?command=full-import
5. Let's check the indexing status during import. To do that we run the following query:
http://localhost:8983/solr/dataimport
Solr will show us a response like the following reponse:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
</lst>
<lst name="initArgs">
<lst name="defaults">
www.it-ebooks.info
Chapter 2
47
<str name="config">db-data-config.xml</str>
</lst>
</lst>
<str name="status">busy</str>
<str name="importResponse">A command is still running...</str>
<lst name="statusMessages">
<str name="Time Elapsed">0:1:15.460</str>
<str name="Total Requests made to DataSource">39547</str>
<str name="Total Rows Fetched">59319</str>
<str name="Total Documents Processed">19772</str>
<str name="Total Documents Skipped">0</str>
<str name="Full Dump Started">2010-10-25 14:28:00</str>
</lst>
<str name="WARNING">This response format is experimental.
It is likely to change in the future.</str>
</response>
6. Running the same query after the importing process is done should result in a
response like the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
</lst>
<lst name="initArgs">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</lst>
<str name="status">idle</str>
<str name="importResponse"/>
<lst name="statusMessages">
<str name="Total Requests made to DataSource">2118645</str>
<str name="Total Rows Fetched">3177966</str>
<str name="Total Documents Skipped">0</str>
<str name="Full Dump Started">2010-10-25 14:28:00</str>
<str name="">Indexing completed. Added/Updated: 1059322
documents. Deleted 0 documents.</str>
<str name="Committed">2010-10-25 14:55:20</str>
<str name="Optimized">2010-10-25 14:55:20</str>
<str name="Total Documents Processed">1059322</str>
<str name="Time taken ">0:27:20.325</str>
</lst>
<str name="WARNING">This response format is experimental.
It is likely to change in the future.</str>
</response>
www.it-ebooks.info
Indexing Your Data
48
How it works...
To illustrate how Data Import Handler works, I decided to index the Polish Wikipedia data.
I decided to store four fields: page identifier, page name, page revision number, and its
contents. The field definition part is fairly simple so I decided to skip commenting on this.
The request handler definition, the Data Import Handler configuration, and command queries
were discussed in the How to properly configure Data Import Handler with JDBC recipe in this
chapter. The portions of interest in this task are in the db-data-config.xml file.
As you can see, we have three entities defined. The first entity gathers data from the page
table and maps two of the columns to the index fields. The next entity is nested inside the
first one and gathers the revision identifier from the table revision with the appropriate
condition. The revision identifier is then mapped to the index field. The last entity is nested
inside the second and gathers data from the pagecontent table again with the appropriate
condition. And again, the returned column is mapped to the index field.
We have the response which shows us that the import is still running (the listing with <str
name="importResponse">A command is still running...</str>). As you can
see there is information about how many data rows were fetched, how many requests to
the database were made, how many Solr documents were processed, and how many were
deleted. There is also information about the start of the indexing process. One thing you
should be aware of: this response can change in the next versions of Solr and Data
Import Handler.
The last listing shows us the summary of the indexing process.
How to import data using Data Import
Handler and delta query
Do you remember the task with the users import from the recipe named How to properly
configure Data Import Handler? We imported all the users from our client database but it took
ages – about two weeks. Our client is very happy with the results. His database is now not
used for searching but only updating. And yes, that is the problem for us – how do we update
data in the index? We can't fetch the whole data every time – it took two weeks. What we can
do is an incremental import which will modify only the data that has changed since the last
import. This task will show you how to do that.
www.it-ebooks.info
Chapter 2
49
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter to
get to know the basics of the Data Import Handler configuration. I assume that Solr is set up
according to the description given in the mentioned recipe.
How to do it...
1. The first thing you should do is add an additional column to the tables you use. So
in our case let's assume that we added a column named last_modified (which
should be a timestamp-based column). Now our db-data-config.xml will look
like the following code:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<document>
<entity name="user" query="SELECT user_id, user_name FROM users"
deltaImportQuery="select user_id, user_name FROM users WHERE user_
id = '${dataimporter.delta.user_id}'"deltaQuery="select user_id
FROM users WHERE last_modified &gt; '${dataimporter.last_index_
time}'">
<field column="user_id" name="id" />
<field column="user_name" name="name" />
<entity name="user_desc" query="select description from
users_description where user_id=${user.user_id}">
<field column="description" name="description" />
</entity>
</entity>
</document>
</dataConfig></dataConfig>
2. After that we run a new kind of query to start delta import:
http://localhost:8983/solr/dataimport?command=delta-import
How it works...
First we modified our database table to include a column named last_modified. We need
to ensure that the column will be modified at the same time as the table is. Solr will not
modify the database, so you have to ensure that your application will do that.
www.it-ebooks.info
Indexing Your Data
50
When running a delta import, Data Import Handler will create a file named dataimport.
properties inside a Solr configuration directory. In that file, the last index time will be
stored as a timestamp. This timestamp will be later used to distinguish whether the data
was changed or not. It can be used in a query by using a special variable: ${dataimporter.
last_index_time}.
You may have already noticed the two differences – two additional attributes defining an
entity named user – deltaQuery and deltaImportQuery. The first one is responsible
for getting the information about which users were modified since the last index. Actually
it only gets the user's unique identifiers. It uses the last_modified field to determine
which users were modified since the last import. Then the second query is executed –
deltaImportQuery. This query gets users with the appropriate unique identifier, to get
all the data which we want to index. One thing worth noticing is the way that I used the user
identifier in deltaImportQuery. I used the delta variable with its user_id (the same
name as the table column name) variable to get it: ${dataimporter.delta.user_id}.
You may have noticed that I left the query attribute in the entity definition. It's left on
purpose; you may need to index the entire data once again, so that configuration will
be useful for full imports as well as for the partial ones.
Next we have a query that shows how to run the delta import. You may have noticed that
compared to the full import, we didn't use the full-import command – we've sent the
delta-import command.
The statuses that are returned by Solr are the same as with the full import, so please refer to
the appropriate chapters to see what information they carry.
One more thing – the delta queries are only supported for the default SqlEntityProcessor
class. This means that you can only use those queries with JDBC data sources.
How to use Data Import Handler with the
URL data source
Do you remember the first example with the Wikipedia data? We asked our fellow DB expert
to import the data dump into PostgreSQL and we fetched the data from there. But what if our
colleague is sick and can't help us, and we need to import that data? We can parse the data
and send it to Solr, but that's not an option – we don't have much time to do that. So what to
do? Yes, you guessed – we can use Data Import Handler and one of its data sources, file data
source. This task will show you how to do that.
www.it-ebooks.info
Chapter 2
51
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter
to get to know the basics of the Data Import Handler configuration. I assume that Solr is
set up according to the description given in the mentioned recipe.
How to do it...
Let's take a look at our data source. To be consistent, I chose to index the Wikipedia data,
which you should already be familiar with.
1. First of all, the index structure. Our field definition part of schema.xml should look
like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="string" indexed="true" stored="true"/>
<field name="revision_id" type="string" indexed="true"
stored="true"/>
<field name="contents" type="text" indexed="true" stored="true"/>
2. The next step is to define a Data Import Handler request handler (put that definition
in the solrconfig.xml file):
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">data-config.xml</str>
</lst>
</requestHandler>
3. And now the data-config.xml file:
<dataConfig>
<dataSource type="FileDataSource" encoding="UTF-8" />
<document>
<entity name="page" processor="XPathEntityProcessor"
stream="true" forEach="/mediawiki/page/" url="/solrcookbook/data/
enwiki-20120802-pages-articles.xml"transformer="RegexTransformer">
<field column="id" xpath="/mediawiki/page/id" />
<field column="name" xpath="/mediawiki/page/title" />
<field column="revision_id" xpath="/mediawiki/page/revision/id"
/>
<field column="contents" xpath="/mediawiki/page/revision/text"
/>
<field column="$skipDoc" regex="^#REDIRECT .*"
replaceWith="true" sourceColName="contents"/>
www.it-ebooks.info
Indexing Your Data
52
</entity>
</document>
</dataConfig>
4. Now let's start indexing by sending the following query to Solr:
http://localhost:8983/solr/dataimport?command=full-import
After the import is done, we will have the data indexed.
How it works...
The Wikipedia data I used in this example is available under the Wikipedia downloads page
at http://download.wikimedia.org/enwiki/. I've chosen the pages-articles.
xml.bz2 file (actually it was named enwiki-20120802-pages-articles.xml.bz2)
which is about 6 GB. We only want to index some of the data from the file: page identifier,
name, revision, and page contents. I also wanted to skip articles that are only linking to
other articles in Wikipedia.
The field definition part of the schema.xml file is fairly simple and contains only four fields
and there is nothing unusual within it, so I'll skip commenting on it.
The solrconfig.xml file contains the handler definition with the information about the
Data Import Handler configuration filename.
Next we have the data-config.xml file where the actual configuration is written. We
have a new data source type here named FileDataSource. This data source will read the
data from a local directory. You can use HttpDataSource if you want to read data from
an outer location. The XML tag defining the data source also specifies the file encoding
(the encoding attribute) and in our example it's UTF-8. Next we have an entity definition,
which has a name under which it will be visible, a processor which will process our data. The
processor attribute is only mandatory when not using a database source. This value must
be set to XPathEntityProcessor in our case. The stream attribute, which is set to true,
informs Data Import Handler to stream the data from the file which is a must in our case
when the data is large. Following that we have a forEach attribute which specifies an XPath
expression – this path will be iterated over. There is a location of the data file defined in the
url attribute and a transformer defined in the transformer attribute. A transformer is a
mechanism that will transform every row of data and process it before sending it to Solr.
Under the entity definition we have field mapping definitions. We have columns which are
the same as the index field names thus I skipped the name field. There is one additional
attribute named xpath in the mapping definitions. It specifies the XPath expression that
defines where the data is located in the XML file. If you are not familiar with XPath please
refer to the http://www.w3schools.com/xpath/default.asp tutorial.
www.it-ebooks.info
Chapter 2
53
We also have a special column named $skipDoc. It tells Solr which documents to skip (if
the value of the column is true then Solr will skip the document). The column is defined
by a regular expression (attribute regex), a column to which the regular expression applies
(attribute sourceColName), and the value that will replace all the occurrences of the given
regular expression (replaceWith attribute). If the regular expression matches (in this case,
if the data in the column specified by the sourceColName attribute starts with #REDIRECT),
then the $skipDoc column will be set to true and thus the document will be skipped.
The actual indexing time was more than four hours on my machine, so if you try to index the
sample Wikipedia data please take that into consideration.
How to modify data while importing with
Data Import Handler
After we indexed the users and made the indexing incremental (the How to properly configure
Data Import Handler and How to import data using Data Import Handler and delta query
recipes), we were asked if we could modify the data a bit. Actually it would be perfect if we could
split name and surname into two fields in the index while those two reside in a single column in
the database. And of course, updating the database is not an option (trust me – it almost never
is). Can we do that? Of course we can, we just need to add some more configuration details in
Data Import Handler and use a transformer. This task will show you how to do that.
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter to
get to know the basics about the Data Import Handler configuration. Also, to be able to run
examples in this chapter, you need to run Solr in the servlet container run on Java 6 or later. I
assume that Solr is set up according to the description given in the mentioned recipe.
How to do it...
Let's assume that we have a database table. To select users from our table we use the
following SQL query:
SELECT user_id, user_name, description FROM users
The response may look like this:
| user_id | user_name | description |
| 1 | John Kowalski | superuser |
| 2 | Amanda Looks | user |
www.it-ebooks.info
Indexing Your Data
54
Our task is to split the name from the surname and place it in two fields: name and surname.
1. First of all change the index structure, so our field definition part of schema.xml
should look like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="firstname" type="string" indexed="true"
stored="true"/>
<field name="surname" type="string" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true"
stored="true"/>
2. Now we have to add a db-data-config.xml file:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<script><![CDATA[
function splitName(row) {
var nameTable = row.get('user_name').split(' ');
row.put('firstname', nameTable[0]);
row.put('surname', nameTable[1]);
row.remove('name');
return row;
}
]]></script>
<document>
<entity name="user" transformer="script:splitName" query="SELECT
user_id, user_name, description from users">
<field column="user_id" name="id" />
<field column="firstname" />
<field column="surname" />
<field column="description" />
</entity>
</document>
</dataConfig>
3. And now you can follow the normal indexing procedure which was discussed in the
How to properly configure Data Import Handler recipe in this chapter.
www.it-ebooks.info
Chapter 2
55
How it works...
The first two listings are the sample SQL query and the result given by a database. Next we
have a field definition part of the schema.xml file which defines four fields. Look at the
example database rows once again. See the difference? We have four fields in our index while
our database rows have only three columns. We must split the contents of the user_name
column into two index fields: firstname and surname. To do that, we will use JavaScript
language and the script transformer functionality of Data Import Handler.
The solrconfig.xml file is the same as the one discussed in the How to properly configure
Data Import Handler recipe in this chapter, so I'll skip that as well.
Next we have the updated contents of the db-data-config.xml file which we use to define
the behavior of Data Import Handler. The first and the biggest difference is the script tag
that will be holding our scripts that parse the data. The scripts should be held in the CDATA
section. I defined a simple function called splitName that takes one parameter, database row
(remember that the functions that operate on entity data should always take one parameter).
The first thing in the function is getting the contents of the user_name column, split it with the
space character, and assign it into a JavaScript table. Then we create two additional columns
in the processed row – firstname and surname. The contents of those rows come from the
JavaScript table we created. Then we remove the user_name column because we don't want it
to be indexed. The last operation is the returning of the processed row.
To enable script processing you must add one additional attribute to the entity definition – the
transformer attribute with the contents such as script:functionName. In our example,
it looks like this: transformer:"script:splitName". It tells Data Import Handler to use
the defined function name for every row returned by the query.
And that's how it works. The rest is the usual indexing process described in the How to
properly configure Data Import Handler task in this chapter.
There's more...
If you want to use a different language other than JavaScript, then you have to specify it in the
language attribute of the <script> tag. Just remember that the scripting language that you
want to use must be supported by Java 6. The example definition would look as follows:
<script language="ECMAScript">…</script>
www.it-ebooks.info
Indexing Your Data
56
Updating a single field of your document
Imagine that you have a system where you store a document your users upload. In addition
to that, your users can add other users to have access to the files they uploaded. As you
probably know, before Solr 4.0, when you wanted to update a single field in a document
you had to re-index the whole document. Solr 4.0 allows you to update a single field if
you fulfill some basic requirements. So let's see how we can do that in Solr 4.0.
How to do it...
For the purpose of the recipe, let's assume we have the following index structure
(put the following entries to your schema.xml file's fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="file" type="text_general" indexed="true" stored="true"/>
<field name="user" type="string" indexed="true" stored="true"
multiValued="true" />
In addition to that, we need the _version_ field:
<field name="_version_" type="long" indexed="true" stored="true"/>
And that's all when it comes to the schema.xml file. In addition to that, let's assume
we have the following data indexed:
<add>
<doc>
<field name="id">1</field>
<field name="file">Sample file</field>
<field name="user">gro</field>
<field name="user">negativ</field>
</doc>
</add>
So, we have a sample file and two user names specifying which users of our system can
access that file. But what if we would like to add another user called jack. Is that possible?
Yes, with Solr 4.0 it is. To add the value to a field which has multiple values, we should send
the following command:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","user":{"add":"jack"}}]'
Let's see if it worked by sending the following query:
http://localhost:8983/solr/select?q=*:*&indent=true
www.it-ebooks.info
Chapter 2
57
The response sent by Solr was as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="file">Sample file</str>
<arr name="user">
<str>gro</str>
<str>negativ</str>
<str>jack</str>
</arr>
<long name="_version_">1411121765349851136</long></doc>
</result>
</response>
As you can see it worked without any problems. Imagine that now one of the users changed
the name of the document, and we would also like to update the file field of that document
to match that change. In order to do so, we should send the following command:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","file":{"set":"New file name"}}]'
And again, we send the same query as before to see if the command succeeded:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
www.it-ebooks.info
Indexing Your Data
58
<str name="id">1</str>
<str name="file">New file name</str>
<arr name="user">
<str>gro</str>
<str>negativ</str>
<str>jack</str>
</arr>
<long name="_version_">1411121902871642112</long></doc>
</result>
</response>
It worked again. So now let's see how Solr does that.
How it works...
As you can see the index structure is pretty simple; we have a document identifier, its name,
and users that can access that file. As you can see all the fields in the index are marked as
stored (stored="true"). This is required for the partial update functionality to work. This
is because, under the hood, Solr takes all the values from the fields and updates the one we
mentioned to be updated. So it is just a typical document indexing, but instead of you having
to provide all the information, it's Solr's responsibility to get it from the index.
Another thing that is required for the partial update functionality to work is the _version_
field. You don't have to set it during indexing, it is used internally by Solr. The example data
we are indexing is also very simple. It is a single document with two users defined.
[{"id":"1","user":{"add":"jack"}}]
The interesting stuff comes with the update command. As you can see, that command
is run against a standard update handler you run indexing against. The commit=true
parameter tells Solr to perform the commit operation right after update. The -H 'Contenttype:
application/json' part is responsible for setting the correct HTTP headers for the
update request. Next we have the request contents itself. It is sent as a JSON object.
We specified that we are interested in the document with the identifier "1" ("id":"1").
We want to change the user field and we want to add the jack value to that field (the
add command). So as you can see, the add command is used when we want to add a
new value to a field which can hold multiple values.
The second command shown as an example shows how to change the value of a
single-valued field. It is very similar to what we had before, but instead of using the
add command, we use the set command. And again, as you can see, it worked perfectly.
www.it-ebooks.info
Chapter 2
59
Handling multiple currencies
Imagine a situation where you run an e-commerce site and you sell your products all over the
world. One day you say that you would like to calculate the currencies by yourself and have
all the goodies that Solr gives you on all the currencies you support. You could of course add
multiple fields, one for each currency. On the other hand, you can use the new functionality
introduced in Solr 3.6 and create a field that will use the provided currency exchange rates.
How to do it...
This recipe will show you how to configure and use multiple currencies using a single field
in the index:
1. Let's start with creating a sample index structure, by modifying the fields section
in your schema.xml file so it looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_general" indexed="true"
stored="true" />
<field name="price" type="currencyField" indexed="true"
stored="true" />
2. In addition to that, we need to provide the definition for the type the price field is
based on (add the following entry to the types section in your schema.xml file):
<fieldType class="solr.CurrencyField" name="currencyField"
defaultCurrency="USD" currencyConfig="currencyExchange.xml" />
3. Another file that we need to create is the currencyExchange.xml file, which should
be placed in the conf directory of your collection and have the following contents:
<currencyConfig version="1.0">
<rates>
<rate from="USD" to="EUR" rate="0.743676" comment="European
Euro" />
<rate from="USD" to="HKD" rate="7.801922" comment="HONG KONG
Dollar" />
<rate from="USD" to="GBP" rate="0.647910" comment="UNITED
KINGDOM Pound" />
</rates>
</currencyConfig>
www.it-ebooks.info
Indexing Your Data
60
4. Now we can index some example data. For this recipe, I decided to index the
following documents:
<add>
<doc>
<field name="id">1</field>
<field name="name">Test document one</field>
<field name="price">10.10,USD</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Test document two</field>
<field name="price">12.01,USD</field>
</doc>
</add>
5. Let's now check if that works. Our second document costs 12.01 USD and we have
defined the exchange rate for European Euro as 0.743676. This gives us about 7.50
EUR for the first document and about 8.90 EUR for the second one. Let's check that
by sending the following query to Solr:
http://localhost:8983/solr/select?q=name:document&fq=price:[8.00,E
UR TO 9.00,EUR]
6. The result returned by Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price:[8.00,EUR TO 9.00,EUR]</str>
<str name="q">name:document</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Test document two</str>
<str name="price">12.01,USD</str></doc>
</result>
</response>
As you can see, we got the document we wanted.
www.it-ebooks.info
Chapter 2
61
How it works...
The idea behind the functionality is simple – we create a field based on a certain type and
we provide a file with a currency exchange rate, and that's all. After that we can query our
Solr instance with the use of all the currencies we defined exchange rates for. But now, let's
discuss all the previous configuration changes in detail.
The index structure is very simple; it contains three fields of which one is responsible for
holding the price of the document and is based on the currencyField type. This type is
based on solr.CurrencyField. Its defaultCurrency attribute specifies the default
currency for all the fields using this type. This is important, because Solr will return prices
in the defined default currency, no matter what currency is used during the query. The
currencyConfig attribute specifies the name of the file with the exchange rate definition.
Our currencyExchange.xml file provides exchange rate for three currencies:
ff EUR
ff HKD
ff GBP
The file should be structured similar to the example one previously shown. This means
that each exchange rate should have the from attribute telling Solr from which currency
the exchange will be done, the to attribute specifying to which currency the exchange will
be done, and the rate attribute specifying the actual exchange rate. In addition to that,
it can also have the comment attribute if we want to include some short comment.
During indexing, we need to specify the currency we want the data to be indexed with. In
the previous example, we indexed data with USD. This is done by specifying the price, a
colon character, and the currency code after it. So 10.10,USD will mean ten dollars
and ten cents in USD.
The last thing is the query. As you can see, you can query Solr with different currencies from
the one used during indexing. This is possible because of the provided exchange rates file.
As you can see, when we use a range query for a price field, we specify the value, the colon
character, and the currency code after it. Please remember that if you provide a currency
code unknown to Solr, it will throw an exception saying that the currency is not known.
There's more...
You can also have the exchange rates being updated automatically by specifying the
currency provider.
www.it-ebooks.info
Indexing Your Data
62
Setting up your own currency provider
Specifying the currency exchange rate file is great, but we need to update that file because
the exchange rates change constantly. Luckily for us, Solr committers thought about it and
gave us the option to provide an exchange rate provider instead of a plain file. The provider
is a class responsible for providing the exchange rate data. The default exchange rate provider
available in Solr uses exchange rates from http://openexchangerates.org, which are
updated hourly. In order to use it, we need to modify our currencyField field
type definition and introduce three new properties (and remove the currencyConfig one):
ff providerClass: This class implements the exchange rates provider,
which in our case will be the default one available in Solr – solr.
OpenExchangeRatesOrgProvider
ff refreshInterval: This determines how often to refresh the rates
(specified in minutes)
ff ratesFileLocation: This determines the location of the file with rates in open
exchange format
So the final configuration should look like the following snippet:
<fieldType name="currencyField" class="solr.CurrencyField"
providerClass="solr.OpenExchangeRatesOrgProvider"
refreshInterval="120" ratesFileLocation="http://192.168.10.10/latest.
json"/>
You can download the sample exchange file from the http://openexchangerates.org
site after creating an account there.
Detecting the document's language
Imagine a situation where you have users from different countries and you would like to give
them a choice to only see content you index that is written in their native language. Sounds quite
interesting, right? Let us see how we can identify the language of the documents during indexing
and store that information along with the documents in the index for later use.
How to do it...
For the language identification we will use one of the Solr contrib modules, but let's start from
the beginning.
1. For the purpose of the recipe, I assume that we will be using the following index
structure (add the following to the fields section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
www.it-ebooks.info
Chapter 2
63
<field name="name" type="text_general" indexed="true"
stored="true"/>
<field name="description" type="text_general" indexed="true"
stored="true" />
<field name="langId" type="string" indexed="true" stored="true" />
We will use the langId field to store the information about the identified language.
2. The next thing we need to do is create a langid directory somewhere on your
filesystem (I'll assume that the directory is created in the same directory that Solr is
installed) and copy the following libraries to that directory:
?? apache-solr-langid-4.0.0.jar (from the dist directory of Apache
Solr distribution)
?? jsonic-1.2.7.jar (from the contrib/langid/lib directory of Apache
Solr distribution)
?? langdetect-1.1.jar (from the contrib/langid/lib directory of
Apache Solr distribution)
3. Next we need to add some information to the solrconfig.xml file. First we need to
inform Solr that we want it to load the additional libraries. We do that by adding the
following entry to the config section of that file:
<lib dir="../../langid/" regex=".*\.jar" />
4. In addition to that we configure a new update processor by adding the following to the
config section of the solrconfig.xml file:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
LangDetectLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
5. Now, we need some data to be indexed. I decided to use the following test data
(stored in a data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">First</field>
<field name="description">>Water is a chemical substance with
the chemical formula H2O. A water molecule contains one oxygen
and two hydrogen atoms connected by covalent bonds. Water is a
www.it-ebooks.info
Indexing Your Data
64
liquid at ambient conditions, but it often co-exists on Earth with
its solid state, ice, and gaseous state (water vapor or steam).
Water also
exists in a liquid crystal state near hydrophilic surfaces.[1]
[2] Under nomenclature used to name chemical compounds, Dihydrogen
monoxide is the scientific name for water, though it is almost
never used.</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Zweite</field>
<field name="description">Wasser (H2O) ist eine chemische
Verbindung aus den Elementen Sauerstoff (O) und Wasserstoff
(H). Wasser ist die einzige chemische Verbindung auf der Erde,
die in der Natur in allen drei Aggregatzuständen vorkommt.
Die Bezeichnung Wasser wird dabei besonders für den flüssigen
Aggregatzustand verwendet. Im festen (gefrorenen) Zustand spricht
man von Eis, im gasförmigen Zustand von Wasserdampf.</field>
</doc>
</add>
6. And now the indexing. To index the above test file I used the following commands:
curl 'http://localhost:8983/solr/update?update.chain=langid'
--data-binary @data.xml -H 'Content-type:application/xml'
curl 'http://localhost:8983/solr/update?update.chain=langid'
--data-binary '<commit/>' -H 'Content-type:application/xml'
7. After sending the previous two commands, we can finally test if that worked. We will
just ask Solr to return all the documents by sending the q=*:* query. The following
results will be returned:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">6</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">First</str>
<str name="description">&gt;Water is a chemical substance with
the
www.it-ebooks.info
Chapter 2
65
chemical formula H2O. A water molecule contains one oxygen and two
hydrogen atoms connected by covalent bonds. Water is a liquid at
ambient conditions, but it often co-exists on Earth with its solid
state, ice, and gaseous state (water vapor or steam). Water also
exists in a liquid crystal state near hydrophilic surfaces.[1]
[2] Under nomenclature used to name chemical compounds, Dihydrogen
monoxide is the scientific name for water, though it is almost
never used.</str>
<str name="langId">en</str></doc>
<doc>
<str name="id">2</str>
<str name="name">Zweite</str>
<str name="description">Wasser (H2O) ist eine chemische
Verbindung
aus den Elementen Sauerstoff (O) und Wasserstoff (H). Wasser ist
die einzige chemische Verbindung auf der Erde, die in der Natur
in allen drei Aggregatzuständen vorkommt. Die Bezeichnung Wasser
wird dabei besonders für den flüssigen Aggregatzustand verwendet.
Im festen (gefrorenen) Zustand spricht man von Eis, im gasförmigen
Zustand von Wasserdampf.</str>
<str name="langId">de</str></doc>
</result>
</response>
As you can see, the langId field was filled with the correct language.
How it works...
The index structure we used is quite simple; it contains four fields and we are most interested
in the langId field which won't be supplied with the data, but instead of that we want Solr
to fill it.
The mentioned libraries are needed in order for the language identification to work. The lib
entry in the solrconfig.xml file tells Solr to look for all the JAR files from the ../../
langid directory. Remember to change that to reflect your setup.
Now the update request processor chain definition comes. We need
that definition to include org.apache.solr.update.processor.
LangDetectLanguageIdentifierUpdateProcessorFactory in order to detect the
document language. The langid.fl property tells the defined processor which fields
should be used to detect the language. langid.langField specifies to which field the
detected language should be written. The last property, langid.fallback, tells the
language detection library what language should be set if it fails to detect a language.
The solr.LogUpdateProcessorFactory and solr.RunUpdateProcessorFactory
processors are there to log the updates and actually run them.
www.it-ebooks.info
Indexing Your Data
66
As for data indexing, in order to use the defined update request processor chain, we need to
tell Solr that we want it to be used. In order to do that, when sending data to Solr we specify
the additional parameter called update.chain with the name of the update chain we want
to use, which in our case is langid. The --data-binary switch tells the curl command to
send that data in a binary format and the -H switch tells curl which content type should
be used. In the end we send the commit command to write the data to the Lucene index.
There's more...
If you don't want to use the previously mentioned processor to detect the document language,
you can use the one that uses the Apache Tika library:
Language identification based on Apache Tika
If LangDetectLanguageIdentifierUpdateProcessorFactory is not good
enough for you, you can try using language identification based on the Apache Tika
library. In order to do that you need to provide all the libraries from the contrib/
extraction directory in the Apache Solr distribution package instead of the ones
from contrib/langid/lib, and instead of using the org.apache.solr.update.
processor.LangDetectLanguageIdentifierUpdateProcessorFactory
processor use org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory. So the final configuration should
look like the following code:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
However, remember to still specify the update.chain parameter during indexing or add the
defined processor to your update handler configuration.
www.it-ebooks.info
Chapter 2
67
Optimizing your primary key field indexing
Most of the data stored in Solr has some kind of primary key. Primary keys are different from
most of the fields in your data as each document has a unique value stored; because they are
primary in most cases they are unique. Because of that, a search on this primary field
is not always as fast as you would expect when you compare it to databases. So, is there
anything we can do to make it faster? With Solr 4.0 we can, and this recipe will show
you how to improve the execution time of queries run against unique fields in Solr.
How to do it...
Let's assume we have the following field defined as a unique key for our Solr collection.
So, in your schema.xml file, you would have the following:
ff In your fields section you would have the following:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
ff After your fields section the following entry could be found:
<uniqueKey>id</uniqueKey>
The following steps will help you optimize the indexing of your primary key field:
1. Now, we would like to use the Lucene flexible indexing and use PulsingCodec
to handle our id field. In order to do that we introduce the following field type (just
place it in the types section of your schema.xml file):
<fieldType name="string_pulsing" class="solr.StrField"
postingsFormat="Pulsing40"/>
2. In addition to that, we need to change the id field definition to use the new type.
So, we should change the type attribute from string to string_pulsing:
<field name="id" type="string_pulsing" indexed="true"
stored="true" required="true" />
3. In addition to that we need to put the following entry in the solrconfig.xml file:
<codecFactory class="solr.SchemaCodecFactory"/>
And that's all. Now you can start indexing your data.
www.it-ebooks.info
Indexing Your Data
68
How it works...
The changes we made use the new feature introduced in Apache Lucene 4.0 and in Solr – the
so-called flexible indexing. It allows us to modify the way data is written into an inverted index
and thus configure it to our own needs. In the previous example, we used PulsingCodec
(postingsFormat="Pulsing40") in order to store the unique values in a special way. The
idea behind that codec is that the data for low frequency terms is written in a special way to
save a single I/O seek operation when retrieving a document or documents for those terms
from the index. That's why in some cases, when you do a noticeable amount of search to your
unique field (or any high cardinality field indexed with PulsingCodec), you can see a drastic
performance increase for that fields.
The last change, the one we made to the solrconfig.xml file, is required; without
it Solr wouldn't let us use specified codes and would throw an exception during startup.
It just specifies which codec factory should be used to create codec instances.
Please keep in mind that the previously mentioned method is very case dependent and
you may not see a great performance increase with the change.
www.it-ebooks.info
3
Analyzing Your
Text Data
In this chapter, we will cover:
ff Storing additional information using payloads
ff Eliminating XML and HTML tags from text
ff Copying the contents of one field to another
ff Changing words to other words
ff Splitting text by CamelCase
ff Splitting text by white space only
ff Making plural words singular without stemming
ff Lowercasing the whole string
ff Storing geographical points in the index
ff Stemming your data
ff Preparing text to perform an efficient trailing wildcard search
ff Splitting text by numbers and non-white space characters
ff Using Hunspell as a stemmer
ff Using your own stemming dictionary
ff Protecting words from being stemmed
www.it-ebooks.info
Analyzing Your Text Data
70
Introduction
The process of data indexing can be divided into parts. One of the parts, actually one of the
last parts of that process, is data analysis. It's one of the crucial parts of data preparation.
It defines how your data will be written into an index. It defines its structure and so on. In Solr,
data behavior is defined by types. A type's behavior can be defined in the context of the indexing
process or the context of the query process, or both. Furthermore, a type definition is composed
of a tokenizer (or multiple ones–one for querying and one for indexing) and filters (both token
filters and character filters).
A tokenizer specifies how your data will be pre-processed after it is sent to the appropriate field.
Analyzer operates on the whole data that is sent to the field. Types can only have one tokenizer.
The result of the tokenizer's work is a stream of objects called tokens. Next in the analysis chain
are the filters. They operate on the tokens in the token stream. They can do anything with the
tokens – change them, remove them, or make them lowercase, for example. Types can have
multiple filters.
One additional type of filter is character filters. They do not operate on tokens from the token
stream. They operate on the data that is sent to the field, and they are invoked before the
data is sent to the analyzer.
This chapter will focus on data analysis and how to handle common day-to-day analysis
questions and problems.
Storing additional information using
payloads
Imagine you have a powerful preprocessing tool that can extract information about all the
words in the text. Your boss would like you to use it with Solr or at least store the information it
returns in Solr. So what can you do? We can use something called payload to store that data.
This recipe will show you how to do it.
How to do it...
I assume that we already have an application that takes care of recognizing the part of
speech in our text data. What we need to add is the data to the Solr index. To do that we
will use a payload – a metadata that can be stored with each occurrence of a term.
www.it-ebooks.info
Chapter 3
71
1. First of all, you need to modify the index structure. To do this, we will add the
new field type to the schema.xml file (the following entries should be added
to the types section):
<fieldtype name="partofspeech" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.DelimitedPayloadTokenFilterFactory"
encoder="integer" delimiter="|"/>
</analyzer>
</fieldtype>
2. Now we'll add the field definition part to the schema.xml file (the following entries
should be added to the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="text" type="text" indexed="true" stored="true" />
<field name="speech" type="partofspeech" indexed="true"
stored="true" multivalued="true" />
3. Now let's look at what the example data looks like (I named it ch3_payload.xml):
<add>
<doc>
<field name="id">1</field>
<field name="text">ugly human</field>
<field name="speech">ugly|3 human|6</field>
</doc>
<doc>
<field name="id">2</field>
<field name="text">big book example</field>
<field name="speech">big|3 book|6 example|1</field>
</doc>
</add>
4. The next step is to index our data. To do that, we run the following command from
the exampledocs directory (put the ch3_payload.xml file there):
java -jar post.jar ch3_payload.xml
www.it-ebooks.info
Analyzing Your Text Data
72
5. To check if the payloads were written to the index, we will use the analysis capabilities
or the Solr administration panel. We will test the test|7 term with the associated
payload. The following is what it looks like:
How it works...
What information can the payload hold? It may hold information that is compatible with the
encoder type you define for the solr.DelimitedPayloadTokenFilterFactory filter.
In our case, we don't need to write our own encoder – we will use the supplied one to store
integers. We will use it to store the boost of the term. For example, nouns will be given a token
boost value of 6, while the adjectives will be given a boost value of 3.
So first, we have the type definition. We defined a new type in the schema.xml file named
partofspeech based on the Solr text field (attribute class="solr.TextField"). Our
tokenizer splits the given text on whitespace characters. Then we have a new filter which
handles our payloads. The filter defines an encoder which in our case is an integer (attribute
encoder="integer"). Furthermore it defines a delimiter which separates the term from
the payload. In our case the separator is the pipe character (|).
www.it-ebooks.info
Chapter 3
73
Finally we have the field definitions. In our example we only define three fields:
ff Identifier
ff Text
ff Recognized speech part with payload
Now let's take a look at the example data. We have two simple fields – id and text. The one
that we are interested in is the speech field. Look at how it is defined. It contains pairs which
are made of a term, a delimiter, and a boost value. For example, book|6. In the example, I
decided to boost nouns with a boost value of 6 and adjectives with the boost value of 3. I also
decided that words that cannot be identified by my application, which is used to identify parts of
speech, will be given a boost of 1. Pairs are separated with a space character, which in our case
will be used to split those pairs – that is the task of the tokenizer which we defined earlier.
To index the documents we use simple post tools provided with the example deployment of
Solr. To use it, invoke the command shown in the example. The post tools will send the data
to the default update handler found under the address http://localhost:8983/solr/
update. The following parameter is the file that is going to be sent to Solr. You can
also post a list of files, not only a single one.
As you can see on the provided screenshot, payload is being properly encoded and
written to the index – you can see [0 0 0 7] in the payload section of the solr.
DelimitedPayloadTokenFilterFactory filter.
Eliminating XML and HTML tags from text
There are many real-life situations when you have to clean your data. Let's assume that you want
to index web pages that your client sends you. You don't know anything about the structure of
that page; one thing you know is that you must provide a search mechanism that will enable
searching through the content of the pages. Of course you could index the whole page, splitting
it by whitespaces, but then you would probably hear the clients complain about the HTML tags
being searchable and so on. So before we enable searching the contents of the page, we need
to clean the data. In this example we need to remove the HTML tags. This recipe will show you
how to do it with Solr.
How to do it...
1. Let's start with assuming that our data looks like this (the ch3_html.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="html"><html><head><title>My page</title></
head><body><p>This is a <b>my</b><i>sample</i> page</body></
html></field>
www.it-ebooks.info
Analyzing Your Text Data
74
</doc>
</add>
2. Now let's take care of the schema.xml file. First add the type definition to the
schema.xml file:
<fieldType name="html_strip" class="solr.TextField">
<analyzer>
<charFilter class="solr.HTMLStripCharFilterFactory"/>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. The next step is to add the following to the field definition part of the schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="html" type="html_strip" indexed="true" stored="false"
/>
4. We can now index our data and have the HTML tags removed, right? Let's check
that, by going to the analysis section of the Solr administration pages and passing
the <html><head><title>My page</title></head><body><p>This is a
<b>my</b><i>sample</i> page</body></html> text to analysis there:
www.it-ebooks.info
Chapter 3
75
How it works...
First of all we have the data example. In the example we see one file with two fields, the identifier
and some HTML data nested in the CDATA section. You must remember to surround HTML data
in CDATA tags if they are full pages and start from HTML tags like our example. Otherwise Solr
will have problems with parsing the data. But if you only have some tags present in the data, you
shouldn't worry.
Next we have the html_strip type definition. It is based on solr.TextField to enable
full text searching. Following that we have a character filter which handles the HTML and the
XML tag stripping. The character filters are invoked before the data is sent to the tokenizer.
This way they operate on un-tokenized data. In our case the character filter strips the HTML
and XML tags, attributes, and so on and then sends the data to the tokenizer which splits the
data by whitespace characters. The one and only filter defined in our type makes the tokens
lowercase to simplify the search.
If you want to check how your data was indexed, remember not to be mistaken when you
choose to store the field contents (attribute stored="true"). The stored value is the
original one sent to Solr, so you won't be able to see the filters in action. If you wish to check
the actual data structures, take a look at the Luke utility (a utility that lets you see the index
structure and field values, and operate on the index). Luke can be found at the following
address: http://code.google.com/p/luke. Instead of using Luke, I decided to use the
analysis capabilities of the Solr administration pages and see how the html field behaves
when we pass the example value provided in the example data file.
Copying the contents of one field to another
Imagine that you have many big XML files that hold information about the books that are
stored on library shelves. There is not much data, just a unique identifier, the name of the
book and the author. One day your boss comes to you and says: "Hey, we want to facet and
sort on the basis of book author". You can change your XML and add two fields, but why do
that, when you can use Solr to do that for you? Well, Solr won't modify your data, but can
copy the data from one field to another. This recipe will show you how to do that.
How to do it...
In order to achieve what we want, we need the contents of the author field to be present in
the fields named author, author_facet, and authorsort.
1. Let's assume that our data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook</field>
www.it-ebooks.info
Analyzing Your Text Data
76
<field name="author">John Kowalsky</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Some other book</field>
<field name="author">Jane Kowalsky</field>
</doc>
</add>
2. Now let's add the following fields' definition to the fields section of your
schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="author" type="text" indexed="true" stored="true"
multiValued="true"/>
<field name="name" type="text" indexed="true" stored="true"/>
<field name="author_facet" type="string" indexed="true"
stored="false"/>
<field name="author_sort" type="alphaOnlySort" indexed="true"
stored="false"/>
3. In order to make Solr copy data from the author field to the author_facet
and author_sort field we need to define the copy fields in the schema.xml
file (place the following entries right after the field section):
<copyField source="author" dest="author_facet"/>
<copyField source="author" dest="author_sort"/>
4. Now we can index our example data file by running the following command from the
exampledocs directory (put the data.xml file there):
java -jar post.jar data.xml
How it works...
As you can see in the example, we only have three fields defined in our sample data XML file.
There are two fields which we are not particularly interested in – id and name. The field that
interests us the most is the author field. As I have previously mentioned, we want to place
the contents of that field into three fields:
ff author (the actual field that will be holding the data)
ff author_sort
ff author_facet
www.it-ebooks.info
Chapter 3
77
To do that we use copy fields. Those instructions are defined in the schema.xml file, right
after the field definitions; that is, after the </fields> tag. To define a copy field, we need
to specify a source field (attribute source) and a destination field (attribute dest).
After the definitions, like those in the example, Solr will copy the contents of the source fields
to the destination fields during the indexing process. There is one thing that you have to be
aware of – the content is copied before the analysis process takes place. That means that
the data is copied as it is stored in the source.
There's more...
Solr also allows us to do more with copy fields than a simple copying from one field to another.
Copying contents of dynamic fields to one field
You can also copy multiple fields' content to one field. To do that you should define a copy field
like so:
<copyField source="*_author" dest="authors"/>
The definition, like the one previously mentioned, would copy all of the fields that end with
_author to one field named authors. Remember that if you copy multiple fields to one
field, the destination field should be defined as multi-valued.
Limiting the number of characters copied
There may be situations where you only need to copy a defined number of characters from
one field to another. To do that we add the maxChars attribute to the copy field definition.
It can look like the following line of code:
<copyField source="author" dest="author_facet" maxChars="200"/>
The preceding definition tells Solr to copy up to 200 characters from the author field to the
author_facet field. This attribute can be very useful when copying the content of multiple
fields to one field.
Changing words to other words
Let's assume we have an e-commerce client and we are providing a search system based on
Solr. Our index has hundreds of thousands of documents which mainly consist of books. And
everything works fine! Then one day, someone from the marketing department comes into
your office and says that he wants to be able to find books that contain the word "machine"
when he types "electronics" into the search box. The first thing that comes to mind is, "Hey, I'll
do it in the source and index that". But that is not an option this time, because there can be
many documents in the database that have those words. We don't want to change the whole
database. That's when synonyms come into play and this recipe will show you how to use them.
www.it-ebooks.info
Analyzing Your Text Data
78
How to do it...
To make the example as simple as possible, I assumed that we only have two fields in our index.
1. Let's start by defining our index structure by adding the following field definition
section to the schema.xml file (just add it to your schema.xml file in the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="description" type="text_syn" indexed="true"
stored="true" />
2. Now let's add the text_syn type definition to the schema.xml file as shown in
the following code snippet:
<fieldType name="text_syn" class="solr.TextField">
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.SynonymFilterFactory" synonyms="synonyms.
txt" ignoreCase="true" expand="false" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. As you have noticed there is a file mentioned – synonyms.txt. Let's take a look at
its contents:
machine => electronics
The synonyms.txt file should be placed in the same directory as other
configuration files, which is usually the conf directory.
4. Finally we can look at the analysis page of the Solr administration panel to see if the
synonyms are properly recognized and applied:
www.it-ebooks.info
Chapter 3
79
How it works...
First we have our field definition. There are two fields, an identifier and a description. The second
one should be of interest r to us ight now. It's based on the new type text_syn which is shown
in the second listing.
Now about the new type, text_syn – it's based on the solr.TextField class. Its definition
is divided; it behaves in one way while indexing and in a different way while querying. So the
first thing we see is the query time analyzer definition. It consists of the tokenizer that splits the
data on the basis of whitespace characters, and then the lowercase filter converts all the tokens
to lowercase. The interesting part is the index time behavior. It starts with the same tokenizer,
but then the synonyms filter comes into play. Its definition starts like all the other filters – with
a factory definition. Next we have a synonyms attribute which defines which file contains the
synonyms definition. Following that we have the ignoreCase attribute which tells Solr to ignore
the case of the tokens and the contents of the synonyms file.
www.it-ebooks.info
Analyzing Your Text Data
80
The last attribute named expand is set to false. This means that Solr won't be expanding
the synonyms – all equivalent synonyms will be reduced to the first synonym in the line.
If the attribute is set to true, all synonyms will be expanded to all equivalent forms.
The example synonyms.txt file tells Solr that when the word "machine" appears in the field
based on the text_syn type it should be replaced by "electronics". But not vice versa. Each
synonym rule should be placed in a separate line in the synonyms.txt file. Also remember
that the file should be written in the UTF-8 file encoding. This is crucial and you should always
remember it because Solr will expect the file to be encoded in UTF-8.
As you can see in the provided screenshot from the Solr administration pages, the defined
synonym was properly applied during the indexing phase.
There's more...
There is one more thing associated to using synonyms in Solr.
Equivalent synonyms setup
Let's get back to our example for a second. What if the person from the marketing
department says that he/she wants not only to be able to find books that have the word
"machine" to be found when entering the word "electronics", but also all the books that
have the word "electronics", to be found when entering the word "machine". The answer
is simple. First, we would set the expand attribute (of the filter) to true. Then we would
change our synonyms.txt file to something like this:
machine, electronics
As I said earlier Solr would expand synonyms to equivalent forms.
Splitting text by CamelCase
Let's suppose that you run an e-commerce site with an electronic assortment. The marketing
department can be a source of many great ideas. Imagine that your colleague from this
department comes to you and says that they would like your search application to be able to
find documents containing the word "PowerShot" by entering the words "power" and "shot" into
the search box. So can we do that? Of course, and this recipe will show you how.
How to do it...
1. Let's start by creating the following index structure (add this to your schema.xml file
to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
www.it-ebooks.info
Chapter 3
81
<field name="description" type="text_split" indexed="true"
stored="true" />
2. To split text in the description field, we should add the following type definition
to the schema.xml file:
<fieldType name="text_split" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.WordDelimiterFilterFactory"
generateWordParts="1" splitOnCaseChange="1"/>
<filter class="solr.LowerCaseFilterFactory" />
</analyzer>
</fieldType>
3. Now let's index the following XML file:
<add>
<doc>
<field name="id">1</field>
<field name="description">TextTest</field>
</doc>
</add>
4. Finally, let's run the following query in the web browser:
http://localhost:8983/solr/select?q=description:test
You should get the indexed document as the response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:test</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="description">TextTest</str></doc>
</result>
</response>
www.it-ebooks.info
Analyzing Your Text Data
82
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml file.
This is pretty straightforward. We have two fields defined–one that is responsible for holding
information about the identifier (the id field) and the second one that is responsible for the
product description (the description field).
Next we see the interesting part. We name our type text_split and have it based on
a text type, solr.TextField. We also told Solr that we want our text to be tokenized
by whitespaces by adding the whitespace tokenizer (the tokenizer tag). To do what we
want to do–split by case change–we need more than this. Actually we need a filter named
WordDelimiterFilter which is created by the solr.WordDelimiterFilterFactory
class and a filter tag. We also need to define the appropriate behavior of the filter, so we
add two attributes – generateWordParts and splitOnCaseChange. The values of those
two parameters are set to 1 which means that they are turned on. The first attribute tells Solr
to generate word parts, which means that the filter will split the data on non-letter characters.
We also add the second attribute which tells Solr to split the tokens by case change.
What will that configuration do with our sample data? As you can see we have one document
sent to Solr. The data in the description field will be split into two words: text and test.
Please remember that we won't see the analyzed text in the Solr response, we only see the
stored fields and the original content of those, not the analyzed one.
Splitting text by whitespace only
One of the most common problems that you probably came across is having to split text
with whitespaces in order to segregate words from each other, to be able to process it further.
This recipe will show you how to do it.
How to do it...
1. Let's start with the assumption that we have the following index structure (add this
to your schema.xml file in the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="description_string" type="string" indexed="true"
stored="true" />
<field name="description_split" type="text_split" indexed="true"
stored="true" />
2. To split the text in the description field, we should add the following type definition:
<fieldType name="text_split" class="solr.TextField">
<analyzer>
www.it-ebooks.info
Chapter 3
83
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
</analyzer>
</fieldType>
3. To test our type, I've indexed the following XML file:
<add>
<doc>
<field name="id">1</field>
<field name="description_string">test text</field>
<field name="description_text">test text</field>
</doc>
</add>
4. Finally, let's run the following query in the web browser:
http://localhost:8983/solr/select?q=description_split:text
In the response to the preceding query, we got the indexed document:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description_split:text</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="description_string">test text</str>
<str name="description_split">test text</str></doc>
</result>
</response>
5. On the other hand, we won't get the indexed document in the response after
running the following query:
http://localhost:8983/solr/select?q=description_string:text
The response to the preceding query:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
www.it-ebooks.info
Analyzing Your Text Data
84
<str name="q">description_string:text</str>
</lst>
</lst>
<result name="response" numFound="0" start="0">
</result>
</response>
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml
file. This is pretty straightforward. We have three fields defined – one for the identifier of the
document (the id field), and one named description_string which is based on a string
field and thus not analyzed. The third one is the descriptionsplit
field which is based
on our text_split type and will be tokenized on the basis of whitespace characters.
Next we see the interesting part. We named our type text_split and had it based on a text
type – solr.TextField. We told Solr that we want our text to be tokenized by whitespaces
by adding a whitespace tokenizer (the tokenizer tag). Because there are no filters defined,
the text will only be tokenized by whitespace characters and nothing more.
That's why our sample data in the field description_text will be split into two words,
test and text. On the other hand, the text in the description_string field won't be
split. That's why the first example query will result in one document in the response, while
the second example won't find the example document. Please remember that we won't see
the analyzed text in the Solr response, we only see stored fields and we see the original
content of those, not the analyzed one.
Making plural words singular without
stemming
Nowadays it's nice to have stemming algorithms (algorithms that will reduce words to their
stems or root form) in your application, which will allow you to find the words such as cat
and cats by typing cat. But let's imagine you have a search engine that searches through
the contents of books in the library. One of the requirements is changing the plural forms
of the words from plural to singular – nothing less, nothing more. Can Solr do that? Yes,
the newest version can and this recipe will show you how to do that.
How to do it...
1. First of all let's start with a simple two field index (add this to your schema.xml
file to the field definition section):
www.it-ebooks.info
Chapter 3
85
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="description" type="text_light_stem" indexed="true"
stored="true" />
2. Now let's define the text_light_stem type which should look like this (add this
to your schema.xml file):
<fieldType name="text_light_stem" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.EnglishMinimalStemFilterFactory" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. Now let's check the analysis tool of the Solr administration pages. You should
see that words such as ways and, keys are changed to their singular forms. Let's
check the for that words using the analysis page of the Solr administration pages:
www.it-ebooks.info
Analyzing Your Text Data
86
How it works...
First of all we need to define the fields in the schema.xml file. To do that we add the
contents from the first example into that file. It tells Solr that our index will consist of two
fields – the id field which will be responsible for holding information about the unique
identifier of the document, and the description file which will be responsible for holding
the document description.
The description field is actually where the magic is being done. We defined a new field type
for that field and we called it text_light_stem. The field definition consists of a tokenizer
and two filters. If you want to know how this tokenizer behaves please refer to the Splitting text
by whitespace only recipe in this chapter. The first filter is a new one. This is the light stemming
filter that we will use to perform minimal stemming. The class that enables Solr to use that
filter is solr.EnglishMinimalStemFilterFactory. This filter takes care of the process of
light stemming. You can see that using the analysis tools of the Solr administration panel. The
second filter defined is the lowercase filter – you can see how it works by referring to the How
to lowercase the whole string recipe in this chapter.
After adding this to your schema.xml file you should be able to use the light stemming
algorithm.
There's more...
Light stemming supports a number of different languages. To use the light stemmers for
your respective language, add the following filters to your type:
Language Filter
Russian solr.RussianLightStemFilterFactory
Portuguese solr.PortugueseLightStemFilterFactory
French solr.FrenchLightStemFilterFactory
German solr.GermanLightStemFilterFactory
Italian solr.ItalianLightStemFilterFactory
Spanish solr.SpanishLightStemFilterFactory
Hungarian solr.HungarianLightStemFilterFactory
Swedish solr.SwedishLightStemFilterFactory
Finish solr.FinnishLightStemFilterFactory
Indonesian solr.IndonesianStemFilterFactory
(with stemDerivational="false" attribute)
Norwegian solr.NorwegianLightStemFilterFactory
In the case of solr.IndonesianStemFilterFactory, you need to add the
stemDerivational="false" attribute in order to have it working as a light stemmer.
www.it-ebooks.info
Chapter 3
87
Lowercasing the whole string
Imagine you have a system where you only want to have perfect matches for names of
the documents. No matter what the cause of such a decision is, you would want such a
functionality. However there is one thing you would like to have – you would like your search
to be case independent, so it doesn't matter if the document or query is lower cased or
uppercased. Can we do something with that in Solr? Of course Solr can do that, and this
recipe will describe how to do it.
How to do it...
1. We start by defining the following index structure (add this to your schema.xml file
in the field definition section):
<field name="id " type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="string_lowercase" indexed="true"
stored="true" />
<field name="description" type="text" indexed="true" stored="true"
/>
2. To make our strings lowercase, we should add the following type definition to the
schema.xml file:
<fieldType name="string_lowercase" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. In order to test if everything is working as it should we need to index the following
XML file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook</field>
<field name="description">Simple description</field>
</doc>
</add>
4. Then we will run the following query in the web browser:
http://localhost:8983/solr/select?q=name:"solr cookbook"
You should get the indexed document in response. You should also be able to get the
indexed document in response to the following query:
http://localhost:8983/solr/select?q=name:"solr Cookbook"
www.it-ebooks.info
Analyzing Your Text Data
88
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml file.
This is pretty straightforward. We have three fields defined. First, the field named id which is
responsible for holding our unique identifier. The second one is the name field which is actually
our lowercased string field. The third field will hold the description of our documents and is
based on the standard text type defined in the example Solr deployment.
Now let's get back to our name field. It's based on the string_lowercase type. The string_
lowercase type consists of an analyzer which is defined as a tokenizer and one filter. The
solr.KeywordTokenizerFactory filter tells Solr that the data in that field should not be
tokenized in any way. It just should be passed as a single token to the token stream. Next we
have our filter, which changes all the characters to their lowercased equivalents. And that's how
this field analysis is performed.
The example queries show how the field behaves. It doesn't matter if you type lowercase or
uppercase characters, the document will be found anyway. What matters is that you must type
the whole string as it is because we used the keyword tokenizer which, as I already said, is not
tokenizing but just passing the whole data through the token stream as a single token.
Storing geographical points in the index
Imagine that up till now your application stores information about companies – not much
information, just unique identification and the company name. But now, your client wants
to store the location of the companies. In addition to that, your users would like to sort by
distance and filter by distance from a given point. Is this doable with Solr? Of course it is
and this recipe will show you how to do it.
How to do it...
1. For the purpose of this recipe, let's create a sample index structure. To do this, describe
the companies that we store in the index with three fields which are defined as follows
(add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="location" type="location" indexed="true"
stored="true" />
www.it-ebooks.info
Chapter 3
89
2. Next we will also add one dynamic field (add this to your schema.xml file in the field
definition section):
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
3. The next step is to define the location type which should look like the following code:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
4. In addition to that, we will need the tdouble field type, which should look like the
following code:
<fieldType name="tdouble" class="solr.TrieDoubleField"
precisionStep="4" positionIncrementGap="0"/>
5. The next step is to create the example data looking like the following code (I named
the data file task9.xml):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr.pl company</field>
<field name="location">54.02,23.10</field>
</doc>
</add>
6. And now let's index our data. To do that, we run the following command from the
exampledocs directory (put the task9.xml file there):
java -jar post.jar task9.xml
7. After indexing we should be able to use the query, such as the following one, to get
our data:
http://localhost:8983/solr/select?q=*:*&fq={!geofilt
sfield=location}&pt=54.00,23.00&d=10
The response should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="pt">54.00,23.00</str>
www.it-ebooks.info
Analyzing Your Text Data
90
<str name="d">10</str>
<str name="fq">{!geofiltsfield=location}</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr.pl company</str>
<str name="location">54.02,23.10</str>
</doc>
</result>
</response>
How it works...
First of all we have three fields and one dynamic field defined in our schema.xml file. The first
field is the one responsible for holding the unique identifier. The second one holds the name of
the company. The third one named location is responsible for holding geographical points
and is based on the location type. The dynamic field – *_coordinate will be used internally
by our location type. It uses the tdouble field which was taken from the schema.xml file
distributed with Solr.
Next we have our location type definition. It's based on the solr.LatLonType class which
is specially designed for spatial search and is defined by a single attribute – subFieldSuffix.
That attribute specifies which fields (in our case it's the dynamic *_coordinate field) will be
used internally for holding the actual values of latitude and longitude.
So how does this type of field actually work? When defining a two-dimensional field, like we
did, there are actually three fields created in the index. The first field is named like the field we
added in the schema.xml file, so in our case it is location. This field will be responsible for
holding the stored value of the field. And one more thing – this field will only be created when
we set the field attribute store to true.
The next two fields are based on the defined dynamic field. Their names will be location
_0_coordinate and location_1_coordinate in our case. First we have the field
name, the _ character, then the index of the value, and finally the suffix defined by the
subFieldSuffix attribute of the type.
We can now look at the way the data is indexed. Please take a look at the example data file.
You can see that the values in each pair are separated by the comma character, and that's
how you can add the data to the index:
http://localhost:8983/solr/select?q=*:*&fq={!geofilt sfield=location}
&pt=54.00,23.00&d=10
www.it-ebooks.info
Chapter 3
91
Querying is a bit different. We send a query to retrieve all the documents from the index
(q=*:*). In addition to that, we want to filter the results by distance (the geofilt filter) with
the use of the location field (sfield=location). fq={!geofiltsfield=location}
uses the Solr local params syntax to send a distance filter. It can look strange comparing it to
a standard query, but it works. In addition to that, we've specified the point we will calculate
the distance from (the pt parameter) as 54.00,23.00. This is a pair of latitude and longitude
values separated by a comma character. The last parameter is d, which specifies the maximum
distance that documents can be, from the given point, to be considered as a match. We
specified it as 10 kilometers (d=10). As you can see, even though our document had its point
defined as 54.02,23.10 we found it with our query because of the distance we specified.
Stemming your data
One of the most common requirements I meet is stemming – the process of reducing the
word to their root form (or stems). Let's imagine the book e-commerce store, where you store
the books' names and descriptions. We want to be able to find words such as shown or showed
when you type the word show and vice versa. To achieve that we can use stemming algorithms.
This recipe will show you how to add stemming to your data analysis.
How to do it...
1. We need to start with the index structure. Let's assume that our index consists
of three fields (add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="description" type="text_stem" indexed="true"
stored="true" />
2. Now let's define our text_stem type which should look like the following code:
<fieldType name="text_stem" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.SnowballPorterFilterFactory" />
</analyzer>
</fieldType>
3. Now we can index our data – to do that we need to create an example data file,
for example, the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
www.it-ebooks.info
Analyzing Your Text Data
92
<field name="description">This is a book that I'll show</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr cookbook 2</field>
<field name="description">This is a book I showed</field>
</doc>
</add>
4. After indexing, we can test how our data was analyzed. To do that, let's run the
following query:
http://localhost:8983/solr/select?q=description:show
The result we get from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:show</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<arr name="description">
<str>This is a book that I'll show</str>
</arr>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr cookbook 2</str>
<arr name="description">
<str>This is a book I showed</str>
</arr>
</doc>
</result>
</response>
www.it-ebooks.info
Chapter 3
93
That's right, Solr found two documents matching the query which means that our fields and
types are working as intended.
How it works...
Our index consists of three fields; one holding the unique identifier of the document, the
second one holding the name of the document, and the third one holding the document
description. The last field is the field that will be stemmed.
The stemmed field is based on a Solr text field and has an analyzer that is used at query
and indexing time. It is tokenized on the basis of the whitespace characters, and then the
stemming filter is used. What does the filter do? It tries to bring the words to its root form,
which means that words such as shows, showing, and show will all be changed to show
– or at least they should be changed to that form.
Please note that in order to properly use stemming algorithms they should be used at query
and indexing time. This is a must because of the stemming results.
As you can see, our test data consists of two documents. Take a look at the description.
One of the documents contains the word showed and the other has the word show in
their description fields. After indexing and running the sample query, Solr would return
two documents in the results which means that the stemming did its job.
There's more...
There are too many languages that have stemming support integrated into Solr to mention
them all. If you are using a language other than English, please refer to the http://wiki.
apache.org/solr/LanguageAnalysis page of the Solr Wiki to find the appropriate filter.
Preparing text to perform an efficient
trailing wildcard search
Many users coming from traditional RDBMS systems are used to wildcard searches. The most
common of them are the ones using * characters which means zero or more characters. You
have probably seen searches like the one as follows:
AND name LIKE 'ABC12%'
So how to do that with Solr and not kill our Solr server? This task will show you how to prepare
your data and make efficient searches.
www.it-ebooks.info
Analyzing Your Text Data
94
How to do it...
1. The first step is to create a proper index structure. Let's assume we have the following
one (add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="string_wildcard" indexed="true"
stored="true" />
2. Now, let's define our string_wildcard type (add this to the schema.xml file):
<fieldType name="string_wildcard" class="solr.TextField">
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.EdgeNGramFilterFactory" minGramSize="1"
maxGramSize="25" side="front"/>
</analyzer>
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
</analyzer>
</fieldType>
3. The third step is to create the example data which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">XYZ1234ABC12POI</field>
</doc>
</add>
4. Now send the following query to Solr:
http://localhost:8983/solr/select?q=name:XYZ1
The Solr response for the previous query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:XYZ1</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
www.it-ebooks.info
Chapter 3
95
<str name="id">1</str>
<str name="name">XYZ1234ABC12POI</str>
</doc>
</result>
</response>
As you can see, the document has been found, so our setup is working as intended.
How it works...
First of all let's look at our index structure defined in the schema.xml file. We have two fields
– one holding the unique identifier of the document (the id field) and the second one holding
the name of the document (the name field) which is actually the field we are interested in.
The name field is based on the new type we defined – string_wildcard. This type is
responsible for enabling trailing wildcards, the ones that will enable the LIKE 'WORD%' SQL
queries. As you can see the field type is divided into two analyzers, one for the data analysis
during indexing and the other for query processing. The querying analyzer is straight; it just
tokenizes the data on the basis of whitespace characters. Nothing more, nothing less.
Now the indexing time analysis (of course we are talking about the name field). Similar to
the query time, during indexing the data is tokenized on the basis of whitespace characters,
but there is also an additional filter defined. The solr.EdgeNGramFilterFactory class
is responsible for generating the filter called n-grams. In our setup, we tell Solr that the
minimum length of an n-gram is 1 (the minGramSize attribute) and the maximum length is
25 (the maxGramSize attribute). We also defined that the analysis should be started from the
beginning of the text (the side attribute set to front). So what would Solr do with our example
data? It will create the following tokens from the example text: X, XY, XYZ, XYZ1, XYZ12, and
so on. It will create tokens by adding the next character from the string to the previous token,
up to the maximum length of the n-gram filter that is given in the filter configuration.
So by typing the example query, we can be sure that the example document will be found
because of the n-gram filter defined in the configuration of the field. We also didn't define
the n-gram filter in the querying stage of analysis because we didn't want our query to be
analyzed in such a way that it could lead to false positive hits.
This functionality, as described, can also be used successfully to provide autocomplete
features to your application (if you are not familiar with the autocomplete feature please
take a look at http://en.wikipedia.org/wiki/Autocomplete).
Please remember that using n-grams will make your index a bit larger. Because of that you
should avoid having n-grams on all the fields in the index. You should carefully decide which
fields should use n-grams and which should not.
www.it-ebooks.info
Analyzing Your Text Data
96
There's more...
If you would like your field to be able to simulate SQL LIKE '%ABC' queries, you should
change the side attribute of the solr.EdgeNGramFilterFactory class to the back
value. The configuration should look like the following code snippet:
<filter class="solr.EdgeNGramFilterFactory" minGramSize="1"
maxGramSize="25" side="back"/>
It would change the end from which Solr starts to analyze the data. In our case it would start
from the end, and thus would produce n-grams as follows: I, OI, POI,2POI, 12POI, and so
on.
See also
ff If you want to propose another solution for that kind of search, please refer to the
recipe Splitting text by numbers and non-whitespace characters in this chapter
Splitting text by numbers and
non-whitespace characters
Analyzing the text data is not only about stemming, removing diacritics (if you are not familiar
with the word, please take a look at http://en.wikipedia.org/wiki/Diacritic), and
choosing the right format for the data. Let's assume that our client wants to be able to search
by words and numbers that construct product identifiers. For example, he would like to be able
to find the product identifier ABC1234XYZ by using ABC, 1234, or XYZ.
How to do it...
1. Let's start with the index that consists of three fields (add this to your schema.xml
file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="description" type="text_split" indexed="true"
stored="true" />
2. The second step is to define our text_split type which should look like the
following code (add this to your schema.xml file):
<fieldType name="text_split" class="solr.TextField">
<analyzer>
www.it-ebooks.info
Chapter 3
97
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.WordDelimiterFilterFactory"
generateWordParts="1" generateNumberParts="1" splitOnNumerics="1"
/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. Now you can index your data. To do that let's create an example data file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Test document</field>
<field name="description">ABC1234DEF BL-123_456
adding-documents</field>
</doc>
</add>
4. After indexing we can test how our data was analyzed. To do that let's run the
following query:
http://localhost:8983/solr/select?q=description:1234
Solr found our document which means that our field is working as intended.
The response from Solr will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:1234</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Test document</str>
<str name="description">ABC1234DEF BL-123_456 addingdocuments</
str></doc>
</result>
</response>
www.it-ebooks.info
Analyzing Your Text Data
98
How it works...
We have our index defined as three fields in the schema.xml file. We have a unique identifier
(an id field) indexed as a string value. We have a document name (the name field) indexed as
text (type which is provided with the example deployment of Solr), and a document description
(a description field) which is based on the text_split field which we defined ourselves.
Our type is defined to make the same text analysis, both on query time and on index time.
It consists of the whitespace tokenizer and two filters. The first filter is where the magic is
done. The solr.WordDelimiterFilterFactory behavior, in our case, is defined by
the following parameters:
ff generateWordParts: If this parameter is set to 1, it tells the filter to generate
parts of the word that are connected by non-alphanumeric characters such as
the dash character. For example, token ABC-EFG would be split into ABC and EFG.
ff generateNumberParts: If this parameter is set to 1, it tells the filter to generate
words from numbers connected by non-numeric characters, such as the dash
character. For example, token 123-456 would be split into 123 and 456.
ff splitOnNumerics: If this parameter is set to 1, it tells the filter to split letters
and numbers from each other. This means that token ABC123 would be split in
to ABC and 123.
The second filter is responsible for changing the words that lowercased the equivalents and
is discussed in the recipe How to lowercase the whole string in this chapter.
Therefore, after sending our test data to Solr we can run the example query to see if we
defined our filter properly. In addition, you probably know the result; yes, the result will contain
one document – the one that we send to Solr. That is because the word ABC1234DEF is split
into ABC, 1234, and DEF tokens, and thus can be found by the example query.
There's more...
In case you would like to preserve the original token that is passed to solr.
WordDelimiterFilterFactory, add the following attribute to the filter definition:
preserveOriginal="1"
See also
ff If you would like to know more about solr.WordDelimiterFilterFactory,
please refer to the recipe Splitting text by CamelCase in this chapter
www.it-ebooks.info
Chapter 3
99
Using Hunspell as a stemmer
Solr supports numerous stemmers for various languages. You can use various stemmers for
English, and there are ones available for French, German, and most of the European languages.
But sometimes they provide stemming results that are not of great quality. Alternatively, maybe
you are wondering if there is a stemmer out there that supports your language, which is not
included in Solr. No matter what the reason, if you are looking for a different stemmer you
should look at the Hunspell filter if it suits your needs, and this recipe will show you how to
use it in Solr.
Getting ready
Before starting, please check the http://wiki.openoffice.org/wiki/Dictionaries
page to see if Hunspell supports your language.
How to do it...
1. We should start by creating an index structure (just add the following entries to the
fields section of your schema.xml file) which looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text_english" indexed="true"
stored="true"/>
<field name="description" type="text_english" indexed="true"
stored="true" />
2. Now we should define the text_english type as follows (if you don't have it in your
schema.xml file, please add it to the types section of the file):
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
3. Let's assume that we are not satisfied with the quality of solr.
PorterStemFilterFactory and we would like to have that improved
by using Hunspell. In order to do that, we need to change the solr.
PorterStemFilterFactory definition to the following one:
<filter class="solr.HunspellStemFilterFactory" dictionary="en_
GB.dic" affix="en_GB.aff" ignoreCase="true" />
www.it-ebooks.info
Analyzing Your Text Data
100
So the final text_english type configuration would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.HunspellStemFilterFactory" dictionary=
"en_GB.dic" affix="en_GB.aff" ignoreCase="true" />
</analyzer>
</fieldType>
4. The last thing we need to do is place the en_GB.dic and en_GB.aff files in the
Solr conf directory (the one where you have all your configuration files stored). Those
files can be found at the http://wiki.openoffice.org/wiki/Dictionaries
page. They are the dictionaries for English used in Great Britain. And that's all;
nothing more needs to be done.
How it works...
Our index structure is very simple – it contains three fields of which two (name and
description) are used for full text searching, and we want those fields to use the text_
english field type and thus use solr.HunspellStemFilterFactory for stemming.
The configuration of the solr.HunspellStemFilterFactory filter factory is not difficult.
Of course, there are a few attributes of the filter tag that need to be specified:
ff class: This specifies the class implementing the filter factory we want to use,
which in our case is solr.HunspellStemFilterFactory.
ff dictionary: This specifies the name of the .dic file of the dictionary we want
to use.
ff affix: This specifies the name of the .aff file of the dictionary we want to use.
ff ignoreCase: This is used to ignore cases when matching words against the
dictionary. In our case, we want to ignore cases.
The last thing we need to do is provide Solr with the dictionary files so that the Hunspell filter
can do its work. Although this is simple, this part is crucial. The dictionaries define how well
Hunspell will work. Before using a new dictionary, you should always properly conduct A/B
testing and see if things did not get worse in your case.
One last thing about the dictionaries. If you would like to use other languages with Hunspell,
the only thing you will need to do is provide the new dictionary file and change the name
of the dictionaries, so change the dictionary and affix attributes of the solr.
HunspellStemFilterFactory definition.
www.it-ebooks.info
Chapter 3
101
Using your own stemming dictionary
Sometimes, stemmers provided with Lucene and Solr don't do what you would like them
to do. That's because most of them are based on an algorithmic approach and even the
best algorithms can come to a place where you won't like the results of their work and you
would like to make some modifications. Of course, modifications to the algorithm code can
be challenging and we don't usually do that. The good thing is that Solr supports a method
of overriding the stemmer work and this recipe will show you how to use it.
Getting ready
Before we continue please remember that the method described in this recipe may not work
with custom stemmers that are not provided with Solr.
How to do it... Let's say that we want some of the words to be stemmed in a way we want. For
example,
we want the word dogs to be stemmed as doggie (of course that's only an example).
1. What we have to do first is write the words dogs and doggie in a file (let's call it
override.txt). Words should be separated from each other by a tab character
and each line of the file should contain a single stemming overwrite. For example,
our override.txt file could look like this:
dogs doggie
2. Now we should put the override.txt file in the same directory as the schema.
xml file (usually its conf). Please remember to have that file written in UTF-8
encoding. If you have characters from the classic ASCII character set, they won't
be recognized properly if you don't use UTF-8.
3. Next we need to add the solr.StemmerOverrideFilterFactory filter to
our text types. I assume we only use text_english with the following definition
(put the following definition to your types section of the schema.xml file):
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
www.it-ebooks.info
Analyzing Your Text Data
102
In order for our list of protected words to work, we need to put solr.
StemmerOverrideFilterFactory before the stemming, which is solr.
PorterStemFilterFactory in our case. The final type definition for text_
english would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.StemmerOverrideFilterFactory"
dictionary="dict.txt" />
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
This is what the analysis page of the Solr administration pages shows:
4. That's all. Now, the fields that are based on the text_english type will not
be stemmed.
www.it-ebooks.info
Chapter 3
103
How it works...
The work of the solr.StemmerOverrideFilterFactory class is simple – it changes the
words we want it to change and then marks them as protected so that the stemmer won't do
any further processing of those words. In order for this functionality to work properly, you should
remember to put solr.StemmerOverrideFilterFactory before any stemmers in your
analysis chain.
The actual configuration of solr.StemmerOverrideFilterFactory is pretty simple and
similar to other filters. It requires two attributes; the usual class attribute, which informs Solr
which filter factory should be used in order to create the filter, and the dictionary attribute,
which specifies the name of the file containing the dictionary that we want to use
for our custom stemming.
Looking at the analysis page of the Solr administration pages, we can see that our dogs
word was protected from being stemmed with the default stemmer and changed to what
we wanted, that is, doggie.
Protecting words from being stemmed
Sometimes, the stemming filters available in Solr do more than you would like them to do.
For example, they can stem brand names or the second name of a person. Sometimes, you
would like to protect some of the words that have a special meaning in your system or you
know that some words would cause trouble to a stemmer or stemmers. This recipe will show
you how to do it.
Getting started
Before we continue, please remember that the method described in this recipe may not work
with custom stemmers that are not provided with Solr.
How to do it...
In order to have the defined words protected we need a list of them. Let's say that we don't
want the words cats and dogs to be stemmed.
1. To achieve that, we should start by writing the words we want to be protected from
stemming into a file. Let's create the file called dontstem.txt with the following
contents:
cats
dogs
www.it-ebooks.info
Analyzing Your Text Data
104
2. Now let's put the created file in the same directory as the schema.xml file (usually
it's the conf directory). Please remember to have that file written in UTF-8 encoding.
If you have characters from the classic ASCII character set they won't be recognized
properly if you don't use UTF-8.
3. Now, we need to add the solr.KeywordMarkerFilterFactory filter to our text
types. I assume we only use the text_english type with the following definition:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
In order for our list of protected words to work, we need to put solr.
KeywordMarkerFilterFactory before the stemming, which is solr.
PorterStemFilterFactory in our case. So the final type definition for the text_
english type would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.KeywordMarkerFilterFactory"
protected="dontstem.txt" />
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
www.it-ebooks.info
Chapter 3
105
This is what the analysis page of the Solr administration pages shows:
4. That's all. Now, the fields that are based on the text_english type won't
be stemmed.
www.it-ebooks.info
Analyzing Your Text Data
106
How it works...
The whole idea is pretty simple. With the use of solr.KeywordMarkerFilterFactory,
we mark the protected words and that information is used by the stemmers available in Solr
and Lucene. In order for this functionality to work properly, you should remember to put the
solr.KeywordMarkerFilterFactory filter before any stemmers in your analysis chain.
The actual configuration of solr.KeywordMarkerFilterFactory is pretty simple and
similar to other filters. It requires two attributes; the usual class attribute, which informs
Solr which filter factory should be used in order to create the filter, and the attribute protected
which specifies the name of the file containing words that we want to protect from stemming.
Looking at the analysis page of the Solr administration pages, we can see that our
dogs word was protected from being stemmed, compared to the birds word which
was changed to bird.
www.it-ebooks.info
4
Querying Solr
In this chapter, we will cover:
ff Asking for a particular field value
ff Sorting results by a field value
ff How to search for a phrase, not a single word
ff Boosting phrases over words
ff Positioning some documents over others in a query
ff Positioning documents with words closer to each other first
ff Sorting results by a distance from a point
ff Getting documents with only a partial match
ff Affecting scoring with functions
ff Nesting queries
ff Modifying returned documents
ff Using parent-child relationships
ff Ignoring typos in terms of the performance
ff Detecting and omitting duplicate documents
ff Using field aliases
ff Returning a value of a function in the results
www.it-ebooks.info
Querying Solr
108
Introduction
Making a simple query is not a hard task, but making a complex one, with faceting, local
params, parameters dereferencing, and phrase queries can be a challenging task. On the top
of all that, you must remember to write your query with performance factors in mind. That's
why something that is simple at first sight can turn into something more challenging such as
writing a good, complex query. This chapter will try to guide you through some of the tasks you
may encounter during your everyday work with Solr.
Asking for a particular field value
There are many cases where you will want to ask for a particular field value. For example,
when searching for the author of a book in the Internet library or an e-commerce shop. Of
course Solr can do that, and this recipe will show you how to do it.
How to do it...
1. Let's start with the following index structure (just add the following to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. To ask for a value in the author field, send the following query to Solr:
http://localhost:8983/solr/select?q=author:rafal
That's all. The documents you'll get from Solr will be the ones with the requested value in the
author field. Remember that the query shown in the example uses the standard query parser,
not DisMax.
How it works...
We defined three fields in the index, but this was only for the purpose of the example. As
you can see in the previous query, to ask for a particular field value, you need to send a q
parameter syntax such as FIELD_NAME:VALUE, and that's all there is to it. Of course you can
add the logical operator to the query to make it more complex. Remember that if you omit the
field name from the query your values will be checked again in the default search field that is
defined in the schema.xml file.
www.it-ebooks.info
Chapter 4
109
There's more...
When asking for a particular field value, there are a few things that are useful to know:
Querying for a particular value using the DisMax query parser
Sometimes you may need to ask for a particular field value when using the DisMax query parser.
Unfortunately the DisMax query parser doesn't support full Lucene query syntax and thus you
can't send a query like that, but there is a solution to it. You can use the extended DisMax query
parser which is an evolved DisMax query parser. It has the same list of functionalities as DisMax
and it also supports full Lucene query syntax. The following is the query shown in this task, but
by using edismax, it would look like the following:
http://localhost:8983/solr/select?q=author:rafal&defType=edismax
Querying for multiple values in the same field
You may sometimes need to ask for multiple values in a single field. For example, let's
suppose that you want to find the solr and cookbook values in the title field. To do
that you should run the following query (notice the brackets surrounding the values):
http://localhost:8983/solr/select?q=author:(solr cookbook)
Sorting results by a field value
Imagine an e-commerce site where you can't choose the sorting order of the results, you
can only browse the search results page-by-page and nothing more. That's terrible, right?
That's why with Solr you can specify the sort fields and order in which your search results
should be sorted. This recipe will show you how to do it.
How to do it...
Let's assume that you want to sort your data by an additional field, for example, the field that
contains the name of the author of the book.
1. First we add the following to your schema.xml file's field section:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. Now, let's create a simple data file which will look like the following code:
<add>
<doc>
www.it-ebooks.info
Querying Solr
110
<field name="id">1</field>
<field name="title">Solr cookbook</field>
<field name="author">Rafal Kuc</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Solr results</field>
<field name="author">John Doe</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr perfect search</field>
<field name="author">John Doe</field>
</doc>
</add>
3. As I wrote earlier, we want to sort the result list by author name in ascending order.
Additionally, we want the books that have the same author to be sorted by relevance
in the descending order. To do that we must send the following query
to Solr:
http://localhost:8983/solr/select?q=solr&sort=author+asc,
score+desc
The results returned by Solr are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">solr</str>
<str name="sort">author asc,score desc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="title">Solr results</str>
<str name="author">John Doe</str>
</doc>
<doc>
<str name="id">3</str>
www.it-ebooks.info
Chapter 4
111
<str name="title">Solr perfect search</str>
<str name="author">John Doe</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solr cookbook</str>
<str name="author">Rafal Kuc</str>
</doc>
</result>
</response>
As you can see, our data is sorted exactly how we wanted it to be.
How it works...
As you can see I defined three fields in our index. The most interesting to us is the author
field, based on which we will perform the sorting operation. Notice one thing – the type on
which the field is based is the string type. In order to sort the values of the field from the
index, you need to prepare your data well, that is, use the appropriate number types (the ones
based on the Trie types), and to sort the text field using the string field type (or text type
using the KeywordTokenizer type and a lowercase filter).
The following what you see is the data which is very simple – it only adds three documents to
the index.
I've added one additional parameter to the query that was sent to Solr – the sort parameter.
This parameter defines the sort field with the order. Each field must be followed by the order in
which the data should be sorted; asc which tells Solr to sort the data in the ascending order,
and desc which tells Solr to sort in the descending order. Pairs of field and order should be
delimited with the comma character as shown in the example.
The result list that Solr returned tells us that we did a perfect job on defining the sort order.
How to search for a phrase, not a single
word
Imagine that you have an application that searches within millions of documents that
are generated by a law company. One of the requirements is to search the titles of the
documents as a phrase, but with stemming and lowercasing. So a string-based field is
not an option. In that case, is it possible to achieve this using Solr? Yes, and this recipe will
show you how to do that.
www.it-ebooks.info
Querying Solr
112
How to do it...
1. First let's define the following type (add this part to your schema.xml file):
<fieldType name="text" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.SnowballPorterFilterFactory"
language="English"/>
</analyzer>
</fieldType>
2. Now let's add the following fields to our schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
3. The third step is to create an example data which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">2012 report</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">2009 report</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">2012 draft report</field>
</doc>
</add>
4. Now let's try to find the documents that have the phrase 2012 report in them.
To do that, make the following query to Solr:
http://localhost:8983/solr/select?q=title:"2012 report"
The result should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Chapter 4
113
<int name="QTime">1</int>
<lst name="params">
<str name="q">title:"2012 report"</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="title">2012 report</str>
</doc>
</result>
</response>
The debug query (the debugQuery=on parameter) shows us the Lucene query that
was created:
<str name="parsedquery">PhraseQuery(title:"2012 report")</str>
As you can see we only got one document which is perfectly good. Now let's see how
that happened.
How it works...
As I said in the Introduction section, our requirement was to search for phrases over fields
that are stemmed and lowercased. If you want to know more about stemming please refer
to the Stemming your data recipe in Chapter 3, Analyzing Your Text Data. Lowercasing is
described in the Lowercasing the whole string recipe in Chapter 3.
We only need two fields because we will only search the title, and return the title and
unique identifier of the field; thus the configuration is as shown in the example.
The example data is quite simple so I'll skip commenting on it.
The query is something that we should be more interested in. The query is made to the standard
Solr query parser, thus we can specify the field name and the value we are looking for. The query
differs from the standard word searching query by the use of the " character at the start and
end of the query. It tells Solr to use the phrase query instead of the term query. Using the phrase
query means that Solr will search for the whole phrase not a single word. That's why only the
document with identifier 1 was found, because the third document did not match the phrase.
The debug query only ensured that the phrase query was made instead of the usual term
query, and Solr showed us that we created the right query.
www.it-ebooks.info
Querying Solr
114
There's more...
When using queries there is one thing that is very useful to know.
Defining the distance between words in a phrase
You may sometimes need to find documents that match a phrase, but are separated by some
other words. Let's assume that you would like to find the first and third document in
our example. This means that you want documents that could have an additional word
between the word 2010 and report. To do that, we add a so-called phrase slop to the
phrase. In our case the distance (slop) between words can be the maximum of one word, so
we add the ~1 part after the phrase definition:
http://localhost:8983/solr/select?q=title:"2012 report"~1
Boosting phrases over words
Imagine you are a search expert at a leading e-commerce shop in your region. One day
disaster strikes and your marketing department says that the search results are not good
enough. They would like you to favor documents that have the exact phrase typed by the
user over the documents that have matches for separate words. Can you do it? Of course
you can, and this recipe will show you how to achieve it.
Getting ready
Before you start reading this task I suggest you read the How to search for a phrase not a
single word recipe in this chapter. It will allow you to understand the recipe better.
How to do it...
I assume that we will be using the DisMax query parser, not the standard one. We will also
use the same schema.xml file that was used in the How to search for a phrase not a single
word recipe in this chapter.
1. Let's start with our sample data file which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">Annual 2012 report last draft</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">2011 report</field>
www.it-ebooks.info
Chapter 4
115
</doc>
<doc>
<field name="id">3</field>
<field name="title">2012 draft report</field>
</doc>
</add>
2. As I already mentioned, we would like to boost those documents that have phrase
matches over others matching the query. To do that, run the following query to your
Solr instance:
http://localhost:8983/solr/select?defType=dismax&pf=title^100&q=20
12+report&qf=title&q.op=AND
You should get the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="pf">title^100</str>
<str name="q">2012 report</str>
<str name="qf">title</str>
<str name="q.op">AND</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="title">Annual 2012 report last draft</str>
</doc>
<doc>
<str name="id">3</str>
<str name="title">2012 draft report</str>
</doc>
</result>
</response>
3. To visualize the results better, I decided to include the results returned by Solr for the
same query but without adding the pf parameter, and received the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
www.it-ebooks.info
Querying Solr
116
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="qf">title</str>
<str name=" defType">dismax</str>
<str name="q.op">AND</str>
<str name="q">2012 report</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">3</str>
<str name="title">2012 draft report</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Annual 2012 report last draft</str>
</doc>
</result>
</response>
As you can see we fulfilled our requirement.
How it works...
Some of the parameters that are present in the example query may be new to you. The first
parameter is defType that tells Solr which query parser we will be using. In this example we
will be using the DisMax query parser (if you are not familiar with the DisMax query parser
please have a look at the following address http://wiki.apache.org/solr/DisMax).
One of the features of this query parser is the ability to tell what field should be used to search
for phrases, and we do that by adding the pf parameter. The pf parameter takes a list of
fields with the boost that corresponds to them, for example, pf=title^100 which means
that the phrase found in the title field will be boosted with a value of 100. The q parameter is
the standard query parameter that you are familiar with. This time we passed the words we
are searching for and the logical operator AND. This means that we are looking for documents
which contain the words 2012 and report. You should remember that you can't pass queries
such as fieldname:value to the q parameter and use the DisMax query parser. The fields
you are searching against should be specified using the qf parameter. In our case we told Solr
that we will be searching against the title field. We also included the q.op=AND parameter
because we want AND to be our logical operator for the query.
The results show us that we found two documents. The one that matches the exact query is
returned first and that is what we intended to achieve.
www.it-ebooks.info
Chapter 4
117
There's more...
You can of course boost phrases with standard query parsers, but that's not as elegant as the
DisMax query parser method. To achieve similar results, you should run the following query to
your Solr instance:
http://localhost:8983/solr/select?q=title:(2012+AND+report)+OR+title:
"2012+report"^100
The above query tells Solr to search for the words 2010 and report in a title field, and
search for a 2012 report phrase and, if found, to boost that phrase with the value of 100.
Positioning some documents over others on
a query
Imagine a situation when your client tells you that he/she wants to promote some of his/her
products by placing them at the top of the search result list. Additionally, the client would like
the product list to be flexible, that is, he/she would like to be able to define the list for some
queries and not for others. Many thoughts come into your mind such as boosting, index time
boosting, or maybe some special field to achieve that. But don't bother, Solr can help you with
a component that is known as solr.QueryElevationComponent.
How to do it...
The following recipe will help you to place document over others based on your priorities:
1. First of all let's modify the solrconfig.xml document. We need to add
the component definition. To do that add the following section to your
solrconfig.xml file:
<searchComponent name="elevator" class="solr.
QueryElevationComponent" >
<str name="queryFieldType">string</str>
<str name="config-file">elevate.xml</str>
</searchComponent>
2. Now let's add the proper request handler that will include the elevation component.
We will name it /promotion. Add this to your solrconfig.xml file:
<requestHandler name="/promotion" class="solr.SearchHandler">
<lst name="defaults">
www.it-ebooks.info
Querying Solr
118
<str name="echoParams">explicit</str>
<int name="rows">10</int>
<str name="df">name</str>
</lst>
<arr name="last-components">
<str>elevator</str>
</arr>
</requestHandler>
3. You may notice that the query elevation component contained information about
a mysterious elevate.xml file. Let's assume that we want the documents with
identifiers 3 and 1 to be in the first two places in the results list for the solr
query. For now you need to create that file in the configuration directory of your
Solr instance and paste the following content:
<?xml version="1.0" encoding="UTF-8" ?>
<elevate>
<query text="solr">
<doc id="3" />
<doc id="1" />
</query>
</elevate>
4. Now it's time for the schema.xml file. Our field definition part of the file should
contain the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
5. Now let's index the following data file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr master pieces</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr annual report</field>
</doc>
</add>
www.it-ebooks.info
Chapter 4
119
6. Now we can run Solr and test our configuration. To do that let's run the following query:
http://localhost:8983/solr/promotion?q=solr
The previous query should return the following result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">solr</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Solr annual report</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr master pieces</str>
</doc>
</result>
</response>
The query without using the elevation component returned the following result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">solr</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
</doc>
www.it-ebooks.info
Querying Solr
120
<doc>
<str name="id">2</str>
<str name="name">Solr master pieces</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Solr annual report</str>
</doc>
</result>
</response>
As you can see the component worked. Now let's see how it works.
How it works...
The first part of the configuration defines a new search component with a name under
which the component will be visible to other components and the search handler (the
name attribute). In our case the component name is elevator and it's based on the
solr.QueryElevationComponent class (the class attribute). The following that
we have are two additional attributes that define the elevation component's behavior:
ff queryFieldType: This attribute tells Solr what type of field should be used to parse
the query text that is given to the component (for example, if you want the component
to ignore the letter case, you should set this parameter to the field type that
lowercases its contents)
ff config-file: This specifies the configuration file which will be used by the component
The next part of the solrconfig.xml configuration procedure is the search handler
definition. It simply tells Solr to create a new search handler with the name of /promotion
(to be the value of the name attribute) and using the solr.SearchHandler class (the class
attribute). In addition to that, the handler definition also tells Solr to include the component
named elevator in this search handler. This means that this search handler will use our
defined component. For your information, you can use more than one search component in a
single search handler. We've also included some standard parameters to the handler, such as
df, which specifies the default search field.
What we see next is the actual configuration of the elevator component. You can see that
there is a query defined (the query XML tag) with an attribute text="solr". This defines
the behavior of the component when a user passes solr to the q parameter. Under this tag
you can see a list of the documents' unique identifiers that will be placed at the top of the
results list for the defined query. Each document is defined by a doc tag and an id attribute
(which have to be defined on the basis of solr.StrField) which holds the unique identifier.
You can have multiple query tags in a single configuration file which means that the elevation
component can be used for a variety of queries.
www.it-ebooks.info
Chapter 4
121
The index configuration and example datafile are fairly simple. The index contains two fields
that are responsible for holding information about the document. In the example datafile,
we can see three documents present. As the explanation is not crucial, I'll skip discussing
it further.
The query you see in the example returns all the documents. The query is made to our new
handler with just a simple one word q parameter (the default search field is set to name in the
schema.xml file). Recall the elevate.xml file and the documents we defined for the query
we just passed to Solr. We told Solr that we want the document with id=3 in the first place of
the results list and we want the document with id=1 in the second place of the results list. As
you can see, the documents were positioned exactly as we wanted them so it seems that the
component did its job.
There's more...
There is one more thing I would like to say about the query elevation functionality in Solr.
Excluding documents with QueryElevationComponent
The elevate component can not only place documents on top of the results list, but it can also
exclude documents from the results list. To do that you should add the exclude="true"
attribute to the document definition in your elevate.xml file. This is what the example file
would look like:
<?xml version="1.0" encoding="UTF-8" ?>
<elevate>
<query text="solr">
<doc id="3" />
<doc id="1" exclude="true" />
<doc id="2" exclude="true" />
</query>
</elevate>
See also
If you would like to know how to mark the documents that were positioned by the solr.
QueryElevationComponent class, please read the Modifying returned documents recipe
in this chapter.
www.it-ebooks.info
Querying Solr
122
Positioning documents with words closer to
each other first
Imagine an e-commerce book shop where the users have only one way to find books, that is,
by searching. Most of the users requested that the OR operator should be the default logical
operator, so that we can have many results for most of the popular queries. Once every few
days an angry user calls the call center and says that by typing "solr cookbook" the first few
pages are not relevant to the query he/she typed in, so in other words this is not what he/
she searched for. So that's the problem, now what can be done? The answer is to boost
documents with query words closer to each other. This recipe will show you how to do it.
How to do it...
For the purpose of this task I will be using the DisMax query parser.
1. Let's start with the following index structure (just add the following to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. Now, let's index the following data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solr perfect search cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Solr example cookbook</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
3. In addition to that we need to define a new request handler in the solrconfig.xml
file, which looks like the following code:
<requestHandler name="/closer" class="solr.
StandardRequestHandler">
www.it-ebooks.info
Chapter 4
123
<lst name="defaults">
<str name="q">_query_:"{!dismax qf=$qfQuery mm=1 pf=$pfQuery
bq=$boostQuery v=$mainQuery}"</str>
<str name="qfQuery">title</str>
<str name="pfQuery">title^1000</str>
<str name="boostQuery">_query_:"{!dismax qf=$boostQueryQf
mm=100% v=$mainQuery}"^100</str>
<str name="boostQueryQf">title</str>
<str name="df">title</str>
</lst>
</requestHandler>
4. As I wrote earlier, we want to get the documents with the words typed by the user close
to each other first in the result list. Let's assume our user typed in the dreaded solr
cookbook query. To handle the query we use the new /closer request handler we
defined earlier and we send the query using the mainQuery parameter, not the q one
(I'll describe why this is so later). So the whole query looks as follows:
http://localhost:8983/solr/closer?mainQuery=solr+cookbook&fl=score,
id,title
The result list returned by Solr is the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
</lst>
<result name="response" numFound="3" start="0"
maxScore="0.93303263">
<doc>
<str name="id">3</str>
<str name="title">Solr cookbook</str>
<float name="score">0.93303263</float>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solr perfect search cookbook</str>
<float name="score">0.035882458</float>
</doc>
<doc>
<str name="id">2</str>
<str name="title">Solr example cookbook</str>
<float name="score">0.035882458</float>
www.it-ebooks.info
Querying Solr
124
</doc>
</result>
</response>
We received the documents in the way we wanted. Now let's look at how that happened.
How it works...
First of all we have the index structure. For the purpose of the example, I assumed that our
book description will consist of three fields and we will be searching with the use of the title
field which is based on the standard text field defined in the standard schema.xml
file provided with the Solr distribution.
As you can see in the provided data file example there are three books. The first book has
two other words between the words solr and cookbook. The second book has one word
between the given words, and the third book has the words next to each other. In a perfect
situation, we would like to have the third book from the example file as the first one in the
result list, the second book from the file in the second place, and the first book from the
example data file as the last in the results list.
Now let's take a look at our new request handler. We defined it to be available under a name
/closer (the name attribute of the requestHandler tag). We also said that it should be
based on the solr.StandardRequestHandler class (in the class attribute). Next, in
the defaults list we have a number of parameters defining the query behavior we want to
achieve. First of all we have the q parameter. This contains the query constructed with the
use of local params. The _query_:"…" part of the q parameter is a way of specifying the
new query. We tell Solr that we want to use DisMax query parser (the !dismax part) and we
want to pass the value of the qfQuery parameter as the qf DisMax parser parameter. We
also want the "minimum should match" parameter to be equal one (mm=1), we want a phrase
query to be used (the one which is defined in the pfQuery (pf=$pfQuery) parameter) and
we want the boost query to be used – the one that is defined in the boostQuery parameter
(bq=$boostQuery). Finally we specify that we will pass the actual user query not with the q
parameter, but instead with the mainQuery parameter (v=$mainQuery).
Next we have boostQuery – another query constructed using local parameters. As you can
see we use the DisMax query parser (the !dismax query part), and we specify the qf and
mm DisMax query parser parameters. The value of the qf parameter will be taken from the
boostQueryQf parameter and the value of the mm parameter is set to 100%, so we want the
boost query to return only the documents that have all the words specified by the user. The
v attribute is responsible for passing the actual query, which in our case will be stored in the
mainQuery (v=$mainQuery part) parameter. We also said that we want our boost query
to be boosted by 100 (the ^100 part of the query). The boostQueryQf parameter is used
by the boost query to specify which fields should be used for search, in the query used for
boosting. Finally, the df parameter specifies the default search field.
www.it-ebooks.info
Chapter 4
125
Now let's discuss what the query is actually doing. The first query tells Solr to return all the
documents with at least one of the words that the user entered (this is defined by setting the
mm parameter to 1). But we also say that we want to boost phrases; that's why we use a phrase
query on the title field. In addition to that we specified that we want to use our boost query,
which will increase the boost of all the documents that have all the words entered by the user
(mm=100%). Combining all those factors we will end up with results that have the top documents
occupied by those documents that have all the words entered by the user present in the title
field and where all those words are close to each other.
The query is simple. We specify that we want to get a calculated score in the results for
each document, we want the id field, and the title field. We also pass the mainQuery
parameter because we have the v attribute of both the q and boostQuery parameters set
to the $mainQuery parameter. This means that Solr will take the mainQuery parameter
value and pass it to the v parameter of those queries. Because we prepared our request
handler configuration and pasted it into solrconfig.xml, now at query time we only
need to pass a single parameter that passes the words specified by our users.
The last thing is the results list. As you can see the documents are sorted in the way we
wanted them to be. You should take a look at one thing – the score field. This field shows
how relevant the document is to the query we sent to Solr.
Sorting results by a distance from a point
Suppose we have a search application that is storing information about the companies.
Every company is described by a name and two floating point numbers that represent the
geographical location of the company. One day your boss comes to your room and says that
he/she wants the search results to be sorted by distance from the user's location. This recipe
will show you how to do it.
Getting ready
Before continuing please read the Storing geographical points in the index recipe from
Chapter 3, Analyzing Your Text Data.
How to do it...
1. Let's begin with the following index (add the following to your schema.xml file to the
fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"/>
www.it-ebooks.info
Querying Solr
126
<field name="location" type="location" indexed="true"
stored="true" />
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
2. We also have the following type defined in the schema.xml file:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
I assumed that the user location will be provided from the application that is making
a query.
3. Now let's index our example data file, which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="location">56.4,40.2</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="location">50.1,48.9</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="location">23.18,39.1</field>
</doc>
</add>
4. So our user is standing at the North Pole and is using our search application. Now
let's assume that we want to get the companies sorted in such a way that the ones
that are nearer the user are at the top of the results list. The query to find such
companies could look like the following query:
http://localhost:8983/solr/select?q=company&sort=geodist(location,
0.0,0.0)+asc
The result of that query would look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Chapter 4
127
<int name="QTime">1</int>
<lst name="params">
<str name="q">company</str>
<str name="sort">geodist(location,0.0,0.0) asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="location">23.18,39.1</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="location">56.4,40.2</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company 2</str>
<str name="location">50.1,48.9</str>
</doc>
</result>
</response>
If you would like to calculate the distance by hand, you would see that the results are
sorted as they should be.
How it works...
As you can see in the index structure and in the data, every company is described by the
following three fields:
ff id: This specifies the unique identifier
ff name: This specifies the company name
ff location: This specifies the latitude and longitude of the company location
I'll skip commenting on how the actual location of the company is stored. If you want to
read more about it, please refer to the Storing geographical points in the index recipe
from Chapter 3, Analyzing Your Text Data.
www.it-ebooks.info
Querying Solr
128
We wanted to get the companies that match the given query and are sorted in the ascending
order from the North Pole. To do that we run a standard query with a non-standard sort.
The sort parameter consists of a function name, geodist, which calculates the distance
between points. In our example the function takes three parameters:
ff The first parameter specifies the field in the index that should be used to calculate
the distance
ff The second parameter is the latitude value of the point from which the distance will
be calculated
ff The third parameter is the longitude value of the point from which the distance will
be calculated
After the function there is the order of the sort which in our case is asc (ascending order).
See also
If you would like to learn how to return the calculated distance that we used for sorting please
refer to the Returning the value of a function in results recipe in this chapter.
Getting documents with only a partial match
Imagine a situation where you have an e-commerce library and you want to make a search
algorithm that tries to bring the best search results to your customers. But you noticed that
many of your customers tend to make queries with too many words, which result in an empty
results list. So you decided to make a query that will require the maximum of two of the
words that the user entered to be matched. This recipe will show you how to do it.
Getting ready
This method can only be used with the DisMax query parser. The standard query parser doesn't
support the mm parameter.
How to do it...
1. Let's begin with creating our index that has the following structure (add this to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
As you can see our books are described by two fields.
www.it-ebooks.info
Chapter 4
129
2. Now let's look at the example data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solrcook book revised</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book that was revised</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Another revised book</field>
</doc>
</add>
3. The third step is to made a query that will satisfy the requirements. Such a query
could look like the following:
http://localhost:8983/solr/select?q=book+revised+another+
different+word+that+doesnt+count&defType=dismax&mm=2&q.op=AND
The preceding query will return the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q.op">AND</str>
<str name="mm">2</str>
<str name="q">book revised another different word that doesnt
count</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
</doc>
<doc>
<str name="id">2</str>
www.it-ebooks.info
Querying Solr
130
<str name="title">Some book that was revised</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
</doc>
</result>
</response>
As you can see, even though the query was made up of too many words, the result list contains
all the documents from the example file. Now let's see how that happened.
How it works...
The index structure and the data are fairly simple. Every book is described by two fields:
a unique identifier and a title.
The query is the thing that we are interested in. We have passed about eight words to Solr
(the q parameter), we defined that we want to use the DisMax query parser (the defType
parameter), and we sent the mysterious mm parameter set to the value of 2. Yes, you are right,
the mm parameter, also called minimum should match, tells the DisMax query parser how
many of the words passed into the query must be matched with the document, to ascertain
that the document is a match. In our case we told the DisMax query parser that there should
be two or more words matched to identify the document as a match. We've also included
q.op=AND, so that the default logical operator for the query would be set to AND.
You should also note one thing – the document that has three words matched is at the top
of the list. The relevance algorithm is still there, which means that the documents with more
words that matched the query will be higher in the result list than those that have fewer
words that matched the query. The documentation about the mm parameter can be found at
http://wiki.apache.org/solr/DisMaxQParserPlugin.
Affecting scoring with functions
There are many situations where you would want to have an influence on how the score of the
documents is calculated. For example, you would perhaps like to boost the documents on the
basis of the purchases of it. Like in an e-commerce boost store, you would like to show relevant
results, but you would like to influence them by adding yet another factor to their score. Is it
possible? Yes, and this recipe will show you how to do it.
www.it-ebooks.info
Chapter 4
131
How to do it...
1. Let's start with the following index structure (just add the following to the field section
in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="sold" type="int" indexed="true" stored="true" />
2. The example data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solrcook book revised</field>
<field name="sold">5</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book revised</field>
<field name="sold">200</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Another revised book</field>
<field name="sold">60</field>
</doc>
</add>
3. So we want to boost our documents on the basis of a sold field while retaining the
relevance sorting. Our user typed revised into the search box, so the query would
look like the following:
http://localhost:8983/solr/select?defType=dismax&qf=title&q=revise
d&fl=*,score
And the results would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">title</str>
www.it-ebooks.info
Querying Solr
132
<str name="fl">*,score</str>
<str name="q">revised</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="0.35615897">
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
<int name="sold">5</int>
<float name="score">0.35615897</float>
</doc>
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
<int name="sold">200</int>
<float name="score">0.35615897</float>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
<int name="sold">60</int>
<float name="score">0.35615897</float>
</doc>
</result>
</response>
4. Now let's add the sold factor by adding the following to the query:
bf=product(sold)
So our modified query would look like this:
http://localhost:8983/solr/select?defType=dismax&qf=title&q=revise
d&fl=*,score&bf=product(sold)
And the results for the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">36</int>
<lst name="params">
<str name="fl">*,score</str>
www.it-ebooks.info
Chapter 4
133
<str name="q">revised</str>
<str name="qf">title</str>
<str name="bf">product(sold)</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="163.1048">
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
<int name="sold">200</int>
<float name="score">163.1048</float>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
<int name="sold">60</int>
<float name="score">49.07608</float>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
<int name="sold">5</int>
<float name="score">4.279089</float>
</doc>
</result>
</response>
As you can see, adding the parameter changed the whole results list. Now let's see why
that happened.
How it works...
The schema.xml file is simple. It contains the following three fields:
ff id: This field is responsible for holding the unique identifier of the book
ff title: This specifies the book title
ff sold: This specifies the number of pieces that have been sold during the last month
In the data we have three books. Each of the books has the same number of words in the title.
That's why when typing the first query all documents got the same score. As you can see,
the first book is the one with the fewest pieces sold and that's not what we want to achieve.
www.it-ebooks.info
Querying Solr
134
For the same reason we added the bf parameter. It tells Solr what function to use to affect
the scoring computation (in this case the result of the function will be added to the score of
the document). In our case it is the product function that returns the product of the values
we provide as its arguments; in our case the one and only argument of the function will be
the value of the book's sold field.
The result list of the modified query clearly shows how the scoring was affected by the function.
In the first place of the results list we have the book that was most popular during the last week.
The next book is the one which was less popular than the first book, but more popular than the
last book. The last book in the results is the least popular book.
See also
If you would like to know more about the functions available in Solr, please go to the Solr wiki
page at the following address: http://wiki.apache.org/solr/FunctionQuery.
Nesting queries
Imagine a situation where you need a query nested inside another query. Let's imagine that
you want to run a query using the standard request handler but you need to embed a query
that is parsed by the DisMax query parser inside it. This is possible with Solr 4.0 and this
recipe will show you how to do it.
How to do it...
1. Let's start with a simple index that has the following structure (just add the following
to the field section in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
2. Now let's look at the example data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Revised solrcook book</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book revised</field>
</doc>
<doc>
www.it-ebooks.info
Chapter 4
135
<field name="id">3</field>
<field name="title">Another revised little book</field>
</doc>
</add>
3. Imagine you are using the standard query parser to support the Lucene query syntax,
but you would like to boost phrases using the DisMax query parser. At first it seems
that it is impossible, but let's assume that we want to find books that have the words
book and revised in their title field, and we want to boost the book revised
phrase by 10. Let's send a query like so:
http://localhost:8983/solr/select?q=book+revised+_query_:"{!dismax
qf=title pf=title^10 v=$qq}"&qq=book+revised&q.op=AND
The results of the preceding query should look like the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
<lst name="params">
<str name="q.op">AND</str>
<str name="qq">book revised</str>
<str name="q">book revised _query_:"{!dismax qf=title
pf=title^10 v=$qq}"</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Revised solrcook book</str>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised little book</str>
</doc>
</result>
</response>
As you can see, the results list was sorted exactly the way we wanted. Now let's see how it works.
www.it-ebooks.info
Querying Solr
136
How it works...
As you can see our index is very simple. It consists of two fields – one holding the unique
identifier (the id field) and another one holding the title of the book (the title field).
Let's look at the query. The q parameter is built from two parts. The first one, book+revised,
is just a usual query composed from two terms. The second part of the query starts with a
strange looking expression, that is, _query_. This expression tells Solr that another query
should be made that will affect the results list. Notice that the expression is surrounded
with " characters. Then we will see the expression tells Solr to use the DisMax query parser
(the !dismax part) and the parameters that will be passed to the parser (qf and pf). The
v parameter is used to pass the value of the q parameter. The value passed to the DisMax
query parser in our case will be book+revised. This is called parameter dereferencing.
By using the $qq expression, we tell Solr to use the value of the qq parameter. Of course,
we could pass the value to the v parameter, but I wanted to show you how to use the
dereferencing mechanism. The qq parameter is set to book+revised and it is used by Solr
as a parameter for the query that was passed to the DisMax query parser. The last parameter,
q.op=AND tells Solr which logical operator should be used as the default one.
The results show that we achieved exactly what we wanted.
Modifying returned documents
Let's say we are using the elevate component that Solr provides to promote some books
when necessary. But as you may already know, the standard Solr response doesn't include
the information about document being elevated or not. What we would like to achieve is to
get that information somehow from Solr. Actually we would like it to be as simple as running
a Solr query and getting the results back. This recipe will show you how to use document
transformers with the elevation component.
How to do it...
1. First of all, let's assume we have the following index structure defined in the fields
section of our schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
2. We also need to have the elevation component defined along with the search
component (place the following entries in your solrconfig.xml file):
<requestHandler name="/select" class="solr.SearchHandler">
<lst name="defaults">
www.it-ebooks.info
Chapter 4
137
<str name="echoParams">explicit</str>
<int name="rows">10</int>
<str name="df">name</str>
</lst>
<arr name="last-components">
<str>elevator</str>
</arr>
</requestHandler>
<searchComponent name="elevator" class="solr.
QueryElevationComponent">
<str name="queryFieldType">string</str>
<str name="config-file">elevate.xml</str>
</searchComponent>
3. The contents of the elevate.xml file located in the conf directory look like the
following code:
<elevate>
<query text="book">
<doc id="3" />
</query>
</elevate>
4. Our example data that we indexed looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Promoted document</field>
</doc>
</add>
5. Now let's query Solr with the following query:
http://localhost:8983/solr/select?q=book&df=name&fl=*,[elevated]
www.it-ebooks.info
Querying Solr
138
And the response we get from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">book</str>
<str name="df">name</str>
<str name="fl">*,[elevated]</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Promoted document</str>
<bool name="[elevated]">true</bool>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<bool name="[elevated]">false</bool>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Book 2</str>
<bool name="[elevated]">false</bool>
</doc>
</result>
</response>
As you can see each document does not only have its name and identifier, but also the
information about whether it was elevated or not.
How it works...
Our index structure consists of two fields, the id field which is our unique key and the name
field used for holding the name of the document. Please remember that in order to use the
elevation component you have to have a unique key defined in your schema.xml file, and this
field has to be based on the string type.
The /select request handler configuration is quite standard, although we've added the
last-components sections that define what component should be used during a query.
We defined that we want to use the component named elevator.
www.it-ebooks.info
Chapter 4
139
The next thing we did is the elevator search component definition. It is based on the solr.
QueryElevationComponent class (the class attribute) and we set its name to elevator
(the attribute name). In addition to that, we specified two attributes needed by the query
elevation component:
ff queryFieldType: This specifies the name of the type that will be used to analyze
the incoming text. We specified the string type because we want only exact
matches to include elevated documents.
ff config-file: This specifies the name of the configuration file that stores the
elevation definitions.
The elevate.xml file we use for storing the query elevation component is simple. The root tag
is named elevate and can have multiple query tags inside it. Each query tag is responsible
for elevating documents for a query defined with the text attribute. Inside the query tag we
can have multiple doc tags with an id attribute, which should have a value of the identifier of
the document to which we want add to results or modify positions. In our case, we want the
document with an identifier value of 3 to be placed in the first position when users enter the
book query.
The query we sent was simple; we asked for documents that have book in the default field
(the df parameter) which is name in our case. In addition to that, we want all stored fields to
be returned (the * part of the fl parameter) and we also want to activate one of the document
transformers, which is responsible for marking the documents that were elevated by the query
elevation component, by adding the [elevated] part of the fl parameter. This transformer
adds the <bool name="[elevated]">true</bool> field if the document was elevated,
and <bool name="[elevated]">false</bool> if the document wasn't elevated.
Using parent-child relationships
When using Solr you are probably used to having a flat structure of documents without any
relationships. However, there are situations where decomposing relationships is a cost we
can't take. Because of that Solr 4.0 comes with a join functionality that allows us to use some
basic relationships. For example, imagine that our index consists of books and workbooks and
we would like to use that relationship. This recipe will show you how to do it.
How to do it...
1. First of all, let's assume that we have the following index structure (just place the
following in the fields section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"
multiValued="false"/>
www.it-ebooks.info
Querying Solr
140
<field name="type" type="string" indexed="true" stored="true"/>
<field name="book" type="string" indexed="true" stored="true"/>
2. Now let's look at our test data that we are going to index:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="type">book</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="type">book</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook A</field>
<field name="type">workbook</field>
<field name="book">1</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook B</field>
<field name="type">workbook</field>
<field name="book">2</field>
</doc>
</add>
3. Now, let's assume we want to get all the books from Solr that have workbooks
for them. Also we want to narrow the books we got to only those that have the
character 2 in their names. In order to do that, we run the following query:
http://localhost:8983/solr/select/?q={!join from=book to=id}
type:workbook&fq=name:2
The Solr response for the preceding query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
www.it-ebooks.info
Chapter 4
141
<lst name="params">
<str name="fq">name:2</str>
<str name="q">{!join from=book to=id}type:workbook</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Book 2</str>
<str name="type">book</str>
</doc>
</result>
</response>
As you can see, the returned document was exactly the one we expected.
How it works...
Although the example index structure is simple I would like to comment on it. The id field is
responsible for holding the unique identifier of the document, the name field is the document
name, and the type field holds the document's types. The book field is optional and specifies
the identifier of the parent document. So you can see that in our example data, we have two
parent documents (those with an id field value of 1 and 2) and two child documents (those
with an id field value of 3 and 4).
Let's pause for a bit now before looking at the query, and look at our example data. If we only
query for workbooks, we would get documents with identifier values of 3 and 4. The parent
for the document with the id field equal to 3 is 1, and the parent for the document with the
id field equal to 4 is 2. If we filter 1 and 2 with the filter fq=name:2, we should only get the
document with the id field value equal to 2 as the result. So looking at the query result it
works as intended, but how does the query actually work?
I'll begin the description from the join part, that is, q={!join from=book to=id}
type:workbook. As you can see we used local params to choose the different type of
query parser – the join query parser (the !join part of the query). We specified that child
documents should use the book field (the from parameter) and join it with the id field (the to
parameter). The type:workbook part specifies the query we run, that is, we want only those
documents that have the workbook value in the type field. The fq parameter, which narrows
the result set to only those documents that have the value 2 in the name field, is applied after
the join is executed, so we only apply it to the parent documents.
www.it-ebooks.info
Querying Solr
142
Ignoring typos in terms of performance
Sometimes there are situations where you would like to have some kind of functionality that
would allow you to give your user the search results even though he/she made a typo or even
multiple typos. In Solr, there are multiple ways to undo that: using a spellchecker component
to try and correct the user's mistake, using the fuzzy query, or for example, using the ngram
approach. This recipe will concentrate on the third approach and show you how to use ngrams
to handle user typos.
How to do it...
For the purpose of the recipe, let's assume that our index is built up of four fields:
identifier, name, description, and the description_ngram field which
will be processed with the ngram filter.
1. So let's start with the fields definition of our index which should look like the following
code (place this in your schema.xml file in the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true" stored="true"
/>
<field name="description_ngram" type="text_ngram" indexed="true"
stored="false" />
2. As we want to use the ngram approach, we will include the following filter in our
text_ngram field type definition:
<filter class="solr.NGramFilterFactory" minGramSize="2"
maxGramSize="2" />
The filter will be responsible for dividing the indexed data and queries into two
bi-grams. To better illustrate what I mean, take a look at the following screenshot,
which shows how the filter worked for the word "multiple":
www.it-ebooks.info
Chapter 4
143
So the whole text_ngram type definition will look like the following code:
<fieldType name="text_ngram" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.NGramFilterFactory" minGramSize="2"
maxGramSize="2" />
</analyzer>
</fieldType>
3. We also need to add the copy field definition to our schema.xml file, to automatically
copy the value of the description field to the description_ngram field. The copy
field definition looks as follows:
<copyField source="description" dest="description_ngram" />
4. Now we can index our data. For the purpose of the recipe I used the following
data sample:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook 4.0</field>
<field name="description">Solr Cookbook 4.0 contains multiple
recipes helping you with your every day work with Solr :)</field>
</doc>
</add>
5. After indexing it, I decided to test if my query can handle a single typo in each of the
words provided to the query, so I've sent the following query to Solr, where the words
I was really interested in were "contains" and "multiple":
q=description:(kontains+multyple) description_
ngram:(kontains+multyple)&q.op=OR
The result of the query was as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">description:(kontains multyple) description_
ngram:(kontains multyple)</str>
www.it-ebooks.info
Querying Solr
144
<str name="q.op">OR</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<str name="description">Solr Cookbook 4.0 contains multiple
recipes helping you with your every day work with Solr :)</
str></doc>
</result>
</response>
As you can see the document we were interested in was found. So let's see how that worked.
How it works...
As you can see from the index structure, we have two fields, namely name and description,
which we defined to use the text_ngram field because we want these fields to support the
returning of the search results even when the user enters a typo of some sort. To allow this
we use the solr.NGramFilterFactory filter with two attributes defined, namely, the
minGramSize which sets the minimum size of the produced ngram, and the maxGramSize
which sets the maximum size of the produced ngram. With both of these attributes set to 2,
we configured the solr.NGramFilterFactory filter to produce tokens called 2-grams, that
are built of two characters. The third attribute of the filter tag is the class attribute that
specifies
the filter factory class we want to use.
Let's concentrate on the provided screenshot (refer to step 2 in the How to do it... section) to
discuss how the solr.NGramFilterFactory filter works in our case. As I wrote earlier, we
want the ngram filter to produce grams built up of two characters. You can see how the filter
we've chosen works. From the word multiple it created the following bi-grams (n-grams built
from 2 characters):
mu ul lt ti ip pl le
So, the idea of the algorithm is quite simple – divide the word, so that we take the first
character and the character after it, and we make a bi-gram from it. Then we take the next
character and the character after it and create the second bi-gram and so on until we can't
make any more bi-grams.
www.it-ebooks.info
Chapter 4
145
Now if you look at the query there are two words we are looking for and both of them contain
a typo. The kontains word should be contain without a typo and the multyple should be
multiple without a typo. Our query also specifies that the logical query operator we want to
use is the OR operator. We use it because we want to match all documents with even a single
match to any bi-gram. If we turn the kontains and multyple tokens into bi-grams, we would
get the following (I'll use the pipe (|) character to separate the words from each other):
ko on nt ta ai in ns | mu ul lt ty yp pl le
If we turn the contains multiple tokens into bi-grams we would get the following:
co on nt ta ai in ns | mu ul lt ti ip pl le
If you compare those bi-grams you would see that only three of those differ between the proper
words and the ones with typos. The rest of them are the same. Because of that our query
finds the document we indexed. You may wonder why we queried both the description and
description_ngram fields. We did that because we don't know if the client's query is the
one with typos or without. If it is without, we want the documents with better matches to be
higher up on the results lists, than the ones that are not perfectly matched.
Of course all of that doesn't come without any downsides. One of the major downsides of this
approach is the growth of the index size because of the number of tokens produced by the
ngram filter. The second downside is the number of results produced with such an approach;
there will be many more results than you are used to and that's why we did a query to both the
description and description_ngram fields. We wanted to increase the score value of
the perfectly matched documents (you can also boost the description field higher during a
query). You can also try having the same approach work with the edismax query parser and
the "minimum should match" (mm) parameter, but this is beyond the scope of this recipe.
Detecting and omitting duplicate documents
Imagine your data consists of duplicates because they come from different sources. For
example, you have books that come from different suppliers, but you are only interested in a
single book with the same name. Of course you could use the field collapsing feature during the
query, but that affects query performance and we would like to avoid that. This recipe will show
you how to use the Solr deduplication functionality.
How to do it...
1. We start with the simple index structure. This should be placed in the fields section
of your schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
www.it-ebooks.info
Querying Solr
146
<field name="name" type="text" indexed="true" stored="true"
multiValued="false"/>
<field name="type" type="string" indexed="true" stored="true"
multiValued="false"/>
2. For the purpose of the recipe, we assume that we have the following data stored in
the data.xml file:
<add>
<doc>
<field name="name">This is a book we are indexing and we think
it will be a dupe because it's almost the same as the second document
we are going to index</field>
<field name="type">book</field>
</doc>
<doc>
<field name="name">This is the book we are indexing and we think
it will be a dupe because it's almost the same as the second
document we are going to index</field>
<field name="type">book</field>
</doc>
</add>
As you can see, the file contains two documents and they only differ by a single word;
the first document contains is the is a book phrase, while the second contains the
is the book phrase. In my opinion the second document is a dupe of the first one.
3. In order to have those two documents detected and overwritten, we need to create
a new update request processor chain called dedupe and configure org.apache.
solr.update.processor.SignatureUpdateProcessorFactory as the first
update processor. So the appropriate section of our solrconfig.xml file should
look like the following code:
<updateRequestProcessorChain name="dedupe">
<processor class="org.apache.solr.update.processor.
SignatureUpdateProcessorFactory">
<bool name="enabled">true</bool>
<bool name="overwriteDupes">true</bool>
<str name="signatureField">id</str>
<str name="fields">name</str>
<str name="signatureClass">org.apache.solr.update.processor.
TextProfileSignature</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
www.it-ebooks.info
Chapter 4
147
4. Now let's index our data by running the following command:
curl 'http://localhost:8983/solr/update?update.
chain=dedupe&commit=true' --data-binary @data.xml -H 'Contenttype:
application/xml'
5. If everything went well, we should only see the second document as the first one
should be overwritten. So we should check that by running the following query:
http://localhost:8983/solr/select?q=*:*
The response to it was the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="name">This is the book we are indexing and we think it
will be a dupe because it's almost the same as the second
document we are going to index</str>
<str name="type">book</str>
<str name="id">a095014df10f76513387af0450768ffb</str>
</doc>
</result>
</response>
As you can see we got only a single document, and if you look again at the example data,
you would notice that it is the second document we sent, so the first one was overwritten.
How it works...
Our index structure is simple and consists of three fields – the id field which holds the unique
identifier, the name field which is a name of the book, and the type field which holds the type
of the book.
The example data you see doesn't contain the id field, which isn't a mistake, it was prepared
this way on purpose. We want our deduping to use the id field to generate a unique identifier
for us and use it to overwrite duplicate documents. Also, you can see that the two sample
documents are almost the same, so they should be marked as dupes and we should only
see one of them in the index, probably the second one.
www.it-ebooks.info
Querying Solr
148
Next we define a new update request processor chain in the solrconfig.xml file
with the name dedupe (the name property). The first processor we need to add in order
to have the deduping functionality is org.apache.solr.update.processor.
SignatureUpdateProcessorFactory. We do so by setting the class attribute of the
processor tag to the mentioned class. The next few properties configure the org.apache.
solr.update.processor.SignatureUpdateProcessorFactory behavior. By setting
the enabled property to true, we turn on the deduping mechanism. overwriteDupes set to
true tells Solr that we want the duplicate documents to be overwritten. The signatureField
field configures the name of the field where the generated signature will be stored, which in our
case is the id field. This is crucial, because Solr will use that information to identify duplicate
documents. The fields field contains information of which fields (a list separated by the
comma character) should be used to identify the duplication. We decided to use the name
field. Finally, the signatureClass class is the class implementing the signature calculation.
We've chosen org.apache.solr.update.processor.TextProfileSignature because
it works best on longer text and we expect that. You can also choose org.apache.solr.
update.processor.MD5Signature and org.apache.solr.update.processor.
Lookup3Signature. The last two processors, solr.LogUpdateProcessorFactory and
solr.RunUpdateProcessorFactory, write information about the update to the log file and
run the update.
As you can see in the response for our "match all documents" query, only the second document
is present. This is because when the index was empty the first document was indexed. Then, the
second document came and it was identified as a dupe and thus it overwrote the first one.
Using field aliases
Imagine your products have multiple prices, and depending on your client's location you search
one of the defined fields. So you have a field for price in US dollars, in Euros, and so on. But what
you would like to do is return the field you are using for displaying the price of the document as a
"price" no matter what field you use. This recipe will show you how to do it.
How to do it...
1. Let's begin with the following index structure (put all the entries in the fields
section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="price_usd" type="double" indexed="true" stored="true"
/>
<field name="price_eur" type="double" indexed="true" stored="true"
/>
<field name="price_pln" type="double" indexed="true" stored="true"
/>
www.it-ebooks.info
Chapter 4
149
2. We will also use the following test data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook 4.0</field>
<field name="price_usd">40.00</field>
<field name="price_pln">120.00</field>
<field name="price_eur">30.00</field>
</doc>
</add>
3. Let's assume that we have a client from the United States of America and he/she
searches for the word solr and for products with the price in US dollars ranging from
20 to 50. The query would look like the following:
q=name:solr&fq=price_usd:[20+TO+50]&fl=id,name,price_usd
And the results of the preceding query would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price_usd:[20 TO 50]</str>
<str name="fl">id,name,price_usd</str>
<str name="q">name:solr</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<double name="price_usd">40.0</double>
</doc>
</result>
</response>
4. As you can see, we have our sample document returned but we've got the price_
usd value returned as well. We would like it to be named price. So let's modify
our fl parameter value, and instead of specifying id,name,price_usd we pass
id,name,price:price_usd. So the whole query would look as follows:
q=name:solr&fq=price_usd:[20+TO+50]&fl=id,name,price:price_usd
www.it-ebooks.info
Querying Solr
150
And the returned results would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price_usd:[20 TO 50]</str>
<str name="fl">id,name,price:price_usd</str>
<str name="q">name:solr</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<double name="price">40.0</double>
</doc>
</result>
</response>
As you can see in the result document we got a field called price instead of price_usd
field. Now, let's see how that works.
How it works...
The index structure is pretty simple, it only contains the field responsible for holding the
identifier, name of the document, and three prices in different currencies. All the fields
are marked as stored because we want to return them (not all at the same time though)
at query time. The sample data is also simple so I decided to skip commenting on that.
The first query is simple. We are searching for the value solr in the name field and we
want only the documents with the value of the price_usd field to be between 20 and 50.
We also want to return (the fl parameter) the following fields as a document: id, name,
and price_usd.
The interesting things come with the second query. As you can see there is a different fl
parameter that you may be used to. The first part of the fl parameter is pretty obvious;
we want to return the id and name fields. The second part is new though; we specified
the following value: price:price_usd. This means that we want the price_usd field
to be returned as price. That is how field aliasing works; you add the value ALIAS_
NAME:FIELD_NAME to the fl parameter and in the results, instead of FIELD_NAME,
Solr will return ALIAS_NAME.
www.it-ebooks.info
Chapter 4
151
Returning a value of a function in the results
Imagine you have a service where your users can search for different companies. Your users
can enter a simple keyword(s) and then return all the companies matching that keyword(s).
But a day comes when you give your users the ability to choose their location, and you would
like to show how far they are from each company returned in the results. This recipe will show
you how to do it.
Getting ready
Before reading further I advise you to read the Using field aliases recipe in the current
chapter and the Storing geographical points in the index recipe from Chapter 3, Analyzing
Your Text Data.
How to do it...
1. For the purpose of the recipe, let's assume that we have the following index structure
(put the following field's definition into your schema.xml file in the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="loc" type="location" indexed="true" stored="true"/>
<dynamicField name="*_coordinate" type="double" indexed="true"
stored="false" />
2. Next, we need to define the location field type. It should look like the following
code (put the following definition in to your schema.xml file in the types section):
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
3. Let's also assume that we have the following data indexed:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="loc">56.4,40.2</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="loc">50.1,48.9</field>
</doc>
www.it-ebooks.info
Querying Solr
152
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="loc">23.18,39.1</field>
</doc>
</add>
4. Now, in order to get all the documents with the word company in the name field
we would run the following query:
q=name:company&fl=*
5. We have the information that our client's location is 50.0, 28.0 and we would like to
show our client the distance from his/her location to each of the companies we return
in the results. In order to do that we add the following part to the fl parameter:
dist:geodist(loc,50.0,28.0)
So the whole query looks like the following:
q=name:company&fl=*,dist:geodist(loc,50.0,28.0)
And the response from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">name:company</str>
<str name="fl">*,dist:geodist(loc,50.0,28.0)</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="loc">56.4,40.2</str>
<double name="dist">1077.4200268973314</double>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company 2</str>
<str name="loc">50.1,48.9</str>
<double name="dist">1487.4260767512278</double>
</doc>
www.it-ebooks.info
Chapter 4
153
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="loc">23.18,39.1</str>
<double name="dist">3134.746384852772</double>
</doc>
</result>
</response>
As you can see, in addition to all the stored fields, Solr returned the additional field called
dist. Let's now see how that worked.
How it works...
The index structure is simple, it contains the identifier (the id field), name of the company
(the name field), and the geographical location of the company (the loc field). Description
of how geographical points should be stored were described in Chapter 3, Analyzing Your
Text Data, in the Storing geographical points in the index recipe, so please refer to that for
the explanation.
The initial query returning all the companies that have the word company in their name field
returns all the stored fields (the fl=* part of the query). The interesting part comes with the
dist:geodist(loc,50.0,28.0) part of the fl parameter. As you remember from the
Using field aliases recipe, we told Solr that we want to have a new field called dist returned
and we want it to be a value of the dist function query which takes three parameters: the field
in the index (in our case it is loc), the latitude, and the longitude, and returns the distance
between the point stored in the loc field, and the point described by the latitude and longitude.
The value is then returned as the dist field of each of the returned documents.
www.it-ebooks.info
www.it-ebooks.info
5
Using the Faceting
Mechanism
In this chapter we will cover:
ff Getting the number of documents with the same field value
ff Getting the number of documents with the same value range
ff Getting the number of documents matching the query and the sub query
ff Removing filters from faceting results
ff Sorting faceting results in alphabetical order
ff Implementing the autosuggest feature using faceting
ff Getting the number of documents that don't have a value in the field
ff Having two different facet limits for two different fields in the same query
ff Using decision tree faceting
ff Calculating faceting for relevant document groups
Introduction
One of the advantages of Solr is the ability to group results on the basis of some fields'
contents. The Solr classification mechanism, called faceting, provides the functionalities
which can help us in several tasks that we need to do in everyday work, from getting the
number of documents with the same values in a field (for example, the companies from the
same city) using the ability of date and range faceting, to the autocomplete features based on
the faceting mechanism. This chapter will show you how to handle some of the common tasks
when using the faceting mechanism.
www.it-ebooks.info
Using the Faceting Mechanism
156
Getting the number of documents with the
same field value
Imagine a situation where besides the search results, you have to return the number of
documents with the same field value. For example, imagine that you have an application
that allows the user to search for companies in Europe, and your client wants the number
of companies in the cities where the companies that were found by the query are located.
To do this, you could of course run several queries but Solr provides a mechanism called
faceting that can do that for you. This recipe will show you how to do it.
How to do it...
For getting the number of documents with the same field value, follow these steps:
1. To start, let's assume that we have the following index structure (just add this
to your schema.xml file in the field definition section; we will use the city field
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
2. The next step is to index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="city">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="city">New Orleans</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="city">New York</field>
</doc>
</add>
3. Let's suppose that our hypothetical user searches for the word company. The query
that will get us what we want should look like this:
http://localhost:8983/solr/select?q=name:company&facet=true&facet.
field=city
www.it-ebooks.info
Chapter 5
157
The result of the query should be like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.field">city</str>
<str name="q">name:company</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="city">New York</str>
<str name="id">1</str>
<str name="name">Company 1</str>
</doc>
<doc>
<str name="city">New Orleans</str>
<str name="id">2</str>
<str name="name">Company 2</str>
</doc>
<doc>
<str name="city">New York</str>
<str name="id">3</str>
<str name="name">Company 3</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">2</int>
<int name="New Orleans">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, besides the normal results list, we got faceting results with the numbers that
we wanted. Now let's look at how that happened.
www.it-ebooks.info
Using the Faceting Mechanism
158
How it works...
The index structure and the data are pretty simple and they make the example easier to
understand. The company is described by three fields. We are particularly interested in the
city field. This is the field that we want to use to get the number of companies that have
the same value in this field—which basically means that they are in the same city.
To do that, we run a query to Solr and inform the query parser that we want the documents
that have the word company in the title field. Additionally we say that we want to enable
the faceting mechanism, by using the facet=true parameter. The facet.field parameter
tells Solr which field to use to calculate faceting numbers. You can specify the facet.field
parameter multiple times to get faceting numbers for different fields in the same query.
As you can see in the results list, the results of all types of faceting are grouped in the list
with the name="facet_counts" attribute. The field based faceting is grouped under the
list with the name="facet_fields" attribute. Every field that you specified using the
facet.field parameter has its own list which has the attribute name, the same as the
value of the parameter in the query—in our case it is city. Then finally you can see the
results that we are interested in: the pairs of values (name attribute) and how many
documents have the value in the specified field.
There's more...
There are two more things I would like to share about field faceting:
ff How to show facets with counts greater than zero: The default behavior of Solr is to
show all the faceting results irrespective of the counts. If you want to show only the
facets with counts greater than zero than you should add the facet.mincount=1
parameter to the query (you can set this parameter to another value if you are
interested in any arbitrary value).
ff Lexicographical sorting of the faceting results: If you want to sort the faceting
results lexicographically, and not by the highest count (which is the default behavior),
then you need to add the facet.sort=index parameter.
Getting the number of documents with the
same value range
Imagine that you have an application where users can search the index to find a car for rent.
One of the requirements of the application is to show a navigation panel, where the user can
choose the price range for the cars that they are interested in. To do it in an efficient way, we
will use range faceting and this recipe will show you how to do it.
www.it-ebooks.info
Chapter 5
159
How to do it...
For getting the number of documents with the same value range, follow these steps:
1. Let's begin with the following index structure (just add this to your schema.xml
file in the field definition section; we will use the price field to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
2. The example data that we will use is like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Super Mazda</field>
<field name="price">50</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mercedes Benz</field>
<field name="price">210</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Bentley</field>
<field name="price">290</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Super Honda</field>
<field name="price">99.90</field>
</doc>
</add>
3. Now, as you recall, our requirement was to show the navigation panel with
price ranges. To do that, we need to get that data from Solr. We also know
that the minimum price for car rent is 1 dollar and the maximum is 400
dollars. To get the price ranges from Solr, we send the following query:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
range=price&facet.range.start=0&facet.range.end=400&facet.range.
gap=100
www.it-ebooks.info
Using the Faceting Mechanism
160
The query will produce the following result list:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="facet.range.start">0</str>
<str name="facet.range">price</str>
<str name="facet.range.end">400</str>
<str name="facet.range.gap">100</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="4" start="0"/>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges">
<lst name="price">
<lst name="counts">
<int name="0.0">2</int>
<int name="100.0">0</int>
<int name="200.0">2</int>
<int name="300.0">0</int>
</lst>
<float name="gap">100.0</float>
<float name="start">0.0</float>
<float name="end">400.0</float>
</lst>
</lst>
</lst>
</response>
So we got exactly what we wanted. Now let's see how it works.
www.it-ebooks.info
Chapter 5
161
How it works...
As you can see, the index structure is simple. There are three fields, one responsible for
the unique identifier, one responsible for the car name, and the last one responsible for
the price of rent.
The query is where all the magic is done. As we are not interested in the search results, we
ask for all documents in the index (q=*:* parameter) and we tell Solr not to return the search
results (rows=0 parameter). Then we tell Solr that we want the faceting mechanism to be
enabled for the query (facet=true parameter). We will not be using the standard faceting
mechanism, that is, the field based faceting. Instead we will use range faceting which is
optimized to work with ranges. So, we tell Solr which field will be used for range faceting by
adding the parameter facet.range with the price value. That means that the price field
will be used for the range faceting calculation. Then we specify the lower boundary from which
the range faceting calculation will begin. We do this by adding the facet.range.start
parameter; in our example we set it to 0. Next we have the facet.range.end parameter
which tells Solr when to stop the calculation of the range faceting. The last parameter
(facet.range.gap) informs Solr about the length of the periods that will be calculated.
Remember that when using the range faceting mechanism you must specify the
three parameters:
ff facet.range.start
ff facet.range.end
ff facet.range.gap
Otherwise, the range faceting mechanism won't work.
In the faceting results you can see the periods and the number of documents that were found
in each of them. The first period can be found under the <int name="0.0"> tag. This period
consists of prices from 0 to 100 (in mathematical notation it would be <0; 100>). It contains
two cars. The next period can be found under the <int name="100.0"> tag and consists of
prices from 100 to 200 (in mathematical notation it would be <100; 200>), and so on.
Getting the number of documents matching
the query and subquery
Imagine a situation where you have an application that has a search feature for cars. One of
the requirements is not only to show search results, but also to show the number of cars with
the price period chosen by the user. There is also another thing—those queries must be fast
because of the number of queries that will be run. Can Solr handle that? The answer is yes.
This recipe will show you how to do it.
www.it-ebooks.info
Using the Faceting Mechanism
162
How to do it...
For getting the number of documents matching the query and subquery, follow these steps:
1. Let's start with creating an index with the following structure (just add this to
your schema.xml file in the field definition section; we will use the price field
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
2. Now let's index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Car 1</field>
<field name="price">70</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Car 2</field>
<field name="price">101</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Car 3</field>
<field name="price">201</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Car 4</field>
<field name="price">99.90</field>
</doc>
</add>
Now, recall our requirement cars that match the query (let's suppose that our user
typed car), and show the counts in the chosen price periods. For the purpose of the
recipe let's assume that the user has chosen two periods of prices:
?? 10 to 80
?? 90 to 300
www.it-ebooks.info
Chapter 5
163
3. The query to achieve such a requirement should look like this:
http://localhost:8983/solr/select?q=name:car&facet=true&facet.
query=price:[10 TO 80]&facet.query=price:[90 TO 300]
The result list of the query should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<arr name="facet.query">
<str>price:[10 TO 80]</str>
<str>price:[90 TO 300]</str>
</arr>
<str name="q">name:car</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="id">1</str>
<str name="name">Car 1</str>
<float name="price">70.0</float>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Car 2</str>
<float name="price">101.0</float>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Car 3</str>
<float name="price">201.0</float>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Car 4</str>
<float name="price">99.9</float>
</doc>
</result>
www.it-ebooks.info
Using the Faceting Mechanism
164
<lst name="facet_counts">
<lst name="facet_queries">
<int name="price:[10 TO 80]">1</int>
<int name="price:[90 TO 300]">3</int>
</lst>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
</lst>
</response>
How it works...
As you can see, the index structure is simple. There are three fields, one responsible for the
unique identifier, one responsible for the car name, and the last one responsible for the price.
Next we have the query. First you can see a standard query where we tell Solr that we want to
get all the documents that have the word car in the name field (the q=name:car parameter).
Next, we say that we want to use the faceting mechanism by adding the facet=true
parameter to the query. This time we will use the query faceting type. This means that we
can pass the query to the faceting mechanism and as a result we will get the number of
documents that match the given query. In our example case, we wanted two periods like this:
ff One from the price of 10 to 80
ff Another from the price of 90 to 300
This is achieved by adding the facet.query parameter with the appropriate value. The first
period is defined as a standard range query to the price field (price:[10 TO 80]). The
second query is very similar, just with different values. The value passed to the facet.query
parameter should be a Lucene query written using the default query syntax.
As you can see in the results, the query faceting results are grouped under the <lst
name="facet_queries"> XML tag with the names exactly as in the queries sent to Solr.
You can see that Solr correctly calculated the number of cars in each of the periods, which
means that this is a perfect solution for us when we can't use the range faceting mechanism.
Removing filters from faceting results
Let's assume for the purpose of this recipe that you have an application that can search for
companies within a city and state. But the requirements say that you should show not only
the search results but also the number of companies in each city and the number of
companies in each state (in the Solr way we say that you want to exclude the filter query
from the faceting results). Can Solr do that in an efficient way? Sure it can, and this recipe
will show you how to do it.
www.it-ebooks.info
Chapter 5
165
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. We start with the following index structure (just add this to your schema.xml file in
the field definition section; we will use the city and state fields to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
<field name="state" type="string" indexed="true" stored="true />
2. The second step would be to index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="city">New Orleans</field>
<field name="state">Luiziana</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Company 4/field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
</add>
www.it-ebooks.info
Using the Faceting Mechanism
166
3. Let's suppose that our hypothetical user searched for the word company, and told
our application that he needs the companies matching the word in the state of New
York. In that case, the query that will fulfill our requirement should look like this:
http://localhost:8983/solr/select?q=name:company&facet=true
&fq={!tag=stateTag}state:"New York"&facet.field={!ex=stateTag}
city&facet.field={!ex=stateTag}state
The result for the query will look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<arr name="facet.field">
<str>{!ex=stateTag}city</str>
<str>{!ex=stateTag}state</str>
</arr>
<str name="fq">{!tag=stateTag}state:"New York"</str>
<str name="q">name:company</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Company 4</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
www.it-ebooks.info
Chapter 5
167
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">3</int>
<int name="New Orleans">1</int>
</lst>
<lst name="state">
<int name="New York">3</int>
<int name="Luiziana">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
Now let's see how it works.
How it works...
The index structure is pretty simple—it contains four fields that describe the company.
The search will be performed against the name field, while the filtering and the faceting
is done with the use of the state and city fields.
So let's get on with the query. As you can see, we have some typical elements there. First
the q parameter, which just tells Solr where and what to search for. Then the facet=true
parameter that enables the faceting mechanism. So far, so good. Following that, you have
a strange looking filter query (the fq parameter) with the value of fq={!tag=stateTag}
state:"New York". It tells Solr to only show those results that have New York in the
state field. By adding the {!tag=stateTag} part, we basically gave that filter query
a name (stateTag), which we will use further.
Now, look at the two facet.field parameters. Our requirement was to show the number
of companies in all states and in all cities. The only thing that was preventing us from getting
those numbers was the filter query we added to the query. So let's exclude it from the faceting
results. How to do it ? It's simple—just add {!ex=stateTag} to the beginning of each of the
facet.field parameters, like this: facet.field={!ex=stateTag}city. It tells Solr to
exclude the filter with the passed name.
As you can see in the results list, we got the correct numbers which means that the exclude
works as intended.
www.it-ebooks.info
Using the Faceting Mechanism
168
Sorting faceting results in alphabetical
order
Imagine a situation where you have a website, where you present some kind of
advertisements, for example, house rental advertisements. One of the requirements is to
show a list of cities in which the offer, that matched the query typed by the user, are located.
So the first thing you think is to use the faceting mechanism – and that's a good idea. But
then, your boss tells you that he is not interested in the counts and you have to sort the
results in the alphabetical order. So, is Solr able to do it? Of course it is and this recipe will
show you how to do it.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. For the purpose of the recipe let's assume that we have the following index structure
(just add this to your schema.xml file to the field definition section; we will use the
city field to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
2. This index structure is responsible for holding information about companies and their
location. Now, let's index the example data matching the presented index structure:
<add>
<doc>
<field name="id">1</field>
<field name="name">House 1</field>
<field name="city">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">House 2</field>
<field name="city">Washington</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">House 3</field>
www.it-ebooks.info
Chapter 5
169
<field name="city">Washington</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">House 4</field>
<field name="city">San Francisco</field>
</doc>
</add>
3. Let's assume that our hypothetical user typed house in the search box. The query
to return the search results with the faceting results sorted alphabetically should
be like this:
http://localhost:8983/solr/select?q=name:house&facet=true&facet.
field=city&facet.sort=index
The results returned by Solr for the query should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.field">city</str>
<str name="facet.sort">index</str>
<str name="q">name:house</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="city">New York</str>
<str name="id">1</str>
<str name="name">House 1</str>
</doc>
<doc>
<str name="city">Washington</str>
<str name="id">2</str>
<str name="name">House 2</str>
</doc>
<doc>
<str name="city">Washington</str>
<str name="id">3</str>
<str name="name">House 3</str>
www.it-ebooks.info
Using the Faceting Mechanism
170
</doc>
<doc>
<str name="city">San Francisco</str>
<str name="id">4</str>
<str name="name">House 4</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">1</int>
<int name="San Francisco">1</int>
<int name="Washington">2</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see the faceting results returned by Solr are not sorted by counts but in
alphabetical order. Now let's see how it works.
How it works...
The index structure and the example data are only here to help us make a query so I'll skip
discussing them.
The query shown in the recipe differs from the standard faceting query by only one
parameter—facet.sort. It tells Solr how to sort the faceting results. The parameter
can be assigned one of two values:
ff count – which tells Solr to sort the faceting results placing the highest counts first
ff index – which tells Solr to sort the faceting results by index order, which means that
the results will be sorted lexicographically
For the purpose of the recipe we chose the second option and as you can see in the returned
results, we got what we wanted.
www.it-ebooks.info
Chapter 5
171
Implementing the autosuggest feature using
faceting
There are plenty of web-based applications that help users choose what they want to search
for. One of the features that helps users is the autocomplete (or autosuggest) feature, like
the one that most of the most used search engines have. Let's assume that we have an
e-commerce library and we want to help the user to choose a book title—we want to enable
autosuggest on the basis of the title. This recipe will show you how to do that.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. Let's begin with the assumption of having the following index structure (just add
this to your schema.xml file in the fields definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="title_autocomplete" type="lowercase" indexed="true"
stored="true">
2. We also want to add some field copying to do some operations automatically.
To do that we need to add the following line after the fields section in your
schema.xml file:
<copyField source="title" dest="title_autocomplete" />
3. The lowercase field type should look like this (just add this to your schema.xml
file to the type definitions):
<fieldType name="lowercase" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory" />
</analyzer>
</fieldType>
www.it-ebooks.info
Using the Faceting Mechanism
172
4. Now, let's index a sample data file which could look like this:
<add>
<doc>
<field name="id">1</field>
<field name="title">Lucene or Solr ?</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">My Solr and the rest of the world</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr recipes</field>
</doc>
<doc>
<field name="id">4</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
5. Let's assume that our hypothetical user typed the letters so in the search box and
we want to give him the first 10 suggestions with the highest counts. We also want
to suggest the whole titles, not just single words. To do that, we should send the
following query to Solr:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
field=title_autocomplete&facet.prefix=so
As a result for the query, Solr returned the following output:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">16</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="facet.prefix">so</str>
<str name="facet.field">title_autocomplete</str>
<str name="rows">0</str>
</lst>
</lst>
www.it-ebooks.info
Chapter 5
173
<result name="response" numFound="4" start="0"/>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="title_autocomplete">
<int name="solr cookbook">1</int>
<int name="solr recipes">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, we got what we wanted in the faceting results. Now let's see how it works.
How it works...
You can see that our index structure defined in the schema.xml file is pretty simple.
Every book is described by two fields, id and title. The additional field will be used
to provide the autosuggest feature.
The copy field section is there to automatically copy the contents of the title field to
the title_autocomplete field.
The lowercase field type is a type we will use to provide the autocomplete feature; this is
the same for lowercase words typed by the users as well as uppercase words. If we want to
show different results for uppercased and lowercased letters then the string type will be
sufficient.
Now let's take a look at the query. As you can see we are searching the whole index (the
parameter q=*:*), but we are not interested in any search results (the rows=0 parameter).
We tell Solr that we want to use the faceting mechanism (facet=true parameter) and that
it will be field-based faceting on the basis of the title_autocomplete field (the facet.
field=title_autocomplete parameter). The last parameter, facet.prefix, can be
something new. Basically it tells Solr to return only those faceting results that begin with the
prefix specified as the value of this parameter, which in our case is the value of so. The use of
this parameter enables us to show the suggestions that the user is interested in, and we can
see in the results that we achieved what we wanted.
There's more...
There is one more thing I would like to say about autosuggestion functionality.
www.it-ebooks.info
Using the Faceting Mechanism
174
Suggesting words not whole phrases
If you want to suggest words instead of a whole phrase you don't have to change much
of the previous configuration. Just change the type of title_autocomplete to the type
based on solr.TextField (for example, the text_ws field type). You should remember,
though, not to use heavily analyzed text (like stemmed text) to be sure that your word won't
be modified too much.
Getting the number of documents that don't
have a value in the field
Let's imagine we have an e-commerce library where we put some of our books on a special
promotion, for example, we give them away for free. We want to share that knowledge with our
customers and say: Hey! You searched for Solr, we found this, but we also have X books that
are free! To do that, we index the books that are free without the price defined. But how do
you make a query to Solr to retrieve the data that we want? This recipe will show you how.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents matching the query and the subquery recipe in this chapter.
How to do it...
1. Let's begin with the following index structure (just add this to your schema.xml
file in the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true">
2. We will also use the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Lucene or Solr ?</field>
<field name="price">11</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">My Solr and the rest of the world</field>
<field name="price">44</field>
</doc>
www.it-ebooks.info
Chapter 5
175
<doc>
<field name="id">3</field>
<field name="title">Solr recipes</field>
<field name="price">15</field>
</doc>
<doc>
<field name="id">4</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
As you can see, the first three documents have a value in the price field, while
the last one doesn't. So now, for the purpose of the example, let's assume that
our hypothetical user is trying to find books that have solr in their title field.
3. Besides the search results, we want to show the number of documents that don't
have a value in the price field. To do that, we send the following query to Solr:
http://localhost:8983/solr/select?q=title:solr&facet=true&facet.
query=!price:[* TO *]\
The query should result in the following output from Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.query">!price:[* TO *]</str>
<str name="q">title:solr</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="id">3</str>
<float name="price">15.0</float>
<str name="title">Solr recipes</str>
</doc>
<doc>
<str name="id">4</str>
<str name="title">Solr cookbook</str>
</doc>
<doc>
www.it-ebooks.info
Using the Faceting Mechanism
176
<str name="id">1</str>
<float name="price">11.0</float>
<str name="title">Lucene or Solr ?</str>
</doc>
<doc>
<str name="id">2</str>
<float name="price">44.0</float>
<str name="title">My Solr and the rest of the world</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries">
<int name="!price:[* TO *]">1</int>
</lst>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see we got the proper results. Now let's see how it works.
How it works...
You can see that our index structure defined in the schema.xml file is pretty simple. Every
book is described by three fields, id, title, and price. Their names speak for the type of
information they will hold.
The query is in most parts something you should be familiar with. First, we tell Solr that we
are searching for documents that have the word solr in the title field (the q=title:solr
parameter). Then we say that we want to have the faceting mechanism enabled by adding the
facet=true parameter. Then we add a facet query parameter that tells Solr to return the
number of documents that don't have a value in the price field. We do that by adding the
facet.query=!price:[* TO *] parameter. How does that work? You should be familiar
with how the facet.query parameter works, so I'll skip that part. The price:[* TO *]
expression tells Solr to count all the documents that have a value in the price field. By
adding the ! character before the fieldname, we tell Solr to negate the condition and in fact
we get the number of documents that don't have any value in the specified field.
www.it-ebooks.info
Chapter 5
177
Having two different facet limits for two
different fields in the same query
Imagine a situation where you have a database of cars in your application. Besides the
standard search results, you want to show two faceting by field results. The first of those
two faceting results, the number of cars in each category, should be shown without any
limits, while the second faceting, the one showing the cars by their manufacturer, should
be limited to a maximum of 10 results. Can we achieve it in one query? Yes, we can, and
this recipe will show you how to do it.
Getting ready
Before you start reading this recipe please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. For the purpose of the recipe, let's assume that we have the following index structure
(just add this to your schema.xml file in the field definition section; we will use the
category and manufacturer fields to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="manufacturer" type="string" indexed="true"
stored="true" />
2. We will need some sample data. For example we can use a file that has the
following content:
<add>
<doc>
<field name="id">1</field>
<field name="name">Super Mazda car</field>
<field name="category">sport</field>
<field name="manufacturer">mazda</field>
</doc>
<doc>
www.it-ebooks.info
Using the Faceting Mechanism
178
<field name="id">2</field>
<field name="name">Mercedes Benz car</field>
<field name="category">limousine</field>
<field name="manufacturer">mercedes</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Bentley car</field>
<field name="category">limousine</field>
<field name="manufacturer">bentley</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Super Honda car</field>
<field name="category">sport</field>
<field name="manufacturer">honda</field>
</doc>
</add>
3. For the purpose of the example, let's assume that our hypothetical user is trying to
search the index for the word car. To do that we should send Solr the following query:
http://localhost:8983/solr/select?q=name:car&facet=true&facet.
field=category&facet.field=manufacturer&f.category.facet.limit=-
1&f.manufacturer.facet.limit=10
The query resulted in the following response from Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="f.category.facet.limit">-1</str>
<str name="facet">true</str>
<str name="q">name:car</str>
<arr name="facet.field">
<str>category</str>
<str>manufacturer</str>
</arr>
<str name="f.manufacturer.facet.limit">10</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
www.it-ebooks.info
Chapter 5
179
<str name="id">3</str>
<str name="name">Bentley car</str>
<str name="category">limousine</str>
<str name="manufacturer">bentley</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Super Mazda car</str>
<str name="category">sport</str>
<str name="manufacturer">mazda</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Mercedes Benz car</str>
<str name="category">limousine</str>
<str name="manufacturer">mercedes</str>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Super Honda car</str>
<str name="category">sport</str>
<str name="manufacturer">honda</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="category">
<int name="limousine">2</int>
<int name="sport">2</int>
</lst>
<lst name="manufacturer">
<int name="bentley">1</int>
<int name="honda">1</int>
<int name="mazda">1</int>
<int name="mercedes">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
Now let's see how it works.
www.it-ebooks.info
Using the Faceting Mechanism
180
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml
file and the example data, every document is described by four fields—id, name, category,
and manufacturer. I think that their names speak for themselves and I don't need to
discuss them.
The first parts of the query are pretty standard. We ask for documents which have the word
car in their name field. Then we tell Solr to enable faceting (the facet=true parameter) and
we tell it what field will be used to calculate faceting results (the facet.field=category
and the facet.field=manufacturer parameters). Then we specify the limits. By adding
the parameter limits in a way shown in the example (f.FIELD_NAME.facet.limit) we
tell Solr to set the limits for the faceting calculation for the particular field. In our example
query, by adding the f.category.facet.limit=-1 parameter we told Solr that we don't
want any limits on the number of faceting results for the category field. By adding the
f.manufacturer.facet.limit=10 parameter we told Solr that we want a maximum of 10
faceting results for the manufacturer field.
Following the pattern you can specify per-field values for faceting properties such as sorting
and minimum count.
Using decision tree faceting
Imagine that in our store we have products divided into categories. In addition to that, we
store information about the stock of the items. Now, we want to show our crew how many of
the products in the categories are in stock and how many we are missing. The first thing that
comes to mind is using the faceting mechanism and some additional calculation. But why
bother, when Solr 4.0 can do that calculation for us with the use of so called pivot faceting.
This recipe will show you how to use it.
How to do it...
The following steps illustrate the use of pivot faceting:
1. Let's start with the following index structure (just add this to your schema.xml
file in the field definition section; we will use the category and stock fields
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="stock" type="boolean" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 5
181
2. Now let's index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook 1</field>
<field name="category">workbooks</field>
<field name="stock">false</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook 2</field>
<field name="category">workbooks</field>
<field name="stock">true</field>
</doc>
</add>
3. Let's assume we are running a query from the administration panel of our
shop and we are not interested in the documents at all; we only want to know
how many documents are in stock or out of stock for each of the categories.
The query implementing that logic should look like this:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
pivot=category,stock
The response to the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">76</int>
<lst name="params">
<str name="facet">true</str>
www.it-ebooks.info
Using the Faceting Mechanism
182
<str name="indent">true</str>
<str name="facet.pivot">category,stock</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
<lst name="facet_pivot">
<arr name="category,stock">
<lst>
<str name="field">category</str>
<str name="value">books</str>
<int name="count">2</int>
<arr name="pivot">
<lst>
<str name="field">stock</str>
<bool name="value">true</bool>
<int name="count">2</int>
</lst>
</arr>
</lst>
<lst>
<str name="field">category</str>
<str name="value">workbooks</str>
<int name="count">2</int>
<arr name="pivot">
<lst>
<str name="field">stock</str>
<bool name="value">false</bool>
<int name="count">1</int>
</lst>
<lst>
<str name="field">stock</str>
<bool name="value">true</bool>
<int name="count">1</int>
</lst>
</arr>
</lst>
</arr>
</lst>
</lst>
</response>
You will notice that we received what we wanted, now let's see how it works.
www.it-ebooks.info
Chapter 5
183
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml file
and the example data, every document is described by four fields—id, name, category, and
stock. I think that their names speak for themselves and I don't need to discuss them.
The interesting things start with the query. We specified that we want the query to match all
the documents (q=*:* parameter), but we don't want to see any documents in the response
(rows=0 parameter). In addition to that, we want to have faceting calculation (facet=true
parameter) and we want to use the decision tree faceting, also known as pivot faceting. We
do that by specifying which fields should be included in the tree faceting. In our case we
want the top level of the pivot facet to be calculated on the basis of the category field,
and the second level (the one nested in the category field calculation) should be based
on the values available in the stock field. Of course, if you would like to have another value
of another field nested under the stock field you can do that by adding another field to the
facet.pivot query parameter. Assuming you would like to see faceting on the price field
nested under the stock field, your facet.pivot parameter would look like this: facet.
pivot=category,stock,price.
As you can see in the response, each nested faceting calculation result is written inside
the <arr name="pivot"> XML tag. So let's look at the response structure. The first
level of your facet pivot tree is based on the category field. You can see two books (<int
name="count">2</int>) in the books category (<str name="value">books</str>),
and these books have the stock field (<str name="field">stock</str>) set to true
(<bool name="value">true</bool>). For the workbooks category, the situation is a
bit different, because you can see two different sections there—one for documents with the
stock field equal to false, and the other with the stock field set to true. But in the end,
the calculation is correct and that's what we wanted!
Calculating faceting for relevant documents
in groups
If you have ever used the field collapsing functionality of Solr you may be wondering if there is a
possibility of using that functionality and faceting. Of course there is, but the default behavior still
works so that you get the faceting calculation on the basis of documents not document groups.
In this recipe, we will learn how to query Solr so that it returns facets calculated for the most
relevant document in each group in order for your user facet counts to be more or less grouped.
Getting ready
Before reading this recipe please look at the Using field to group results, Using query to group
results, and Using function query to group results recipes in Chapter 8, Using Additional Solr
Functionalities. Also, if you are not familiar with faceting functionality, please read the first
three recipes in this chapter.
www.it-ebooks.info
Using the Faceting Mechanism
184
How to do it...
1. As a first step we need to create an index. For the purpose of the recipe let's assume
that we have the following index structure (just add this to your schema.xml file to
the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="stock" type="boolean" indexed="true" stored="true" />
2. The second step is to index the data. We will use some example data which looks
like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook 1</field>
<field name="category">workbooks</field>
<field name="stock">false</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook 2</field>
<field name="category">Workbooks</field>
<field name="stock">true</field>
</doc>
</add>
www.it-ebooks.info
Chapter 5
185
3. So now it's time for our query. So, let's assume we want our results to be grouped
on the values of the category field, and we want the faceting to be calculated on
the stock field. And remember that we are only interested in the most relevant
document from each result group when it comes to faceting. So, the query that
would tell Solr to do what we want should look like this:
http://localhost:8983/solr/select?q=*:*&facet=true&facet.
field=stock&group=true&group.field=category&group.truncate=true
The results for the query would look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="group.truncate">true</str>
<str name="group.field">category</str>
<str name="group">true</str>
<str name="facet.field">stock</str>
</lst>
</lst>
<lst name="grouped">
<lst name="category">
<int name="matches">4</int>
<arr name="groups">
<lst>
<str name="groupValue">books</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<str name="category">books</str>
<bool name="stock">true</bool></doc>
</result>
</lst>
<lst>
<str name="groupValue">workbooks</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">3</str>
www.it-ebooks.info
Using the Faceting Mechanism
186
<str name="name">Workbook 1</str>
<str name="category">workbooks</str>
<bool name="stock">false</bool>
</doc>
</result>
</lst>
</arr>
</lst>
</lst>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="stock">
<int name="false">1</int>
<int name="true">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see everything worked as it should. Now let's see how it works.
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml file
and the example data, every document is described by four fields—id, name, category, and
stock. I think that their names speak for themselves and I don't need to discuss them.
As it comes to the query, we fetch all the documents from the index (the q=*:* parameter).
Next, we say that we want to use faceting and we want it to be calculated on the stock field.
We want a grouping mechanism to be active and we want to group documents on the basis of
the category field (all the query parameters responsible for defining the faceting and grouping
behavior are described in the appropriate recipes in this book, so please look at those if you
are not familiar with those parameters). And finally something new—the group.truncate
parameter is set to true. If set to true, like in our case, facet counts will be calculated using
only the most relevant document in each of the calculated groups. So in our case, for the group
with the category field equal to books, we have the true value in the stock field and for the
second group we have false in the stock field. Of course we are looking at the most relevant
documents, so the first ones in our case. So, as you can easily see, we've got two facet counts
for the stock field, both with a count of 1, which is what we would expect.
There is one thing more—at the time of writing this book, the group.truncate parameter
was not supported when using distributed search, so please be aware of that.
www.it-ebooks.info
6
Improving Solr
Performance
In this chapter we will cover:
ff Paging your results quickly
ff Configuring the document cache
ff Configuring the query result cache
ff Configuring the filter cache
ff Improving Solr performance right after the start up or commit operation
ff Caching whole result pages
ff Improving faceting performance for low cardinality fields
ff What to do when Solr slows down during indexing
ff Analyzing query performance
ff Avoiding filter caching
ff Controlling the order of execution of filter queries
ff Improving the performance of numerical range queries
Introduction
Performance of the application is one of the most important factors. Of course, there are
other factors, such as usability and availability—we could recite many more—but one of the
most crucial is performance. Even if our application is perfect in terms of usability, the
users won't be able to use it if they will have to wait for minutes for the search results.
www.it-ebooks.info
Improving Solr Performance
188
The standard Solr deployment is fast enough, but sooner or later a time will come when
you will have to optimize your deployment. This chapter and its recipes will try to help you
with the optimization of Solr deployment.
If your business depends on Solr, you should keep monitoring it even after optimization.
There are numerous solutions available in the market, from the generic and open-sourced
ones such as Gangila (http://ganglia.sourceforge.net/) to search-specific ones
such as Scalable Performance Monitoring (http://www.sematext.com/spm/index.
html) from Sematext.
Paging your results quickly
Imagine a situation where you have a user constantly paging through the search results.
For example, one of the clients I was working for was struggling with the performance of his
website. His users tend to search for a word and then page through the result pages – the
statistical information gathered from the application logs showed that typical users changed
the page about four to seven times. Apart from improving the query relevance (which isn't
what we will talk about in this recipe), we decided to optimize the paging. How do we do that?
This recipe will show you.
How to do it...
So, let's get back to my client deployment. As I mentioned, typical users typed a word into
the search box and then used the paging mechanism to go through a maximum of seven
pages. My client's application was showing 20 documents on a single page. So, it can be
easily calculated that we need about 140 documents in advance, apart from the first 20
documents returned by the query.
1. So what we did was actually pretty simple. First of all, we modified the
queryResultWindowSize property in the solrconfig.xml file and changed
it to the following value:
<queryResultWindowSize>160</queryResultWindowSize>
2. We then changed the maximum number of documents that can be cached for a
single query to 160, by adding the following property to the solrconfig.xml file:
<queryResultMaxDocsCached>160</queryResultMaxDocsCached>
We also modified queryResultCache, but that's a discussion for another recipe. To learn
how to change that cache, please refer to the How to configure the query result cache recipe
in this chapter.
www.it-ebooks.info
Chapter 6
189
How it works...
So how does Solr behave with the changes proposed in the preceding section? First of all,
queryResultWindowSize tells Solr to store (in documentCache) a maximum of the 160
documents IDs with every query. Therefore, after doing the initial query, we gather more
documents than we actually need. Because of this we are sure that when a user clicks on
the next page button, which is present in our application, the results will be taken from the
cache. So there won't be a need for intensive I/O operations. You must remember that the
160 documents IDs will be stored in the cache and won't be visible in the results list, as the
result size is controlled by the rows parameter.
The queryResultMaxDocsCached property tells Solr about the maximum number of
document IDs that can be cached for a single query (please remember than in this case, the
cache stores the document identifiers and not whole documents). We told Solr that we want
a maximum of 160 document IDs for a single query, because the statistics showed us that
we don't need more, at least for a typical user.
Of course, there is another thing that should be done – setting the query result cache size,
but that is discussed in another recipe.
Configuring the document cache
Cache can play a major role in your deployment's performance. One of the caches that you can
configure when setting up Solr is the document cache. It is responsible for storing the Lucene
internal documents that have been fetched from the disk. The proper configuration of this
cache can save precious I/O calls and therefore boost the whole deployment performance.
This recipe will show you how to properly configure the document cache.
How to do it...
For the purpose of this recipe, I assumed that we are dealing with the deployment of Solr
where we have about 100, 000 documents. In our case, a single Solr instance is getting a
maximum of 10 concurrent queries and the maximum number of documents that a query
can fetch is 256.
With the preceding parameters, our document cache should look similar to the following code
snippet (add this code to the solrconfig.xml configuration file):
<documentCache
class="solr.LRUCache"
size="2560"
initialSize="2560"/>
www.it-ebooks.info
Improving Solr Performance
190
Notice that we didn't specify the autowarmCount parameter—this is because the document
cache uses Lucene's internal ID to identify documents. These identifiers can't be copied
between index changes and thus we can't automatically warm this cache.
How it works...
The document cache configuration is simple. We define it in the documentCache XML tag
and specify a few parameters that define the document cache's behavior. First of all, the
class parameter tells Solr which Java class should be used for implementation. In our
example, we use solr.LRUCache because we will be adding more information into the
cache than we will be fetching from it. When you see that you are getting more information
than you add, consider using solr.FastLRUCache. The next parameter tells Solr the
maximum size of the cache (the size parameter). As the Solr wiki says, we should always
set this value to more than the maximum number of results returned by the query multiplied
by the maximum concurrent queries than we think will be sent to the Solr instance. This will
ensure that we always have enough place in the cache, so that Solr will not have to fetch the
data from the index multiple times during a single query.
The last parameter tells Solr the initial size of the cache (the initialSize parameter). I
tend to set it to the same value as the size parameter to ensure that Solr won't be wasting
its resources on cache resizing.
The more fields marked as stored in the index structure, the higher
the memory usage of this cache will be.
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way. Remember
that having a very large cache with very low hit rate can be worse than having no cache at all.
Along with everything else, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you
have a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one
of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll
repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the query result cache
The major Solr role in a typical e-commerce website is handling user queries. Of course, users
of the site can type multiple queries in the Search box and we can't easily predict how many
unique queries there may be. But, using the logs that Solr gives us, we can check how many
different queries there were in the last day, week, month, or year. Using this information, we
can configure the query result cache to suit our needs in the most optimal way, and this recipe
will show you how to do it.
www.it-ebooks.info
Chapter 6
191
How to do it...
For the purpose of this recipe, let's assume that one Solr instance of our e-commerce website
is handling about 10 to 15 queries per second. Each query can be sorted by four different
fields (the user can choose by which field). The user can also choose the order of sort. By
analyzing the logs for the past three months, we know that there are about 2000 unique
queries that users tend to type in the search box of our application. We also noticed that
our users don't usually use the paging mechanism.
On the basis of this information, we configure our query results cache as follows (add this
code to the solrconfig.xml configuration file):
<queryResultCache
class="solr.LRUCache"
size="16000"
initialSize="16000"
autowarmCount="4000"/>
How it works...
Adding the query result cache to the solrconfig.xml file is a simple task. We define it in
the queryResultCache XML tag and specify a few parameters that define the query result's
cache behavior. First of all, the class parameter tells Solr which Java class should be used
for implementation. In our example, we use solr.LRUCache because we will be adding
more information into the cache than we will fetching from it. When you see that you are get
more information than you add, consider using solr.FastLRUCache. The next parameter
tells Solr about the maximum size of the cache (the size parameter). This cache should be
able to store the ordered identifiers of the objects that were returned by the query with its
sort parameter and the range of documents requested. This means that we should take
the number of unique queries, multiply it by the number of sort parameters and the number
of possible orders of sort. So in our example, the size should be at least the result of the
following equation:
size = 2000 * 4 * 2
In our case, it is 16,000.
I tend to set the initial size of this cache to the maximum size; so in our case, I set the
initialSize parameter to a value of 16000. This is done to avoid the resizing of the cache.
The last parameter (autowarmCount) says how many entries should be copied when Solr
invalidates caches (for example, after a commit operation). I tend to set this parameter to
a quarter of the maximum size of the cache. This is done because I don't want the caches
to be warming for too long. However, please remember that the auto-warming time depends
on your deployment and the autowarmCount parameter should be adjusted if needed.
www.it-ebooks.info
Improving Solr Performance
192
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything else, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you
have a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one
of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll
repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the filter cache
Almost every client of mine who uses Solr, tends to forget or simply doesn't know how to use
filter queries or simply filters. People tend to add another clause with a logical operator to the
main query—they forget how efficient filters can be, at least when used wisely. And that's why
whenever I can, I tell people using Solr to use filter queries. But when using filter queries, it is
nice to know how to set up a cache that is responsible for holding the filters results – the filter
cache. This recipe will show you how to properly set up the filter cache.
How to do it...
For the purpose of this recipe, let's assume that we have a single Solr slave instance to
handle all the queries coming from the application. We took the logs from the last three
months and analyzed them. From this we know, that our queries are making about 2000
different filter queries. By getting this information, we can set up the filter cache for our
instance. This configuration should look similar to the following code snippet (add this
code to the solrconfig.xml configuration file):
<filterCache
class="solr.FastLRUCache"
size="2000"
initialSize="2000"
autowarmCount="1000"/>
That's it. Now let's see what those values mean.
How it works...
As you may have noticed, adding the filter cache to the solrconfig.xml file is a simple
task; you just need to know how many unique filters your Solr instance is receiving. We define
this in the filterCache XML tag and specify a few parameters that define the query result
cache behavior. First of all, the class parameter tells Solr which Java class should be used
for implementation. In our example, we use solr.LRUCache because we will be adding more
information into the cache than we will fetching from it. When you see that you are getting
more information than you add, consider using solr.FastLRUCache.
www.it-ebooks.info
Chapter 6
193
The next parameter tells Solr the maximum size of the cache (the size parameter). In our
case, we said that we have about 2000 unique filters and we set the maximum size to that
value. This is done because each entry of the filter cache stores the unordered sets of Solr
document identifiers that match the given filter. In this way, after the first use of the filter,
Solr can use the filter cache to apply filtering and thus save the I/O operations.
The next parameter – initialSize tells Solr about the initial size of the filter cache. I tend
to set it's value to the same as that of the size parameter to avoid cache resizing. So in our
example, we set it to the value of 2000.
The last parameter (autowarmCount) says how many entries should be copied when Solr
invalidates caches (for example, after a commit operation). I tend to set this parameter to
a quarter of the maximum size of the cache. This is done because I don't want the caches
to be warming for too long. However, please remember that the auto-warming time depends
on your deployment and the autowarmCount parameter should be adjusted if needed.
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you have
a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one of those
things in Apache Solr that is very dependent on your data, queries, and users; so I'll repeat once
again—keep an eye on your caches and don't be afraid to react and change them. For example,
take a look at the following screenshot that shows that the filter cache is probably too small,
because the evictions are happening (this is a screenshot of the Solr administration panel):
www.it-ebooks.info
Improving Solr Performance
194
Improving Solr performance right after the
startup or commit operation
Anyone with some experience with Solr would have noticed that – right after the startup, Solr
doesn't have as much of an improved query performance as after running a while. This happens
because Solr doesn't have any information stored in caches, the I/O is not optimized, and so on.
Can we do something about it? Of course we can, and this recipe will show you how to do it.
How to do it...
The following steps will explain how we can enhance Solr performance right after the startup
or commit operation:
1. First of all, we need to identify the most common and the heaviest queries that we send
to Solr. I have two ways of doing this—first of all, I analyze the logs that Solr produces
and see how queries behave. I tend to choose those queries that are run often and
those that run slowly in my opinion. The second way of choosing the right queries is by
analyzing the application that use Solr and seeing what queries they produce, which
queries will be the most crucial, and so on. Based on my experience, the log-based
approach is usually much faster and can be done using self-written scripts.
But let's assume that we have identified the following queries as good candidates:
q=cats&fq=category:1&sort=title+desc,value+desc,score+desc
q=cars&fq=category:2&sort=title+desc
q=harry&fq=category:4&sort=score+desc
2. What we will do next is just add the so called warming queries to the solrconfig.
xml file. So the listener XML tag definition in the solrconfig.xml file should
look similar to the following code snippet:
<listener event="firstSearcher"
class="solr.QuerySenderListener">
<arr name="queries">
<lst>
<str name="q">cats</str>
<str name="fq">category:1</str>
<str name="sort">
title desc,value desc,score desc
</str>
<str name="start">0</str>
www.it-ebooks.info
Chapter 6
195
<str name="rows">20</str>
</lst>
<lst>
<str name="q">cars</str>
<str name="fq">category:2</str>
<str name="sort">title desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">harry</str>
<str name="fq">category:4</str>
<str name="sort">score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
</arr>
</listener>
Basically we added the so-called warming queries to the startup of Solr. Now let's
see how it works.
How it works...
By adding the preceding fragment of configuration to the solrconfig.xml file, we told
Solr that we want it to run those queries whenever a firstSearcher event occurs. The
firstSearcher event is fired whenever a new searcher object is prepared and there is
no searcher object available in the memory. So basically, the firstSearcher event
occurs right after Solr startup.
So what happens after Solr startup? After adding the preceding fragment, Solr runs each
of the defined queries. By doing this, the caches get populated with the entries that are
significant for the queries that we identified. This means that if we did the job right, we
have Solr configured and ready to handle the most common and heaviest queries right
after its startup.
Let's just go over what all the configuration options mean. The warm up queries are always
defined under the listener XML tag. The event parameter tells Solr what event should
trigger the queries; in our case, it is firstSearcher. The class parameter is the Java
class that implements the listener mechanism. Next, we have an array of queries that
are bound together by the array tag with the name="queries" parameter. Each of
the warming queries is defined as a list of parameters that are grouped by the lst tag.
www.it-ebooks.info
Improving Solr Performance
196
There's more...
There is one more thing that I would like to mention (in the following section).
Improving Solr performance after commit operations
If you are interested in improving the performance of your Solr instance, you should also look
at the newSearcher event. This event occurs whenever a commit operation is performed
by Solr (for example, after replication). Assuming that we identified the same queries as
before as good candidates to warm the caches, we should add the following entries to the
solrconfig.xml file:
<listener event="newSearcher" class="solr.QuerySenderListener">
<arr name="queries">
<lst>
<str name="q">cats</str>
<str name="fq">category:1</str>
<str name="sort">title desc,value desc,score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">cars</str>
<str name="fq">category:2</str>
<str name="sort">title desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">harry</str>
<str name="fq">category:4</str>
<str name="sort">score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
</arr>
</listener>
Please remember that the warming queries are especially important for the caches that
can't be automatically warmed.
www.it-ebooks.info
Chapter 6
197
Caching whole result pages
Imagine a situation where you have an e-commerce library and your data changes rarely. What
can you do to take away the stress on your search servers? The first thing that comes to mind
is caching; for example, HTTP caching. And yes, that is a good point. But do we have to set up
external caches prior to Solr, or can we tell Solr to use its own caching mechanism? We can
use Solr to cache whole result pages and this recipe will show you how to do it.
Getting ready
Before you continue to read this recipe, it would be nice for you to know some basics about
the HTTP cache headers. To learn something about it, please refer to the RFC document that
can be found on the W3 site at http://www.w3.org/Protocols/rfc2616/rfc2616-
sec13.html.
How to do it...
So let's configure the HTTP cache. To do this, we need to configure the Solr request
dispatcher. Let's assume that our index changes every 60 minutes.
1. Let's start by replacing the request dispatcher definition in the solrconfig.xml
file with the following content:
<requestDispatcher handleSelect="true">
<httpCaching lastModifiedFrom="openTime"
etagSeed="Solr">
<cacheControl>max-age=3600, public</cacheControl>
</httpCaching>
</requestDispatcher>
2. Now, let's try sending a query similar to the following to see the HTTP headers:
http://localhost:8983/solr/select?q=book
We get the following HTTP headers:
HTTP/1.1 200 OK
Cache-Control: max-age=3600, public
Expires: Tue, 11 Sep 2012 16:44:56 GMT
Last-Modified: Tue, 11 Sep 2012 15:43:24 GMT
ETag: "YzAwMDAwMDAwMDAwMDAwMFNvbHI="
Content-Type: application/xml; charset=UTF-8
Transfer-Encoding: chunked
From this we can tell that cache works.
www.it-ebooks.info
Improving Solr Performance
198
How it works...
The cache definition is defined inside the requestDispatcher XML tag. The
handleSelect="true" attribute describes error handling and it should be set to
true. Then, we see the httpCaching tag (notice the lack of the <httpCaching
never304="true"> XML tag), which actually configures the HTTP caching in Solr. The
lastModifiedFrom="openTime" attribute defines that the last modified HTTP header
will be relative to when the current searcher object was opened (for example, relative to the
last replication execution date). You can also set this parameter value to dirLastMod to
be relative to when the physical index was modified. Next, we have the eTagSeed attribute,
which is responsible for generating the ETag HTTP cache header.
The next configuration tag is the cacheControl tag, which can be used to specify the
generation of the cache control HTTP headers. In our example, adding the max-age=3600
parameter tells Solr that it should generate an additional HTTP cache header, which will
confirm that the cache is valid for a maximum of one hour. The public directive means
that the response can be cached by any cache type.
As you can see from the response, the headers that we got as a part of the results returned
by Solr tell us that we got what we wanted.
Improving faceting performance for low
cardinality fields
Let's assume that our data which we use to calculate faceting can be considered to have low
distinct values. For example, we have an e-commerce shop with millions of products – clothes.
Each document in our index, apart from name and price, is also described by additional
information – target size. So, we have values such as XS, S, M, L, XL, and XXL (that is, six
distinct values), and each document can only be described with a single value. In addition to
this, we run field faceting on that information and it doesn't work fast by default. This recipe
will show you how to change that.
How to do it...
The following steps will explain how we can improve faceting performance for low
cardinality fields:
1. Let's begin with the following index structure (add the following entries to your
schema.xml fields section):
<field name="id " type="string" indexed="true"
stored="true" required="true" />
www.it-ebooks.info
Chapter 6
199
<field name="name " type="text " indexed="true"
stored="true" />
<field name="size" type="string" indexed="true"
stored="true" />
The size field is the one in which we store our XS, S, M, L, XL, and XXL values
(remember: one value per document).
2. Assuming that our user typed black skirt into the Search box, our query would
look similar to the following code snippet:
q=name:(black+skirt)&q.op=AND&facet=true&facet.field=size
Assuming that the query is matching one-fourth of our documents, we can expect
the query to be executing longer than usual. This is because the default faceting
calculation is optimized for fields that have many unique values in the index and
we have the opposite—we have many documents but few unique terms.
3. In order to speed up faceting in our case, let's add the facet.method=enum
parameter to our query, so that it looks similar to the following code snippet:
q=name:(black+skirt)&q.op=AND&facet=true&facet.field=size&facet.
method=enum
If you measure the performance before and after the change you will notice the difference;
let's discuss why.
How it works...
Let's take a look at the query—we search for the given words in the name field using the AND
logical operator (q.op parameter). As our requirements state, we also run faceting on the
size field (facet=true and facet.field=size parameters).
We know that our fields have only six distinct values, and we also assumed that our queries
can return vast amount of documents. To handle such faceting calculation faster than the
default method, we decided to use the enum method of facet calculation. The default faceting
calculation method (facet.method=fc) iterates over documents that match the query and
sums the terms that appear in the field that we are calculating faceting on. The enum method
does the other thing – it enumerates all the terms in the field that we want to calculate
faceting on, and intersects the documents that match the query with the documents that
match the enumerated terms. In this way, less time and processing is needed to calculate
field faceting for low cardinality fields, such as size in our case, and thus we see faster
query execution.
It is good to know that for field faceting on Boolean fields, Solr uses the enum faceting method
by default.
www.it-ebooks.info
Improving Solr Performance
200
There's more...
You can also use the faceting method for each field you perform faceting upon.
Specifying faceting method per field
If you have multiple fields on which you run faceting, then you may only want to change the
method for one of them (or more than one, but not all). To do that, instead of adding the
facet.method=enum parameter, you can add the facet.FIELD_NAME.method=enum
parameter for each field whose faceting calculation method you would want to change. For
example, if you would like to change the faceting method for the size field, you can add the
following parameter:
facet.size.method=enum
What to do when Solr slows down during
indexing
One of the most common problems when indexing a vast amount of data is the indexing time.
Some of the problems with indexing time are not easily resolvable, but others are. Imagine that
you need to index about 300,000 documents that are in a single XML file. You run the post.
sh bash script that is provided with Solr and you wait, wait, and wait. Something is wrong –
when you index 10,000 documents you need about a minute, but now you are waiting about
an hour and the commit operation didn't take place. Is there something we can do to speed
it up? Sure, and this recipe will tell you how to.
How to do it...
The solution to the situation is very simple – just add the commit operation every now and then.
But as you may have noticed, I mentioned that our data is written in a single XML file. So, how do
we add the commit operation to that kind of data? Send it in parallel to the indexing process?
No, we need to enable the auto commit mechanism. To do that, let's modify the solrconfig.
xml file, and change the update handler definition to the following one:
<updateHandler class="solr.DirectUpdateHandler2">
<autoCommit>
<maxTime>60000</maxTime>
<openSearcher>true</openSearcher>
</autoCommit>
</updateHandler>
www.it-ebooks.info
Chapter 6
201
If you start the indexing described in the indexing process, you will notice that a commit
command will be sent once a minute while the indexing process is takes place. Now, let's
see how it works.
How it works...
Solr tends to slow down the indexing process when indexing a vast amount of data without the
commit commands being sent once in a while. This behavior is completely understandable
and is bound to the memory and how much of it Solr can use.
We can avoid the slowing down behavior by adding the commit command after the set
amount of time or set amount of data. In this recipe, we choose the first approach.
We assumed that it would be good to send the commit command once every minute. So we add
the <autoCommit> section with the <maxTime> XML tag set to a value of 60000. This value is
specified in milliseconds. We've also specified that we want the search to be reopened after the
commit and thus the data available for search (the <openSearcher>true</openSearcher>
option). If you would only like to write the data to the index and not have it available for search,
just change the <openSearcher>true</openSearcher> option to false. That's all we
need to do. After this change, Solr will send a commit command after every minute passes
during the indexing operation, and we don't have to worry that Solr indexing speed will
decrease over time.
There's more...
There are two more things about automatic commits that should be mentioned.
Commit after a set amount of documents
Sometimes, there is a need to rely not on the time between commit operations, but on the
amount of documents that were indexed. If this is the case, we can choose to automatically
send the commit command after a set amount of documents are processed. To do this, we
add the <maxDocs> XML tag with the appropriate amount. For example, if we want to send
the commit command after every 50000 documents, the update handler configuration
should look similar to the following code snippet:
<updateHandler class="solr.DirectUpdateHandler2">
<autoCommit>
<maxDocs>50000</maxDocs>
<openSearcher>true</openSearcher>
</autoCommit>
</updateHandler>
www.it-ebooks.info
Improving Solr Performance
202
Commit within a set amount of time
There may be situations when you want some of the document to be committed faster than
the auto commit settings. In order to do that, you can add the commitWithin attribute to
the <add> tag of your data XML time. This attribute will tell Solr to commit the documents
within the specified time (specified in milliseconds). For example, if we want the portion of
documents to be indexed within 100 milliseconds, our data file would look similar to the
following code snippet:
<add commitWithin="100">
<doc>
<field name="id">1</field>
<field name="title">Book 1</field>
</doc>
</add>
Analyzing query performance
Somewhere along the experience with Apache Solr (and not only Solr), you'll end up at a point
where some of your queries are not running as you would like them to run – some of them are
just slow. Of course, such a situation is not desirable and we have to do something to make
those queries run faster. But how do we know which part of the query is the one we should
look at ? This recipe will tell you what information you can get from Solr.
How to do it...
The following steps will help you analyze query performance:
1. Let's start with the assumption that we have a query that has parts that are not as
fast as we would like it to be. The query is as follows:
http://localhost:8983/solr/select?q=metal&facet=true&facet.
field=date&facet.query=from:[10+TO+2000]
2. In order to get the information we want, we need to add the debugQuery=true
parameter to our query, so that it looks similar to the following code snippet:
http://localhost:8983/solr/select?q=metal&facet=true&facet.
field=date&facet.query=from:[10+TO+2000]&debugQuery=true
The response from Solr is as follows (I've cut some parts of the response, because
it is quite large and we are only interested in the last section):
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">427</int>
www.it-ebooks.info
Chapter 6
203
</lst>
<result name="response" numFound="61553" start="0">
<doc>
(...)
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries">
<int name="from:[10 TO 2000]">50820</int>
</lst>
<lst name="facet_fields">
<lst name="date">
<int name="0">61553</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
<lst name="debug">
<str name="rawquerystring">metal</str>
<str name="querystring">metal</str>
<str name="parsedquery">Body:metal</str>
<str name="parsedquery_toString">Body:metal</str>
<lst name="explain">
(...)
</lst>
<str name="QParser">LuceneQParser</str>
<lst name="timing">
<double name="time">426.0</double>
<lst name="prepare">
<double name="time">15.0</double>
<lst name="org.apache.solr
.handler.component.QueryComponent">
<double name="time">14.0</double>
</lst>
<lst name="org.apache.
solr.handler.component.FacetComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.MoreLikeThisComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.HighlightComponent">
<double name="time">0.0</double>
</lst>
www.it-ebooks.info
Improving Solr Performance
204
<lst name="org.apache.solr
.handler.component.StatsComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.DebugComponent">
<double name="time">0.0</double>
</lst>
</lst>
<lst name="process">
<double name="time">411.0</double>
<lst name="org.apache.solr
.handler.component.QueryComponent">
<double name="time">43.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.FacetComponent">
<double name="time">360.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.MoreLikeThisComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.HighlightComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.StatsComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.DebugComponent">
<double name="time">8.0</double>
</lst>
</lst>
</lst>
</lst>
</response>
As you can see in the preceding response, there is some information about query time.
So let's see what it means.
www.it-ebooks.info
Chapter 6
205
How it works...
Let's not concentrate on the query, because it is only an example that allows us to discuss
what we want to achieve. We've added a single parameter to the query – debugQuery=true.
This parameter turns on the debug mode in Solr, as you can see in the response.
The debug mode is divided into few categories. All these categories are nested inside the
<lst name="debug"> XML tag. The first few entries let you see how the query parser
parses your query and how it is passed to Lucene, but it's beyond the scope of this chapter
to explain this. Similar information is nested inside the <lst name="explain"> XML tag;
we will talk about it in Chapter 9, Dealing with Problems.
What we are interested in is the information nested inside the <lst name="timing">
XML tag. The first information you see under this tag is the total time of your query, which
in our case is 426 milliseconds (<double name="time">426.0</double>). We have
the following two lists:
ff <lst name="prepare"> holds information regarding the query preparation time
ff <lst name="process"> holds information regarding the query execution time
You can see that nested inside those lists are components and their time.
The prepare list tells us how much time each component spends during the query
preparation phase. For example, we can see that org.apache.solr.handler.
component.QueryComponent spent 14.0 milliseconds during preparation time.
The process list tells us how much time was spent during the query processing phase,
which is the phase that is usually the longest one, because of all the computation and
I/O operations needed to execute the query. You can see that in our case, there were three
components that were working for longer than 0 milliseconds. The last one (org.apache.
solr.handler.component.DebugComponent) is the component that we added with
the query parameter, and we can skip it as it won't be used during production queries.
The second component, which was running for 43 milliseconds, was org.apache.solr.
handler.component.QueryComponent, which is responsible for parsing the query
and running it. It still takes about 10 percent time of the whole query, which is not what
we are looking for. The component that took the most amount of the query execution time
is org.apache.solr.handler.component.FacetComponent; it was working for
360 milliseconds, so for almost 90 percent of the query execution time.
As you can see, with the use of the debugQuery parameter, we identified which part of the
query is problematic and we can start optimizing it; But it's beyond the scope of this recipe.
www.it-ebooks.info
Improving Solr Performance
206
Avoiding filter caching
Imagine that some of the filters you use in your queries are not good candidates for caching.
You may wonder why, for example, those filters have a date and time with seconds or are
spatial filters scattered all over the world. Such filters are quite unique and when added to
the cache, their entries can't be reused much. Thus they are more or less useless. Caching
such filters is a waste of memory and CPU cycles. Is there something you can do to avoid filter
queries caching? Yes, there is a way and this recipe will show you how to do it.
How to do it...
Let's assume we have the following query being used to get the information we need:
q=solr+cookbook&fq=category:books&fq=date:2012-06-12T13:22:12Z
The filter query we don't want to cache is the one filtering our documents on the basis of the
date field. Of course, we still want the filtering to be done. In order to turn off caching, we
need to add {!cache=false} to our filter with the date field, so that our query should look
similar to the following code snippet:
q=solr+cookbook&fq=category:books&fq={!cache=false}date:2012-06-
12T13:22:12Z
Now let's take a look at how this works.
How it works...
The first query is very simple; we just search for the words solr cookbook and we want the
result set to be narrowed in the books category. We also want to narrow the results further to
only those that have 2012-06-12T13:22:12Z in the date field.
As you can imagine, if we have many filters with such dates as the one in the query, the
filter cache can be filled very fast. In addition to this, if you don't reuse the same value for
that field, the entry in the field cache becomes pretty useless. That's why, by adding the
{!cache=false} part to the filter query, we tell Solr that we don't want the filter query
results to be put into the filter cache. With such an approach we won't pollute the filter cache
and thus save some CPU cycles and memory. There is one more thing – the filters that are
not cached will be executed in parallel with the query, so this may be an improvement to your
query execution time.
www.it-ebooks.info
Chapter 6
207
Controlling the order of execution of filter
queries
If you use filter queries extensively, which isn't a bad thing at all, you may be wondering if
there is something you can do to improve the execution time of some of your filter queries.
For example, if you have some filter queries that use heavy function queries, you may want
to have them executed only on the documents that passed all the other filters. Let's see how
we can do this.
Getting ready
Before continuing reading please read the Avoiding filter caching recipe in this chapter.
How to do it...
The following steps will explain how we can control the order of execution of filter queries:
1. Let's assume we have the following query being used to get the information we need:
q=solr+cookbook&fq=category:books&fq={!frange l=10 u=100}log(sum(s
qrt(popularity),100))&fq={!frange l=0 u=10}if(exists(price_a),sum(
0,price_a),sum(0,price))
2. For the purpose of this recipe, let's also assume that fq={!frange l=10 u=100}
log(sum(sqrt(popularity),100)) and fq={!frange l=0 u=10}if(exis
ts(price_a),sum(0,price_a),sum(0,price)) are the filter queries that are
heavy and we would like those filters to be executed as the previous ones. We would
also like the second filter to execute only on the documents that were narrowed by
other filters. In order to do this, we need to modify our query so that it looks similar to
the following code snippet:
q=solr+cookbook&fq=category:books&fq={!frange l=10 u=100
cache=false cost=50}log(sum(sqrt(popularity),100))&fq={!frange l=0
u=10 cache=false cost=150}if(exists(price_promotion),sum(0,price_
promotion),sum(0,price))
As you can see, we've added other two attributes: cache=false and cost having values as
50 and 150. Let's see what they mean.
www.it-ebooks.info
Improving Solr Performance
208
How it works...
As you can see, we search for the words solr cookbook in the first query and we want the
result set to be narrowed by book category. We also want the documents to be narrowed to
only those that have a value of the log(sum(sqrt(popularity),100)) function between
10 and 100. In addition to this, the last filter query specifies that we want our documents to
be filtered to only those that have a price_promotion field (price if price_promotion
isn't filled) value between 0 and 10.
Our requirements are such that the second filter query (the one with log function query)
should be executed after the fq=category:books filter query and the last filter should
be executed in the end, only on the documents narrowed by other filters. To do this, we set
those two filters to not cache and we introduced the cost parameter. The cost parameter
in filter queries specifies the order in which non-cached filter queries are executed; the higher
the cost value, the later the filter query will be executed. So our second filter (the one with
cost=50) should be executed after the fq=category:books filter query and the last filter
query (the one with cost=150) are executed. In addition to this, because the cost of the
second non-cached filter query is higher or equal to 100, this filter will be executed only on
the documents that matched the main query and all the other filters. So our requirements
have been completed.
Forgive me, but I have to say it once again—please remember that the cost attribute only
works when the filter query is not cached.
Improving the performance of numerical
range queries
Let's assume we have the Apache Solr 4.0 deployment where we use range queries. Some
of those are run against string fields, while others are run against numerical fields. Using
different techniques, we identified that our numerical range queries execute slower than we
would like. The usual question arises – is there something that we can do ? Of course, and
this recipe will show you what.
How to do it...
The following steps will explain how we can control the order of execution of numerical
range queries:
1. Let's begin with the definition of a field that we use to run our numerical
range queries:
<field name="price" type="float" indexed="true" stored="true"/>
www.it-ebooks.info
Chapter 6
209
2. The second step is to define the float field type:
<fieldType name="float" class="solr.TrieFloatField"
precisionStep="8" positionIncrementGap="0"/>
3. Now the usual query that is run against the preceding field is as follows:
q=*:*&fq=price:[10.0+TO+59.00]&facet=true&facet.field=price
4. In order to have your numerical range queries performance improved, there is just a
single thing you need to do – decrease the precisionStep attribute of the float
field type; for example, from 8 to 4. So, our field type definition would look similar to
the following code snippet:
<fieldType name="float" class="solr.TrieFloatField"
precisionStep="4" positionIncrementGap="0"/>
After the preceding change, you will have to re-index your data and your numerical
queries should be run faster. How faster, depends on your setup. Now let's take a
look at how it works.
How it works...
As you can see, in the preceding examples, we used a simple float-based field to run
numerical range queries. Before the changes, we specified precisionStep on our field type
as 8. This attribute (specified in bits) tells Lucene (which Solr is built on top of) how many
tokens should be indexed for a single value in such a field. Smaller precisionStep values
(when precisionStep > 0) will lead to more tokens being generated by a single value and
thus make range queries faster. Because of this, when we decreased the precisionStep
value from 8 to 4, we saw a performance increase.
However, please remember that decreasing the precisionStep value will lead to slightly
larger indices. Also, setting the precisionStep value to 0 turns off indexing of multiple
tokens per value, so don't use that value if you want your range queries to perform faster.
www.it-ebooks.info
www.it-ebooks.info
7
In the Cloud
In this chapter we will cover:
ff Creating a new SolrCloud cluster
ff Setting up two collections inside a single cluster
ff Managing your SolrCloud cluster
ff Understanding the SolrCloud cluster administration GUI
ff Distributed indexing and searching
ff Increasing the number of replicas on an already live cluster
ff Stopping automatic document distribution among shards
Introduction
As you know, Apache Solr 4.0 introduced the new SolrCloud feature that allows us to use
distributed indexing and searching on a full scale. We can have automatic index distribution
across multiple machines, without having to think about doing it in our application. In this
chapter, we'll learn how to manage our SolrCloud instances, how to increase the number
of replicas, and have multiple collections inside the same cluster.
Creating a new SolrCloud cluster
Imagine a situation where one day you have to set up a distributed cluster with the use of Solr.
The amount of data is just too much for a single server to handle. Of course, only you can set
up a second server or go for another master database with another set of data. But before
Solr 4.0, you would have to take care of the data distribution yourself. In addition to this, you
would also have to take care of setting up replication, thinking about data duplication, and so
on. You don't have to do this now because Solr 4.0 can do it for you. Let's see how.
www.it-ebooks.info
In the Cloud
212
Getting ready
Before continuing, I advise you to read the Installing standalone ZooKeeper recipe in Chapter
1, Apache Solr Configuration. This recipe shows how to set up a ZooKeeper cluster ready for
production use. However, if you already have ZooKeeper running, you can skip that recipe.
How to do it...
Let's assume we want to create a cluster that will have four Solr servers. We would also like
to have our data divided between four Solr servers in such a way that we would have the
original data sharded to two machines. In addition to this we would also have a copy of each
shard available, in case something happens with one of the Solr instances. I also assume that
we already have our ZooKeeper cluster setup, ready, and available at the 192.168.0.10
address on port 9983.
1. Let's start with populating our cluster configuration into the ZooKeeper cluster. In
order to do this, you need to run the following command:
java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.
configName=twoShardsTwoReplicasConf -DnumShards=2
-DzkHost=192.168.0.10:9983 -jar start.jar
2. Now that we have our configuration populated, let's start the second node with the
following command:
java -DzkHost=192.168.0.10:9983 -jar start.jar
3. We now have our two shards created and want to create replicas. This is very simple
since we have already created the configuration. We just need to start two additional
servers with the following command run on each of them:
java -DzkHost=192.168.0.10:9983 -jar start.jar
If you look at the cloud configuration of the Solr administration panel, you will see that you
have a cluster that has four nodes, where the first two nodes act as leaders for the shards
and the other two nodes act as their replicas. You can start indexing your data to one of the
servers now, and Solr will take care of data distribution and will also automatically copy the
data to the replicas. Let's see how this works.
www.it-ebooks.info
Chapter 7
213
How it works...
What we need to do first is send all the configuration files to ZooKeeper in order for the Solr
servers to be able to fetch it from there. That's why, when running the first server (only during
the first start of it), we add the -Dboostrap_confdir and -Dcollection.configName
parameters. The first parameter specifies the location of the directory with the configuration
files that we would like to put into ZooKeeper. The second parameter specifies the name of
your configuration. During the first start, we also need to specify the number of shards that
should be available in our cluster, and in this example we set it to 2 (the -DnumShards
parameter). The -DzkHost parameter is used to tell Solr about the location and the port
used by the Zookeeper cluster.
As you can see, all the other commands are similar to the ones you used while running the
Solr instances. The only difference is that we specify one additional parameter, -DzkHost,
which tells Solr where to look for the ZooKeeper server on the cluster.
When setting up the SolrCloud cluster, please remember to choose the number of shards
wisely, because you can't change that for your existing cluster, at least not right now. You can
add replicas to an already created cluster, but the number of shards will remain constant.
There's more...
There is one more thing that I would like to mention – the possibility of running a ZooKeeper
server embedded into Apache Solr 4.0.
Starting the embedded ZooKeeper server
You can also start an embedded ZooKeeper server shipped with Solr for your test
environment. In order to do this, you should pass the -DzkRun parameter instead
of -DzkHost=192.168.0.10:9983, but only in the command that sends our
configuration to the ZooKeeper cluster. So the final command should look similar
to the following code snippet:
java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName
=twoShardsTwoReplicasConf -DzkHost=192.168.0.10:9983 -DnumShards=2 -jar
start.jar
www.it-ebooks.info
In the Cloud
214
Setting up two collections inside a single
cluster
Imagine that you would like to have more than a single collection inside the same Apache
Solr 4.0 cluster. For example, you would like to store books in one collection and users in
the second one. SolrCloud allows that, and this recipe will show you how to do it.
Getting ready
Before continuing, I advise you to read the Installing standalone ZooKeeper recipe in Chapter
1, Apache Solr Configuration, because this recipe assumes that we already have ZooKeeper up
and running. We assume that ZooKeeper is running on localhost and is listening on port 2181.
How to do it...
1. Since we want to start a new SolrCloud cluster that doesn't have any collections
defined, we should start with the solr.xml file. On both instances of Solr, the
solr.xml file should look similar to the following code snippet:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores"
defaultCoreName="collection1" host="${host:}"
hostPort="${jetty.port:}"
hostContext="${hostContext:}"
zkClientTimeout="${zkClientTimeout:15000}">
</cores>
</solr>
2. Let's assume that we have two SolrCloud instances that form a cluster, both running
on the same physical server, one on port 8983 and the second one on 9983. They
are started with the following commands:
java -Djetty.port=8983 -DzkHost=localhost:2181 -jar start.jar
java -Djetty.port=9983 -DzkHost=localhost:2181 -jar start.jar
3. Now, we need to add the configuration files, which we want to create collections with,
to ZooKeeper. Let's assume that we have all the configuration files stored in /usr/
share/config/books/conf for the books collection, and the configuration files for
the users collection stored in /usr/share/config/users/conf. To send these files
to ZooKeeper, we should run the following commands from our $SOLR_HOME directory:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/books/conf -confnamebookscollection
www.it-ebooks.info
Chapter 7
215
And:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/users/conf -confnameuserscollection
4. We have pushed our configurations into the ZooKeeper, so we can now create the
collections we want. In order to do this, we use the following commands:
curl 'http://localhost:8983/solr/admin/collections?action=CREATE&n
ame=bookscollection&numShards=
2&replicationFactor=0'
And:
curl 'http://localhost:8983/solr/admin/collections?action=CREATE&n
ame=userscollection&numShards=
2&replicationFactor=0'
5. Now, just to test if everything went well, we will query the newly created collections
as follows:
curl 'http://localhost:8983/solr/bookscollection/select?q=*:*'
The response to the preceding command will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">39</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="0" start="0"
maxScore="0.0">
</result>
</response>
As you can see, Solr responded correctly. But as we don't have any data indexed,
we got 0 documents.
How it works...
As you can see, our solr.xml file on both the instances is the same and it doesn't contain
any information about the cores. This is done on purpose, since we want to have a clean
cluster – one without any collections present.
www.it-ebooks.info
In the Cloud
216
The mentioned configuration directories should store all the files (solrconfig.xml,
schema.xml, and stopwords.txt) that are needed for your Solr instance to work, if you
use one. Please remember this before sending them to ZooKeeper or else Solr will fetch
those files from ZooKeeper and create collections using them.
Now, let's look at the most interesting aspect – the scripts used to upload the configuration
files to ZooKeeper. We used the zkcli.sh script provided with the standard Solr 4.0
distribution and placedit in the cloud-scripts directory by default. The first thing is the cmd
parameter, which specifies what we want to do – in this case upconfig means that we want
to upload the configuration files. The zkhost parameter allows us to specify the host and port
of the ZooKeeper instance we want to put the configuration to. confdir is one of the most
crucial parameters and it specifies the directory in which the Solr configuration files are stored
– the ones that should be sent to ZooKeeper (in our case, /usr/share/config/users/
conf and /usr/share/config/books/conf). Finally the last parameter, confname,
specifies the name of the collection we will use the configuration with.
The command in the fourth step lets us create the actual collection in the cluster. In order to do
this, we send a request to the /admin/collections handler, which uses the newly introduced
collections API. We tell Solr that we want to create a new collection (the action=CREATE
parameter) with the name of bookscollection (name=bookscollection). Please note that
the name specified in the name parameter is the same as the confname parameter value used
during configuration files upload. The last two parameters specify the number of shards and
replicas that the collection should be created with. The number of shards is the initial number of
cores that will be used to hold the data in the collection (numShards). The number of replicas
(replicationFactor) is the exact number of copies of the shards that can be distributed
among many servers, and may increase query throughput and reliability.
Managing your SolrCloud cluster
In addition to creating a new collection with the API exposed by SolrCloud, we are also allowed
to use two additional operations. The first is to delete our collection and the second one is
to reload the whole collection. Along with the ability to create new collections, we are able
to dynamically manage our cluster. This recipe will show you how to use the delete and
reload operations and where they can be useful.
www.it-ebooks.info
Chapter 7
217
Getting ready
The content of this recipe is based on the Setting up two collections inside a single cluster
recipe in this chapter. Please read it before continuing.
How to do it...
I assume that we already have two collection deployed on our cluster –bookscollection
and userscollection – the same ones that we configured in the Setting up two
collections inside a single cluster recipe in this chapter. So our cluster view looks
similar to the following screenshot:
1. First, let's delete one of the collections – userscollection. To do this, we send the
following command:
curl 'http://localhost:8983/solr/admin/collections?action=DELETE&n
ame=userscollection'
www.it-ebooks.info
In the Cloud
218
2. Now, let's look at our cluster view once again:
As you can see, the userscollection collection was deleted.
3. Now, let's see how the reloading of collections works. In order to test it, let's
update the spellings.txt file located at /usr/share/config/books/conf
directory. The original file looks similar to the following code snippet:
pizza
history
After the update, it will look similar to the following code snippet:
after
update
4. Now, we need to update the collection configuration in ZooKeeper. To do this we
use the following command, which is run from our Solr instance's home directory:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/books/conf -confnamebookscollection
5. Now that we have the updated version of our configuration files to
bookscollection in ZooKeeper, we can send the reload command to Solr:
curl 'http://localhost:8983/solr/admin/collections?action=RELOAD&n
ame=bookscollection'
www.it-ebooks.info
Chapter 7
219
6. First, let's check if the Solr administration panel sees the changes in ZooKeeper.
To do this, we'll use the tree view of the cloud section and navigate to /configs/
bookscollection/spellings.txt. We should be able to see something similar
to the following screenshot:
7. In the final check, let's see if Solr itself sees the update. In order to do this we run the
following command:
curl 'http://localhost:8983/solr/bookscollection/admin/
file?file=spellings.txt'
The response of the preceding command would be as follows:
after
update
So it seems like everything is working as it should. Now let's see how it works.
How it works...
We begin with a cluster that contains two collections. But we want to delete one of them and
update the second one. In order to do this we use the collections API provided by Solr 4.0.
www.it-ebooks.info
In the Cloud
220
We start by sending the delete action (action=DELETE) to the /solr/admin/
collections URL, which is the default address that the collections API is available at.
In addition to this, we need to provide the name of the collection we want to delete – to do
this, we use the name parameter with the name of the collection that we want to delete. After
sending the command and refreshing the Solr administration panel, we see that the second
collection was deleted just as we wanted.
Now, let's discuss the process of updating the second collection. First of all, we've changed
the contents of the spellings.txt file in order to see how it works. However, be careful
when updating collections, because some changes may force you to re-index your data; but
let's get back to our update. So after we update the file, we use one of the scripts provided
with Solr 4.0 in order to upload all the configuration files that belong to this collection into the
ZooKeeper ensemble (if you are not familiar with that command, please see the Setting up
two collections inside a single cluster recipe, later in this chapter). Now, we needed to tell Solr
to reload our collection by sending the reload command (action=RELOAD) to the same
URL as the delete command. Of course, just like with the delete command, we needed to
provide the name of the collection we want to reload using the name parameter.
As you can see, on the previous screenshot, the collection was updated at least in the
ZooKeeper ensemble. However, we want to be sure that Solr sees those changes, so we use
the /admin/file handler to get the contents of the spellings.txt file. In order to do this,
we pass the file=spellings.txt parameter to that handler. As you can see, Solr returned
the changed contents, so the collection was updated and reloaded successfully.
Understanding the SolrCloud cluster
administration GUI
With the release of Solr 4.0, we've got the ability to use a fully-distributed Solr cluster
with fully-distributed indexing and searching. Along with this comes the reworked Solr
administration panel with parts concentrated on Cloud functionalities. This recipe will
show you how to use this part of the administration panel; for example, how to see your
cluster distribution and detailed information about shards and replicas.
Getting ready
This recipe assumes that the SolrCloud cluster is up and running. If you are not familiar with
setting up the SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe
in this chapter.
www.it-ebooks.info
Chapter 7
221
How to do it...
1. First of all, let's see how we can check how our cluster distribution looks. In order
to do this, let's open our web browser to http://localhost:8983/solr/ (or
the address of the host and port of any of the Solr instances that form the cluster)
and open the Cloud graph view. We should be able to see something similar to the
following screenshot:
2. There is also a second view of the same information that can be accessed by viewing
the Graph (Radial) section, and it should look similar to the following screenshot:
www.it-ebooks.info
In the Cloud
222
3. Looks nice, doesn't it? However, there is some additional information that can be
retrieved. So now, let's look at the Tree section of the Cloud administration panel:
As you can see, there is some very detailed information available. So now, let's look at what
it means.
How it works...
First of all, remember that the best way to get used to the new administration panel is to just
run a simple SolrCloud cluster by yourself and play with it. However, let's look at the provided
examples to see what information we have there.
As you can see, in the first two screenshots provided, our cluster consists of a single collection
named collection1. It consists of two shards (shard1 and shard2) and each shard
lives on a single node. One of each shards is the primary one (the ones at gr0-vaio:8983
and gr0-vaio:7983), and each of them has a replica (the ones at gr0-vaio:6983 and
gr0-vaio:5983). Both diagrams shown in the screenshots provide the same amount of
information and they only differ in the way they present the data.
www.it-ebooks.info
Chapter 7
223
Now, let's look and discuss the last screenshot – the Tree view of the Cloud section of the
Solr administration panel. As you can see, there is much more information available there.
The tree presented in the administration panel is what your ZooKeeper ensemble sees.
The first thing is clusterstate.json, which holds detailed information about the current
state of the cluster.
Next, you can see the collections section, which holds information about each collection
deployed in the cluster – you can see the information about each shard and its replicas, such
as leaders, and some detailed information needed by the Solr and ZooKeeper.
In addition to the preceding information, you can also see the configuration files
(the /configs section) that were sent to the ZooKeeper and are used as the
configuration files for your collection or collections.
Not visible in the screenshot is the additional information connected to ZooKeeper,
which is not needed during the usual work with Solr, so I decided to omit discussing it.
Distributed indexing and searching
Having a distributed SolrCloud cluster is very useful; you can have multiple shards and replicas,
which are automatically handled by Solr itself. This means that your data will be automatically
distributed among shards and replicated between replicas. However, if you have your data
spread among multiple shards, you probably want them to be queried while you send the
query. With earlier versions of Solr before 4.0, you had to manually specify the list of shards
that should be queried. Now you don't need to do that, and this recipe will show you how to
make your queries distributed.
Getting ready
If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter. If you are not familiar with how to modify the returned
documents using the fl parameter, please read the Modifying returned documents recipe in
Chapter 4, Querying Solr.
www.it-ebooks.info
In the Cloud
224
How to do it...
1. First of all, let's assume we have a cluster that consist of three nodes and we have
a single collection deployed on that cluster; a collection with three shards. For the
purpose of this recipe, I'm using the example configuration files provided with Solr
and the example documents stored in the XML files in the exampledocs directory
of the Solr distribution package. If we look at the Solr administration panel, this is
what the Cloud graph will show:
2. Now, the best thing about distributed indexing and searching—if you are using Solr
4.0 and its distributed searching and indexing capabilities—is that you don't need to
do anything in addition to sending the proper indexing and searching requests to one
of the shards. So, in order to have the example files indexed, I've run the following
command from the exampledocs directory of the Solr instance running on port 8983:
java -jar post.jar *.xml
3. Now, let's use the non-distributed queries to check if the documents were sent to all
the shards. In order to do this, we run three queries. The first query is run to the Solr
instance holding the first shard:
curl 'http://localhost:8983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
www.it-ebooks.info
Chapter 7
225
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="8" start="0">
</result>
</response>
4. The second query is run to the Solr instance holding the second shard:
curl 'http://localhost:7983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="10" start="0">
</result>
</response>
5. The third query is run to the Solr instance holding the last shard:
curl 'http://localhost:6983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="14" start="0">
</result>
</response>
www.it-ebooks.info
In the Cloud
226
6. Everything seems to be in the perfect order now, at least by judging the number of
documents. So now, let's run the default distributed query to see if all the shards
were queried. In order to do this we run the following query:
curl 'http://localhost:8983/solr/select?q=*:*&fl=id,[shard]&ro
ws=50'
Since the response was quite big, I decided to cut it a bit and show only a single
document from each shard:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">58</int>
<lst name="params">
<str name="fl">id,[shard]</str>
<str name="q">*:*</str>
<str name="rows">50</str>
</lst>
</lst>
<result name="response" numFound="32" start="0"
maxScore="1.0">
<doc>
<str name="id">SP2514N</str>
<str name="[shard]">gr0-vaio:6983/solr/collection1/
</str>
</doc>
...
<doc>
<str name="id">GB18030TEST</str>
<str name="[shard]">gr0-vaio:7983/solr/collection1/
</str>
</doc>
...
<doc>
<str name="id">IW-02</str>
<str name="[shard]">gr0-vaio:8983/solr/collection1/
</str>
</doc>
...
</result>
</response>
As you can see, we got documents from each shard that builds our cluster, so it works as
intended. Now, let's look at exactly how it works.
www.it-ebooks.info
Chapter 7
227
How it works...
As you can see, as shown in the previous screenshot, our test cluster created for the purpose
of this recipe contains thee Solr instances, where each of them contains a single shard of the
collection deployed on the cluster. This means that the data indexed to any of the shards will
be automatically divided and distributed among the shards. In order to choose which shard
the document should go to, Solr uses a hash value of the identifier of the document.
During indexing, we sent the documents to the Solr instance that is working on port
8983. However, as our example queries show, when querying only a particular shard (the
distrib=false parameter), each of them hosts different amount of documents, which
is expected. If we had many more documents, the amount of documents on each shard
would be probably almost the same if not equal. As you must have guessed by now, the
distrib=false parameter forces the query to be run on the Solr server that it was sent
to in a non-distributed manner, and we want such behavior to see how many documents
are hosted on each of the shards.
Let's now focus on the query that was used to fetch all the documents in the cluster. It's a
query that you are probably used to – fetching all the documents (q=*:*) and returning a
maximum of 50 documents (rows=50). In addition, we specify the fl parameter in such a
way that the returned document contains the id field and the information about the shard
the document was fetched from (fl=id,[shard]). As you can see, we got documents
coming from all the shards that build the collection in the response. This is because when
using the SolrCloud deployment, Solr automatically queries all the relevant shards that are
needed to be queried in order to query the whole collection. The information about shards
(and replicas, if they exist) is fetched from ZooKeeper, so we don't need to specify it.
Increasing the number of replicas on an
already live cluster
If you used Solr before the release of the 4.0 version, you are probably familiar with replication.
The way deployments usually worked is that there was a single master server and multiple slave
servers that were pulling the index from the master server. In Solr 4.0, we don't have to worry
about replication and pulling interval – it's done automatically. We can also set up our instances
in a way to achieve a similar setup as that of multiple replicas of a single shard where data is
stored. This recipe will show you how to do it.
www.it-ebooks.info
In the Cloud
228
Getting ready
If you are not familiar with setting up a SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter.
How to do it...
For the purpose of this recipe, I'll assume that we want to have a cluster with a single shard
running just like the usual Solr deployment, and we want to add two additional replicas to that
shard. So, we have more servers to handle the queries.
1. The first step is starting a new Solr 4.0 server. We will use the configuration provided
with the example Solr server, but you can use your own if you want. We will also use
the ZooKeeper server embedded into Solr, but again, you can use the standalone one.
So finally, the command that we use for starting the first instance of Solr is as follows:
java -Dbootstrap_confdir=solr/collection1/conf -Dcollection.
configName=collection1 -DzkRun -DnumShards=1 -jar start.jar
2. Now, let's take a look at the Solr administration panel to see how our cluster
state looks:
As you can see, we have a single shard in our collection that has a single replica. This
can be a bit misleading, because the single replica is actually the initial shard we've
created. So we actually have a single shard and zero copies of it. As we said earlier,
we want to change that in order to have two additional replicas of our shard. In order
to do this, we need to run two additional Solr instances. I'll run them on the same
machine as the first one on ports 7893 and 6893. But in a real life situation, you'd
probably want to have them on different servers.
www.it-ebooks.info
Chapter 7
229
3. In order to run these two additional Solr servers, we use the following commands:
java -Djetty.port=7983 -DzkHost=localhost:9983 -jar start.jar
java -Djetty.port=6983 -DzkHost=localhost:9983 -jar start.jar
4. Now, let's see how our cluster state changes, by looking at the cluster state in
the Solr administration panel again. The cluster state information looks similar
to the following screenshot after we start the two additional instances of Solr:
As you see, we still have our initial shard. But right now, we also have two additional replicas
present that will be automatically updated and will hold the same data as the primary shard
that we created in the beginning.
www.it-ebooks.info
In the Cloud
230
How it works...
We start our single shard instance with the command that allows us to run the embedded
ZooKeeper server along with Solr. The embedded ZooKeeper server is started at the port
whose number is the Solr port + 1000, which in our case if 9983. bootstrap_confdir
specifies the directory where the Solr configuration files are stored, which will be sent to the
ZooKeeper. collection.configName specifies the name of the collection, numShards
specifies the amount of shards the collection should have, and zkRun tells Solr that we want
the embedded ZooKeeper to be run. Of course, this was only used as an example, and in a
production environment you should set up a standalone ZooKeeper server.
As shown in the previous screenshot, you can see that our collection consists of a single
shard and the only replica we have is this shard. So, we have a single primary shard with
no data replication at all. In order to create two replicas that will be automatically populated
with exactly the same data as the primary shard, we just need to start the two additional Solr
servers. For the purpose of the recipe, we started these new instances on the same machine,
but usually in a production environment you would set them up on separate machines.
As you can see in the second screenshot, after adding these two new Solr instances, our
cluster is composed of a primary shard and two replicas, which will have their contents
updated automatically. So we've got what we wanted.
Stopping automatic document distribution
among shards
In most cases, the standard distribution of documents between your SolrCloud instances
will be enough, and what's more, it will be the right way to go. However, there are situations
where controlling the documents distribution outside of Solr (that is, in your application) may
be better. For example, imagine that you'll only allow your users to search in the data they
indexed. In such situations, it would be good to have documents for a single client stored in a
single shard (if that's possible). In such cases, automatic documents distribution based on the
documents identifier may not be the best way. Solr allows us to turn off automatic document
distribution and this recipe will show you how to do that.
Getting ready
If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter. If you are not familiar with how to modify the returned
documents using the fl parameter, please read the Modifying the returned documents recipe
in Chapter 4, Querying Solr.
www.it-ebooks.info
Chapter 7
231
How to do it...
1. Let's assume that we have the following index structure (schema.xml) defined,
and that we already have it stored in ZooKeeper:
<fields>
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="userName" type="string" indexed="true"
stored="true" />
<field name="data" type="text" indexed="true"
stored="true" />
<field name="_version_" type="long" indexed="true"
stored="true"/>
</fields>
2. We have two files that contain user data. One is called data1.xml, and it holds the
data for user1 and looks similar to the following code snippet:
<add>
<doc>
<field name="id">1</field>
<field name="userName">user1</field>
<field name="data">Data of user1</field>
</doc>
</add>
The second one is called data2.xml, and it holds the data for user2:
<add>
<doc>
<field name="id">2</field>
<field name="userName">user2</field>
<field name="data">Data of user2</field>
</doc>
<doc>
<field name="id">3</field>
<field name="userName">user2</field>
<field name="data">Another data of user2</field>
</doc>
</add>
3. In order to be able to stop the automatic document distribution between shards,
we need the following update request processor chain to be defined in the
solrconfig.xml file:
<updateRequestProcessorChain>
<processor class="solr.LogUpdateProcessorFactory" />
www.it-ebooks.info
In the Cloud
232
<processor class="solr.RunUpdateProcessorFactory" />
<processor class="
solr.NoOpDistributingUpdateProcessorFactory" />
</updateRequestProcessorChain>
4. I assume that we already have a cluster containing at least two nodes up and
running, these nodes use the preceding configuration files, and that our collection
name is collection1. One of the nodes is running on a server with the IP address
as 192.168.1.1 and the second one is running on a server with the IP address as
192.168.1.2.
5. As we discussed earlier, we want to manually distribute the data to Solr instances.
In our case, we would like the data from the data1.xml file to be indexed on the Solr
server running at 192.168.1.1, and the data from data2.xml to be indexed on the
Solr instance running on 192.168.1.2. So, we use the following commands to index
the data:
java -Durl=http://192.168.1.1:8983/solr/collection1/update -jar
post.jar data1.xml
java -Durl=http://192.168.1.2:8983/solr/collection1/update -jar
post.jar data2.xml
6. Now, let's test if it works. In order to do this, we will use the Solr functionality that
enables us to see which shard the document is stored at. In our case, it will be the
following query:
curl http://localhost:7983/solr/select?q=*:*&fl=*,[shard]
The response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">24</int>
<lst name="params">
<str name="q">*:*</str>
<str name="fl">*,[shard]</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="1.0">
<doc>
<str name="id">2</str>
www.it-ebooks.info
Chapter 7
233
<str name="userName">user2</str>
<str name="data">Data of user2</str>
<str name="[shard]">
192.168.1.2:8983/solr/collection1/
</str>
</doc>
<doc>
<str name="id">3</str>
<str name="userName">user2</str>
<str name="data">Another data of user2</str>
<str name="[shard]">
192.168.1.2:8983/solr/collection1/
</str>
</doc>
<doc>
<str name="id">1</str>
<str name="userName">user1</str>
<str name="data">Data of user1</str>
<str name="[shard]">
192.168.1.1:8983/solr/collection1/
</str>
</doc>
</result>
</response>
It seems that we have achieved what we wanted, so let's see how it works.
How it works...
Our schema.xml file is very simple. It contains three fields that are used by our data
files at the _version_ field used internally by Solr. The actual data is nothing new
as well, so I'll skip discussing it.
The thing we want to look at is the update request processor chain definition.
As you can see, apart from the standard solr.LogUpdateProcessorFactory
and solr.RunUpdateProcessorFactory processors, it contains a solr.
NoOpDistributingUpdateProcessorFactory processor. You can think
of this additional processor as the one that forces the update command to
be indexed on the node it was sent to.
www.it-ebooks.info
In the Cloud
234
We used the standard post.jar library distributed with Solr in order to index the data. In
order to specify which server the data should be sent to, we use the –Durl parameter. We
use two available servers to send the data to – the one running at 192.168.1.1 that should
contain one document after indexing, and the one running at 192.168.1.2 that should
contain two documents. In order to check this, we use a query that returns all the documents
(q=*:*). In addition, we specify the fl parameter in such a way that the returned document
contains not only all the stored fields, but also the shard the document was fetched from
(fl=*,[shard]).
As you can see, in the results returned by Solr, the documents that belong to user2 (the ones
with id field equal to 2 and 3) were fetched from the Solr server running at 192.168.1.2
(<str name="[shard]">192.168.1.2:8983/solr/collection1/</str>), and
the one belonging to user1 came from the Solr instance running at 192.168.1.1 (<str
name="[shard]">192.168.1.1:8983/solr/collection1/</str>). So, everything
is just as we wanted it to be.
One more thing: please remember that when turning off automatic documents distribution,
you may end up with shards being uneven. This is because of the different number of
documents being stored in each of them. So, you have to carefully plan your distribution.
www.it-ebooks.info
8
Using Additional Solr
Functionalities
In this chapter we will cover:
ff Getting more documents similar to those returned in the results list
ff Highlighting matched words
ff How to highlight long text fields and get good performance
ff Sorting results by a function value
ff Searching words by how they sound
ff Ignoring defined words
ff Computing statistics for the search results
ff Checking the user's spelling mistakes
ff Using field values to group results
ff Using queries to group results
ff Using function queries to group results
Introduction
There are many features of Solr that we don't use every day. You may not encounter
highlighting words, ignoring words, or statistics computation in everyday use, but they
can come in handy in many situations. In this chapter, I'll try to show how to overcome
some typical problems that can be fixed by using some of the Solr functionalities. In
addition to that we will see how to use the Solr grouping mechanism in order to get
documents that have some fields in common.
www.it-ebooks.info
Using Additional Solr Functionalities
236
Getting more documents similar to those
returned in the results list
Imagine a situation where you want to show similar documents to those returned by Solr. Let's
imagine a situation where you have an e-commerce library shop, and you want to show users
the books similar to the ones they found while using your application. This recipe will show you
how to do that.
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"
termVectors="true" />
2. Next, let's use the following test data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our hypothetical user wants to find books that have cookbook and
second in their names. But, we also want to show him/her similar books. To do that
we send the following query:
http://localhost:8983/solr/select?q=cookbook+second&mm=2&qf=name&d
efType=edismax&mlt=true&mlt.fl=name&mlt.mintf=1&mlt.mindf=1
www.it-ebooks.info
Chapter 8
237
The results returned by Solr for the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="mm">2</str>
<str name="mlt.mindf">1</str>
<str name="mlt.fl">name</str>
<str name="q">cookbook second</str>
<str name="mlt.mintf">1</str>
<str name="qf">name</str>
<str name="mlt">true</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Solr Cookbook second edition</str>
<long name="_version_">1415606105364496384</long>
</doc>
</result>
<lst name="moreLikeThis">
<result name="2" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook first edition</str>
<long name="_version_">1415606105279561728</long>
</doc>
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
<long name="_version_">1415606105366593536</long>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Solr by example first edition</str>
<long name="_version_">1415606105365544960</long>
</doc>
</result>
</lst>
</response>
Now let's see how it works.
www.it-ebooks.info
Using Additional Solr Functionalities
238
How it works...
As you can see the index structure and the data are really simple. One thing to note is the
termVectors attribute set to true in the name field definition. It is a good thing to have
when using the more like this component, and should be used whenever possible in
the fields on which we plan to use this component.
Now let's take a look at the query. As you can see, we added some additional parameters
besides the standard q one (and the ones such as mm and defType which specify how our
query should be handled). The parameter mlt=true says that we want to add the more
like this component to the result processing. The mlt.fl parameter specifies which
fields we want to use with the more like this component. In our case we will use the
name field. The mlt.mintf parameter asks Solr to ignore terms from the source document
(the ones from the original result list) with the term frequency below the given value. In our
case we don't want to include the terms that will have a frequency lower than 1. The last
parameter, mlt.mindf, tells Solr that words appearing less than the value of the parameter
documents should be ignored. In our case we want to consider words that appear in at least
one document.
Last is the search results. As you can see, there is an additional section (<lst
name="moreLikeThis">) that is responsible for showing us the more like this
component results. For each document in the results there is one more like this section
added to the response. In our case, Solr added a section for the document with the unique
identifier 3 (<result name="3" numFound="3" start="0">), and there were three
similar documents found. The value of the id attribute is assigned the value of the unique
identifier of the document for which the similar documents are calculated for.
Highlighting matched words
Imagine a situation where you want to show your users which words were matched in the
document shown in the results list. For example, you want to show which words in the book
name were matched and display that to the user. Do you have to store the documents and
do the matching on the application side? The answer is no. We can force Solr to do that for
us and this recipe will show you how to do that.
How to do it...
1. We begin by creating the following index structure (just add this to your schema.xml
file, to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 8
239
2. Our test data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our user is searching for the word book. To tell Solr that we want
to highlight the matches, we send the following query:
http://localhost:8983/solr/select?q=name:book&hl=true
The response from Solr should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="hl">true</str>
<str name="q">name:book</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
</doc>
</result>
www.it-ebooks.info
Using Additional Solr Functionalities
240
<lst name="highlighting">
<lst name="4">
<arr name="name">
<str>My &lt;em&gt;book&lt;/em&gt; second edition</str>
</arr>
</lst>
</lst>
</response>
As you can see, besides the normal results list we got the highlighting results (the highlighting
results are grouped by the <lst name="highlighting"> XML tag). The word book is
surrounded by the <em> and </em> HTML tags. So everything is working as intended. Now
let's see how it works.
How it works...
As you can see the index structure and the data are really simple, so I'll skip discussing this
part of the recipe. Please note that in order to use the highlighting mechanism, your fields
should be stored and not analysed by aggressive filters (such as stemming). Otherwise the
highlighting results can be misleading to the users. Let's think of a simple example of such
behavior – imagine the user types the word bought in the search but Solr highlighted the
word buy because of the stemming algorithm.
The query is also not complicated. We can see the standard q parameter that passes the
query to Solr. But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
As you can see in the results list, in addition to the standard results, there is a new section
<lst name="highlighting">, which contains the highlighting results. For every
document, in our case the only one found (<lst name="4"> means that the highlighting
result is presented for the document with the unique identifier value of 4), there is a list
of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
You should also remember one other thing: if you are using the standard LuceneQParser
query parser then the default field used for highlighting will be the one set in the schema.
xml file. If you are using DismaxQParser then the default fields used for highlighting are
the ones specified by the qf parameter.
There's more...
There are a few things that can be useful when using the highlighting mechanism.
www.it-ebooks.info
Chapter 8
241
Specifying the fields for highlighting
In many real life situations we want to decide what fields we would want to show the
highlighting for. To do that, you must add an additional parameter – hl.fl with the list
of fields separated by the comma character. For example, if we would like to show the
highlighting for the fields name and description, our query should look as follows:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.
fl=name,description
Changing the default HTML tags that surround the matched word
There are situations where you would like to change the default <em> and </em> HTML tags
to the ones of your choice. To do that you should add the hl.simple.pre and hl.simple.
post parameters. The first one specifies the prefix that will be added in front of the matched
word and the second one specifies the postfix that will be added after the matched word. For
example, if you would like to surround the matched word with the <b> and </b> HTML tags
the query would look like this:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.simple.
pre=<b>&hl.simple.post=</b>
How to highlight long text fields and get
good performance
In certain situations, the standard highlighting mechanism may not be performing as well as
you would like it to be. For example, you may have long text fields and you want the highlighting
mechanism to work with them. This recipe will show you how to do that.
How to do it...
1. We begin the index structure configuration which looks as follows (just add this
to your schema.xml file, to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"
termVectors="true" termPositions="true" termOffsets="true" />
2. The next step is to index the data. We will use the test data which looks like the
following code:
<add>
<doc>
<field name="id">1</field>
www.it-ebooks.info
Using Additional Solr Functionalities
242
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our user is searching for the word book. To tell Solr that we
want to highlight the matches, we send the following query:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.
useFastVectorHighlighter=true
The response from Solr should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">15</int>
<lst name="params">
<str name="hl">true</str>
<str name="q">name:book</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
</doc>
</result>
<lst name="highlighting">
<lst name="4">
<arr name="name">
<str>My &lt;em&gt;book&lt;/em&gt; second edition</str>
</arr>
</lst>
</lst>
</response>
As you can see everything is working as intended. Now let's see how.
www.it-ebooks.info
Chapter 8
243
How it works...
As you can see the index structure and the data are really simple, but there is a difference
between using the standard highlighter and the new FastVectorHighlighting feature.
To be able to use the new highlighting mechanism, you need to store the information about
term vectors, position, and offsets. This is done by adding the following attributes to the
field definition or to the type definition: termVectors="true" termPositions="true"
termOffsets="true".
Please note that in order to use the highlighting mechanism, your fields should be stored and
not analysed by aggressive filters (such as stemming). Otherwise the highlighting results can
be misleading to the users. An example of such a behavior is simple – imagine that the user
types the word bought in the search box but Solr highlighted the word buy because of the
stemming algorithm.
The query is also not complicated. We can see the standard q parameter that passes the
query to Solr. But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
In addition we add the parameter to tell Solr to use the FastVectorHighlighting
feature: hl.useFastVectorHighlighter=true.
As you can see in the results list, in addition to the standard results, there is a new section
called <lst name="highlighting"> that contains the highlighting results. For every
document, in our case the only one found (<lst name="4"> means that the highlighting
result is presented for the document with the unique identifier value of 4), there is a list
of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
Sorting results by a function value
Let's imagine that you have an application that allows the user to search through the
companies that are stored in the index. You would like to add an additional feature to your
application to sort the results on the basis of the distance of a certain geographical point.
Is this possible with Solr? Yes, and this recipe will show you how to do that.
Getting ready
The following recipe uses spatial search. If you are not familiar with geographical search in
Solr please read the Storing geographical points in the index recipe in Chapter 3, Analyzing
Your Text Data.
www.it-ebooks.info
Using Additional Solr Functionalities
244
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="geo" type="location" indexed="true" stored="true" />
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
2. Our test data that we want to index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company one</field>
<field name="geo">10.1,10.1</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company two</field>
<field name="geo">11.1,11.1</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company three</field>
<field name="geo">12.2,12.2</field>
</doc>
</add>
3. In addition to that we also need to define the following field type in the schema.xml
file in the types section:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
4. Let's assume that our hypothetical user searches for the word company and the user
is in the location with the geographical point of(13, 13). So, in order to show the
results of the query and sort them by the distance from the given point, we send the
following query to Solr:
http://localhost:8983/solr/select?q=name:company&sort=geodist(geo,
13,13)+asc
The results list returned by the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
www.it-ebooks.info
Chapter 8
245
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">name:company</str>
<str name="sort">geodist(geo,13,13) asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company three</str>
<str name="geo">12.2,12.2</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company two</str>
<str name="geo">11.1,11.1</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Company one</str>
<str name="geo">10.1,10.1</str>
</doc>
</result>
</response>
As you can see, everything is working as it should be. So now let's see exactly how this works.
How it works...
Let's start from the index structure. We have four fields – one for holding the unique
identifier (the id field), one for holding the name of the company (the name field), and
one field responsible for the geographical location of the company (the geo field). The
last field, the dynamic one, is needed for the location type to work. The data is pretty
simple so let's just skip discussing that.
Besides the standard q parameter responsible for the user query, you can see the sort
parameter. But the sort is a bit different from the ones you are probably used to. It uses the
geodist function to calculate the distance from the given point, and the value returned by
the function is then used to sort the documents in the results list. The first argument of the
geodist function (the geo value) tells Solr which field to use to calculate the distance. The
next two arguments specify the point from which the distance should be calculated. Of course
as with every sort we specify the order in which we want the sort to take place. In our case we
want to sort from the nearest to the furthest company (the asc value).
As you can see in the results, the documents were sorted as they should be.
www.it-ebooks.info
Using Additional Solr Functionalities
246
Searching words by how they sound
One day your boss comes to your office and says "Hey, I want our search engine to be able to
find the same documents when I enter phone or fone into the search box". You tried to say
something, but your boss is already at the other side of the door to your office. So, you wonder
if this kind of functionality is available in Solr. I think you already know the answer – yes it is,
and this recipe will show you how to configure it and use with Solr.
How to do it...
1. We start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="phonetic" indexed="true" stored="true" />
2. Next we define the phonetic type, which looks like the following code (paste it into
the schema.xml file):
<fieldtype name="phonetic" stored="false" indexed="true"
class="solr.TextField" >
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
</analyzer>
</fieldtype>
3. Now we need to index our test data, which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Phone</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Fone</field>
</doc>
</add>
4. Now let's assume that our user wants to find documents that have the word that
sounds like fon. So, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:fon
www.it-ebooks.info
Chapter 8
247
The result list returned by the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:fon</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Phone</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Fone</str>
</doc>
</result>
</response>
So, the filter worked! We got two documents in the results list. Now let's see how it worked.
How it works...
Let's start with the index structure. As you can see we have two fields, the id field responsible
for holding the unique identifier of the product and the name field responsible for holding the
name of the product.
The name field is the one that will be used for phonetic search. For that we defined a new
field type named phonetic. Besides the standard parts (such as class among many
others) we defined a new filter: DoubleMetaphoneFilterFactory. It is responsible
for analysis and checking how the words sound. This filter uses an algorithm named double
metaphone to analyse the phonetics of the words. The additional attribute inject="false"
tells Solr to replace the existing tokens instead of inserting additional ones, which mean that
the original tokens will be replaced by the ones that the filter produces.
As you can see from the query and the data, the fon word was matched to the word phone
and also to the word fone, which means that the algorithm (and thus the filter) works quite
well. But take into consideration that this is only an algorithm, so some words that you think
should be matched will not match.
www.it-ebooks.info
Using Additional Solr Functionalities
248
See also
If you would like to know other phonetic algorithms, please take a look at the Solr Wiki page
that can be found at the following URL address: http://wiki.apache.org/solr/
AnalyzersTokenizersTokenFilters.
Ignoring defined words
Imagine a situation where you would like to filter the words that are considered vulgar from
the data we are indexing. Of course, by accident, such words can be found in your data and
you don't want them to be searchable thus you want to ignore them. Can we do that with Solr?
Of course we can, and this recipe will show you how to do that.
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_ignored" indexed="true"
stored="true" />
2. The second step is to define the text_ignored type, which looks like the following
code:
<fieldType name="text_ignored" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="ignored.txt" enablePositionIncrements="true" />
</analyzer>
</fieldType>
3. Now we create the ignored.txt file, whose contents looks as follows:
vulgar
vulgar2
vulgar3
4. The next step is to index our test data, which looks as follows:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company name</field>
</doc>
</add>
www.it-ebooks.info
Chapter 8
249
5. Now let's assume that our user wants to find the documents that have the words
Company and vulgar. So, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:(Company+AND+vulgar)
In the standard situation there shouldn't be any results because we don't have
a document that matches the two given words. But let's look at what Solr returned
to us as the preceding query's result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:(Company AND vulgar)</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company name</str>
</doc>
</result>
</response>
6. Hmm… it works. To be perfectly sure, let's look at the analysis page found at the
administration interface, as shown in the following screenshot:
As you can see the word vulgar was cut and thus ignored.
www.it-ebooks.info
Using Additional Solr Functionalities
250
How it works...
Let's start with the index structure. As you can see we have two fields, the id field responsible
for holding the unique identifier of the product and the name field responsible for holding the
name of the product.
The name field is the one we will use to mention the ignoring functionalities of Solr –
StopFilterFactory. As you can see the text_ignored type definition is analysed
the same way both in the query and index time. The unusual thing is the new filter –
StopFilterFactory. The words attribute of the filter definition specifies the name of the file,
encoded in UTF-8, which consists of words (a new word at every file line) that should be ignored.
The defined file should be placed in the same directory in which we placed the schema.xml file.
The ignoreCase attribute set to true tells the filter to ignore the case of the tokens and the
words defined in the file. The last attribute, enablePositionIncrements=true, tells Solr to
increment the position of the tokens in the token stream. The enablePositionIncrements
parameter should be set to true if you want to preserve the next token after the discarded one
to increment its position in the token stream.
As you can see in the query, our hypothetical user queried Solr for two words with the logical
operator AND, which means that both words must be present in the document. But, the filter
we added cut the word vulgar and thus the results list consists of the document that has
only one of the words. The same situation occurs when you are indexing your data. The words
defined in the ignored.txt file will not be indexed.
If you look at the provided screenshot from the analysis page of the Solr administration
interface (refer to step 6 of the How to do it... section), you can see that the word vulgar
was cut during the processing of the token stream in the StopFilterFactory filter.
Computing statistics for the search results
Imagine a situation where you want to compute some basic statistics about the documents
in the results list. For example, you have an e-commerce shop where you want to show the
minimum and the maximum price of the documents that were found for a given query. Of course
you could fetch all the documents and count them by yourself, but imagine Solr doing it for you.
Yes, it can! And this recipe will show you how to use that functionality.
How to do it...
1. Let's start with the index structure (just add this to your schema.xml file, to the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 8
251
2. The example data that we index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="price">39.99</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="price">30.11</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Book 3</field>
<field name="price">27.77</field>
</doc>
</add>
3. Let's assume that we want our statistics to be computed for the price field.
To do that, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:book&stats=true&stats.
field=price
The response Solr returned should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:book</str>
<str name="stats">true</str>
<str name="stats.field">price</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<float name="price">39.99</float>
</doc>
<doc>
www.it-ebooks.info
Using Additional Solr Functionalities
252
<str name="id">2</str>
<str name="name">Book 2</str>
<float name="price">30.11</float>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Book 3</str>
<float name="price">27.77</float>
</doc>
</result>
<lst name="stats">
<lst name="stats_fields">
<lst name="price">
<double name="min">27.770000457763672</double>
<double name="max">39.9900016784668</double>
<long name="count">3</long>
<long name="missing">0</long>
<double name="sum">97.87000274658203</double>
<double name="sumOfSquares">3276.9852964233432</double>
<double name="mean">32.62333424886068</double>
<double name="stddev">6.486119174232198</double>
<lst name="facets"/>
</lst>
</lst>
</response>
As you can see, in addition to the standard results list, there was an additional section
available. Now let's see how it worked.
How it works...
The index structure is pretty straightforward. It contains three fields – one for holding the
unique identifier (the id field), one for holding the name (the name field), and one for holding
the price (the price field).
The file that contains the example data is simple, so I'll skip discussing it.
The query is interesting. In addition to the q parameter we have two new parameters. The
first one, stats=true, tells Solr that we want to use StatsComponent – the component
which will calculate the statistics for us. The second parameter, stats.field=price tells
StatsComponent which field to use for the calculation. In our case, we told Solr to use the
price field.
www.it-ebooks.info
Chapter 8
253
Now let's look at the result returned by Solr. As you can see, StatsComponent, added an
additional section to the results. The section contains the statistics generated for the field
that we told Solr we wanted the statistics for. The following statistics are available:
ff min: This is the minimum value that was found in the field, for the documents that
matched the query
ff max: This is the maximum value that was found in the field, for the documents that
matched the query
ff sum: This is the sum of all values in the field, for the documents that matched
the query
ff count: This specifies how many non-null values were found in the field for the
documents that matched the query
ff missing: This specifies the number of documents that matched the query but
didn't have any value in the specified field
ff sumOfSquares: This specifies the sum of all values squared in the field, for the
documents that matched the query
ff mean: This specifies the average for the values in the field, for the documents that
matched the query
ff stddev: This specifies the standard deviation for the values in the field, for the
documents that matched the query
You should also remember that you can specify a number of the stats.field parameters
to calculate the statistics for the different fields in a single query.
Please be careful when using this component on the multivalued fields as it can be a
performance bottleneck.
Checking the user's spelling mistakes
Most modern search sites have some kind of user spelling mistakes correction mechanism.
Some of those sites have a sophisticated mechanism, while others just have a basic one. But
actually that doesn't matter. If all search engines have it then there is a high probability that
your client or boss will want one too. Is there a way to integrate such a functionality into Solr?
Yes there is, and this recipe will show you how to do it.
Getting ready
In this recipe we'll learn how to use the Solr spellchecker component. The detailed information
about setting up the spellchecker component can be found in the Configuring spellchecker to
not use its own index recipe in Chapter 1, Apache Solr Configuration.
www.it-ebooks.info
Using Additional Solr Functionalities
254
How to do it...
1. Let's begin with the index structure (just add this to your schema.xml file, to the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
2. The data that we are going to index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mechanics cookbook</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Other book</field>
</doc>
</add>
3. Our spell checking mechanism will work on the basis of the name field. Now,
let's add the appropriate search component to the solrconfig.xml file:
<searchComponent name="spellcheck" class="solr.
SpellCheckComponent">
<str name="queryAnalyzerFieldType">name</str>
<lst name="spellchecker">
<str name="name">direct</str>
<str name="field">name</str>
<str name="classname">solr.DirectSolrSpellChecker</str>
<str name="buildOnCommit">true</str>
</lst>
</searchComponent>
4. In addition to that we would like to have it integrated into our search handler,
so we make the default search handler definition the same as in the following
code (add this to your solrconfig.xml file):
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">name</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck">on</str>
www.it-ebooks.info
Chapter 8
255
<str name="spellcheck.collate">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
5. Now let's check how it works. To do that we will send a query that contains a spelling
mistake. We will send the words other boak instead of other book. The query
doing that should look like as follows:
http://localhost:8983/solr/spell?q=name:(othar boak)
The Solr response for that query looks like the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
</lst>
<result name="response" numFound="0" start="0">
</result>
<lst name="spellcheck">
<lst name="suggestions">
<lst name="other">
<int name="numFound">1</int>
<int name="startOffset">6</int>
<int name="endOffset">11</int>
<arr name="suggestion">
<str>other</str>
</arr>
</lst>
<lst name="boak">
<int name="numFound">1</int>
<int name="startOffset">12</int>
<int name="endOffset">16</int>
<arr name="suggestion">
<str>book</str>
</arr>
</lst>
<str name="collation">name:(other book)</str>
</lst>
</lst>
</response>
As you can see for the preceding response, Solr corrected the spelling mistake we made.
Now let's see how that happened.
www.it-ebooks.info
Using Additional Solr Functionalities
256
How it works...
The index structure is pretty straightforward. It contains two fields, one for holding the unique
identifier (the id field), one for holding the name (the name field). The file that contains the
example data is simple, so I'll skip discussing it.
The spellchecker component configuration is something we discussed already in the
Configuring spellchecker to not use its own index recipe in the first chapter. So again,
I'll look at only the most important fragments.
As you can see in the configuration, we've defined a spellchecker component that will use
Solr DirectSolrSpellChecker in order to not store its index on the hard disk drive. In
addition to that, we configured it to use the name field for spellchecking and also to use
that field analyzer to process queries. Our /spell handler is configured to automatically
include spellchecking results (<str name="spellcheck">on</str>), to create collation
(<str name="spellcheck.collate">true</str>), and to use direct dictionary (<str
name="spellcheck.dictionary">direct</str>). All those properties were already
discussed in the previously mentioned recipe.
Now let's look at the query. We send the boak and othar words in the query parameter (q).
The spellchecker component will be activated automatically because of the configuration of
our /spell handler, and that's actually all there is to it when it comes to the query.
Finally we come to the results returned by Solr. As you can see there were no documents
found for the word boak and the word other, that's what we actually were expecting.
But as you can see there is a spellchecker component section added to the results list
(the <lst name="spellcheck"> tag). For each word there is a suggestion returned
by Solr (the tag <lst name="boak"> is the suggestion for the word boak). As you can
see, the spellchecker component informed us about the number of suggestions found
(<int name="numFound">), about the start and end offset of the suggestion (<int
name="startOffset">and <int name="endOffset">), and about the actual
suggestions (the <arr name="suggestion"> array). The only suggestion that Solr
returned was the book word (<str>book</str> under the suggestion array). The
same goes for the second word.
There is an additional section in the spellchecker component results generated by the
spellcheck.collate=true parameter, <str name="collation">name:(other
book)</str>. This tells us what query Solr suggested to us. We can either show the query
to the user or send it automatically to Solr and show to the user the corrected results list
and this one is up to you.
www.it-ebooks.info
Chapter 8
257
Using field values to group results
Imagine a situation where your data set is divided into different categories, subcategories,
price ranges, and things like that. What if you would like to not only get information about
counts in such a group (with the use of faceting), but would also like to show the most
relevant documents in each of the groups? Is there a grouping mechanism of some kind
in Solr? Yes there is, and this recipe will show you how to use this functionality in order
to divide documents into groups on the basis of field values.
How to do it...
1. Let's start with the index structure. Let's assume that we have the following fields
in our index (just add this to the schema.xml file to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="price" type="tfloat" indexed="true" stored="true" />
2. The example data, which we are going to index, looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
<field name="category">it</field>
<field name="price">39.99</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mechanics cookbook</field>
<field name="category">mechanics</field>
<field name="price">19.99</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">ElasticSearch book</field>
<field name="category">it</field>
<field name="price">49.99</field>
</doc>
</add>
www.it-ebooks.info
Using Additional Solr Functionalities
258
3. Let's assume that we would like to get our data divided into groups on the basis of
their category. In order to do that we send the following query to Solr:
http://localhost:8983/solr/select?q=*:*&group=true&group.
field=category
The results returned by the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="group.field">category</str>
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="category">
<int name="matches">3</int>
<arr name="groups">
<lst>
<str name="groupValue">it</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<str name="category">it</str>
<float name="price">39.99</float>
</doc>
</result>
</lst>
<lst>
<str name="groupValue">mechanics</str>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Mechanics cookbook</str>
<str name="category">mechanics</str>
<float name="price">19.99</float>
</doc>
</result>
www.it-ebooks.info
Chapter 8
259
</lst>
</arr>
</lst>
</lst>
</response>
As you can see the grouped results are different from the ones returned during a usual
search. But as you can see we got a single document per group which means it worked.
So now let's see how.
How it works...
Our index structure is very simple. It consist four fields – one responsible for the document
identifier (the id field), one used for holding the name of the book (the name field), its
category (the category field), and the last one used to hold the price of the book (the
price field). Our example data is also very simple, but please know that the first and second
book belongs to the same it category and the second book belongs to another category.
Let's look at our query now. We said that we want to have our documents divided on the basis
of contents of the category field. In order to do that, we've added a new parameter called
group, which is set to true. This tells Solr that we want to enable the grouping functionality.
And similar to faceting, we've added a second parameter we are not familiar with. The group.
field parameter is set to the name of the field holding books category, and
that's all we need.
If we look at the results returned by Solr, they are a bit different than the usual results. You
can see the usual response header, however, the resulting groups are returned in the <lst
name="grouped"> tag. The <lst name="category"> tag is generated for each group.
field parameter passed in the query; this time it tells us that the following results will be
for the category field. The <int name="matches">3</int> tag informs us how many
documents were found for our query. This is the same as the numFound value during our
usual query.
Next we have the groups array, which holds the information about the groups that were
created by Solr in the results. Each group is described by the it value, that is, the <str
name="groupValue">it</str> section for the first group, which means that all documents
in that group have the it value in the field used for grouping. In the result tag we can see
the documents returned for the group. By default Solr will return the most relevant document
for each group. I'll skip commenting on the result tag as it is almost identical to the results
Solr returns for a non-grouped query and we are familiar with those, right?
One last thing – you can specify multiple group.field parameters with different fields in a
single query in order to get multiple grouping.
www.it-ebooks.info
Using Additional Solr Functionalities
260
There's more...
There is one more thing about grouping on the basis of field values and I would like to share a
few thoughts about that.
More than a single document in a group
Sometimes you may need to return more than a single document in a group. In order to do
that you will need to use the group.limit parameter and set it to the maximum number of
documents you want to have. For example, if we would like to have 10 documents per group
of results, we would send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.
field=category&group.limit=10
Using queries to group results
Sometimes grouping results on the basis of field values is not enough. For example, imagine
that we would like to group documents in price brackets, that is, we would like to show the
most relevant document for documents with price range of 1.0 to 19.99, a document for
documents with price range of 20.00 to 50.0, and so on. Solr allows us to group results on
the basis of query results. This recipe will show you how to do that.
Getting ready
In this chapter we will use the same index structure and test data as we used in the Using
field values to group results recipe in this chapter. Please read it before continuing.
How to do it…
As we are reusing the data and index structure from the Using field values to group results
recipe, we can start with the query. In order to group our documents on the basis of query
results, we can send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.query=price:
[20.0+TO+50.0]&group.query=price:[1.0+TO+19.99]
The results of the preceding query look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
www.it-ebooks.info
Chapter 8
261
<arr name="group.query">
<str>price:[20.0 TO 50.0]</str>
<str>price:[1.0 TO 19.99]</str>
</arr>
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="price:[20.0 TO 50.0]">
<int name="matches">3</int>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<str name="category">it</str>
<float name="price">39.99</float>
</doc>
</result>
</lst>
<lst name="price:[1.0 TO 19.99]">
<int name="matches">3</int>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Mechanics cookbook</str>
<str name="category">mechanics</str>
<float name="price">19.99</float>
</doc>
</result>
</lst>
</lst>
</response>
So now let's look at how it works.
How it works...
As you can see in the query we told Solr that we want to use the grouping functionality
by using the group=true parameter. In addition to that we specify that we want to have
two groups calculated on the basis of the queries. The first group should contain the
documents that match the following range query price=[20.0+TO+50.00] (the group.
query=price:[1.0+TO+19.99] parameter), and the second group should contain
documents that match the following range query price=[1.0+TO+19.99] (the group.
query=price:[1.0+TO+19.99] parameter).
www.it-ebooks.info
Using Additional Solr Functionalities
262
If you look at the results, they are very similar to the ones for grouping on the basis of field
values. The only difference is in the name of the groups. When using the field values for
grouping, groups were named after the used field names. However, when using queries
to group documents, groups are named as our grouping queries. So in our case, we have
two groups – one named price:[1.0+TO+19.99] (the <lst name="price:[1.0
TO 19.99]"> tag) and a second one named price:[20.0 TO 50.0] (the <lst
name="price:[20.0 TO 50.0]"> tag).
Using function queries to group results
Imagine that you would like to group results not by using queries or field contents, but instead
you would like to use a value returned by a function query. Imagine you could group documents
on the basis of their distance from a point. Sounds good, Solr allows that and in the following
recipe we will see how we can use a simple function query to group results.
Getting ready
In this chapter we will use the same index structure and test data we used in the Sorting
results by a function value recipe in this chapter. We will also use some knowledge that
we gained in the Using field values to group results recipe in this chapter. Please read
them before continuing.
How to do it...
I assume that we would like to have our documents grouped on the basis of the distance
from a given point (in real life we would probably like to have some kind of bracket calculated,
but let's skip that for now).
As we are using the same index structure and test data as we used in the Sorting results by
a function value recipe in this chapter, we'll start with the query. In order to achieve what we
want we send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.
func=geodist(geo,0.0,0.0)
The following results were returned by Solr after running the preceding query:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="group.func">geodist(geo,0.0,0.0)</str>
www.it-ebooks.info
Chapter 8
263
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="geodist(geo,0.0,0.0)">
<int name="matches">3</int>
<arr name="groups">
<lst>
<double name="groupValue">1584.126028923632</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company one</str>
<str name="geo">10.1,10.1</str>
</doc>
</result>
</lst>
<lst>
<double name="groupValue">1740.0195023531824</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Company two</str>
<str name="geo">11.1,11.1</str>
</doc>
</result>
</lst>
<lst>
<double name="groupValue">1911.187477467305</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company three</str>
<str name="geo">12.2,12.2</str>
</doc>
</result>
</lst>
</arr>
</lst>
</lst>
</response>
Everything worked as it should have, so now let's see how it worked.
www.it-ebooks.info
Using Additional Solr Functionalities
264
How it works...
As you can see, the query is very similar to the one we used when grouping our documents on
the basis of field values. So, we again pass the group=true parameter to enable grouping,
but this time in addition to that we pass the group.func parameter with the value, that is,
our function query based on whose results Solr should group our documents.
If you look at the results, they are again very similar to the ones seen in grouping on the basis
of field values. The only difference is in the names of the groups. When using the field values
for grouping, groups were named after the used field names. However, when using function
queries to group documents, groups are named by the result of the function query. So in our
case, we have three groups because our function query returned three different results, as
illustrated in the following list:
ff The group named 1584.126028923632 (the <double name="groupVal
ue">1584.126028923632</double> tag)
ff The group named 1740.0195023531824 (the <double name="groupVal
ue">1740.0195023531824</double> tag)
ff The group named 1911.187477467305 (the <double name="groupVal
ue">1911.187477467305</double> tag)
www.it-ebooks.info
9
Dealing with Problems
In this chapter we will cover:
ff How to deal with too many opened files
ff How to deal with out-of-memory problems
ff How to sort non-English languages properly
ff How to make your index smaller
ff Diagnosing Solr problems
ff How to avoid swapping
Introduction
Every Solr deployment will, sooner or later, have some kind of problem. It doesn't matter
if the deployment is small and simple or if it's a big and complicated deployment containing
multiple servers and shards. In this chapter I'll try to help you with some of the problems you
can run into when running Solr. I hope this will help you and make your task easier.
How to deal with too many opened files
Sometimes you might encounter a strange error, something that lies on the edge between
Lucene and the operating system—the "too many files opened" exception. Is there something
we can do about it? Yes, we can, and this recipe will show you how.
www.it-ebooks.info
Dealing with Problems
266
How to do it...
The following steps show how to deal with too many opened files:
1. So, for the purpose of the recipe let's assume that the header of the exception thrown
by Solr looks like this:
java.io.FileNotFoundException: /use/share/solr/data/index/_1.fdx
(Too many open files)
2. What can you do instead of pulling your hair out? First of all, this probably occurred on
a Unix-/Linux-based operating system. So, let's start with setting the opened files' limit
higher. To do that, you need to edit the /etc/security/limits.conf file of your
operating system and set the following values (I assume Solr is running as solr user):
solr soft nofile 32000
solr hard nofile 32000
3. Now let's add the following line to the .bash_profile file in the solr user home
directory:
ulimit -n 32000
The probable cause of the "too many files opened" exception is the number of files the
index is built of. The more segments the index is built of, the more files will be used.
4. The next thing sometimes worth considering is lowering the mergeFactor
parameter. To make things simple, the lower the mergeFactor setting, the fewer
files will be used to construct the index (please read the How it works... section
that follows, about the dangers of having a very low merge factor). So, let's set
mergeFactor to 2. We modify the following line in the solrconfig.xml file and
set it with the appropriate value (2 in our case):
<mergeFactor>2</mergeFactor>
After we set that configuration value, we need to run the optimization of the index. Now let's
see what the options mean.
How it works...
We don't discuss the operating system's internal working in this book, but in this section we
will make an exception. The mentioned limits.conf file in the /etc/security directory
lets you specify the opened files limit for the users of your system. In the example shown
earlier, we set the two necessary limits to 32000 for the user solr, so if you had problems
with the number of opened files in the default setup you should see the difference after
restarting Solr. However, remember that if you are working as the user and you change
the limits then you may need to log out and log in again to see those changes.
www.it-ebooks.info
Chapter 9
267
Next, we have the mergeFactor parameter. This configuration parameter lets you determine
how often Lucene segments will be merged. The lower the value of mergeFactor, the
smaller the number of index files will be. However, you have to remember that having a small
mergeFactor value will lead to more background merges being done by Lucene, and thus
the indexing speed will be lower compared to the ones with a higher mergeFactor value
and your node's I/O system will be used more extensively. On the other hand, lower values
of mergeFactor will speed up searching.
How to deal with out-of-memory problems
As with every application written in Java, sometimes memory problems happen. When talking
about Solr, those problems are usually related to heap size. They usually happen when the
heap size is too low. This recipe will show you how to deal with those problems and what to
do to avoid them.
How to do it...
Let's consider what to do when we see an exception like this:
SEVERE: java.lang.OutOfMemoryError: Java heap space
Firstly, you can do something to make your task easier. You can add more memory that the
Java virtual machine can use if you have some free physical memory available in your system.
To do that, you need to add the Xmx and, preferably, the Xms parameter to the start-up
script of your servlet container (Apache Tomcat or Jetty). To do that, I used the default
Solr deployment and modified the parameters. This is how Solr was run with more than
the default heap size:
java –Xmx1024M –Xms512m –jar start.jar
How it works...
So what do the Xmx and Xms Java virtual machine parameters do? The Xms parameter
specifies how much heap memory should be assigned by the virtual machine at the start and
thus this is the minimal size of the heap memory that will be assigned by the virtual machine.
The Xmx parameter specifies the maximum size of the heap. The Java virtual machine will not
be able to assign more memory for the heap than the Xmx parameter.
You should remember one thing—sometimes it's good to set the Xmx and Xms parameters to
the same values. It will ensure that the virtual machine won't be resizing the heap size during
application execution and thus won't lose precious time in heap resizing.
One additional thing—be careful when setting the heap size to be too big. It is usually not
advised to give the heap size more than 60 percent of your total memory available in the
system, because your operating system's I/O cache will suffer.
www.it-ebooks.info
Dealing with Problems
268
There's more...
There are a few more things I would like to discuss when it comes to memory issues.
Monitoring heap when an out-of-memory error occurs
If the out-of-memory errors occurs even after the actions you've done, you should start
monitoring your heap. One of the easiest ways to do that is to add the appropriate Java
virtual machine parameters. Those parameters are XX:+HeapDumpOnOutOfMemory and
XX:HeapDumpPath. Those two parameters tell the virtual machine to dump the heap on the
out-of-memory error and write it to a file created in the specified directory. So the default Solr
deployment's start command would look like this:
java –jar –XX:+HeapDumpOnOutOfMemoryError –XX:HeapDumpPath=/var/log/dump/
start.jar
Reducing the amount of memory needed by Solr
However there are times (even if your system has a large amount of memory available), when
you may be forced to think about Solr memory consumption reduction. In such cases there is
no general advice, but these are a few things that you can keep in mind:
ff Look at your queries and consider how they are built
ff How you use the faceting mechanism and so on (facet.method=fc tends to use
less memory when the field has many unique terms in the index)
ff Remember that fetching too many documents at a time may cause Solr to run out of
heap memory (for example, when setting a large value for the query result window)
ff Reduce the number of calculated faceting results (facet.limit parameter)
ff Check the memory usage of your caches—this can also be one of the reasons for
the problems with memory
ff If you don't need to use the normalization factor for text fields, you can set
omitNorms="true" for such fields and save some additional memory too
ff Remember that grouping mechanisms requires memory; for big result sets and
high numbers of groups, a vast amount of memory may be needed
How to sort non-English languages properly
As you probably already know, Solr supports UTF-8 encoding and thus can handle data
in many languages. But, if you ever needed to sort some languages that have characters
specific to them you probably know that it doesn't work well on a standard Solr string
type. This recipe will show you how to deal with sorting in Solr.
www.it-ebooks.info
Chapter 9
269
How to do it...
These steps tell us how to sort non-English languages properly:
1. For the purpose of this recipe, I have assumed that we will have to sort text that
contains Polish characters. To show the good and bad sorting behaviour we need
to create the following index structure (add this to your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="name_sort_bad" type="string" indexed="true"
stored="true" />
<field name="name_sort_good" type="text_sort" indexed="true"
stored="true" />
2. Now let's define some copy fields to automatically fill the name_sort_bad and
name_sort_good fields. Here is how they are defined (add this after the fields
section in the schema.xml file):
<copyField source="name" dest="name_sort_bad" />
<copyField source="name" dest="name_sort_good" />
3. The last thing about the schema.xml file is the new type. So the text_sort
definition looks like this:
<fieldType name="text_sort" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory" />
<filter class="solr.CollationKeyFilterFactory" language="pl"
country="PL" strength="primary" />
</analyzer>
</fieldType>
4. The test we need to index looks like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Laka</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Lalka</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Zab</field>
</doc>
</add>
www.it-ebooks.info
Dealing with Problems
270
5. First, let's take a look at how the incorrect sorting order looks. To do this, we send the
following query to Solr:
http://localhost:8983/solr/select?q=*:*&sort=name_sort_bad+asc
And now the response that was returned for the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">*:*</str>
<str name="sort">name_sort_bad asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lalka</str>
<str name="name_sort_bad">Lalka</str>
<str name="name_sort_good">Lalka</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Zab</str>
<str name="name_sort_bad">Zab</str>
<str name="name_sort_good">Zab</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Laka</str>
<str name="name_sort_bad">Laka</str>
<str name="name_sort_good">Laka</str>
</doc>
</result>
</response>
6. Now let's send the query that should return the documents sorted in the correct
order. The query looks like this:
http://localhost:8983/solr/select?q=*:*&sort=name_sort_good+asc
www.it-ebooks.info
Chapter 9
271
And the results returned by Solr are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">6</int>
<lst name="params">
<str name="q">*:*</str>
<str name="sort">name_sort_good asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lalka</str>
<str name="name_sort_bad">Lalka</str>
<str name="name_sort_good">Lalka</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Laka</str>
<str name="name_sort_bad">Laka</str>
<str name="name_sort_good">Laka</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Zab</str>
<str name="name_sort_bad">Zab</str>
<str name="name_sort_good">Zab</str>
</doc>
</result>
</response>
As you can see the order is different and believe me it's correct. Now let's see how it works.
How it works...
Every document in the index is built on four fields. The id field is responsible for holding the
unique identifier of the document. The name field is responsible for holding the name of the
document. The last two fields are used for sorting.
www.it-ebooks.info
Dealing with Problems
272
The name_sort_bad field is nothing new; it's just a field based on string, which
is used to perform sorting. The name_sort_good field is based on a new type, the
text_sort field type. The field is based on the solr.TextField type and on solr.
KeywordTokenizerFactory, which basically means that our text won't be tokenized. We
used this trick because we want to sort on that field and thus we don't want the data in it to
be tokenized, but we want to use a special filter on that field. The filter that allows Solr to sort
correctly is the solr.CollationKeyFilterFactory filter. We used three attributes of
this filter. First, the language attribute, which tells Solr about the language of the field. The
second attribute is country which tells Solr about the country variant (this can be skipped
if necessary). The strength attribute informs Solr about the collation strength used. More
information about those parameters can be found in the JDK documentation. One thing that is
crucial is that you need to create an appropriate field and set the appropriate attribute's value
for every non-English language you want to sort on.
The two queries you can see in the examples differ in only one thing, the field used for sorting.
The first query uses the string-based field, name_sort_bad. When sorting on this field, the
document order will be incorrect when there are non-English characters present. However,
when sorting on the name_sort_good field everything will be in the correct order as shown
in the example.
How to make your index smaller
There may be situations where you would like to make your index smaller. The reasons may be
different—you may want to have a smaller index so that it would fit into the operating system's
I/O cache or you want to store your index in RAMDirectory. This recipe will try to help you
with the process of index slimming.
How to do it...
The following steps tell us how to make your index smaller:
1. For the purpose of this recipe, I assumed that we will have four fields that describe the
document. I created the following index structure (add this to your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="description" type="text" indexed="true" stored="true"
/>
<field name="price" type="string" indexed="true" stored="true" />
Let's assume that our application has the following requirements:
?? We need to search on name and description fields
?? We need to show two fields in the results: id and price
?? We don't use highlighting and spellchecker
www.it-ebooks.info
Chapter 9
273
2. So the first thing we should do is set the stored="false" attribute for the name
and description fields.
3. Next, we set the indexed="false" attribute for the price field.
4. Now, the last thing to do is add the term options. We add the
termVectors="false", termPositions="false", and
termOffsets="false" attributes to the name and description fields.
The modified schema looks like this:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="false"
termVectors="false" termPositions="false" termOffsets="false"/>
<field name="description" type="text" indexed="true"
stored="false" termVectors="false" termPositions="false"
termOffsets="false"/>
<field name="price" type="string" indexed="false" stored="true" />
Let's check the index size now. I've indexed 1,000,000 sample documents with the use
of the original schema.xml file. The index size was 329,237,331 bytes. After changing
the schema.xml file and indexing the same data the index size was 84,301,603 bytes.
So as you can see, the index size was reduced.
Now let's see why we see this reduction in the index size.
How it works...
The first schema.xml file you see is the standard index structure provided with Solr example
deployment, at least when talking about the types. We have four fields, all of them indexed
and stored, which means all of them are searchable and are shown in the result list.
Now let's look at the requirements. First of all we only need to search on the name and
description fields, which mean that the rest of the fields can be set up as not indexed
(indexed="false" attribute). We set that for the price field, while we set the id field to be
searchable, as we need that to avoid duplicates. When the indexed attribute is set to false,
the information in that field is not indexed which basically means that it isn't written into the
Lucene-inverted index and thus it is not available; this saves index space. Of course you can't
set this attribute to false if you need this field to be searchable.
The second requirement tells us what fields we are obligated to show in the search results.
Those field are the ones that need the stored attribute set to true, and the rest can have
this attribute set to false. When we set this attribute to false, we tell Solr that we don't
want to store the original value—the one before analysis—thus we don't want this field to be
included in the search results. Setting this attribute to true on many fields will increase
the index size substantially.
www.it-ebooks.info
Dealing with Problems
274
The last requirement is actually information; we don't need to worry about highlighting
functionality so we can reduce the index size in a greater way. To do that we add the
termVectors="false", termPositions="false", and termOffsets="false"
attributes to the name and description fields. By doing that we tell Solr not to store
any information about terms in the index. This basically means that we can't use the
highlighting functionalities of Solr, but we have reduced our index size substantially
and we don't need highlighting.
If we don't need index time boosting and we do not care about length normalization, we could
also turn on the omitting of that factor (the omitNorms="true" attribute) for the fields
based on the text type (for primitive types such as string, integer, and so on it's turned
on by default in Solr 4.0). This would shrink the index a bit more and in addition to that save
us some memory during queries.
Last few words. Every time you think about reducing the index size, first do the optimization,
then look at your schema.xml file and see if you need all those fields. Then check which
fields shouldn't be stored and which you can omit when indexing. The last thing should
be removing the information about terms, because there may come a time when you will
need this information and the only thing you will be able to do is a full indexation of millions
of documents.
There's more...
There is one additional thing I would like to mention.
Estimating your index size and memory usage
Sometimes it's necessary to have a rough estimate of the index size and the memory
usage of your Solr instance. Currently there is a draft of the Microsoft Excel spreadsheet
that lets you do that kind of estimation. If you are interested in it, download the following
file: http://svn.apache.org/repos/asf/lucene/dev/trunk/dev-tools/sizeestimator-
lucene-solr.xls.
Diagnosing Solr problems
There are many tools out there that can help you diagnose problems with Solr. You can
monitor your operating system by yourself by using different operating system commands
such as vmstat, dstat, and iostat. You can use different Java tools such as jconsole
and jvisualvm to look at the JMX mbeans, you can monitor your garbage collector work,
and so on. However in order to properly diagnose what's happening with your Apache Solr
cluster you'll need to see the whole view as well as the specifics. There are different tools
out there that you can use, however this recipe will show you what you can find in one of
them—Scalable Performance Monitoring.
www.it-ebooks.info
Chapter 9
275
Getting ready
This recipe assumes that you have Scalable Performance Monitoring installed and running. If
you don't, please go to http://sematext.com/spm/index.html, create a free account,
and download the client that's suitable for you. The installation is very simple and you'll be
guided by the Scalable Performance Monitoring installer from the beginning to the end.
How to do it...
1. Let's assume that we want to check our Solr instance health by looking at the GUI of
Scalable Performance Monitoring. After logging we would get the following view:
This is an overview of the system, however we would like to see some details.
www.it-ebooks.info
Dealing with Problems
276
2. Let's start with the information about indices.
3. Now let's have a look at the cache usage:
4. By now we know what our index and Solr caches' usage looks like and we know if we
need to tune them or not, so now let's look at the query rate and its latency:
www.it-ebooks.info
Chapter 9
277
5. Here we can see the warm-up queries' time and execution:
We've got all the information that is connected to queries, so now we can go and see
the other crucial information such as memory and CPU usage, Java heap usage, and
how Java garbage collector works.
6. Let's start with the memory and CPU usage:
www.it-ebooks.info
Dealing with Problems
278
7. And now we can see the JVM heap statistics:
8. And finally we can see how the garbage collector works:
That's all we need during the usual work when we want to see how different parts of Solr work.
If we would like to go in depth and see how the I/O subsystem works or the swap usage we can
use other aggregated reports available in any of the monitoring systems, or you could just use
the appropriate system commands like the ones mentioned in the introduction to the recipe.
How it works...
Let's discuss the provided statistics in a bit more dtail. On the first screenshot provided you
can see the overview of the system. This part of Scalable Performance Monitoring will be
shown to you as soon as you log in to the system. You'll usually use it to get the whole idea
about the system, but you'll want to look at the detailed reports in order to see a higher
granularity of your data.
On the second screenshot you can see the index statistics (or indices depending on the
options you've chosen). You can see the information about the number of documents in
the index, the maximum size of the index, the number of segments, and the delta, which is
calculated as the maximum number of documents minus the current number of documents.
Not shown on the screenshot are the filesystem statistics which tell you about the size of the
index on the disk. With the use of this data you can see the complete information about your
core's or collection's disk drive usage.
www.it-ebooks.info
Chapter 9
279
The third screenshot is one of the most important ones—the information about Apache Solr
caches. Although I haven't shown all the information here, you can see a single cache on the
screenshot—the query result cache (we didn't show the document cache and the filter cache).
You can see information about the size of the cache, the maximum size of the cache, the
number of evictions, and so on. Remember, if your cache is too low, its size will be equal
to the maximum size and you'll start seeing evictions, which is not good and you'll probably
want to change the cache configuration.
The query rate and latency report shown in the fourth screenshot provides information about
the number of queries and their average latency. You can see how your queries were executed
and if you need to start thinking about the optimization of your system.
In order to check how your warm-up queries were executed you can look at the fifth of
the provided screenshots. You can see the amount of time for which your warm-up queries
were executed and how long it took to auto-warm your query result cache and your filter
cache. This information can be valuable when dealing with problems such as Solr hanging
during the opening of new or first searches.
The last three screenshots provide the information that is not directly connected to Apache
Solr, but very valuable from our point of view, when we have to see what is happening with
our Solr instance. So let's discuss them.
The sixth screenshot shows information about the CPU and memory usage. For the CPU
information you can see how it works; the percent of time spent idling, working on user-run
software, working with operating system software, handling interruptions, and so on. If you
look at the memory graph you will find the total, used, free, cached, and buffered statistics.
That's basically all you need in order to see if your CPU is 100 percent utilized and how your
system memory is utilized. This is crucial when your system is not working in the way that you
would like it to.
The seventh screenshot provides information about the Java virtual machine. You can see the
heap memory statistics and the threading information (which is not shown in the screenshot).
The heap usage graph allows us to see if the amount of memory we specified for our Solr
instance is enough to handle all the operations that need to be done.
The final screenshot provides information about how your JVM garbage collector works.
In most situations you will want it to run more frequently, but for a shorter period of time
stop the world events which may cause your Solr instances to stop handling queries or
indexing for a short period of time.
To sum up, all the information can be gathered manually by using different system and Java
tools. The crucial part of every monitoring system is the ability to show you graphs that will let
you point to a certain event in time. We looked at a single monitoring solution, but there are
many more available and if you don't like Scalable Performance Monitoring you can use any
available. One more thing; please remember that we only scraped the surface in this recipe
and the book (or e-book) you are holding in your hands doesn't describe all the information
regarding monitoring and dealing with problems. However I hope that this recipe will help you
at least start with this topic.
www.it-ebooks.info
Dealing with Problems
280
If you don't want to use Scalable Performance Monitoring, you can choose some other
technology that is available like Ganglia (http://ganglia.sourceforge.net/), Mumin
(http://munin-monitoring.org/), Zabix (http://www.zabbix.com/), Cacti (http://
www.cacti.net/), or any commercial ones like New Relic (http://newrelic.com/).
How to avoid swapping
One of the crucial things when running your Solr instance in production is performance. What
you want is to give your clients relevant results in the blink of an eye. If your clients have to
wait for results for too long, some of them may choose other vendors or sites that provide
similar services. One of the things to remember when running a Java application such as
Apache Solr is to ensure that the operating system won't write the heap to disk. This ensures
that the part of the memory used by Solr won't be swapped at all. This recipe will show you
how to achieve that on a Linux operating system.
Getting ready
Please note that the following recipe is only valid when running Apache Solr on a Linux
operating system. In addition to that, please be advised that turning off swapping should
only be done when you have enough memory to handle all the necessary application in
your system and you want to be sure that there won't be any swapping.
How to do it...
1. Before turning off swapping let's look at the amount of swap memory used by
our operating system. In order to do that let's look at the main page of the Solr
administration panel:
www.it-ebooks.info
Chapter 9
281
2. As you can see some swap memory is being used. In order to demonstrate how to
turn off swap usage I've freed some memory on the virtual machine I was using for
tests and after that I've run the following commands:
sudo sysctl -w vm.swappiness=0
sudo /sbin/swapoff -a
3. After the second command is done running, refresh the main page of the Solr admin
instance and this is what it will show:
4. It seems like it is working, but in order to be sure I've run the following command:
free -m
And the response of it was:
total used free shared buffers cached
Mem: 3001 2326 675 0 3 97
-/+ buffers/cache: 2226 775
Swap: 0 0 0
And again we can see that there is no swap usage. Now let's see how this works.
www.it-ebooks.info
Dealing with Problems
282
How it works...
On the first provided screenshot you can see that there is a bit more than 183 MB of
swap memory being used. This is not good; in a production environment you want to avoid
swapping, of course, if you have the necessary amount of memory. Swapping will make the
contents of the memory to be written onto the hard disk drive, thus making your operating
system and applications execute slower. This can also affect Solr.
So, in order to turn off swapping in a Linux operating system, we've run two commands. The
first one sets the vm.swappiness operating system property to 0, which means that we want
to avoid swapping. We needed to use sudo, because in order to set that property with the
use of the sysctl command we need administration privileges. The second command (the /
sbin/swapoff -a one) disables swapping on all known devices.
As you can see on the second screenshot, the Solr administration panel didn't even include
the swapping information so we may suspect that it was turned off. However in order to be
sure, we've used another Linux command, the free command with the -m switch, in order to
see the memory usage on our system. As you can see, the Swap section shows 0, so we can
now be sure that swapping was turned off.
www.it-ebooks.info
Real-life Situations
In this chapter we will cover:
ff How to implement a product's autocomplete functionality
ff How to implement a category's autocomplete functionality
ff How to use different query parsers in a single query
ff How to get documents right after they were sent for indexation
ff How to search your data in a near real-time manner
ff How to get documents with all the query words at the top of the results set
ff How to boost documents based on their publication date
Introduction
In the previous nine chapters, we discussed about the different Apache Solr functionalities
and how to overcome some common problems and situations. However, I decided that we
will describe a few of the most common problems that arise on the Apache Solr mailing list
and during our work with our clients. This chapter is dedicated to describing how to handle
such situations, and I hope that you'll find it useful.
www.it-ebooks.info
Real-life Situations
284
How to implement a product's autocomplete
functionality
The autocomplete functionality is very popular now. You can find it in most e-commerce sites,
on Google, Bing, and so on. It enables your users or clients to find what they want and do it fast.
In most cases, the autocomplete functionality also increases the relevance of your search by
pointing to the right author, title, and so on, right away without looking at the search results.
What's more, sites that use autocomplete report higher revenue after deploying it in comparison
to the situation before implementing it. Seems like a win-win situation, both for you and your
clients. So, let's look at how we can implement a product's autocomplete functionality in Solr.
How to do it...
Let's assume that we want to show the full product name whenever our users enter a part
of the word that the product name is made up of. In addition to this, we want to show the
number of documents with the same names.
1. Let's start with an example data that is going to be indexed:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
</doc>
</add>
2. We will need two main fields in the index – one for the document identifier and one
for the name. We will need two additional fields – one for autocomplete and one for
faceting that we will use. So, our index structure will look similar to the following code
snippet (we should add it to the schema.xml fields section):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="name_autocomplete" type="text_autocomplete"
indexed="true" stored="false" />
<field name="name_show" type="string" indexed="true"
stored="false" />
www.it-ebooks.info
Appendix
285
3. In addition to this, we want Solr to automatically copy the data from the name field to
the name_autocomplete and name_show fields. So, we should add the following
copy fields section to the schema.xml file:
<copyField source="name" dest="name_autocomplete"/>
<copyField source="name" dest="name_show"/>
4. Now, the final thing about the schema.xml file — that is, the text_autocomplete
field type — it should look similar to the following code snippet (place it in the types
section of the schema.xml file):
<fieldType name="text_autocomplete"
class="solr.TextField" positionIncrementGap="100">
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.EdgeNGramFilterFactory"
minGramSize="1" maxGramSize="25" />
</analyzer>
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
5. That's all. Now, if we would like to show all the products that start with the word sol
to our users, we would send the following query:
curl 'http://localhost:8983/solr/select?q=name_autocomplete:sol&q.
op=AND&rows=0&&facet=true&facet.field=name_show&facet.
mincount=1&facet.limit=5'
The response returned by Solr would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="fl">name</str>
<str name="facet.mincount">1</str>
<str name="q">name_autocomplete:sol</str>
<str name="facet.limit">5</str>
<str name="q.op">AND</str>
<str name="facet.field">name_show</str>
www.it-ebooks.info
Real-life Situations
286
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="name_show">
<int name="First Solr 4.0 CookBook">1</int>
<int name="Second Solr 4.0 CookBook">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, the faceting results returned by Solr are exactly what we were looking
for. So now, let's see how it works.
How it works...
Our example documents are pretty simple – they are only built of an identifier and a name
that we will use to make autocomplete. The index structure is where things are getting
interesting. The first two fields are the ones that you would have expected – they are used
to hold the identifier of the document and its name. However, we have two additional fields
available; the name_autocomplete field that will be used for querying and name_show that
will be used for faceting. The name_show field is based on a string type, because we want to
have a single token per name when using faceting.
With the use of the copy field sections, we can let Solr automatically copy the values of the
fields defined by the source attribute to the field defined by the dest field. Copying is done
before any analysis.
The name_autocomplete field is based on the text_autocomplete field type, which is
defined differently for indexing and querying. During query time, we divide the entered query
on the basis of white space characters using solr.WhitespaceTokenizerFactory, and
we lowercase the tokens with the use of solr.LowerCaseFilterFactory. For query time,
this is what we want because we don't want any more processing. For index time, we not only
use the same tokenizer and filter, but also solr.NGramFilterFactory. This is because
we want to allow our users to efficiently search for prefixes, so that when someone enters the
word sol, we would like to show all the products that have a word starting with that prefix,
and solr.NGramFilterFactory allows us to do that. For the word solr, it will produce
the tokens s, so, sol, and solr.
www.it-ebooks.info
Appendix
287
We've also said that we are interested in grams starting from a single character (the
minGramsSize property) and the maximum size of grams allowed is 25 (the maxGramSize
property).
Now comes the query. As you can see, we've sent the prefix of the word that the users have
entered to the name_autocomplete field (q=name_autocomplete:sol). In addition to
this, we've also said that we want words in our query to be connected with the logical AND
operator (the q.op parameter), and that we are not interested in the search results (the
rows=0 parameter). As we said, we will use faceting for our autocomplete functionality,
because we need the information about the number of documents with the same titles, so
we've turned faceting on (the facet=true parameter). We said that we want to calculate
the faceting on our name_show field (the facet.field=name_show parameter). We are
also only interested in faceting a calculation for the values that have at least one document
in them (facet.mincount=1), and we want the top five results (facet.limit=5).
As you can see, we've got two distinct values in the faceting results; both with a single
document with the same title, which matches our sample data.
How to implement a category's
autocomplete functionality
Sometimes we are not just interested in our product's name for autocomplete. Imagine that
we want to show the category of our products in the autocomplete box along with the number
of products in each category. Let's see how we can use faceting
to do that.
How to do it...
This recipe will show how we can implement a category's autocomplete functionality.
1. Let's start with the example data, which is going to be indexed and which looks
similar to the following code snippet:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
<field name="category">Books</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
<field name="category">Books And Tutorials</field>
</doc>
</add>
www.it-ebooks.info
Real-life Situations
288
2. The fields section of the schema.xml configuration file that can handle the
preceding data should look similar to the following code snippet:
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="category" type="text_lowercase"
indexed="true" stored="true" />
3. One final thing is the text_lowercase type definition, which should be placed in
the types section of the schema.xml file. It should look similar to the following
code snippet:
<fieldType name="text_lowercase" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
4. So now, if we would like to get all the categories that start with boo, along with the
number of products in those categories, we would send the following query:
curl 'http://localhost:8983/solr/select?q=*:*&rows=0&facet=tr
ue&facet.field=category&facet.mincount=1&facet.limit=5&facet.
prefix=boo'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.mincount">1</str>
<str name="indent">true</str>
<str name="q">*:* </str>
<str name="facet.limit">5</str>
<str name="facet.prefix">boo</str>
<str name="facet.field">category</str>
<str name="rows">0</str>
www.it-ebooks.info
Appendix
289
</lst>
</lst>
<result name="response" numFound="2" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="category">
<int name="books">1</int>
<int name="books and tutorials">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, we have two categories, each containing a single product. So this is
what matches our example data. Let's now see how it works.
How it works...
Our data is very simple. We have three fields for each of our documents – one for the
identifier fields, one for holding the name of the document, and one for its category.
We will use the category field to do the autocomplete functionality, and we will use
faceting for it.
If you look at the index structure, for the category field, we use a special type – the text_
lowercase one. What it does is that it stores the category as a single token in the index
because of solr.KeywordTokenizerFactory. We also lowercase with the appropriate
filter. This is because we want to send the lowercased queries while using faceting.
The query is quite simple – we query for all the documents (q=*:* parameter), and
we don't want any results returned (the rows=0 parameter). We will use faceting for
autocomplete, so we turn it on (facet=true) and we specify the category field to calculate
the faceting (facet.field=category). We are also only interested in faceting a calculation
for the values that have at least one document in them (facet.mincount=1), and we want
the top five results (facet.limit=5). One of of the most important parameters in the query
is facet.prefix – using it we can return on those results in faceting that start with the
prefix defined by the mentioned parameter, which can be seen in the results.
www.it-ebooks.info
Real-life Situations
290
How to use different query parsers in a
single query
Sometimes, it is good to be able to choose different query parsers in the same query.
For example, imagine that you would like to use the Extended DisMax query parser for
the main query, but in addition to this, we would like to use the field query parser for
filter queries. This recipe will show you how to do it.
How to do it...
This recipe will show how we can use different query parsers in a single query.
1. Let's start with the following index structure (this should go to the field section
in the schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="category" type="string" indexed="true"
stored="true" />
2. Now, let's index the following data:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
<field name="category">Books</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
<field name="category">Books And Tutorials</field>
</doc>
</add>
3. So, if we search for all the documents using the Extended DisMax query parser and
want to narrow our results to the Books And Tutorials category, then we can send
the following query:
curl 'http://localhost:8983/solr/select?q=*:*&defType=edismax&fq={
!term f=category}Books And Tutorials'
www.it-ebooks.info
Appendix
291
The results returned by Solr would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">
{!term f=category}Books And Tutorials
</str>
<str name="q">*:*</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Second Solr 4.0 CookBook</str>
<str name="category">Books And Tutorials</str>
</doc>
</result>
</response>
As you can see, we got what we expected. So let's see how it works.
How it works...
Our index structure and example data are not that relevant for this recipe, so I'll skip
discussing them.
What we want to achieve is be sure that the data we filter will be properly processed, and
we want to avoid thinking about any kind of query parsing and Lucene special characters
escaping. In order to do this, we use the term query parser. To inform Solr that we want to
use this query parser in the filter query (the fq parameter), we use local parameter syntax
and send this filter query: {!term f=category}Books And Tutorials. The !term
part of the filter query says which query parser we want to use, and the f property specifies
the field to which we want to send the provided Books And Tutorials value.
That's all; as you can see in the provided results, everything works as intended.
www.it-ebooks.info
Real-life Situations
292
How to get documents right after they were
sent for indexation
Let's say that we would like to get our documents as soon as they were sent for indexing, but
without any commit (both hard and soft) operation occurring. Solr 4.0 comes with a special
functionality called real-time get, which uses the information of uncommitted documents
and can return them as documents. Let's see how we can use it.
How to do it...
This recipe will show how we can get documents right after they were sent for indexation.
1. Let's begin with defining the following index structure (add it to the field section
in your schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
2. In addition to this, we need the _version_ field to be present, so let's also add
the following field to our schema.xml file in its field section:
<field name="_version_" type="long" indexed="true"
stored="true"/>
3. The third step is to turn on the transaction log functionality in Solr. In order to do
this, we should add the following section to the updateHandler configuration
section (in the solrconfig.xml file):
<updateLog>
<str name="dir">${solr.data.dir:}</str>
</updateLog>
4. The last thing we need to do is add a proper request handler configuration to our
solrconfig.xml file:
<requestHandler name="/get"
class="solr.RealTimeGetHandler">
<lst name="defaults">
<str name="omitHeader">true</str>
<str name="indent">true</str>
<str name="wt">xml</str>
</lst>
</requestHandler>
www.it-ebooks.info
Appendix
293
5. Now, we can test how the handler works. In order to do this, let's index the following
document (which we've stored in the data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 4.0 CookBook</field>
</doc>
</add>
6. In order to index it, we use the following command:
curl 'http://localhost:8983/solr/update' --data-binary @data.xml
-H 'Content-type:application/xml'
7. Now, let's try two things. First, let's search for the document we've just added.
In order to do this, we run the following query:
curl 'http://localhost:8983/solr/select?q=id:1'
8. As you can imagine, we didn't get any documents returned, because we didn't
send any commit command – not even the soft commit one. So now, let's use
our defined handler:
curl 'http://localhost:8983/solr/get?id=1'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<doc name="doc">
<str name="id">1</str>
<str name="name">Solr 4.0 CookBook</str>
<long name="_version_">1418467767663722496</long>
</doc>
</response>
As you can see, our document is returned by our get handler. Let's see how it
works now.
How it works...
Our index structure is simple, and there is only one relevant piece of information there
– the _version_ field. The real-time get functionality needs that field to be present in
our documents, because the transaction log relies on it. However, as you can see in the
provided example data, we don't need to worry about this field, because its filled and
updated automatically by Solr.
www.it-ebooks.info
Real-life Situations
294
But let's backtrack a bit and discuss the changes made to the solrconfig.xml file.
There are two things there. The first one is the update log (the updateLog section),
which Solr uses to store the so-called transaction log. Solr stores recent index changes
there (until hard commit), in order to provide write durability, consistency, and the ability
to provide the real-time get functionality.
The second thing is the handler we defined under the name of /get with the use of the
solr.RealTimeGetHandler class. It uses the information in the transaction log to get
the documents we want by using their identifier. It can even retrieve the documents that
weren't committed and are only stored in the transaction log. So, if we want to get the
newest version of the document, we can use it. The other configuration parameters are
the same as with the usual request handler, so I'll skip commenting them.
The next thing we do is send the update command without adding the commit command,
so that we shouldn't be able to see the document during a standard search. If you look at the
results returned by the first query, you'll notice that we didn't get that document. However, when
using the /get handler that we previously defined, we get the document we requested. This is
because Solr uses the transaction log in order to even the uncommitted document.
How to search your data in a near real-time
manner
Sometimes, we need our data to be available as soon as possible. Imagine that we have a
SolrCloud cluster up and running, and we want to have our documents available for searching
with only a slight delay. For example, our application can be a content management system
where it would be very weird if a user adds a new document, and it would take some time for
it to be searchable. In order to achieve this, Solr exposes the soft commit functionality, and
this recipe will show you how to set it up.
How to do it...
This recipe will show how we can search for data in a near real-time manner.
1. For the purpose of this recipe, let's assume that we have the following index
structure (add it to the field section in your schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
2. In addition to this, we need to set up the hard and soft automatic commits,
for which we will need to add the following section to the updateHandler
section in the solrconfig.xml file:
www.it-ebooks.info
Appendix
295
<autoCommit>
<maxTime>60000</maxTime>
<openSearcher>false</openSearcher>
</autoCommit>
<autoSoftCommit>
<maxTime>1000</maxTime>
</autoSoftCommit>
3. Let's test if that works. In order to do this, let's index the following document
(which we've stored in the data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 4.0 CookBook</field>
</doc>
</add>
4. In order to index it, we use the following command:
curl 'http://localhost:8983/solr/update' --data-binary @data.xml
-H 'Content-type:application/xml'
5. We didn't send any commit command, so we shouldn't see any documents, right?
I think there will be one available – the one we've just send for indexation. But, let's
check that out by running the following simple search command:
curl 'http://localhost:8983/solr/select?q=id:1'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">id:1</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr 4.0 CookBook</str>
</doc>
</result>
</response>
As you can see, our document was returned. So, let's see how it works.
www.it-ebooks.info
Real-life Situations
296
How it works...
As you may know, the standard commit operation is quite resource-intensive – it flushes the
changes since the last commit to the disk to the new segment. If you would like to do that every
second, we could run into a problem of a very high amount of I/O writes and thus our searches
would suffer (of course, this depends on the situation). That's why, with Lucene and Solr 4.0,
the new commit type was introduced – the soft commit, which doesn't flush the changes to
disk, but just reopens the searcher object and allows us to search the data that is stored in
the memory.
As we are usually lazy and don't want to remember when it's time to send the commit and
when to use soft commit, we'll let Solr manage that so we properly need to configure the
update handler. First, we add the standard auto commit by adding the autoCommit section
and saying that we want to commit after every 60 seconds (the maxTime property is specified
in milliseconds), and that we don't want to reopen the searcher after the standard commit
(the openSearcher property is set to false).
The next thing is to configure the soft auto commit functionality by adding the softAutoCommit
section to the update handler configuration. We've specified that we want the soft commit to be
fired every second (the maxTime property is specified in milliseconds), and thus our searcher
will be reopened every second if there are changes.
As you can see, even though we didn't specify the commit command after our update
command, we are still able to find the document we've sent for indexation.
How to get the documents with all the query
words to the top of the results set
One of the most common problems that users struggle with when using Apache Solr is how to
improve the relevancy of their results. Of course, relevancy tuning is, in most cases, connected
to your business needs, but one of the common problems is to have documents that have all the
query words in their fields at the top of the results list. You can imagine a situation where you
search for all the documents that match at least a single query word, but you would like to show
the ones with all the query words first. This recipe will show you how to achieve that.
How to do it...
This recipe will show how we can get the documents with all the query words to the top of the
results set.
1. Let's start with the following index structure (add it to the field section in your
schema.xml file):
www.it-ebooks.info
Appendix
297
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="description" type="text" indexed="true"
stored="true" />
2. The second step is to index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr and all the others</field>
<field name="description">This is about Solr</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Lucene and all the others</field>
<field name="description">
This is a book about Solr and Lucene
</field>
</doc>
</add>
3. Let's assume that our usual queries look similar to the following code snippet:
http://localhost:8983/solr/select?q=solr book&defType=edismax&mm=1
&qf=name^10000+description
Nothing complicated; however, the results of such query don't satisfy us, because
they look similar to the following code snippet:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">name^10000 description</str>
<str name="mm">1</str>
<str name="q">solr book</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
www.it-ebooks.info
Real-life Situations
298
<doc>
<str name="id">1</str>
<str name="name">Solr and all the others</str>
<str name="description">This is about Solr</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Lucene and all the others</str>
<str name="description">
This is a book about Solr and Lucene
</str>
</doc>
</result>
</response>
4. In order to change this, let's introduce a new handler in our solrconfig.xml file:
<requestHandler name="/better"
class="solr.StandardRequestHandler">
<lst name="defaults">
<str name="indent">true</str>
<str name="q">
_query_:"{!edismaxqf=$qfQuery mm=$mmQuerypf=
$pfQuerybq=$boostQuery v=$mainQuery}"
</str>
<str name="qfQuery">name^100000 description</str>
<str name="mmQuery">1</str>
<str name="pfQuery">name description</str>
<str name="boostQuery">
_query_:"{!edismaxqf=$boostQueryQf mm=100%
v=$mainQuery}"^100000
</str>
<str name="boostQueryQf">name description</str>
</lst>
</requestHandler>
5. So, let's send a query to our new handler:
http://localhost:8983/solr/better?mainQuery=solr book
We get the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Appendix
299
<int name="QTime">2</int>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lucene and all the others</str>
<str name="description">
This is a book about Solr and Lucene
</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr and all the others</str>
<str name="description">This is about Solr</str>
</doc>
</result>
</response>
As you can see, it works. So let's discuss how.
How it works...
For the purpose of the recipe, we've used a simple index structure that consists of a
document identifier, its name, and description. Our data is very simple as well; it just
contains two documents.
During the first query, the document with the identifier 1 is placed at the top of the query
results. However, what we would like to achieve is be able to boost the name. In addition to
this, we would like to have the documents with words from the query close to each other at
the top of the results.
In order to do this, we've defined a new request handler named /better, which will
leverage the local params. The first thing is the defined q parameter, which is the standard
query. It uses the Extended DisMax parser (the {!edismax part of the query), and defines
several additional parameters:
ff qf: This defines the fields against which edismax should send the query. We tell
Solr that we will provide the fields by specifying the qfQuery parameter by using
the $qfQuery value.
ff mm: This is the "minimum should match" parameter, which tells edismax how
many words from the query should be found in a document for the document to
be considered a match. We tell Solr that we will provide the fields by specifying
the mmQuery parameter, by using the $mmQuery value.
www.it-ebooks.info
Real-life Situations
300
ff pf: This is the phrase fields definition which specifies the fields on which Solr should
generate phrase queries automatically. Similar to the previous parameters that we've
specified, we will provide the fields by specifying the pfQuery parameter, by using
the $pfQuery value.
ff bq: This is the boost query that will be used to boost the documents. Again, we use
the parameter dereferencing functionality and tell Solr that we will provide the
value in the bqQuery parameter, by using the $bqQuery value.
ff v: This is the final parameter which specifies the content of the query; in our case,
the user query will be specified in the mainQuery parameter.
Basically, the preceding queries say that we will use the edismax query parser, phrase,
and boost queries. Now let's discuss the values of the parameters.
The first thing is the qfQuery parameter, which is exactly the same as the qf parameter in
the first query we sent to Solr. Using it, we just specify the fields that we want to be searched
and their boosts. Next, we have the mmQuery parameter set to 1 that will be used as mm in
edismax, which means that a document will be considered a match when a single word
from the query will be found in it. As you will remember, the pfQuery parameter value will
be passed to the pf parameter, and thus the phrase query will be automatically made on
the fields defined in those fields.
Now, the last and probably the most important part of the query, the boostQuery parameter,
specifies the value that will be passed to the bq parameter. Our boost query is very similar to
our main query, however, we say that the query should only match the documents that have
all the words from the query (the mm=100% parameter). We also specify that the documents
that match that query should be boosted by adding the ^100000 part at the end of it.
To sum up all the parameters of our query, they will promote the documents with all the words
from the query present in the fields we want to search on. In addition to this, we will promote
the documents that have phrases matched. So finally, let's look at how the newly created
handler work. As you can see, when providing our query to it with the mainQuery parameter,
the previous document is now placed as the first one. So, we have achieved what we wanted.
How to boost documents based on their
publishing date
Imagine that you would like to place documents that are newer above the ones that are older.
For example, you have a book store and want to promote the books that have been published
recently, and place them above the books that have been present in our store for a long time.
Solr lets us do this, and this recipe will show you how.
www.it-ebooks.info
Appendix
301
How to do it...
This recipe will show how we can boost documents based on their publishing date.
1. Let's begin with the following index structure (add it to the field section in your
schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="published" type="date" indexed="true"
stored="true" default="NOW" />
2. Now, let's index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 3.1 CookBook</field>
<field name="published">2011-02-02T12:00:00Z</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr 4.0 CookBook</field>
<field name="published">2012-10-01T12:00:00Z</field>
</doc>
</add>
3. Now, let's run a simple query:
curl 'http://localhost:8983/solr/select?q=solr+cookbook&qf=name&de
fType=edismax'
For the preceding query, Solr will return the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">name</str>
<str name="q">solr cookbook</str>
<str name="defType">edismax</str>
</lst>
</lst>
www.it-ebooks.info
Real-life Situations
302
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr 3.1 CookBook</str>
<date name="published">2011-02-02T12:00:00Z</date>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr 4.0 CookBook</str>
<date name="published">2012-10-01T12:00:00Z</date>
</doc>
</result>
</response>
4. As you can see, the newest document is the second one, which we want to avoid. So,
we need to change our query to the following one:
curl 'http://localhost:8983/solr/select?q=solr+cookbook&qf=name&bf
=recip(ms(NOW/HOUR,published),3.16e-11,1,1)defType=edismax'
Now, the response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="qf">name</str>
<str name="bf">
recip(ms(NOW/HOUR,published),3.16e-11,1,1)
</str>
<str name="q">solr cookbook</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">2</str>
<str name="name">Solr 4.0 CookBook</str>
<date name="published">2012-10-01T12:00:00Z</date>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr 3.1 CookBook</str>
www.it-ebooks.info
Appendix
303
<date name="published">2011-02-02T12:00:00Z</date>
</doc>
</result>
</response>
So, we have achieved what we wanted. Now, let's see how it works.
How it works...
Our index structure consists of three fields; one responsible for holding the identifier of the
document, one for the name of the document, and the last one; the one which we will be
most interested in, in which we hold the publishing date.
The published field has one nice feature – if we don't define it in the document and send
it for indexation, then it will get the value of the date and time when it is processed (the
default="NOW" attribute).
As you can see, the first query that we sent to Solr returned results not in a way we would
like them to be sorted. The most recent document is the second one. Of course, we could
have sorted them by date, but we don't want to do that, because we would like to have the
most recent and the most relevant documents at the top, not only the newest ones.
In order to achieve this, we use the bf (boost function) parameter. We specify the boosting
function. At first, it can look very complicated, but it's not. In order to boost our documents,
we use the recip(ms(NOW/HOUR,published),3.16e-11,1,1) function query. 3.16e10
specifies the number of milliseconds that are in a single year, so we use 3.16e-11 to invert
that, and we use the reciprocal function (recip) to calculate the scaling value, which will
return values near 1 for recent documents, 1/2 for documents from about a year, 1/3 for
documents that are about two years old, 1/4 for documents that are about three years old,
and so on.
We've also used NOW/HOUR to reduce the precision of the published field, in order for
our function query to consume less memory and because we don't need that granularity;
our results will be just fine.
As you can see, our query with the bf parameter and the time-based function query work
as intended.
There's more...
If you want to read more about function queries, please refer to the http://wiki.apache.
org/solr/FunctionQuery Solr wiki page.
www.it-ebooks.info
www.it-ebooks.info
Index
Symbols
-DnumShards parameter 213
-DzkHost parameter 213
-DzkRun parameter 213
<script> tag 55
A
add command 58
administration GUI, SolrCloud
cluster 220-223
adminPath property 17
adminPath variable 9
alphabetical order
faceting results, sorting in 168-170
analyzer 70
Apache Nutch
URL, for downloading 27
URL, for info 30
Apache Solr
URL, for tutorial 5
Apache Tika 36
Apache Tika library
used, for detecting language 66
Apache Tomcat
Solr, running on 10-13
URL 11
apt-get command 6, 8
automatic document distribution
stopping, among shards 230-234
autosuggest feature
implementing, faceting used 171-173
autowarmCount parameter 190, 193
B
binary files
metadata, extracting from 40-42
bqQuery parameter 300
buffer overflow 10
C
cache 22
caches, Solr
document 22, 26
filter 22, 25
query result 22, 26
CamelCase
used, for splitting text 80, 82
Catalina context file 12
category’s autocomplete functionality
implementing 287-289
working 289
CDATA tags 75
character filters 70
clientPort property 15
cluster
collections, setting up 214-216
replica count, increasing 227-230
collections
setting up, in cluster 214-216
commit command 295
commit operation
about 200
Solr performance, improving after 194-196
conf directory 13
config-file 120
configuration, document cache 189, 190
www.it-ebooks.info
306
configuration, filter cache 192, 193
configuration, query result cache 191, 192
configuration, Solr cache
about 23, 24
document cache 26
filter cache 25
filter cache, using with faceting 25
no cache hits 25
query result cache 26
query result window 26
configuration, spellchecker 19, 21
content
copying, of dynamic fields 77
copying, of fields 75-77
context directory 6
contrib modules 62
crawl command 29
crawl-urlfilter.txt file 29
CSV 30
curl command 37
currencyConfig attribute 61
currencyExchange.xml file 61
currency provider
setting up 62
D
data
clustering 15, 17
importing, Data Import Handler used 48-50
indexing, Data Import Handler used 45-48
modifying, in Data Import Handler 53-55
searching, in near real-time manner 294-296
stemming 91-93
data analysis 70
data behavior 70
data-config.xml file 52
dataDir property 15
Data Import Handler
about 42
configuring, with JDBC 42-44
data, modifying 53-55
used, for importing data 48-50
used, for indexing data from database 45-48
using, with URL data source 50, 51
data indexing 70
db-data-config.xml file 43
debug attribute 12
decision tree faceting
using 180-183
defaultCoreName attribute 9, 13
defaultCurrency attribute 61
default HTML tags
modifying 241
default similarity implementation
modifying 32-34
defined words
ignoring 248-250
defType parameter 116
delete operation 216
different query parsers
using, in single query 290, 291
directoryFactory tag 18
directory implementation
selecting 17-19
DirectSolrSpellChecker 256
DisMax query parser
about 116, 122
used, for querying particular value 109
distance
defining, between words in phrase 114
distributed indexing 223-226
docBase attribute 12
document
language, detecting 62-66
single field, updating 56-58
document cache
about 22, 26, 189
configuring 189, 190
document count
getting, by query match 161-164
getting, by subquery match 161-164
getting, without value in field 174-176
getting, with same field value 156-158
getting, with same value range 158-161
document language
detecting 62-66
detecting, Apache Tika library used 66
documents
boosting, based on publishing date 301-303
default HTML tags, modifying 241
excluding, with
QueryElevationComponent 121
faceting, calculating for 183-186
www.it-ebooks.info
307
getting right, after indexation 292, 293
getting, with all query words at top
results set 296-300
modifying 136-138
positioning, over others on query 117-121
positioning, with closer words 122-125
retrieving, with partial match 128-130
DoubleMetaphoneFilterFactory 247
duplicate documents
detecting 145-148
omitting 145-148
dynamic fields
content, copying of 77
E
elevate.xml file 139
embedded ZooKeeper server
starting 213
enablePositionIncrements parameter 250
entities 44
Extended DisMax query parser
parameters 299
using 290, 299
extracting request handler
setting up 30, 31
F
faceting
about 155
calculating, for relevant documents
in groups 183-186
filter cache, using with 25
used, for implementing
autosuggest feature 171-173
faceting method per field
specifying 200
faceting performance
improving, for low cardinality fields 198, 199
faceting results
filters, removing from 164-167
lexicographical sorting 158
sorting, in alphabetical order 168-170
facet limits
for different fields, in same query 177-180
FastVectorHighlighting feature 243
field
updating, of document 56-58
field aliases
using 148-150
fields
content, copying of 75-77
specifying, for highlighting 241
field value
used, for grouping results 257-259
used, for sorting results 109-111
file data source 50
filter cache
about 22, 25, 192
configuring 192, 193
using, with faceting 25
filter caching
avoiding 206
filter queries
order of execution, controlling for 207, 208
filters
removing, from faceting results 164-167
flexible indexing 68
function queries
used, for grouping results 262, 263
functions
scoring, affecting with 130-34
function value
used, for sorting results 243-245
G
Gangila
URL 188
generateNumberParts parameter 98
generateWordParts parameter 98
geodist function 245
geographical points
storing, in index 88-91
global similarity
configuring 34
H
hash value 227
highlighting
fields, specifying for 241
HTML tags
eliminating, from text 73-75
www.it-ebooks.info
308
HttpDataSource 52
Hunspell
about 99
using, as stemmer 99, 100
I
ignoreCase attribute 79
ignored.txt file 248
index
geographical points, storing in 88-91
making, smaller 272, 273
indexing 35
index size
estimating 274
information
storing, payloads used 70-73
initialSize parameter 190
initLimit property 15
installation, ZooKeeper 14, 15
instanceDir attribute 9
issues, Apache Tomcat
Apache Tomcat, running on different port 13
issues, Jetty servlet container
buffer overflow 10
Jetty, running on different port 9
J
Java 6 55
java command 8, 9
JDBC
Data Import Handler, configuring with 42-44
Jetty
Solr, running on 6-9
Jetty servlet container
URL, for downloading 6
jetty.xml file 7, 10
JSON 30
L
language attribute 55
lexicographical sorting, faceting results 158
light stemming 86
logging.properties file 7
low cardinality fields
faceting performance, improving for 198, 199
Lucene directory implementation 17
LuceneQParser query parser 240
Lucene’s internal cache 23
M
matched words
highlighing 238-240
maxChars attribute 77
mergeFactor parameter 267
metadata
extracting, from binary files 40-42
mmQuery parameter
about 299
multiple currencies
configuring 59-61
handling 59
using 59-61
multiple values
querying for 109
N
n-grams
about 95
used, for handling user typos.. 142-145
non-English languages
sorting, properly 268-271
non-whitespace characters
used, for splitting text 96-98
numbers
used, for splitting text 96-98
numerical range queries
performance, improving 208, 209
O
opened files
dealing with 265-267
order of execution
controlling, of filter queries 207, 208
OR operator 122
out-of-memory issues
dealing with 267, 268
www.it-ebooks.info
309
P
parameter dereferencing 136
parameters, Extended DisMax query parser
bq 300
mm 299
pf 300
qf 299
v 300
parent-child relationships
about 139
using 140, 141
partial match
documents, retrieving with 128-130
particular field value
asking for 108
particular value
querying, DisMax query parser used 109
path attribute 12
payload
about 70
used, for storing information 70-73
PDFCreator 36
PDF files
indexing 36-38
performance
about 187
improving, of numerical range
queries 208, 209
pfQuery parameter 300
phrase
searching for 111-113
phrases
boosting, over words 114-116
boosting, with standard query parser 117
phrase slop 114
pivot faceting 180
plural words
singular, making 84-86
PostgreSQL 50
primary key 67
primary key field indexing
optimizing 67, 68
product’s autocomplete functionality
implementing 284, 285
working 286, 287
Q
qfQuery parameter 299
queries
nesting 134-136
used, for grouping results 260-262
queryAnalyzerFieldType property 21
QueryElevationComponent
document, excluding with 121
queryFieldType attribute 120
query parser 291
query performance
analyzing 202-205
query result cache
about 22, 26, 190
configuring 191, 192
queryResultMaxDocsCached property 189
query results
paging 188, 189
query result window 26
queryResultWindowSize property 188
R
real-time get 292
reload operation 216
replicas
increasing, on live cluster 227-230
replication 227
result pages
caching 197, 198
results
grouping, field values used 257-259
grouping, function queries used 262-263
grouping, queries used 260-262
sorting, by distance from point 125-128
sorting, by field value 109-111
sorting, by function value 243-245
value of function, returning in 151-153
S
Scalable Performance Monitoring 25, 188
schema.xml file 7, 29, 38, 52, 84, 133
scoring
affecting, with functions 130-134
searching 223-226
www.it-ebooks.info
310
search results
used, for computing statistics 250-253
Sematext
about 25
URL 188
server.xml file 11
similar documents
returning 236-238
softCommit command 17
Solr
about 36, 99
indexing, issues 200-202
performance, improving after commit
operation 194-196
performance, improving after startup
operation 194-196
result pages, caching 197, 198
running, on Apache Tomcat 10-12
running, on Jetty 6-9
Solr 4.0 211
Solr cache
configuring 23, 24
SolrCloud
about 211
automatic document distribution, stopping
among shards 230-234
collections, setting up in cluster 214-216
distributed indexing 223-226
replicas, increasing on live cluster 227-230
searching 223-226
SolrCloud cluster
about 211
administration GUI 220-223
creating 212
managing 216, 217-219
working 213
solrconfig.xml file 7, 16, 19, 52, 188
solr.DFRSimilarityFactory 34
solr.DirectSolrSpellchecker 19
solr.DirectSolrSpellChecker 21
Solr issues
diagnosing 274-279
solr.MMapDirectoryFactory 18
solr.NIOFSDirectoryFactory 18
solr.NRTCachingDirectoryFactory 19
solr.QueryElevationComponent 117
solr.RAMDirectoryFactory 19
solr.RealTimeGetHandler class 294
solr.SchemaSimilarityFactory 34
solr.SimpleFSDirectoryFactory 18
solr.StandardDirectoryFactory 18
solr.UUIDField 39
solr.war file 6, 8
Solr wiki page 303
solr.xml file 6-13
sounds
used, for searching words 246, 247
spellchecker
about 19
configuring 19, 21
spellchecker component
about 253
using 254-256
spelling mistakes
checking, of user 253-256
splitOnNumerics parameter 98
standard query parser
phrases, boosting with 117
startup operation
Solr performance, improving after 194-196
statistics
computing, for search results 250-253
StatsComponent 252
stemmer
Hunspell, using as 99, 100
stemming
about 91
words, protecting from 103-106
stemming algorithms 84
stemming dictionary
using 101-103
StopFilterFactory 250
string
lowercasing 87, 88
swapping
avoiding 280-282
syncLimit property 15
synonyms attribute 79
synonyms.txt file 78
T
temp directory 6
termVectors attribute 238
www.it-ebooks.info
311
text
HTML tags, eliminating from 73-75
preparing, for wildcard search 93-95
splitting, by CamelCase 80-82
splitting, by non-whitespace characters 96-98
splitting, by numbers 96-98
splitting, by whitespace 82-84
XML tags, eliminating from 73-75
text fields
highlighting 241-243
tickTime property 15
Tika 31
tokenizer 70
tokens 70
transformer 52
types 70
typos
handling, ngrams used 142-145
ignoring, in performance wise way 142-145
U
unique fields
generating, automatically 38, 39
URL data source
Data Import Handler, using with 50-53
UTF-8 file encoding 12
V
value of function
returning, in results 151-153
vQuery parameter 300
W
webapps directory 6
webdefault.xml file 7
web pages
fetching 27-29
indexing 27-29
whitespace
used, for splitting text 82-84
wildcard search
text, preparing for 93-95
words
modifying 77-79
phrases, boosting over 114-116
protecting, from stemming 103-106
searching, by sound 246, 247
X
XML 30
XML tags
eliminating, from text 73-75
XPath expression 52
Z
ZooKeeper
about 14
installing 14, 15
URL, for downloading 14
ZooKeeper cluster 212
www.it-ebooks.info
www.it-ebooks.info
Thank you for buying
Apache Solr 4 Cookbook
About Packt Publishing
Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL
Management" in April 2004 and subsequently continued to specialize in publishing highly focused
books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and
customizing today's systems, applications, and frameworks. Our solution based books give you the
knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past. Our
unique business model allows us to bring you more focused information, giving you more of what
you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge
books for communities of developers, administrators, and newbies alike. For more
information, please visit our website: www.packtpub.com.
About Packt Open Source
In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to
continue its focus on specialization. This book is part of the Packt Open Source brand, home
to books published on software built around Open Source licences, and offering information to
anybody from advanced developers to budding web designers. The Open Source brand also runs
Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project
about whose software a book is sold.
Writing for Packt
We welcome all inquiries from people who are interested in authoring. Book proposals should
be sent to author@packtpub.com. If your book idea is still at an early stage and you would like to
discuss it first before writing a formal book proposal, contact us; one of our commissioning editors
will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing
experience, our experienced editors can help you develop a writing career, or simply get some
additional reward for your expertise.
www.it-ebooks.info
Apache Solr 3 Enterprise
Search Server
ISBN: 978-1-84951-606-8 Paperback: 418 pages
Enhance your search with faceted navigation, result
highlighting relevancy ranked sorting, and more
1. Comprehensive information on Apache Solr
3 with examples and tips so you can focus
on the important parts
2. Integration examples with databases,
web-crawlers, XSLT, Java & embedded-Solr,
PHP & Drupal, JavaScript, Ruby frameworks
3. Advice on data modeling, deployment
considerations to include security, logging,
and monitoring, and advice on scaling Solr
and measuring performance
HBase Administration
Cookbook
ISBN: 978-1-84951-714-0 Paperback: 332 pages
Master HBase configuration and administration for
optimum database performance
1. Move large amounts of data into HBase
and learn how to manage it efficiently
2. Set up HBase on the cloud, get it ready
for production, and run it smoothly with
high performance
3. Maximize the ability of HBase with the
Hadoop eco-system including HDFS,
MapReduce, Zookeeper, and Hive
Please check www.PacktPub.com for information on our titles
www.it-ebooks.info
Hadoop Real World Solutions
Cookbook
ISBN: 978-1-84951-912-0 Paperback: 325 pages
Realistic, simple code examples to solve problems at
scale with Hadoop and related technologies
1. Solutions to common problems when working
in the Hadoop environment
2. Recipes for (un)loading data, analytics, and
troubleshooting
3. In depth code examples demonstrating various
analytic models, analytic solutions, and common
best practices
Cassandra High Performance
Cookbook
ISBN: 978-1-84951-512-2 Paperback: 310 pages
Over 150 recipes to design and optimize large-scale
Apache Cassandra deployments
1. Get the best out of Cassandra using this efficient
recipe bank
2. Configure and tune Cassandra components to
enhance performance
3. Deploy Cassandra in various environments and
monitor its performance
4. Well illustrated, step-by-step recipes to make all
tasks look easy!
Please check www.PacktPub.com for information on our titles
www.it-ebooks.info
Hellerstein, chancellor’s professor of computer science at UC Berkeley.
“If you have people in the loop, it’s not real time. Most people take a
second or two to react, and that’s plenty of time for a traditional transactional
system to handle input and output.”
That doesn’t mean that developers have abandoned the quest for speed.
Supported by a Google grant, Matei Zaharia is working on his Ph.D.
at UC Berkeley. He is an author of Spark, an open source cluster computing
system that can be programmed quickly and runs fast. Spark
relies on “resilient distributed datasets” (RDDs) and “can be used to
interactively query 1 to 2 terabytes of data in less than a second.”
In scenarios involving machine learning algorithms and other multipass
analytics algorithms, “Spark can run 10x to 100x faster than Hadoop
MapReduce,” says Zaharia. Spark is also the engine behind
Shark, a data warehousing system.
According to Zaharia, companies such as Conviva and Quantifind
have written UIs that launch Spark on the back end of analytics dashboards.
“You see the statistics on a dashboard and if you’re wondering
about some data that hasn’t been computed, you can ask a question
that goes out to a parallel computation on Spark and you get back an
answer in about half a second.”
Storm is an open source low latency processing stream processing
system designed to integrate with existing queuing and bandwidth
systems. It is used by companies such as Twitter, the Weather Channel,
Groupon and Ooyala. Nathan Marz, lead engineer at BackType (acquired
by Twitter in 2011), is the author of Storm and other opensource
projects such as Cascalog and ElephantDB.
“There are really only two paradigms for data processing: batch and
stream,” says Marz. “Batch processing is fundamentally high-latency.
So if you’re trying to look at a terabyte of data all at once, you’ll never
be able to do that computation in less than a second with batch processing.”
Stream processing looks at smaller amounts of data as they arrive. “You
can do intense computations, like parallel search, and merge queries
on the fly,” says Marz. “Normally if you want to do a search query, you
need to create search indexes, which can be a slow process on one
machine. With Storm, you can stream the process across many machines,
and get much quicker results.”
10 | Chapter 3: How Real Is Real Time?
GAYLE LAAKMANN
Founder and CEO, CareerCup.com
150 programming interview questions and solutions
Plus:
• Five proven approaches to solving tough algorithm questions
• Ten mistakes candidates make -- and how to avoid them
• Steps to prepare for behavioral and technical questions
• Interviewer war stories: a view from the interviewer’s side
FOURTH
C EDITION RACKING THE
C ODING
I NTERVIEW
CRACKING THE
CODING
INTERVIEW

CRACKING THE
CODING
INTERVIEW
150 Programming Interview
Questions and Solutions
GAYLE LAAKMANN
Founder and CEO, CareerCup.com
CareerCup, LLC
Seattle, WA
CRACKING THE CODING INTERVIEW, FOURTH EDITION
Copyright © 2008 - 2010 by Gayle Laakmann. All rights reserved.
Published by CareerCup, LLC, Seattle, WA. Version 3.21090410302210.
Visit our website at: www.careercup.com. No part of this book may be used or reproduced in any manner without written permission except in the case of brief quotations in critical articles or reviews.
For more information, contact support@careercup.com.
Printed in United States of America
978-1-450-59320-5 9781450593205 (ISBN 13)

Cracking the Coding Interview
1
Table of Contents
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Behind the Scenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
The Microsoft Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
The Amazon Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
The Google Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
The Apple Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
The Yahoo Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Interview War Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Before the Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Resume Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Behavioral Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Technical Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
The Interview and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Handling Behavioral Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Handling Technical Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Five Algorithm Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
The Offer and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Top Ten Mistakes Candidates Make . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Frequently Asked Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
Chapter 1 | Arrays and Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Chapter 2 | Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Chapter 3 | Stacks and Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51
Chapter 4 | Trees and Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Concepts and Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Chapter 5 | Bit Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Chapter 6 | Brain Teasers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2
CareerCup.com
Table of Contents
Chapter 7 | Object Oriented Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Chapter 8 | Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Chapter 9 | Sorting and Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Chapter 10 | Mathematical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Chapter 11 | Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Chapter 12 | System Design and Memory Limits . . . . . . . . . . . . . . . . . . . . . . . . 71
Knowledge Based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
Chapter 13 | C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
Chapter 14 | Java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Chapter 15 | Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Chapter 16 | Low Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Chapter 17 | Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Chapter 18 | Threads and Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Additional Review Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Chapter 19 | Moderate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Chapter 20 | Hard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Index .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
Mock Interviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
About the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304

. CareerCup com 4
Foreword
Dear Readers,
Welcome to the 4th edition of Cracking the Coding Interview. This volume updates the 3rd edition with new content and refreshed information. Be sure to check out our website, www.careercup.com, to connect with other candidates and to discover new resources.
For those of you new to technical interviews, the process can seem overwhelming. Interviewers throw questions at you, expect you to whip up brilliant algorithms on the spot, and then ask you to write beautiful code on a whiteboard. Luckily, everyone else is in the same boat, and you’re already working hard to prepare. Good job!
As you get ready for your interviews, consider these suggestions:
» »Write Code on Paper: Most interviewers won’t give you a computer and will instead expect you to write code on a whiteboard or on paper. To simulate this environment, try answering interview problems by writing code on paper first, and then typing them into a computer as-is. Whiteboard / paper coding is a special skill, which can be mastered with constant practice.
» »Know Your Resume: While technical skills are extremely important, that’s no reason to neglect your own resume. Make sure to prepare yourself to give a quick summary of any project or job you were involved with, and to discuss the hardest and most interesting problems you encountered along the day.
» »Don’t Memorize Solutions: While this book offers a representative sample of interview questions, there are still thousands of interview questions out there. Memorizing solutions is not a great use of your time. Rather, use this book to explore approaches to problems, to learn new concepts, and to practice your skills.
» »Talk Out Loud: Interviewers want to understand how you think and approach problems, so talk out loud while you’re solving problems. Let the interviewer see how you’re tackling the problem, and they just might guide you as well.
And remember -- interviews are hard! In my years of interviewing at Google, I saw some interviewers ask “easy” questions while others ask harder questions. But you know what? Getting the easy questions doesn’t make it any easier to get the offer. Receiving an offer is not about solving questions flawlessly (very few candidates do!), but rather, it is about answering questions better than other candidates. So don’t stress out when you get a tricky question - everyone else probably thought it was hard too!
I'm excited for you and for the skills you are going to develop. Thorough preparation will give you a wide range of technical and communication skills. It will be well-worth it no matter where the effort takes you!
Study hard, practice, and good luck!
Gayle Laakmann
5 Cracking the Coding Interview
Introduction
Something’s Wrong
We walked out of the hiring meeting frustrated, again. Of the ten “passable” candidates we reviewed that day, none would receive offers. Were we being too harsh, we wondered?
I, in particular, was disappointed. We had rejected one of my candidates. A former student. One who I had referred. He had a 3.73 GPA from the University of Washington, one of the best computer science schools in the world, and had done extensive work on open source projects. He was energetic. He was creative. He worked hard. He was sharp. He was a true geek, in all the best ways.
But, I had to agree with the rest of the committee: the data wasn’t there. Even if my emphatic recommendation would sway them to reconsider, he would surely get rejected in the later stages of the hiring process. There were just too many red flags.
Though the interviewers generally believed that he was quite intelligent, he had struggled to develop good algorithms. Most successful candidates could fly through the first question, which was a twist on a well known problem, but he struggled to develop his algorithm. When he came up with one, he failed to consider solutions that optimized for other scenarios. Finally, when he began coding, he flew through the code with an initial solution, but it was riddled with mistakes that he then failed to catch. Though he wasn’t the worst candidate we'd seen by any measure, he was far from meeting “the bar.” Rejected.
When he asked for feedback over the phone a couple of weeks later, I struggled with what to tell him. Be smarter? No, I knew he was brilliant. Be a better coder? No, his skills were on-par with some of the best I'd seen.
Like many motivated candidates, he had prepared extensively. He had read K&R’s classic C book and he'd reviewed CLRS' famous algorithms textbook. He could describe in detail the myriad of ways of balancing a tree, and he could do things in C that no sane programmer should ever want to do.
I had to tell him the unfortunate truth: those books aren’t enough. Academic books prepare you for fancy research, but they’re not going to help you much in an interview. Why? I'll give you a hint: your interviewers haven’t seen Red-Black Trees since they were in school either.
To crack the coding interview, you need to prepare with real interview questions. You must practice on real problems, and learn their patterns.
Cracking the Coding Interview is the result of my first-hand experience interviewing at top companies. It is the result of hundreds of conversations with candidates. It is the result of the thousands of candidate- and interviewer- contributed questions. And it’s the result of seeing so many interview questions from so many firms. Enclosed in this book are 150 of the best interview questions, selected from thousands of potential problems.
. CareerCup com 6
Introduction
My Approach
The focus of Cracking the Coding Interview is algorithm, coding and design questions. Why? Because while you can and will be asked behavioral questions, the answers will be as varied as your resume. Likewise, while many firms will ask so-called “trivia” questions (e.g., “What is a virtual function?”), the skills developed through practicing these questions are limited to very specific bits of knowledge. The book will briefly touch on some of these questions, to show you what they’re like, but I have chosen to allocate space where there’s more to learn.
My Passion
Teaching is my passion. I love helping people understand new concepts, and giving them tools so that they can excel in their passions.
My first experience “officially” teaching was in college at the University of Pennsylvania, when I became a teaching assistant for an undergraduate Computer Science course during my second year. I went on to TA for several other courses, and eventually launched my own CS course at the university focused on “hands-on” skills.
As an engineer at Google, training and mentoring “Nooglers” (yes, that’s really what they call new Google employees!) were some of the things I enjoyed most. I went on to use my “20% time” to teach two Computer Science courses at the University of Washington.
Cracking the Coding Interview and CareerCup.com reflect my passion for teaching. Even now, you can often find me “hanging out” at CareerCup.com, helping users who stop by for assistance.
Join us.
Gayle Laakmann
7 Cracking the Coding Interview
Behind the Scenes
For many candidates, interviewing is a bit of a black box. You walk in, you get pounded with questions from a variety of interviewers, and then somehow or other you return with an offer... or not.
Have you ever wondered:
»»How do decisions get made?
»»Do your interviewers talk to each other?
»»What does the company really care about?
Well, wonder no more!
CareerCup sought out interviewing experts from five top companies - Microsoft, Google, Amazon, Yahoo and Apple - to show you what really happens “behind the scenes.” These experts will walk us through a typical interview day and describe what’s taking place outside of the interviewing room, and what happens after you leave.
Our interviewing experts also told us what’s different about their interview process. From bar raisers (Amazon) to Hiring Committees (Google), each company has its own quirks. Knowing these idiosyncrasies will help you to react better to a super-tough interviewer, or to avoid being intimidated when two interviewers show up at the door (Apple!).
In addition, our specialists offered insight as to what their company stresses in their interviews. While almost all software firms care about coding and algorithms, some companies focus more than others on specific aspects of the interview. Whether this is because of the company’s technology or its history, now you'll know what and how to prepare.
So, join us as we take you behind the scenes at Microsoft, Google, Amazon, Yahoo and Apple...
. CareerCup com 8
Behind the Scenes | The Microsoft Interview
Microsoft wants smart people. Geeks. People who are passionate about technology. You probably won’t be tested on the ins and outs of C++ APIs, but you will be expected to write code on the board.
In a typical interview, you'll show up at Microsoft at some time in the morning and fill out initial paper work. You'll have a short interview with a recruiter where he or she will give you a sample question. Your recruiter is usually there to prep you, and not to grill you on technical questions. Be nice to your recruiter. Your recruiter can be your biggest advocate, even pushing to re-interview you if you stumbled on your first interview. They can fight for you to be hired - or not!
During the day, you'll do four or five interviews, often with two different teams. Unlike many companies, where you meet your interviewers in a conference room, you'll meet with your Microsoft interviewers in their office. This is a great time to look around and get a feel for the team culture.
Depending on the team, interviewers may or may not share their feedback on you with the rest of the interview loop.
When you complete your interviews with a team, you might speak with a hiring manager. If so, that’s a great sign! It likely means that you passed the interviews with a particular team. It’s now down to the hiring manager’s decision.
You might get a decision that day, or it might be a week. After one week of no word from HR, send them a friendly email asking for a status update.
Definitely Prepare:
“Why do you want to work for Microsoft?”
In this question, Microsoft wants to see that you’re passionate about technology. A great answer might be, “I’ve been using Microsoft software as long as I can remember, and I'm really impressed at how Microsoft manages to create a product that is universally excellent. For example, I’ve been using Visual Studio recently to learn game programming, and it’s APIs are excellent.” Note how this shows a passion for technology!
What’s Unique:
You'll only reach the hiring manager if you’ve done well, but if you do, that’s a great sign!
9 Cracking the Coding Interview
Behind the Scenes | The Amazon Interview
Amazon’s recruiting process usually begins with one or two phone screens in which you interview with a specific team. The engineer who interviews you will usually ask you to write simple code and read it aloud on the phone. They will ask a broad set of questions to explore what areas of technology you’re familiar with.
Next, you fly to Seattle for four or five interviews with one or two teams which have selected you based on your resume and phone interviews. You will have to code on a whiteboard, and some interviewers will stress other skills. Interviewers are each assigned a specific area to probe and may seem very different from each other. They can not see other feedback until they have submitted their own and they are discouraged from discussing it until the hiring meeting.
Amazon’s “bar raiser” interviewer is charged with keeping the interview bar high. They attend special training and will interview candidates outside their group in order to balance out the group itself. If one interview seems significantly harder and different, that’s most likely the bar raiser. This person has both significant experience with interviews and veto power in the hiring decision. You will meet with your recruiter at the end of the day.
Once your interviewers have entered their feedback, they will meet to discuss it. They will be the people making the hiring decision.
While Amazon’s recruiters are excellent at following up with candidates, occasionally there are delays. If you haven’t heard from Amazon within a week, we recommend a friendly email.
Definitely Prepare:
Amazon is a web-based company, and that means they care about scale. Make sure you prepare for questions in “Large Scale.” You don’t need a background in distributed systems to answer these questions. See our recommendations in the System Design and Memory Limits Chapter.
Additionally, Amazon tends to ask a lot of questions about object oriented design. Check out the Object Oriented Design chapter for sample questions and suggestions.
What’s Unique:
The Bar Raiser, who is brought in from a different team to keep the bar high.
. CareerCup com 10
Behind the Scenes | The Google Interview
There are many scary stories floating around about Google interviews, but it’s mostly just that: stories. The interview is not terribly different from Microsoft’s or Amazon’s. However, because Google HR can be a little disorganized, we recommend being proactive in communication.
A Google engineer performs the first phone screen, so expect tough technical questions. On your on-site interview, you'll interview with four to six people, one of whom will be a lunch interviewer. Interviewer feedback is kept confidential from the other interviewers, so you can be assured that you enter each interview with blank slate. Your lunch interviewer doesn’t submit feedback, so this is a great opportunity to ask honest questions.
Written feedback is submitted to a hiring committee of engineers to make a hire/no-hire recommendation. Feedback is typically broken down into four categories (Analytical Ability, Coding, Experience and Communication) and you are given a score from 1.0 to 4.0 overall.
The hiring committee understands that you can’t be expected to excel in every interview, but if multiple people raise the same red flag (arrogance, poor coding skills, etc), that can disqualify you. A hiring committee typically wants to see one interviewer who is an “enthusiastic endorser.” In other words, a packet with scores of 3.6, 3.1, 3.1 and 2.6 is better than all 3.1s. Your phone screen is usually not a strong factor in the final decision.
The Google hiring process can be slow. If you don’t hear back within one week, politely ask your recruiter for an update. A lack of response says nothing about your performance.
Definitely Prepare:
As a web-based company, Google cares about how to design a scalable system. So, make sure you prepare for questions from “System Design and Memory Limits” Additionally, many Google interviewers will ask questions involving Bit Manipulation, so please brush up on these questions.
What’s Different:
Your interviewers do not make the hiring decision. Rather, they enter feedback which is passed to a hiring committee. The hiring committee recommends a decision which can be—though rarely is—rejected by Google executives.
11 Cracking the Coding Interview
Behind the Scenes | The Apple Interview
Much like the company itself, Apple’s interview process has minimal beaucracy. The interviewers will be looking for excellent technical skills, but a passion for the position and company is also very important. While it’s not a prerequisite to be a Mac user, you should at least be familiar with the system.
The interview process typically begins with a recruiter phone screen to get a basic sense of your skills, followed up by a series of technical phone screens with team members.
Once you’re invited on campus, you'll typically be greeted by the recruiter who provides an overview of the process. You will then have 6-8 interviews with members of the team for which you’re interviewing, as well as key people with whom your team works.
You can expect a mix of 1-on-1 and 2-on-1 interviews. Be ready to code on a whiteboard and make sure all of your thoughts are clearly communicated. Lunch is with your potential future manager and appears more casual, but is still an interview. Each interviewer is usually focused on a different area and is discouraged from sharing feedback unless there’s something they want subsequent interviewers to drill into.
Towards the end of the day, your interviewers will compare notes and if everyone still feels you’re a viable candidate, you'll interview with the director and then VP of the organization you’re applying to. While this decision is rather informal, it’s a very good sign if you make it. This decision also happens behind the scenes and if you don’t pass, you'll simply be escorted out of the building without ever having been the wiser (until now).
If you made it to the director and VP interviews, all of your interviewers will gather in a conference room to give an official thumbs up or thumbs down. The VP typically won’t be present, but can still veto the hire if they weren’t impressed. Your recruiter will usually follow up a few days later, but feel free to ping your recruiter for updates.
Definitely Prepare:
If you know what team you’re interviewing with, make sure you read up on that product. What do you like about it? What would you improve? Offering specific recommendations can show your passion for the job.
What’s Unique:
Apple does 2-on-1 interviews often, but don’t get stressed out about them - it’s the same as a 1-on-1 interview!
Also, Apple employees are huge Apple fans. You should show this same passion in your interview.
. CareerCup com 12
Behind the Scenes | The Yahoo Interview
Resume Selection & Screening: While Yahoo tends to only recruit at the top 10 – 20 schools, other candidates can still get interviewed through Yahoo’s job board (or – better yet – if they can get an internal referral). If you’re one of the lucky ones selected, your interview process will start off with a phone screen. Your phone screen will be with a senior employee (tech lead, manager, etc).
On-Site Interview: You will typically interview with 6 – 7 people on the same team for 45 minutes each. Each interviewer will have an area of focus. For example, one interviewer might focus on databases, while another interviewer might focus on your understanding of computer architecture. Interviews will often be composed as follows:
5 minutes: General conversation. Tell me about yourself, your projects, etc.
20 minutes: Coding question. For example, implement merge sort.
20 minutes: System design. For example, design a large distributed cache. These questions will often focus on an area from your past experience or on something your interviewer is currently working on.
Decision: At the end of the day, you will likely meet with a Program Manager or someone else for a general conversation (product demos, concerns about the company, your competing offers, etc). Meanwhile, your interviewers will discuss your performance and attempt to come to a decision. The hiring manager has the ultimate say and will weigh the positive feedback against the negative.
If you have done well, you will often get a decision that day, but this is not always the case. There can be many reasons that you might not be told for several days – for example, the team may feel it needs to interview several other people.
Definitely Prepare:
Yahoo, almost as a rule, asks questions about system design, so make sure you prepare for that. They want to know that you can not only write code, but that you can design software. Don’t worry if you don’t have a background in this - you can still reason your way through it!
What’s Unique:
Your phone interview will likely be performed by someone with more influence, such as a hiring manager.
Yahoo is also unusual in that it often gives a decision (if you’re hired) on the same day. Your interviewers will discuss your performance while you meet with a final interviewer.
13 Cracking the Coding Interview
Interview War Stories
The View from the Other Side of the Front, by Peter Bailey
For the eager candidate getting ready for a big job interview, Cracking the Coding Interview is an invaluable reference, containing excellent coaching and practice material that gives you an inside edge on the interview process. However, as you go over your old data structures textbook and drill yourself with homemade discrete math flash cards, don’t make the mistake of thinking of the interview as a kind of high-pressure game show – that if you just give all the right answers to the tech questions, you too can win a shiny new career (this week, on Who Wants to be a Software Engineer?)
While the technical questions on computer science obviously are very important, the most important interview question is not covered in this guidebook. In fact, it’s often the single most important question in your interviewers' minds as they grill you in that little room. Despite the questions on polymorphism and heaps and virtual machines, the question they really want an answer to is ...
Would I have a beer with this guy?
Don’t look at me like that, I'm serious! Well, I may be embellishing a little, but hear me out. The point I'm trying to make is that interviewers, especially those that you might work with, are probably just as anxious as you are. Nonsense, you say, as a nervous young professional, checking your pants for lint while you bite your fingernails, waiting for the interview team to show up in the front lobby. After all, this is the big leagues, and these guys are just waiting for you to slip up so they can rip you apart, laugh at your shriveled corpse, and grind your career dreams to dust beneath the heels of their boots.
Right? Just like pledge week, back in freshman year? Right? Hmmm?
Nothing could be further from the truth. The team of developers and managers interviewing you have their own tasks and projects waiting for them, back at their own desks. Believe me, they’re hoping that every interview is going to be the last one. They'd rather be doing anything else. There might be a batch of upcoming projects looming on their calendar, and they need more manpower if they’re going to even have a prayer of making their deadline. But the last guy the agency sent over was a complete flake who railed about Microsoft’s evil for half an hour. And the one before that couldn’t code his way out of a wet paper bag without using copy-and-paste. Sheesh, they think, where is HR getting these guys? How hard can it be to hire one lousy person?
While they may not literally be asking themselves “Would I have a beer with this guy (or gal)”, they are looking to see how well you would fit in with the team, and how you would affect team chemistry. If they hire you, you’re all going to be spending a lot of time together for
. CareerCup com 14
Interview War Stories
the next few months or years, and they want to know that they can rely on you – and maybe even come to consider you a friend and colleague. They want to know that they can depend on you. And as tempting as it might be to them to just settle and hire the next person who comes along, they know better.
In many companies, particularly large U.S. companies, it’s harder to fire somebody than it is to hire somebody. (Welcome to the US: Land of Lawsuits!) If they hire a dud, they’re stuck with them. That person might be unproductive or, even worse, a drain on the team’s productivity. So they keep interviewing, until they find the right person. They know that it’s better to reject a good candidate than hire a bad one.
Some of those interviews are real doozies. Once you’ve interviewed long enough, you build up a repertoire of horror stories. War stories, of candidates who looked promising on paper until the interviews went terribly, terribly wrong. These war stories are not only humorous – they’re also instructive.
Names have been changed to protect the innocent – or downright ridiculous.
2.1 zyxwvutsrqponmlkjihgfedcba
2.2 ZYXWVUTSRQPONMLKJIHGFEDCBA
2.3 spw~~kjlslen
2.4 0987654321+_=-)(*&^%$#@!`~[]{};':”,./<>?
2.5 ABCDEZYXW
2.6 abcdeyxw
2.7 asdabcdezyxwasdf
2.8 ~~
15 Cracking the Coding Interview
Interview War Stories | Pop Divas
Pop Divas Need Not Apply
Leonard was a very promising C++ coder, three years out of college, with a solid work history and an impressive skill set. He proved on the phone screen that he was above-average technically, and so he was invited in for an interview. We needed a savvy C++ person to work on a piece of middleware that interfaced with our database, and Leonard seemed like a sure fit.
However, once we started talking to him, things went south in a hurry. He spent most of the interview criticizing every tool and platform that we questioned him on. We used SQL Server as our database? Puhleease. We were planning to switch to Oracle soon, right? What’s that? Our team used Tool A to do all our coding in? Unacceptable. He used Tool B, and only Tool B, and after he was hired, we'd all have to switch to Tool B. And we'd have to switch to Java, because he really wanted to work with Java, despite the fact that 75 percent of the codebase would have to be rewritten. We'd thank him later. And oh, by the way, he wouldn’t be making any meetings before ten o'clock.
Needless to say, we encouraged Leonard to seek opportunities elsewhere. It wasn’t that his ideas were bad – in fact, he was “technically” right about many things, and his (strong) opinions were all backed with solid fact and sound reason (except for the ten o'clock thing – we think he may have just been making a “power play”.) But it was obvious that, if hired, Leonard wasn’t going to play well with others – he would have been toxic kryptonite for team chemistry. He actually managed to offend two of the team members during the forty-five minutes of his interview. Leonard also made the mistake of assuming that Code Purity and Algorithm Beauty were always more important than a business deadline.
In the real world, there are always compromises to be made, and knowing how to work with the business analysts is just as important as knowing how to refactor a blob of code. If Leonard would not have gotten along with other IT people, he definitely wouldn’t have gotten along with the business folks. Maybe you can get away with hiring a Leonard if he’s one of the best ten coders in the world (he wasn’t). But he was the classic failure example for the “Would you have a beer with this guy?” test.
. CareerCup com 16
Interview War Stories | Failure to Communicate
What We Have Here is Failure to Communicate
Trisha was a mid-level Java developer with a solid history of middleware and JSP work on her resume. Since she was local, we invited her in for an interview without a phone screen. When we started asking her questions, it quickly became obvious that Trisha was a woman of few words. Her answers were short and often composed of “yes/no” responses, even to questions that were meant to start a dialog. Once she did start opening up, I still wasn’t sure she was actually talking. I saw her lips moving, and heard mumbling sounds coming out, but it wasn’t anything that sounded like English.
I'm not sure if Trisha was nervous or just shy, but either way, I had to ask her numerous times to repeat herself. Now I was the one getting nervous! I didn’t want to be the guy who “ruined” the interview, so I pulled back on my questions. The other folks in the room and I exchanged uneasy glances. We felt like we were on a Seinfeld episode. It was almost impossible to understand Trisha, and when she did speak up, her halting, uncertain, confused speech patterns made us feel more like code breakers than interviewers. I am not exaggerating to say that I did not understand a single answer she gave during the interview.
Knowing, alone, isn’t good enough. You’re going to be talking with other technical people, and you’re going to be talking to customers, and sales reps, and Betty from Marketing. You will write something eventually, whether it’s documentation, or a project plan, or a requirements document. The word processor might correct your spelling, but it won’t correct your lousy writing. The ability to communicate thoughts and ideas, in a clear, concise manner, is an absolutely invaluable skill that employers seek.
The same goes for verbal communication. I used to work with a co-worker who doubled the length of every meeting he was in, because he could not answer a question in less than ten minutes. “Hey, Dennis, what time is it?” “Well, that’s kind of interesting, because I just happened to be reading an article on cesium clocks and leap seconds and the history of the Gregorian Calendar and ... ”
I'll spare you the rest.
17 Cracking the Coding Interview
Interview War Stories | You Can (Maybe) Count On Me
You Can Count on Me, Just Not Until Early Afternoon
Ahhh, 1999. The crest of the dot-com bubble, and the tightest labor market in history. Our company was racing to expand its development team, and we would have hired a German Shepherd if it knew HTML. Instead, we wound up hiring Ian. We should’ve hired the dog.
Ian was a cheerful, friendly guy who had a gift of natural charisma. He got along fantastically with all of the interviewers, and seemed very intelligent. Skill-wise, he was adequate. He hadn’t written a single line of computer code outside of his college courses, and didn’t even have his own e-mail address. When we gave Ian the chance to ask us questions at the end of the interview, he asked about flexible work hours, and how soon he could take vacation time. Instead of showing an interest in the career opportunities, or in company’s growth prospects, he asked whether he could take the all-you-could-drink break room soda home with him. The questions grew more bizarre from there.
Ian was very interested in our Legal Assistance benefit. He wanted to know if it covered the cost of filing lawsuits, if it covered him if he got sued himself, if it applied to any lawsuits he currently was involved in, and if he could “theoretically” use it to sue the company itself. He also asked us if he could use it to help him “fix” some unpaid speeding tickets.
In any other year, that should have been it for Ian right there. But, in 1999, we were hiring anybody who was even remotely competent. Ian collected paychecks from us for eighteen months, and he was about as productive as a traffic cone. He usually sauntered into the office around ten-thirty with some sort of lame excuse (by my count, he had to wait for the cable guy sixteen times in a six-month period). He usually killed the morning by answering e-mail and playing ping-pong, before breaking for a two-hour lunch. After lunch, it was more ping-pong, and maybe an hour of writing bad code, before bolting the office sometime around three. He was the dictionary definition of unreliable.
Remember, your potential future team members need to know that they can rely on you. And they need to know that you won’t need constant supervision and hand-holding. They need to know that you’re able to figure things out on your own. One of the most important messages that you, as a candidate, can convey in your interview is hiring me will make your lives easier. In fact, this is a large part of the reason for the famously difficult interview questions at places like Amazon and Google; if you can handle that kind of unpredictable pressure in an interview, then you stand a good chance of being useful to them on real projects.
To cite a more subtle example, once I was on a four person team that was desperately trying to recruit new members to help work on an old pile of software. It was a real mess; we'd inherited a nasty ball of spaghetti, and we needed people who could jump in, figure things out, and be part of the solution.
There was one very smart fellow, Terry, who would have been a great asset for our team – but we didn’t hire him, despite his excellent technical and personal skills. It was because he
. CareerCup com 18
Interview War Stories | You Can (Maybe) Count On Me
insisted on meticulous written instructions for every step of the coding process. He wasn’t going to make a suggestion or take any initiative – or blow his nose, for that matter – without a mile-long audit trail and a dozen signatures. While he insisted that he worked that way for reasons of quality (a defensible point), we got the impression that it had more to do with butt-covering, and we simply didn’t have the time for that kind of bureaucracy. Terry would have been an excellent fit in a government or aerospace IT department, something that required ISO 9000 procedures. But he would have never fit into our team; he would have been a burden, not an asset.
19 Cracking the Coding Interview
Interview War Stories | Spider Senses
My Spider Senses are Tingling
I can think of lots of interviews that just fell into the general category of weird and uncomfortable:
»»The Java coder who apparently considered hygiene optional, and had the interview room smelling like week-old blue cheese within ten minutes (my eyes were watering).
»»The young fresh-out-of-college graduate with a tongue piercing that kept tick-tick-ticking against his teeth as he talked (after half an hour, it was like Chinese water torture).
»»The girl who wore an iPod through her interview, with the volume turned loud enough that she actually had to ask the interviewers to repeat themselves a few times.
»»The poor, hyper-nervous fellow who was sweating like a marathon runner for half an hour.
»»The girl who wore a T-shirt with an obscene political slogan to her interview.
»»The guy who asked (seriously) at the end of his interview, “So, are there any hot chicks in our department?”
Those are the interviews where we politely thank the people for their time, shake their hand (except for the sweaty guy), then turn to each other after the door closes and ask – did that really just happen?
Nobody is saying that you have to be a bland, boring robot in a Brooks Brothers suit and tie. Remember, the interview team wants you to be “the one”, but they’re also very worried about the possibility that you’re going to be more of a distraction than an asset. Don’t talk or behave in a way that will set off their early warning radar. Whether or not somebody bothers to behave professionally during an interview is often a very good indicator of what kind of teammate they’re going to be.
Rudimentary social skills are part of the answer to “Would I have a beer with this guy?”, or at least, “Will I mind working next to this guy for six months?” From the interviewer’s point of view, they’re picking a neighbor that they’re going to live and work with 200 hours per month for foreseeable future. Would you really want a neighbor that smelled like a hog rendering plant?
Before the Interview
21 Cracking the Coding Interview
Before the Interview | Resume Advice
What Resume Screeners Look For
Resume screeners look for the same things that interviewers do:
»»Are you smart?
»»Can you code?
That means that you should present your resume to show those two things. Your love of tennis, traveling, or magic cards won’t do much to show that, so it’s likely just wasting space.
Keep in mind that recruiters only spend a fixed amount of time (about 20 seconds) looking at your resume. If you limit the content to the best, most impressive, most relevant items, they’ll jump out at the recruiter. Weak items only dilute your resume and distract the recruiter from what you’d like them to see.
Employment History
Relevant Jobs: Your resume does not - and should not - include a full history of every role you’ve ever had. Your job serving ice cream, for example, will not show that you’re smart or that you can code. Include only the relevant things.
Writing Strong Bullets: For each role, try to discuss your accomplishments with the following approach: “Accomplished X by implementing Y which led to Z.” Here’s an example:
»»“Reduced object rendering time by 75% by applying Floyd’s algorithm, leading to a 10% reduction in system boot time.”
Here’s another example with an alternate wording:
»»“Increased average match accuracy from 1.2 to 1.5 by implementing a new comparison algorithm based on windiff.”
Not everything you did will fit into this approach, but the principle is the
Got some extra time to prepare?
If you have at least a couple months before an interview (or if you’re in school and not graduating yet), you may be able to improve your resume.
Go out and get project experience! Take course that have major projects. Get involved in open source. Ask a professor if there is any research you can get involved in, or ask if he/she can sponsor you on an independent study.
This will put you in a better position to have your resume selected down the road. It will also give you lots of things to talk about in an interview.
. CareerCup com 22
Before the Interview | Resume Advice
same: show what you did, how you did it, and what the results were. Ideally, you should try to make the results “measurable” somehow.
Projects
Almost every candidate has some projects, even if they’re just academic projects. List them on your resume! I recommend putting a section called “Projects” on your resume and list your 2 - 4 most significant projects. State what the project was, which languages or technologies it employed, and whether it was an individual or a team project. If your project was not for a course, that’s even better! It shows passion, initiative, and work ethic. You can state the type of project by listing course projects as “Course Project” and your independent projects as “Independent Projects” (or some other wording).
Programming Languages and Software
Software: Generally speaking, I do not recommend listing that you’re familiar with Microsoft Office. Everyone is, and it just dilutes the “real” information. Familiarity with developer-specific or highly technical software (e.g., Visual Studio, Eclipse, Linux) can be useful, but it often doesn’t make much of a difference.
Languages: Knowing which languages to list on your resume is always a tricky thing. Do you list everything you’ve ever worked with? Or only the ones that you’re more comfortable with (even though that might only be one or two languages)? I recommend the following compromise: list most languages you’ve used, but add your experience level. This approach is shown below:
»»“Languages: Java (expert), C++ (proficient), JavaScript (prior experience), C (prior experience)”
Advice for Non-Native English Speakers and Internationals
Proofreading: Some companies will throw out your resume just because of a typo. Please get at least one native English speaker to proofread your resume.
Personal Information: For US positions, do not include age, marital status, or nationality. This sort of personal information is not appreciated by companies, as it creates a legal liability for them. However, you may want to include your current work authorization / visa status, particularly when applying to smaller companies who may be unable to sponsor candidates.
23 Cracking the Coding Interview
Before the Interview | Behavioral Preparation
Why Are Behavioral Questions Asked?
Behavioral questions are asked for a variety of reasons. They can be asked either to get to know your personality, to more deeply understand your resume, or just to ease you into an interview. Either way, these questions are important and can be prepared for.
How To Prepare
Behavioral questions are usually of the form “tell me about a time when you ... ”, and may ask for an example from a specific project or position. I recommend filling in the following “preparation grid” as shown below:
Project 1
Project 2
Project 3
Project 4
Most Challenging
What You Learned
Most Interesting
Hardest Bug
Enjoyed Most
Conflicts with Teammates
Along the top, as columns, you should list all the major aspects of your resume – e.g., your projects, jobs, or activities. Along the side, as rows, you should list the common questions – e.g., what you enjoyed most, what you enjoyed least, what you considered most challenging, what you learned, what the hardest bug was, etc. In each cell, put the corresponding story.
We recommend reducing each story to just a couple keywords that you can write in each cell. This will make the grid easier to study.
In your interview, when you’re asked about a project, you’ll be able to come up with an appropriate story effortlessly. Study this grid before your interview.
NOTE: If you’re doing a phone interview, you may want to have this grid out in front of you.
Some additional advice:
1. When asked about your weaknesses, give a real weakness! Answers like “My greatest weakness is that I work too hard / am a perfectionist / etc” tell your interviewer that you’re arrogant and/or won’t admit to your faults. No one wants to work with someone like that. A better answer conveys a real, legitimate weakness but emphasizes how you work to overcome it. For example: “Sometimes, I don’t have a very good attention to detail. While that’s good because it lets me execute quickly, it also means that I sometimes make careless mistakes. Because of that, I make sure to always have someone else double check my work.”
. CareerCup com 24
Before the Interview | Behavioral Preparation
2. When asked what the most challenging part was, don’t say “I had to learn a lot of new languages and technologies.” This is the “cop out” answer (e.g., you don’t know what else to say). It tells the interviewer that nothing was really that hard.
3. Remember: you’re not just answering their questions, you’re telling them about yourself! Many people try to just answer the questions. Think more deeply about what each story communicates about you.
4. If you think you’ll be asked behavioral questions (e.g., “tell me about a challenging interaction with a team member”), you should create a Behavioral Preparation Grid. This is the same as the one above, but the left side contains things like “challenging interaction”, “failure”, “success”, and “influencing people.”
What questions should you ask the interviewer?
Most interviewers will give you a chance to ask them questions. The quality of your questions will be a factor, whether subconsciously or consciously, in their decisions.
Some questions may come to you during the interview, but you can - and should - prepare questions in advance. Doing research on the company or team may help you with preparing questions.
Questions can be divided into three different categories:
Genuine Questions: These are the questions you actually want to know. Here are a few ideas of questions that are valuable to many candidates:
1. “How much of your day do you spend coding?”
2. “How many meetings do you have every week?”
3. “What is the ratio of testers to developers to product managers? What is the interaction like? How does project planning happen on the team?”
Insightful Questions: These questions are designed to demonstrate your deep knowledge of programming or technologies.
1. “I noticed that you use technology X. How do you handle problem Y?”
2. “Why did the product choose to use the X protocol over the Y protocol? I know it has benefits like A, B, C, but many companies choose not to use it because of issue D.”
Passion Questions: These questions are designed to demonstrate your passion for technology.
1. “I’m very interested in scalability. Did you come in with a background in this, or what opportunities are there to learn about it?”
2. “I’m not familiar with technology X, but it sounds like a very interesting solution. Could you tell me a bit more about how it works?”
25 Cracking the Coding Interview
Before the Interview | Technical Preparation
How to Prepare for Technical Questions
You’ve purchased this book, so you’ve already gone a long way towards good preparation. Nice work!
That said, there are better and worse ways to prepare. Many candidates just read through problems and solutions. Don’t do that! Memorizing or trying to learn specific questions won’t help you! Rather, do this:
1. Try to solve the problem on your own. I mean, really try to solve it. Many questions are designed to be tough - that’s ok! When you’re solving a problem, make sure to think about the space and time efficiency. Ask yourself if you could improve the time efficiency by reducing the space efficiency, or vice versa.
2. Write the code for the algorithm on paper. You’ve been coding all your life on a computer, and you’ve gotten used to the many nice things about it. But, in your interview, you won’t have the luxury of syntax highlighting, code completion, or compiling. Mimic this situation by coding on paper.
3. Type your paper code as-is into a computer. You’ll probably have made a bunch of mistakes. Start a list of all the mistakes you made, so that you can keep these in mind in the real interview.
4. Do a mock interview. CareerCup offers a mock interview service, or you can grab a friend to ask you questions. Though your friend may not be an expert interviewer, he or she may still be able to walk you through a coding or algorithm question.
. CareerCup com 26
Before the Interview | Technical Preparation
What You Need To Know
Most interviewers won’t ask about specific algorithms for binary tree balancing or other complex algorithms. Frankly, they probably don’t remember these algorithms either.
You’re usually only expected to know the basics. Here’s a list of the absolute must-have knowledge:
Data Structures
Algorithms
Concepts
Linked Lists
Breadth First Search
Bit Manipulation
Binary Trees
Depth First Search
Singleton Design Pattern
Tries
Binary Search
Factory Design Pattern
Stacks
Merge Sort
Memory (Stack vs Heap)
Queues
Quick Sort
Recursion
Vectors / ArrayLists
Tree Insert / Find / etc
Big-O Time
Hash Tables
This is not, of course, an all-inclusive list. Questions may be asked on areas outside of these topics. This is merely a “must know” list.
For each of the topics, make sure you understand how to implement / use them, and (where applicable) the space and time complexity.
Practicing implementing the data structures and algorithms. You might be asked to implement them in your interview, or you might be asked to implement a modification of them. Either way, the more comfortable you are with implementations the better.
Do you need to know details of C++, Java, etc?
While I personally never liked asking these sorts of questions (e.g., “what is a vtable?”), many interviewers regretfully do ask them. For big companies like Microsoft, Google, Amazon, etc, I wouldn’t stress too much about these questions. Look up the most common questions and make sure you have answers to them, but I would focus on data structures and algorithms preparation.
At smaller companies, or non-software companies, these questions can be more important. Look up your company on CareerCup.com to decide for yourself. If your company isn’t listed, look up a similar company as a reference.

The Interview and Beyond
29 Cracking the Coding Interview
At the Interview | Handling Behavioral Questions
Why Behavioral Questions
As stated earlier, interviews usually start and end with “chit chat” or “soft skills.” This is a time to answer questions about your resume or general questions, and also an opportunity for you to ask questions. This part of the interview is targeted not only at getting to know you, but also at relaxing you.
Be Specific, Not Arrogant
Arrogance is a red flag, but you still want to make yourself sound impressive. So how do you make yourself sound good without being arrogant? By being specific!
Specificity means giving just the facts and letting the interviewer derive an interpretation. Consider an example:
»»Candidate #1: “I basically did all the hard work for the team.”
»»Candidate #2: “I implemented the file system, which was considered one of the most challenging components because …”
Candidate #2 not only sounds more impressive, but she also appears less arrogant.
Limit Details
When a candidate blabbers on about a problem, it’s hard for an interviewer who isn’t well versed in the subject or project to understand it. CareerCup recommends that you stay light on details and just state the key points. That is, consider something like this: “By examining the most common user behavior and applying the Rabin-Karp algorithm, I designed a new algorithm to reduce search from O(n) to O(log n) in 90% of cases. I can go into more details if you’d like.” This demonstrates the key points while letting your interviewer ask for more details if he wants to.
Ask Good Questions
Remember those questions you came up with while preparing? Now is a great time to use them!
Structure Answers Using S.A.R.
Structure your responses using S.A.R.: Situation, Action, Response. That is, you should start off outlining the situation, then explaining the actions you took, and lastly, describing the result.
Example: “Tell me about a challenging interaction with a teammate.”
» »Situation: On my operating systems project, I was assigned to work with three other
. CareerCup com 30
At the Interview | Handling Behavioral Questions
people. While two were great, the third team member didn’t contribute much. He stayed quiet during meetings, rarely chipped in during email discussions, and struggled to complete his components.
» »Action: One day after class, I pulled him aside to speak about the course and then moved the discussion into talking about the project. I asked him open-ended questions on how he felt it was going, and which components he was excited about tackling. He suggested all the easiest components, and yet offered to do the write-up. I realized then that he wasn’t lazy – he was actually just really confused about the project and lacked confidence. I worked with him after that to break down the components into smaller pieces, and I made sure to complement him a lot on his work to boost his confidence.
» »Result: He was still the weakest member of the team, but he got a lot better. He was able to finish all his work on time, and he contributing more in discussions. We were happy to work with him on a future project.
As you can see, the SAR model helps an interviewer clearly see what you did in a certain situation and what the result was.
31 Cracking the Coding Interview
At the Interview | Handling Technical Questions
General Advice for Technical Questions
Interviews are supposed to be difficult. If you don’t get every – or any – answer immediately, that’s ok! In fact, in my experience, maybe only 10 people out of the 120+ that I’ve interviewed have gotten the question right instantly.
So when you get a hard question, don’t panic. Just start talking aloud about how you would solve it.
And, one more thing: you’re not done until the interviewer says that you’re done! What I mean here is that when you come up with an algorithm, start thinking about the problems accompanying it. When you write code, start trying to find bugs. If you’re anything like the other 110 candidates that I’ve interviewed, you probably made some mistakes.
Five Steps to a Technical Questions
A technical interview question can be solved utilizing a five step approach:
1. Ask your interviewer questions to resolve ambiguity.
2. Design an Algorithm
3. Write pseudo-code first, but make sure to tell your interviewer that you’re writing pseudo-code! Otherwise, he/she may think that you’re never planning to write “real” code, and many interviewers will hold that against you.
4. Write your code, not too slow and not too fast.
5. Test your code and carefully fix any mistakes.
Step 1: Ask Questions
Technical problems are more ambiguous than they might appear, so make sure to ask questions to resolve anything that might be unclear or ambiguous. You may eventually wind up with a very different – or much easier – problem than you had initially thought. In fact, many interviewers (especially at Microsoft) will specifically test to see if you ask good questions.
Good questions might be things like: What are the data types? How much data is there? What assumptions do you need to solve the problem? Who is the user?
Example: “Design an algorithm to sort a list.”
»»Question: What sort of list? An array? A linked list?
»»Answer: An array.
»»Question: What does the array hold? Numbers? Characters? Strings?
»»Answer: Numbers.
. CareerCup com 32
At the Interview | Handling Technical Questions
»»Question: And are the numbers integers?
»»Answer: Yes.
»»Question: Where did the numbers come from? Are they IDs? Values of something?
»»Answer: They are the ages of customers.
»»Question: And how many customers are there?
»»Answer: About a million.
We now have a pretty different problem: sort an array containing a million integers between 0 and 130. How do we solve this? Just create an array with 130 elements and count the number of ages at each value.
Step 2: Design an Algorithm
Designing an algorithm can be tough, but our five approaches to algorithms can help you out (see pg 34). While you’re designing your algorithm, don’t forget to think about:
»»What are the space and time complexities?
»»What happens if there is a lot of data?
»»Does your design cause other issues? (i.e., if you’re creating a modified version of a binary search tree, did your design impact the time for insert / find / delete?)
»»If there are other issues, did you make the right trade-offs?
»»If they gave you specific data (e.g., mentioned that the data is ages, or in sorted order), have you leveraged that information? There’s probably a reason that you’re given it.
Step 3: Pseudo-Code
Writing pseudo-code first can help you outline your thoughts clearly and reduce the number of mistakes you commit. But, make sure to tell your interviewer that you’re writing pseudo-code first and that you’ll follow it up with “real” code. Many candidates will write pseudo-code in order to ‘escape’ writing real code, and you certainly don’t want to be confused with those candidates.
Step 4: Code
You don’t need to rush through your code; in fact, this will most likely hurt you. Just go at a nice, slow methodical pace. Also, remember this advice:
»»Use Data Structures Generously: Where relevant, use a good data structure or define your own. For example, if you’re asked a problem involving finding the minimum age for a group of people, consider defining a data structure to represent a Person. This
33 Cracking the Coding Interview
At the Interview | Handling Technical Questions
shows your interviewer that you care about good object oriented design.
»»Don’t Crowd Your Coding: This is a minor thing, but it can really help. When you’re writing code on a whiteboard, start in the upper left hand corner – not in the middle. This will give you plenty of space to write your answer.
Step 5: Test
Yes, you need to test your code! Consider testing for:
»»Extreme cases: 0, negative, null, maximums, etc
»»User error: What happens if the user passes in null or a negative value?
»»General cases: Test the normal case.
If the algorithm is complicated or highly numerical (bit shifting, arithmetic, etc), consider testing while you’re writing the code rather than just at the end.
Also, when you find mistakes (which you will), carefully think through why the bug is occuring. One of the worst things I saw while interviewing was candidates who recognized a mistake and tried making “random” changes to fix the error.
For example, imagine a candidate writes a function that returns a number. When he tests his code with the number ‘5’ he notices that it returns 0 when it should be returning 1. So, he changes the last line from “return ans” to “return ans+1,” without thinking through why this would resolve the issue. Not only does this look bad, but it also sends the candidate on an endless string of bugs and bug fixes.
When you notice problems in your code, really think deeply about why your code failed before fixing the mistake.
. CareerCup com 34
At the Interview | Five Algorithm Approaches
Five Algorithm Approaches
There’s no sure fire approach to solving a tricky algorithm problem, but the approaches below can be useful. Keep in mind that the more problems you practice, the easier it will to identify which approach to use.
Also, remember that the five approaches can be “mixed and matched.” That is, once you’ve applied “Simplify & Generalize”, you may want to implement Pattern Matching next.
APPROACH I: EXAMPLIFY
Description: Write out specific examples of the problem, and see if you can figure out a general rule.
Example: Given a time, calculate the angle between the hour and minute hands.
Approach: Start with an example like 3:27. We can draw a picture of a clock by selecting where the 3 hour hand is and where the 27 minute hand is.
By playing around with these examples, we can develop a rule:
»»Minute angle (from 12 o’clock): 360 * minutes / 60
»»Hour angle (from 12 o’clock): 360 * (hour % 12) / 12 + 360 * (minutes / 60) * (1 / 12)
»»Angle between hour and minute: (hour angle - minute angle) % 360
By simple arithmetic, this reduces to 30 * hours - 5.5 * minutes.
APPROACH II: PATTERN MATCHING
Description: Consider what problems the algorithm is similar to, and figure out if you can modify the solution to develop an algorithm for this problem.
Example: A sorted array has been rotated so that the elements might appear in the order 3 4 5 6 7 1 2. How would you find the minimum element?
Similar Problems:
»»Find the minimum element in an array.
»»Find a particular element in an array (eg, binary search).
Algorithm:
Finding the minimum element in an array isn’t a particularly interesting algorithm (you could just iterate through all the elements), nor does it use the information provided (that the array is sorted). It’s unlikely to be useful here.
However, binary search is very applicable. You know that the array is sorted, but rotated. So, it must proceed in an increasing order, then reset and increase again. The minimum element is the “reset” point.
35 Cracking the Coding Interview
At the Interview | Five Algorithm Approaches
If you compare the first and middle element (3 and 6), you know that the range is still increasing. This means that the reset point must be after the 6 (or, 3 is the minimum element and the array was never rotated). We can continue to apply the lessons from binary search to pinpoint this reset point, by looking for ranges where LEFT > RIGHT. That is, for a particular point, if LEFT < RIGHT, then the range does not contain the reset. If LEFT > RIGHT, then it does.
APPROACH III: SIMPLIFY & GENERALIZE
Description: Change a constraint (data type, size, etc) to simplify the problem. Then try to solve it. Once you have an algorithm for the “simplified” problem, generalize the problem again.
Example: A ransom note can be formed by cutting words out of a magazine to form a new sentence. How would you figure out if a ransom note (string) can be formed from a given magazine (string)?
Simplification: Instead of solving the problem with words, solve it with characters. That is, imagine we are cutting characters out of a magazine to form a ransom note.
Algorithm:
We can solve the simplified ransom note problem with characters by simply creating an array and counting the characters. Each spot in the array corresponds to one letter. First, we count the number of times each character in the ransom note appears, and then we go through the magazine to see if we have all of those characters.
When we generalize the algorithm, we do a very similar thing. This time, rather than creating an array with character counts, we create a hash table. Each word maps to the number of times the word appears.
APPROACH IV: BASE CASE AND BUILD
Description: Solve the algorithm first for a base case (e.g., just one element). Then, try to solve it for elements one and two, assuming that you have the answer for element one. Then, try to solve it for elements one, two and three, assuming that you have the answer to elements one and two.
Example: Design an algorithm to print all permutations of a string. For simplicity, assume all characters are unique.
Test String: abcdefg
Case “a” --> {a}
Case “ab” --> {ab, ba}
Case “abc” --> ?
This is the first “interesting” case. If we had the answer to P(“ab”), how could we generate P(“abc”). Well, the additional letter is “c”, so we can just stick c in at every possible point. That
. CareerCup com 36
At the Interview | Five Algorithm Approaches
is:
merge(c, ab) --> cab, acb, abc
merge(c, ba) --> cba, bca, bac
Algorithm: Use a recursive algorithm. Generate all permutations of a string by “chopping off” the last character and generating all permutations of s[1… n-1]. Then, insert s[n] into every location of the string.
NOTE: Base Case and Build Algorithms often lead to natural recursive algorithms.
APPROACH V: DATA STRUCTURE BRAINSTORM
Description: This is hacky, but it often works. Simply run through a list of data structures and try to apply each one.
Example: Numbers are randomly generated and stored into an (expanding) array. How would you keep track of the median?
Data Structure Brainstorm:
»»Linked list? Probably not – linked lists tend not to do very well with accessing and sorting numbers.
»»Array? Maybe, but you already have an array. Could you somehow keep the elements sorted? That’s probably expensive. Let’s hold off on this and return to it if it’s needed.
»»Binary tree? This is possible, since binary trees do fairly well with ordering. In fact, if the binary search tree is perfectly balanced, the top might be the median. But, be careful – if there’s an even number of elements, the median is actually the average of the middle two elements. The middle two elements can’t both be at the top. This is probably a workable algorithm, but let’s come back to it.
»»Heap? A heap is really good at basic ordering and keeping track of max and mins. This is actually interesting – if you had two heaps, you could keep track of the biggest half and the smallest half of the elements. The biggest half is kept in a min heap, such that the smallest element in the biggest half is at the root. The smallest half is kept in a max heap, such that the biggest element of the smallest half is at the root. Now, with these data structures, you have the potential median elements at the roots. If the heaps are no longer the same size, you can quickly “rebalance” the heaps by popping an element off the one heap and pushing it onto the other.
Note that the more problems you do, the better instinct you will develop about which data structure to apply.
37 Cracking the Coding Interview
At the Interview | The Offer and Beyond
Congrats! You got the offer!
If you’re lucky enough to get an offer (and you will be!), congratulations! You may now be stressing over which offer to accept and all that fun stuff, so just remember that most likely, all of your options are great and you’ll be happy at any of them.
As far as which offer to take, well, we could tell you that money isn’t that important and blah blah blah… but we’ll skip over that and let you make your own decision about the importance of money. We have some other advice for you.
Negotiating
It’s Always Negotiable! Ok, maybe not always, but usually an offer is negotiable even if a recruiter tells you otherwise. It helps if you have a competing offer. But, don’t lie – Microsoft knows what Google offers, so it just won’t be realistic if you make up numbers. Also, technology is a small world, and people talk. Be honest.
What’s the money like, really?
Think about the full offer package. Many companies will have impressive salaries, but small annual bonuses. Other companies will have huge annual bonuses, but lower salaries. Make sure you look at the full package (salary, signing bonus, health care benefits, raises, annual bonus, relocation, stock, promotions, etc). It’s very confusing, and it’s often not clear which company is offering more.
What about your career options?
Even if money is all that matters, think about the long term career. If you’re lucky enough to have several offers to pick from, consider how each one will impact your long term career. The company with the lowest salary but where you’ll learn the most may just be the best move, even financially.
I can’t give you some magical formula to compute which offer to accept, but here’s what I’d recommend thinking about (in no particular order):
»»Career Path: Make a plan for your career. What do you want to do 5, 10 and 15 years out? What skills will you need to develop? Which company or position will help you get there?
»»Promotion Opportunity: Do you prefer to move into management, or would you prefer to become an increasingly senior developer?
»»Money and Benefits: Of course, the money matters (but if you’re early in your career, it probably doesn’t matter much). As mentioned above, make sure you look at the full package.
. CareerCup com 38
At the Interview | The Offer and Beyond
»»Happiness: Did you like the people? The products? The location? It’s hard to tell, of course, before you work there. What are the options to change teams if you’re unhappy?
»»Brand Name: The company’s brand name can mean a lot for your future career. Some company names will open doors, while others will not as much.
What about company stability? Personally, I think it matters much less than most people think. There are so many software companies out there. If you get laid off and need to find a new job, will it be difficult to find a new one? Only you can answer that.
On the job, and beyond...
Before starting at a company, make a career plan. What would you like your career to look like? What will it take to get there? Make sure you check in on your career plan regularly and are on track.
It’s very easy, particularly at the big companies, to get sucked into staying for a while. They’re great companies with lots of perks, and most people are truly quite happy there. If what you want is to stay an engineer for life, then there is absolutely nothing wrong with that.
However, if you want to run a company one day, or move up into management, you should stop and check your career plan. Is another year at your job going to help you get there? Or is it time to move? You, and only you, can decide.
39 Cracking the Coding Interview
At the Interview | Top Ten Mistakes Candidates Make
#1 | Practicing on a Computer
If you were training for a serious bike race in the mountains, would you practice only by biking on the streets? I hope not. The air is different. The terrain is different. Yeah, I bet you’d practice in the mountains.
Using a compiler to practice interview questions is like this - and you’ve basically been biking on the streets your entire life. Put away the compiler and get out the old pen and paper. Use a compiler only to verify your solutions.
#2 | Not Rehearsing Behavioral Questions
Many candidates spend all their time prepping for technical questions and overlook the behavioral questions. Guess what? Your interviewer is judging those too! And, not only that - your performance on behavioral questions might bias your interviewer’s perception of your technical performance. Behavioral prep is relatively easy and well-worth your time. Looking over your projects and positions and think of the key stories. Rehearse the stories. See pg 29 for more details.
#3 | Not Doing a Mock Interview
Imagine you’re preparing for a big speech. Your whole school, or company, or whatever will be there. Your future depends on this. And all you do to prepare is read the speech to yourself. Silently. In your head. Crazy, right?
Not doing a mock interview to prepare for your real interview is just like this. If you’re an engineer, you must know other engineers. Grab a buddy and ask him/her to do a mock interview for you. You can even return the favor!
#4 | Trying to Memorize Solutions
Quality beats quantity. Try to struggle through and solve questions yourself; don’t flip directly to the solutions when you get stuck. Memorizing how to solve specific problem isn’t going to help you much in an interview. Real preparation is about learning how to approach new problems.
#5 | Talking Too Much
I can’t tell you how many times I’ve asked candidates a simple question like “what was the hardest bug on Project Pod?”, only to have them ramble on and on about things I don’t understand. Five minutes later, when they finally come up for air, I’ve learned nothing - except that they’re a poor communicator. When asked a question, break your answer into three parts (Situation / Action / Response, Issue 1 / Issue 2 / Issue 3, etc) and speak for just a couple sentences about each. If I want more details, I’ll ask!
. CareerCup com 40
At the Interview | Top Ten Mistakes Candidates Make
#6 | Talking Too Little
Psst - let me tell you a secret: I don’t know what’s going on in your head. So if you aren’t talking, I don’t know what you’re thinking. If you don’t talk for a long time, I’ll assume that you aren’t making any progress. Speak up often, and try to talk your way through a solution. This shows your interviewer that you’re tackling the problem and aren’t stuck. And it lets them guide you when you get off-track, helping you get to the answer faster. And it shows your awesome communication skills. What’s not to love?
#7 | Rushing
Coding is not a race, and neither is interviewing. Take your time in a coding problem - don’t rush! Rushing leads to mistakes, and reveals you to be careless. Go slowly and methodically, testing often and thinking through the problem thoroughly. You’ll finish the problem in less time in the end, and with fewer mistakes.
#8 | Not Debugging
Would you ever write code and not run it or test it? I would hope not! So why do that in an interview? When you finish writing code in an interview, “run” (or walk through) the code to test it. Or, on more complicated problems, test the code while writing it.
#9 | Sloppy Coding
Did you know that you can write bug-free code while still doing horribly on a coding question? It’s true! Duplicated code, messy data structures (i.e., lack of object oriented design), etc. Bad, bad, bad! When you write code, imagine you’re writing for real-world maintainability. Break code into sub-routines, and design data structures to link appropriate data.
#10 | Giving Up
Have you ever taken a computer adaptive test? These are tests that give you harder questions the better you do. Take it from me - they’re not fun. Regardless of how well you’re actually doing, you suddenly find yourself stumbling through problems. Yikes!
Interviewing is sort of like this. If you whiz through the easy problems, you’re going to get more and harder problems. Or, the questions might have just started out hard to begin with! Either way, struggling on a question does not mean that you’re doing badly. So don’t give up or get discouraged. You’re doing great!
41 Cracking the Coding Interview
At the Interview | Frequently Asked Questions
Do I have to get every question right?
No. A good interviewer will stretch your mind. They’ll want to see you struggle with a difficult problem. If a candidate is good, they’ll ask harder and tougher questions until he/she is stumped! Thus, if you have trouble on a question, all it means is that the interviewer is doing their job!
Should I tell my interviewer if I know a question?
Yes! You should definitely tell your interviewer if you’ve previously heard the question. This seems silly to some people - if you already know the question (and answer), you could ace the question, right? Not quite.
Here’s why we strongly recommend that you tell your interviewer that you’ve heard the question:
1. Big honesty points. This shows a lot of integrity. That’s huge. Remember that the interviewer is evaluating you as a potential teammate. I don’t know about you, but I personally prefer to work with honest people!
2. The question might have changed ever-so-slightly. You don’t want to risk repeating the wrong answer.
3. If you easily belt out the right answer, it’s obvious to the interviewer. They know how hard a problem is supposed to be. It’s very hard to “pretend” to struggle through a question, because you just can’t approach it the same way other candidates do.
How should I dress?
Generally, candidates should dress one small step above the average employee in their position, or as nice as the nicest dressed employees in their position. In most software firms, this means that jeans (nice jeans with no holes) or slacks with a nice shirt or sweater is fine. In a bank or another more formal institution, avoid jeans and stick with slacks.
What language should I use?
Many people will tell you “whatever language you’re most comfortable with,” but ideally you want to use a language that your interviewer is comfortable with. I’d usually recommend coding in either C, C++ or Java, as the vast majority of interviewers will be comfortable in one of these languages. My personal preference for interviews is Java (unless it’s a question requiring C / C++), because it’s quick to write and almost everyone can read and understand Java, even if they code mostly in C++. (Almost all the solutions in this book are written in Java for this reason.)
I didn’t hear back immediately after my interview. Am I rejected?
. CareerCup com 42
At the Interview | Frequently Asked Questions
Absolutely not. Responses can be held up for a variety of reasons that have nothing to do with a good or bad performance. For example, an interviewer could have gone on vacation right after your interview. A company will always tell you if you’re rejected (or at least I’ve never heard of a company which didn’t).
Can I re-apply to a company after getting rejected?
Almost always, but you typically have to wait a bit (6 months – 1 year). Your first bad interview usually won’t affect you too much when you re-interview. Lots of people got rejected from Google or Microsoft and later got an offer.
How are interview questions selected?
This depends on the company, but any number of ways:
1. Pre-Assigned List of Questions: This is unusual at bigger companies.
2. Assigned Topics: Each interviewer is assigned a specific area to probe, but decides on his/her own questions.
3. Interviewer’s Choice: Each interviewer asks whatever he / she wants. Usually, under this system, the interviewers have a way of tracking which questions were asked to a candidate to ensure a good diversity of questions.
Approach #3 is the most common. This system usually means that interviewers will each have a “stock” set of five or so questions that they ask candidates.
What about experienced candidates?
This depends a lot on the company. On average though, experienced candidates will slightly get more questions about their background, and they might face higher standards when discussing system architecture (if this is relevant to their experience). For the most part though, experienced candidates face much the same process.
Yes, for better or worse, experienced candidate should expect to go through the same coding and algorithm questions. With respect to their performance, they could face either higher standards (because they have more experience) or lower standards (because it’s likely been many years since they worked with certain data structures).
43 Cracking the Coding Interview
Interview Questions
How This Book is Organized?
We have grouped interview questions into categories, with a page preceding each category offering advice and other information. Note that many questions may fall into multiple categories.
Within each category, the questions are sorted by approximate level of difficulty. Solutions for all questions are at the back.
Special Advice for Software Design Engineers in Test (SDETs)
Not only must SDETs master testing, but they also have to be great coders. Thus, we recommend the follow preparation process:
» »Prepare the Core Testing Problems: For example, how would you test a light bulb? A pen? A cash register? Microsoft Word? The Testing Chapter will give you more background on these problems.
» »Practice the Coding Questions: The #1 thing that SDETs get rejected for is coding skills. Make sure that you prepare for all the same coding and algorithm questions that a regular developer would get.
» »Practice Testing the Coding Questions: A very popular format for SDET question is “Write code to do X,” followed up by “OK, now test it.” So, even when the question doesn’t specifically ask for this, you should ask yourself, “how would you test this?” Remember: any problem can be an SDET problem!
Full, Compilable Solutions
For your convenience, you can download the full solutions to the problems at http://www.careercup.com/careercup_book_solutions. This file provides executable code for all the Java solutions. The solutions can be opened and run with Eclipse.
Suggestions and Corrections
While we do our best to ensure that all the solutions are correct, mistakes will be made. Moreover, sometimes there is no “right” answer. If you'd like to offer a suggestion or correction, please submit it at http://xrl.us/ccbook.
Interview Questions

Part 1
Data Structures
Chapter 1 | Arrays and Strings
Cracking the Coding Interview | Data Structures
47
Hash Tables
While not all problems can be solved with hash tables, a shocking number of interview problems can be. Before your interview, make sure to practice both using and implementing hash tables.
1 public HashMap<Integer, Student> buildMap(Student[] students) {
2 HashMap<Integer, Student> map = new HashMap<Integer, Student>();
3 for (Student s : students) map.put(s.getId(), s);
4 return map;
5 }
ArrayList (Dynamically Resizing Array):
An ArrayList, or a dynamically resizing array, is an array that resizes itself as needed while still providing O(1) access. A typical implementation is that when a vector is full, the array doubles in size. Each doubling takes O(n) time, but happens so rarely that its amortized time is still O(1).
1 public ArrayList<String> merge(String[] words, String[] more) {
2 ArrayList<String> sentence = new ArrayList<String>();
3 for (String w : words) sentence.add(w);
4 for (String w : more) sentence.add(w);
5 return sentence;
6 }
StringBuffer / StringBuilder
Question: What is the running time of this code?
1 public String makeSentence(String[] words) {
2 StringBuffer sentence = new StringBuffer();
3 for (String w : words) sentence.append(w);
4 return sentence.toString();
5 }
Answer: O(n^2), where n is the number of letters in sentence. Here’s why: each time you append a string to sentence, you create a copy of sentence and run through all the letters in sentence to copy them over. If you have to iterate through up to n characters each time in the loop, and you’re looping at least n times, that gives you an O(n^2) run time. Ouch!
With StringBuffer (or StringBuilder) can help you avoid this problem.
1 public String makeSentence(String[] words) {
2 StringBuffer sentence = new StringBuffer();
3 for (String w : words) sentence.append(w);
4 return sentence.toString();
5 }
Chapter 1 | Arrays and Strings
48
CareerCup.com
1.1 Implement an algorithm to determine if a string has all unique characters. What if you can not use additional data structures?
_
_________________________________________________________________pg 95
1.2 Write code to reverse a C-Style String. (C-String means that “abcd” is represented as five characters, including the null character.)
_
_________________________________________________________________pg 96
1.3 Design an algorithm and write code to remove the duplicate characters in a string without using any additional buffer. NOTE: One or two additional variables are fine. An extra copy of the array is not.
FOLLOW UP
Write the test cases for this method.
_
_________________________________________________________________pg 97
1.4 Write a method to decide if two strings are anagrams or not.
_
_________________________________________________________________pg 99
1.5 Write a method to replace all spaces in a string with ‘%20’.
_
________________________________________________________________pg 100
1.6 Given an image represented by an NxN matrix, where each pixel in the image is 4 bytes, write a method to rotate the image by 90 degrees. Can you do this in place?
_
________________________________________________________________pg 101
1.7 Write an algorithm such that if an element in an MxN matrix is 0, its entire row and column is set to 0.
_
________________________________________________________________pg 102
1.8 Assume you have a method isSubstring which checks if one word is a substring of another. Given two strings, s1 and s2, write code to check if s2 is a rotation of s1 using only one call to isSubstring (i.e., “waterbottle” is a rotation of “erbottlewat”).
_
________________________________________________________________pg 103
Chapter 2 | Linked Lists
Cracking the Coding Interview | Data Structures
49
How to Approach:
Linked list questions are extremely common. These can range from simple (delete a node in a linked list) to much more challenging. Either way, we advise you to be extremely comfortable with the easiest questions. Being able to easily manipulate a linked list in the simplest ways will make the tougher linked list questions much less tricky. With that said, we present some “must know” code about linked list manipulation. You should be able to easily write this code yourself prior to your interview.
Creating a Linked List:
NOTE: When you’re discussing a linked list in an interview, make sure to understand whether it is a single linked list or a doubly linked list.
1 class Node {
2 Node next = null;
3 int data;
4 public Node(int d) { data = d; }
5 void appendToTail(int d) {
6 Node end = new Node(d);
7 Node n = this;
8 while (n.next != null) { n = n.next; }
9 n.next = end;
10 }
11 }
Deleting a Node from a Singly Linked List
1 Node deleteNode(Node head, int d) {
2 Node n = head;
3 if (n.data == d) {
4 return head.next; /* moved head */
5 }
6 while (n.next != null) {
7 if (n.next.data == d) {
8 n.next = n.next.next;
9 return head; /* head didn’t change */
10 }
11 n = n.next;
12 }
13 }
Chapter 2 | Linked Lists
50
CareerCup.com
2.1 Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
_
________________________________________________________________pg 105
2.2 Implement an algorithm to find the nth to last element of a singly linked list.
_
________________________________________________________________pg 106
2.3 Implement an algorithm to delete a node in the middle of a single linked list, given only access to that node.
EXAMPLE
Input: the node ‘c’ from the linked list a->b->c->d->e
Result: nothing is returned, but the new linked list looks like a->b->d->e
_
________________________________________________________________pg 107
2.4 You have two numbers represented by a linked list, where each node contains a single digit. The digits are stored in reverse order, such that the 1’s digit is at the head of the list. Write a function that adds the two numbers and returns the sum as a linked list.
EXAMPLE
Input: (3 -> 1 -> 5) + (5 -> 9 -> 2)
Output: 8 -> 0 -> 8
_
________________________________________________________________pg 108
2.5 Given a circular linked list, implement an algorithm which returns node at the beginning of the loop.
DEFINITION
Circular linked list: A (corrupt) linked list in which a node’s next pointer points to an earlier node, so as to make a loop in the linked list.
EXAMPLE
input: A -> B -> C -> D -> E -> C [the same C as earlier]
output: C
_
________________________________________________________________pg 109
Chapter 3 | Stacks and Queues
Cracking the Coding Interview | Data Structures
51
How to Approach:
Whether you are asked to implement a simple stack / queue, or you are asked to implement a modified version of one, you will have a big leg up on other candidates if you can flawlessly work with stacks and queues. Practice makes perfect! Here is some skeleton code for a Stack and Queue class.
Implementing a Stack
1 class Stack {
2 Node top;
3 Node pop() {
4 if (top != null) {
5 Object item = top.data;
6 top = top.next;
7 return item;
8 }
9 return null;
10 }
11 void push(Object item) {
12 Node t = new Node(item);
13 t.next = top;
14 top = t;
15 }
16 }
Implementing a Queue
1 class Queue {
2 Node first, last;
3 void enqueue(Object item) {
4 if (!first) {
5 back = new Node(item);
6 first = back;
7 } else {
8 back.next = new Node(item);
9 back = back.next;
10 }
11 }
12 Node dequeue(Node n) {
13 if (front != null) {
14 Object item = front.data;
15 front = front.next;
16 return item;
17 }
18 return null;
19 }
20 }
Chapter 3 | Stacks and Queues
52
CareerCup.com
3.1 Describe how you could use a single array to implement three stacks.
_
________________________________________________________________pg 111
3.2 How would you design a stack which, in addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time.
_
________________________________________________________________pg 113
3.3 Imagine a (literal) stack of plates. If the stack gets too high, it might topple. Therefore, in real life, we would likely start a new stack when the previous stack exceeds some threshold. Implement a data structure SetOfStacks that mimics this. SetOfStacks should be composed of several stacks, and should create a new stack once the previous one exceeds capacity. SetOfStacks.push() and SetOfStacks.pop() should behave identically to a single stack (that is, pop() should return the same values as it would if there were just a single stack).
FOLLOW UP
Implement a function popAt(int index) which performs a pop operation on a specific sub-stack.
_
________________________________________________________________pg 115
3.4 In the classic problem of the Towers of Hanoi, you have 3 rods and N disks of different sizes which can slide onto any tower. The puzzle starts with disks sorted in ascending order of size from top to bottom (e.g., each disk sits on top of an even larger one). You have the following constraints:
(A) Only one disk can be moved at a time.
(B) A disk is slid off the top of one rod onto the next rod.
(C) A disk can only be placed on top of a larger disk.
Write a program to move the disks from the first rod to the last using Stacks.
_
________________________________________________________________pg 118
3.5 Implement a MyQueue class which implements a queue using two stacks.
_
________________________________________________________________pg 120
3.6 Write a program to sort a stack in ascending order. You should not make any assumptions about how the stack is implemented. The following are the only functions that should be used to write this program: push | pop | peek | isEmpty.
_
________________________________________________________________pg 121
Chapter 4 | Trees and Graphs
Cracking the Coding Interview | Data Structures
53
How to Approach:
Trees and graphs questions typically come in one of two forms:
1. Implement a tree / find a node / delete a node / other well known algorithm.
2. Implement a modification of a known algorithm.
Either way, it is strongly recommended to understand the important tree algorithms prior to your interview. If you’re fluent in these, it’ll make the tougher questions that much easier! We’ll list some of the most important.
WARNING: Not all binary trees are binary search trees
When given a binary tree question, many candidates assume that the interviewer means “binary search tree”, when the interviewer might only mean “binary tree.” So, listen carefully for that word “search.” If you don’t hear it, the interviewer may just mean a binary tree with no particular ordering on the nodes. If you aren’t sure, ask.
Binary Trees—”Must Know” Algorithms
You should be able to easily implement the following algorithms prior to your interview:
»»In-Order: Traverse left node, current node, then right [usually used for binary search trees]
»»Pre-Order: Traverse current node, then left node, then right node.
»»Post-Order: Traverse left node, then right node, then current node.
»»Insert Node: On a binary search tree, we insert a value v, by comparing it to the root. If v > root, we go right, and else we go left. We do this until we hit an empty spot in the tree.
Note: balancing and deletion of binary search trees are rarely asked, but you might want to have some idea how they work. It can set you apart from other candidates.
Graph Traversal—”Must Know” Algorithms
You should be able to easily implement the following algorithms prior to your interview:
»»Depth First Search: DFS involves searching a node and all its children before proceeding to its siblings.
»»Breadth First Search: BFS involves searching a node and its siblings before going on to any children.
Chapter 4 | Trees and Graphs
54
CareerCup.com
4.1 Implement a function to check if a tree is balanced. For the purposes of this question, a balanced tree is defined to be a tree such that no two leaf nodes differ in distance from the root by more than one.
_
________________________________________________________________pg 123
4.2 Given a directed graph, design an algorithm to find out whether there is a route between two nodes.
_
________________________________________________________________pg 124
4.3 Given a sorted (increasing order) array, write an algorithm to create a binary tree with minimal height.
_
________________________________________________________________pg 125
4.4 Given a binary search tree, design an algorithm which creates a linked list of all the nodes at each depth (i.e., if you have a tree with depth D, you’ll have D linked lists).
_
________________________________________________________________pg 126
4.5 Write an algorithm to find the ‘next’ node (i.e., in-order successor) of a given node in a binary search tree where each node has a link to its parent.
_
________________________________________________________________pg 127
4.6 Design an algorithm and write code to find the first common ancestor of two nodes in a binary tree. Avoid storing additional nodes in a data structure. NOTE: This is not necessarily a binary search tree.
_
________________________________________________________________pg 128
4.7 You have two very large binary trees: T1, with millions of nodes, and T2, with hundreds of nodes. Create an algorithm to decide if T2 is a subtree of T1.
_
________________________________________________________________pg 130
4.8 You are given a binary tree in which each node contains a value. Design an algorithm to print all paths which sum up to that value. Note that it can be any path in the tree - it does not have to start at the root.
_
________________________________________________________________pg 131

Part 2
Concepts and Algorithms
Cracking the Coding Interview | Concepts and Algorithms
57
Chapter 5 | Bit Manipulation
How to Approach:
Bit manipulation can be a scary thing to many candidates, but it doesn’t need to be! If you’re shaky on bit manipulation, we recommend doing a couple of arithmetic-like problems to boost your skills. Compute the following by hand:
1010 - 0001
1010 + 0110
1100^1010
1010 << 1
1001^1001
1001 & 1100
1010 >> 1
0xFF - 1
0xAB + 0x11
If you’re still uncomfortable, examine very carefully what happens when you do subtraction, addition, etc in base 10. Can you repeat that work in base 2?
NOTE: The Windows Calculator knows how to do lots of operations in binary, including ADD, SUBTRACT, AND and OR. Go to View > Programmer to get into binary mode while you practice.
Things to Watch Out For:
»»It’s really easy to make mistakes on these problems, so be careful! When you’re writing code, stop and think about what you’re writing every couple of lines - or, better yet, test your code mid-way through! When you’re done, check through your entire code.
»»If you’re bit shifting, what happens when the digits get shifted off the end? Make sure to think about this case to ensure that you’re handling it correctly.
And (&):
0 & 0 = 0
1 & 0 = 0
0 & 1 = 0
1 & 1 = 1
Or (|):
0 | 0 = 0
1 | 0 = 1
0 | 1 = 1
1 | 1 = 1
Xor (^):
0 ^ 0 = 0
1 ^ 0 = 1
0 ^ 1 = 1
1 ^ 1 = 0
Left Shift:
x << y means x shifted y bits to the left. If you start shifting and you run out of space, the bits just “drop off”. For example:
00011001 << 2 = 01100100
00011001 << 4 = 10010000
Right Shift:
x >> y means x shifted y bits to the right. If you start shifting and you run out of space, the bits just “drop off” the end. Example:
00011001 >> 2 = 00000110
00011001 >> 4 = 00000001
Chapter 5 | Bit Manipulation
58
CareerCup.com
5.1 You are given two 32-bit numbers, N and M, and two bit positions, i and j. Write a method to set all bits between i and j in N equal to M (e.g., M becomes a substring of N located at i and starting at j).
EXAMPLE:
Input: N = 10000000000, M = 10101, i = 2, j = 6
Output: N = 10001010100
_
________________________________________________________________pg 133
5.2 Given a (decimal - e.g. 3.72) number that is passed in as a string, print the binary representation. If the number can not be represented accurately in binary, print “ERROR”
_
________________________________________________________________pg 134
5.3 Given an integer, print the next smallest and next largest number that have the same number of 1 bits in their binary representation.
_
________________________________________________________________pg 135
5.4 Explain what the following code does: ((n & (n-1)) == 0).
_
________________________________________________________________pg 138
5.5 Write a function to determine the number of bits required to convert integer A to integer B.
Input: 31, 14
Output: 2
_
________________________________________________________________pg 139
5.6 Write a program to swap odd and even bits in an integer with as few instructions as possible (e.g., bit 0 and bit 1 are swapped, bit 2 and bit 3 are swapped, etc).
_
________________________________________________________________pg 140
5.7 An array A[1... n] contains all the integers from 0 to n except for one number which is missing. In this problem, we cannot access an entire integer in A with a single operation. The elements of A are represented in binary, and the only operation we can use to access them is “fetch the jth bit of A[i]”, which takes constant time. Write code to find the missing integer. Can you do it in O(n) time?
_
________________________________________________________________pg 141
Chapter 6 | Brain Teasers
Cracking the Coding Interview | Concepts and Algorithms
59
Do companies really ask brain teasers?
While many companies, including Google and Microsoft, have policies banning brain teasers, interviewers still sometimes ask these tricky questions. This is especially true since people have different definitions of brain teasers.
Advice on Approaching Brain Teasers
Don’t panic when you get a brain teaser. Interviewers want to see how you tackle a problem; they don’t expect you to immediately know the answer. Start talking, and show the interviewer how you approach a problem.
In many cases, you will also find that the brain teasers have some connection back to fundamental laws or theories of computer science.
If you’re stuck, we recommend simplifying the problem. Solve it for a small number of items or a special case, and then see if you can generalize it.
Example
You are trying to cook an egg for exactly fifteen minutes, but instead of a timer, you are given two ropes which burn for exactly 1 hour each. The ropes, however, are of uneven densities - i.e., half the rope length-wise might take only two minutes to burn.
The Approach
1. What is important? Numbers usually have a meaning behind them. The fifteen minutes and two ropes were picked for a reason.
2. Simplify! You can easily time one hour (burn just one rope).
3. Now, can you time 30 minutes? That’s half the time it takes to burn one rope. Can you burn the rope twice as fast? Yes! (Light the rope at both ends.)
4. You’ve now learned: (1) You can time 30 minutes. (2) You can burn a rope that takes X minutes in just X/2 minutes by lighting both ends.
5. Work backwards: if you had a rope of burn-length 30 minutes, that would let you time 15 minutes. Can you remove 30 minutes of burn-time from a rope?
6. You can remove 30 minutes of burn-time from Rope #2 by lighting Rope #1 at both ends and Rope #2 at one end.
7. Now that you have Rope #2 at burn-length 30 minutes, start cooking the egg and light Rope #2 at the other end. When Rope #2 burns up, your egg is done!
Chapter 6 | Brain Teasers
60
CareerCup.com
6.1 Add arithmetic operators (plus, minus, times, divide) to make the following expression true: 3 1 3 6 = 8. You can use any parentheses you’d like.
_
________________________________________________________________pg 143
6.2 There is an 8x8 chess board in which two diagonally opposite corners have been cut off. You are given 31 dominos, and a single domino can cover exactly two squares. Can you use the 31 dominos to cover the entire board? Prove your answer (by providing an example, or showing why it’s impossible).
_
________________________________________________________________pg 144
6.3 You have a five quart jug and a three quart jug, and an unlimited supply of water (but no measuring cups). How would you come up with exactly four quarts of water?
NOTE: The jugs are oddly shaped, such that filling up exactly ‘half’ of the jug would be impossible.
_
________________________________________________________________pg 145
6.4 A bunch of men are on an island. A genie comes down and gathers everyone together and places a magical hat on some people’s heads (i.e., at least one person has a hat). The hat is magical: it can be seen by other people, but not by the wearer of the hat himself. To remove the hat, those (and only those who have a hat) must dunk themselves underwater at exactly midnight. If there are n people and c hats, how long does it take the men to remove the hats? The men cannot tell each other (in any way) that they have a hat.
FOLLOW UP
Prove that your solution is correct.
_
________________________________________________________________pg 146
6.5 There is a building of 100 floors. If an egg drops from the Nth floor or above it will break. If it’s dropped from any floor below, it will not break. You’re given 2 eggs. Find N, while minimizing the number of drops for the worst case.
_
________________________________________________________________pg 148
6.6 There are one hundred closed lockers in a hallway. A man begins by opening all one hundred lockers. Next, he closes every second locker. Then he goes to every third locker and closes it if it is open or opens it if it is closed (e.g., he toggles every third locker). After his one hundredth pass in the hallway, in which he toggles only locker number one hundred, how many lockers are open?
_
________________________________________________________________pg 149
Chapter 7 | Object Oriented Design
Cracking the Coding Interview | Concepts and Algorithms
61
How to Approach
Object oriented design questions are very important, as they demonstrate the quality of a candidate’s code. A poor performance on this type of question raises serious red flags.
Handling Ambiguity in an Interview
OOD questions are often intentionally vague to test if you’ll make assumptions, or if you’ll ask clarifying questions. How do you design a class if the constraints are vague? Ask questions to eliminate ambiguity, then design the classes to handle any remaining ambiguity.
Object Oriented Design for Software
Imagine we’re designing the objects for a deck of cards. Consider the following approach:
1. What are you trying to do with the deck of cards? Ask your interviewer. Let’s assume we want a general purpose deck of cards to implement many different types of card games.
2. What are the core objects—and what “sub types” are there? For example, the core items might be: Card, Deck, Number, Suit, PointValue
3. Have you missed anything? Think about how you’ll use that deck of cards to implement different types of games, changing the class design as necessary.
4. Now, get a little deeper: how will the methods work? If you have a method like Card Deck:.getCard(Suit s, Number n), think about how it will retrieve the card.
Object Oriented Design for Real World Object
Real world objects are handled very similarly to software object oriented design. Suppose you are designing an object oriented design for a parking lot:
1. What are your goals? For example: figure out if a parking spot is taken, figure out how many cars of each type are in the parking lot, look up handicapped spots, etc.
2. Now, think about the core objects (Car, ParkingSpot, ParkingLot, ParkingMeter, etc—Car has different subclasses, and ParkingSpot is also subclassed for handicapped spot).
3. Have we missed anything? How will we represent parking restrictions based on time or payment? Perhaps, we’ll add a class called Permission which handles different payment systems. Permission will be sub-classed into classes PaidPermission (fee to park) and FreeParking (open parking). ParkingLot will have a method called GetPermission which will return the current Permission object based on the time.
4. How will we know whether or not a car is in a spot? Think about how to represent the data so that the methods are most efficient.
Chapter 7 | Object Oriented Design
62
CareerCup.com
7.1 Design the data structures for a generic deck of cards. Explain how you would subclass it to implement particular card games.
_
________________________________________________________________pg 151
7.2 Imagine you have a call center with three levels of employees: fresher, technical lead (TL), product manager (PM). There can be multiple employees, but only one TL or PM. An incoming telephone call must be allocated to a fresher who is free. If a fresher can’t handle the call, he or she must escalate the call to technical lead. If the TL is not free or not able to handle it, then the call should be escalated to PM. Design the classes and data structures for this problem. Implement a method getCallHandler().
_
________________________________________________________________pg 152
7.3 Design a musical juke box using object oriented principles.
_
________________________________________________________________pg 154
7.4 Design a chess game using object oriented principles.
_
________________________________________________________________pg 156
7.5 Design the data structures for an online book reader system.
_
________________________________________________________________pg 157
7.6 Implement a jigsaw puzzle. Design the data structures and explain an algorithm to solve the puzzle.
_
________________________________________________________________pg 159
7.7 Explain how you would design a chat server. In particular, provide details about the various backend components, classes, and methods. What would be the hardest problems to solve?
_
________________________________________________________________pg 161
7.8 Othello is played as follows: Each Othello piece is white on one side and black on the other. When a piece is surrounded by its opponents on both the left and right sides, or both the top and bottom, it is said to be captured and its color is flipped. On your turn, you must capture at least one of your opponent’s pieces. The game ends when either user has no more valid moves, and the win is assigned to the person with the most pieces. Implement the object oriented design for Othello.
_
________________________________________________________________pg 163
7.9 Explain the data structures and algorithms that you would use to design an in-memory file system. Illustrate with an example in code where possible.
_
________________________________________________________________pg 166
7.10 Describe the data structures and algorithms that you would use to implement a garbage collector in C++.
_
________________________________________________________________pg 167
Chapter 8 | Recursion
Cracking the Coding Interview | Concepts and Algorithms
63
How to Recognize
While there is a wide variety of recursive problems, many recursive problems follow similar patterns. A good hint that problem is recursive is that it appears to be built off sub-problems.
When you hear a problem beginning with the following, it’s often (though not always) a good candidate for recursion: “Design an algorithm to compute the nth ... ”; “Write code to list the first n... ”; “Implement a method to compute all... ”; etc.
Again, practice makes perfect! The more problems you do, the easier it will be to recognize recursive problems.
How to Approach
Recursive solutions, by definition, are built off solutions to sub-problems. Many times, this will mean simply to compute f(n) by adding something, removing something, or otherwise changing the solution for f(n-1). In other cases, you might have to do something more complicated. Regardless, we recommend the following approach:
1. Think about what the sub-problem is. How many sub-problems does f(n) depend on? That is, in a recursive binary tree problem, each part will likely depend on two problems. In a linked list problem, it’ll probably be just one.
2. Solve for a “base case.” That is, if you need to compute f(n), first compute it for f(0) or f(1). This is usually just a hard-coded value.
3. Solve for f(2).
4. Understand how to solve for f(3) using f(2) (or previous solutions). That is, understand the exact process of translating the solutions for sub-problems into the real solution.
5. Generalize for f(n).
This “bottom-up recursion” is often the most straight-forward. Sometimes, though, it can be useful to approach problems “top down”, where you essentially jump directly into breaking f(n) into its sub-problems.
Things to Watch Out For
1. All problems that can be solved recursively can also be solved iteratively (though the code may be much more complicated). Before diving into a recursive code, ask yourself how hard it would be to implement this algorithm iteratively. Discuss the trade-offs with your interviewer.
2. Recursive algorithms can be very space inefficient. Each recursive call adds a new layer to the stack, which means that if your algorithm has O(n) recursive calls then it uses O(n) memory. Ouch! This is one reason why an iterative algorithm may be better.
Chapter 8 | Recursion
64
CareerCup.com
8.1 Write a method to generate the nth Fibonacci number.
_
________________________________________________________________pg 169
8.2 Imagine a robot sitting on the upper left hand corner of an NxN grid. The robot can only move in two directions: right and down. How many possible paths are there for the robot?
FOLLOW UP
Imagine certain squares are “off limits”, such that the robot can not step on them. Design an algorithm to get all possible paths for the robot.
_
________________________________________________________________pg 170
8.3 Write a method that returns all subsets of a set.
_
________________________________________________________________pg 171
8.4 Write a method to compute all permutations of a string.
_
________________________________________________________________pg 173
8.5 Implement an algorithm to print all valid (e.g., properly opened and closed) combinations of n-pairs of parentheses.
EXAMPLE:
input: 3 (e.g., 3 pairs of parentheses)
output: ()()(), ()(()), (())(), ((()))
_
________________________________________________________________pg 174
8.6 Implement the “paint fill” function that one might see on many image editing programs. That is, given a screen (represented by a 2 dimensional array of Colors), a point, and a new color, fill in the surrounding area until you hit a border of that color.’
_
________________________________________________________________pg 175
8.7 Given an infinite number of quarters (25 cents), dimes (10 cents), nickels (5 cents) and pennies (1 cent), write code to calculate the number of ways of representing n cents.
_
________________________________________________________________pg 176
8.8 Write an algorithm to print all ways of arranging eight queens on a chess board so that none of them share the same row, column or diagonal.
_
________________________________________________________________pg 177
Chapter 9 | Sorting and Searching
Cracking the Coding Interview | Concepts and Algorithms
65
How to Approach:
Understanding the common sorting algorithms is incredibly valuable, as many sorting or searching solutions require tweaks of known sorting algorithms. A good approach when you are given a question like this is to run through the different sorting algorithms and see if one applies particularly well.
Example: You have a very large array of ‘Person’ objects. Sort the people in increasing order of age.
We’re given two interesting bits of knowledge here: (1) It’s a large array, so efficiency is very important. (2) We are sorting based on ages, so we know the values are in a small range. By scanning through the various sorting algorithms, we might notice that bucket sort would be a perfect candidate for this algorithm. In fact, we can make the buckets small (just 1 year each) and get O(n) running time.
Bubble Sort:
Start at the beginning of an array and swap the first two elements if the first is bigger than the second. Go to the next pair, etc, continuously making sweeps of the array until sorted. O(n^2).
Selection Sort:
Find the smallest element using a linear scan and move it to the front. Then, find the second smallest and move it, again doing a linear scan. Continue doing this until all the elements are in place. O(n^2).
Merge Sort:
Sort each pair of elements. Then, sort every four elements by merging every two pairs. Then, sort every 8 elements, etc. O(n log n) expected and worst case.
Quick Sort:
Pick a random element and partition the array, such that all numbers that are less than it come before all elements that are greater than it. Then do that for each half, then each quarter, etc. O(n log n) expected, O(n^2) worst case.
Bucket Sort:
Partition the array into a finite number of buckets, and then sort each bucket individually. This gives a time of O(n + m), where n is the number of items and m is the number of distinct items.
Chapter 9 | Sorting and Searching
66
CareerCup.com
9.1 You are given two sorted arrays, A and B, and A has a large enough buffer at the end to hold B. Write a method to merge B into A in sorted order.
_
________________________________________________________________pg 179
9.2 Write a method to sort an array of strings so that all the anagrams are next to each other.
_
________________________________________________________________pg 180
9.3 Given a sorted array of n integers that has been rotated an unknown number of times, give an O(log n) algorithm that finds an element in the array. You may assume that the array was originally sorted in increasing order.
EXAMPLE:
Input: find 5 in array (15 16 19 20 25 1 3 4 5 7 10 14)
Output: 8 (the index of 5 in the array)
_
________________________________________________________________pg 181
9.4 If you have a 2 GB file with one string per line, which sorting algorithm would you use to sort the file and why?
_
________________________________________________________________pg 182
9.5 Given a sorted array of strings which is interspersed with empty strings, write a method to find the location of a given string.
Example: find “ball” in [“at”, “”, “”, “”, “ball”, “”, “”, “car”, “”, “”, “dad”, “”, “”] will return 4
Example: find “ballcar” in [“at”, “”, “”, “”, “”, “ball”, “car”, “”, “”, “dad”, “”, “”] will return -1
_
________________________________________________________________pg 183
9.6 Given a matrix in which each row and each column is sorted, write a method to find an element in it.
_
________________________________________________________________pg 184
9.7 A circus is designing a tower routine consisting of people standing atop one another’s shoulders. For practical and aesthetic reasons, each person must be both shorter and lighter than the person below him or her. Given the heights and weights of each person in the circus, write a method to compute the largest possible number of people in such a tower.
EXAMPLE:
Input (ht, wt): (65, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68, 110)
Output: The longest tower is length 6 and includes from top to bottom: (56, 90) (60,95) (65,100) (68,110) (70,150) (75,190)
_
________________________________________________________________pg 185
Chapter 10 | Mathematical
Cracking the Coding Interview | Concepts and Algorithms
67
How to Approach:
Many of these problems read as brain teasers at first, but can be worked through in a logical way. Just remember to rely on the rules of mathematics to develop an approach, and then to carefully translate that idea into code.
Example: Given two numbers m and n, write a method to return the first number r that is divisible by both (e.g., the least common multiple).
The Approach: What does it mean for r to be divisible by m and n? It means that all the primes in m must go into r, and all primes in n must be in r. What if m and n have primes in common? For example, if m is divisible by 3^5 and n is divisible by 3^7, what does this mean about r? It means r must be divisible by 3^7.
The Rule: For each prime p such that p^a \ m (e.g., m is divisible by p^a) and p^b \ n, r must be divisible by p^max(a, b).
The Algorithm:
Define q to be 1.
for each prime number p less than m and n:
find the largest a and b such that p^a \ m and p^b \ n
let q = q * p^max(a, b)
return q
NOTE: An alternate solution involves recognizing that gcd(a, b) * lcm(a, b) = ab. One could then compute the gcd(a, b) using the Euclidean algorithm. Of course, unless you already know this fact, it’s unlikely that this rule would occur to you during an interview.
Things to Watch Out For:
1. Be careful with the difference in precision between floats vs. doubles.
2. Don’t assume that a value (such as the slope of a line) is an int unless you’ve been told so.
Bayes’ Rule and Probability
1. If A and B are independent, then P(A and B) = P(A) * P(B).
2. Else (in general), P(A and B) = P(A given B) * P(B)
3. If A and B are mutually exclusive (e.g., if one happens, the other one can’t),
P(A or B) = P(A) + P(B).
4. Else (in general), P(A or B) = P(A) + P(B) - P(A and B).
Chapter 10 | Mathematical
68
CareerCup.com
10.1 You have a basketball hoop and someone says that you can play 1 of 2 games.
Game #1: You get one shot to make the hoop.
Game #2: You get three shots and you have to make 2 of 3 shots.
If p is the probability of making a particular shot, for which values of p should you pick one game or the other?
_
________________________________________________________________pg 187
10.2 There are three ants on different vertices of a triangle. What is the probability of collision (between any two or all of them) if they start walking on the sides of the triangle?
Similarly find the probability of collision with ‘n’ ants on an ‘n’ vertex polygon.
_
________________________________________________________________pg 188
10.3 Given two lines on a Cartesian plane, determine whether the two lines would intersect.
_
________________________________________________________________pg 189
10.4 Write a method to implement *, - , / operations. You should use only the + operator.
_
________________________________________________________________pg 190
10.5 Given two squares on a two dimensional plane, find a line that would cut these two squares in half.
_
________________________________________________________________pg 192
10.6 Given a two dimensional graph with points on it, find a line which passes the most number of points.
_
________________________________________________________________pg 193
10.7 Design an algorithm to find the kth number such that the only prime factors are 3, 5, and 7.
_
________________________________________________________________pg 195
Chapter 11 | Testing
Cracking the Coding Interview | Concepts and Algorithms
69
Testing Problems: Not Just for Testers!
Although testers are obviously asked more testing problems, developers will often be asked testing problems as well. Why? Because a good developer knows how to test their code!
Types of Testing Problems:
Testing problems generally fall into one of three categories:
1. Explain how you would test this real world object (pen, paperclip, etc).
2. Explain how you would test this computer software (e.g., a web browser).
3. Write test cases / test code to test this specific method.
We’ll discuss type #1, since it’s usually the most daunting. Remember that all three types require you to not make assumptions that the input or the user will play nice. Expect abuse and plan for it.
How to Test A Real World Object
Let’s imagine that you were asked to test a paperclip. The first thing to understand is: what is it expected to be used for and who are the expected users. Ask your interviewer—the answer may not be what you think! The answer could be “by teachers, to hold papers together” or it could be “by artists, to bend into new shapes.” These two use-cases will have very different answers. Once you understand the intended use, think about:
»»What are the specific use cases for the intended purpose? For example, holding 2 sheets of paper together, and up to 30 sheets. If it fails, does it fail gracefully? (see below)
»»What does it mean for it to fail? Answer: “Failing gracefully“ means for the paperclip to not hold paper together. If it snaps easily, that’s (probably) not failing gracefully.
»»Ask your interviewer—what are the expectations of it being used outside of the intended use case? Should we ensure that it has a minimum of usefulness for the other cases?
»»What “stress” conditions might your paperclip be used in? Answer: hot weather, cold weather, frequent re-use, etc.
Chapter 11 | Testing
70
CareerCup.com
11.1 Find the mistake(s) in the following code:
1 unsigned int i;
2 for (i = 100; i <= 0; --i)
3 printf(“%d\n”, i);
_
________________________________________________________________pg 209
11.2 You are given the source to an application which crashes when it is run. After running it ten times in a debugger, you find it never crashes in the same place. The application is single threaded, and uses only the C standard library. What programming errors could be causing this crash? How would you test each one?
_
________________________________________________________________pg 210
11.3 We have the following method used in a chess game: boolean canMoveTo(int x, int y) x and y are the coordinates of the chess board and it returns whether or not the piece can move to that position. Explain how you would test this method.
_
________________________________________________________________pg 211
11.4 How would you load test a webpage without using any test tools?
_
________________________________________________________________pg 212
11.5 How would you test a pen?
_
________________________________________________________________pg 213
11.6 How would you test an ATM in a distributed banking system?
_
________________________________________________________________pg 214
Chapter 12 | System Design and Memory Limits
Cracking the Coding Interview | Concepts and Algorithms
71
How to Approach:
Don’t be scared by these types of questions. Unless you claim to know how to design large systems, your interviewer probably won’t expect you to know this stuff automatically. They just want to see how you tackle these problems.
General Approach
The general approach is as follows: Imagine we’re designing a hypothetical system X for millions of items (users, files, megabytes, etc):
1. How would you solve it for a small number of items? Develop an algorithm for this case, which is often pretty straight-forward.
2. What happens when you try to implement that algorithm with millions of items? It’s likely that you have run out of space on the computer. So, divide up the files across many computers.
»»How do you divide up data across many machines? That is, do the first 100 items appear on the same computer? Or all items with the same hash value mod 100?
»»About how many computers will you need? To estimate this, ask how big each item is and take a guess at how much space a typical computer has.
3. Now, fix the problems that occur when you are using many computers. Make sure to answer the following questions:
»»How does one machine know which machine it should access to look up data?
»»Can data get out of sync across computers? How do you handle that?
»»How can you minimize expensive reads across computers?
Example: Design a Web Crawler
1. Forget about the fact that you’re dealing with billions of pages. How would you design this system if it were just a small number of pages? You should have an understanding of how you would solve the simple, small case in order to understand how you would solve the bigger case.
2. Now, think about the issues that occur with billions of pages. Most likely you can’t fit the data on one machine. How will you divide it up? How will you figure out which computer has a particular piece of data?
3. You now have different pieces of data on different machines. What problems might that create? Can you try to solve them?
And remember, don’t get scared! This is just an ordinary problem solving question.
Chapter 12 | System Design and Memory Limits
72
CareerCup.com
12.1 If you were integrating a feed of end of day stock price information (open, high, low, and closing price) for 5,000 companies, how would you do it? You are responsible for the development, rollout and ongoing monitoring and maintenance of the feed. Describe the different methods you considered and why you would recommend your approach. The feed is delivered once per trading day in a comma-separated format via an FTP site. The feed will be used by 1000 daily users in a web application.
_
________________________________________________________________pg 197
12.2 How would you design the data structures for a very large social network (Facebook, LinkedIn, etc)? Describe how you would design an algorithm to show the connection, or path, between two people (e.g., Me -> Bob -> Susan -> Jason -> You).
_
________________________________________________________________pg 199
12.3 Given an input file with four billion integers, provide an algorithm to generate an integer which is not contained in the file. Assume you have 1 GB of memory.
FOLLOW UP
What if you have only 10 MB of memory?
_
________________________________________________________________pg 202
12.4 You have an array with all the numbers from 1 to N, where N is at most 32,000. The array may have duplicate entries and you do not know what N is. With only 4KB of memory available, how would you print all duplicate elements in the array?
_
________________________________________________________________pg 205
12.5 If you were designing a web crawler, how would you avoid getting into infinite loops?
_
________________________________________________________________pg 206
12.6 You have a billion urls, where each is a huge page. How do you detect the duplicate documents?
_
________________________________________________________________pg 207
12.7 You have to design a database that can store terabytes of data. It should support efficient range queries. How would you do it?
_
________________________________________________________________pg 208

Part 3
Knowledge Based
Chapter 13 | C++
Cracking the Coding Interview | Knowledge Based
75
How To Approach:
A good interviewer won’t demand that you code in a language you don’t profess to know. Hopefully, if you’re asked to code in C++, it’s listed on your resume. If you don’t remember all the APIs, don’t worry—your interviewer probably doesn’t care that much. We do recommend, however, studying up on basic C++ syntax.
Pointer Syntax
1 int *p; // Defines pointer.
2 p = &q; // Sets p to address of q.
3 v = *p; // Set v to value of q.
4 Foo *f = new Foo(); // Initializes f.
5 int k = f->x; // Sets k equal to the value of f’s member variable.
C++ Class Syntax
1 class MyClass {
2 private:
3 double var;
4 public:
5 MyClass(double v) {var = v; }
6 ~MyClass() {};
7 double Update(double v);
8 };
9 double Complex::Update(double v) {
10 var = v; return v;
11 }
C++ vs Java
A very common question in an interview is “describe the differences between C++ and Java.” If you aren’t comfortable with any of these concepts, we recommend reading up on them.
1. Java runs in a virtual machine.
2. C++ natively supports unsigned arithmetic.
3. In Java, parameters are always passed by value (or, with objects, their references are passed by value). In C++, parameters can be passed by value, pointer, or by reference.
4. Java has built-in garbage collection.
5. C++ allows operator overloading.
6. C++ allows multiple inheritance of classes.
Question: Which of these might be considered strengths or weaknesses of C++ or Java? Why? In what cases might you choose one language over the other?
Chapter 13 | C++
76
CareerCup.com
13.1 Write a method to print the last K lines of an input file using C++.
_
________________________________________________________________pg 215
13.2 Compare and contrast a hash table vs. an STL map. How is a hash table implemented? If the number of inputs is small, what data structure options can be used instead of a hash table?
_
________________________________________________________________pg 216
13.3 How do virtual functions work in C++?
_
________________________________________________________________pg 217
13.4 What is the difference between deep copy and shallow copy? Explain how you would use each.
_
________________________________________________________________pg 218
13.5 What is the significance of the keyword “volatile” in C?
_
________________________________________________________________pg 219
13.6 What is name hiding in C++?
_
________________________________________________________________pg 220
13.7 Why does a destructor in base class need to be declared virtual?
_
________________________________________________________________pg 221
13.8 Write a method that takes a pointer to a Node structure as a parameter and returns a complete copy of the passed-in data structure. The Node structure contains two pointers to other Node structures.
_
________________________________________________________________pg 223
13.9 Write a smart pointer (smart_ptr) class.
_
________________________________________________________________pg 224
Chapter 14 | Java
Cracking the Coding Interview | Knowledge Based
77
How to Approach:
While Java related questions are found throughout this book, this chapter deals with questions about the language and syntax. You generally will not find too many questions like this at the larger software companies (though they are sometimes asked), but these questions are very common at other companies.
What do you do when you don’t know the answer?
If you don’t know the answer to a question about the Java language, try to figure it out by doing the following: (1) Think about what other languages do. (2) Create an example of the scenario. (3) Ask yourself how you would handle the scenario if you were designing the language.
Your interviewer may be equally—or more—impressed if you can derive the answer than if you automatically knew it. Don’t try to bluff though. Tell the interviewer, “I’m not sure I can recall the answer, but let me see if I can figure it out. Suppose we have this code…”
Classes & Interfaces (Example)
1 public static void main(String args[]) { … }
2 interface Foo {
3 void abc();
4 }
5 class Foo extends Bar implements Foo { … }
final:
»»Class: Can not be sub-classed
»»Method: Can not be overridden.
»»Variable: Can not be changed.
static:
»»Method: Class method. Called with Foo.DoIt() instead of f.DoIt()
»»Variable: Class variable. Has only one copy and is accessed through the class name.
abstract:
»»Class: Contains abstract methods. Can not be instantiated.
»»Interface: All interfaces are implicitly abstract. This modifier is optional.
»»Method: Method without a body. Class must also be abstract.
Chapter 14 | Java
78
CareerCup.com
14.1 In terms of inheritance, what is the effect of keeping a constructor private?
_
________________________________________________________________pg 225
14.2 In Java, does the finally block gets executed if we insert a return statement inside the try block of a try-catch-finally?
_
________________________________________________________________pg 226
14.3 What is the difference between final, finally, and finalize?
_
________________________________________________________________pg 227
14.4 Explain the difference between templates in C++ and generics in Java.
_
________________________________________________________________pg 228
14.5 Explain what object reflection is in Java and why it is useful.
_
________________________________________________________________pg 229
14.6 Suppose you are using a map in your program, how would you count the number of times the program calls the put() and get() functions?
_
________________________________________________________________pg 230
Chapter 15 | Databases
Cracking the Coding Interview | Knowledge Based
79
How to Approach:
You could be asked about databases in a variety of ways: write a SQL query, design a database to hold certain data, or design a large database. We’ll go through the latter two types here.
Small Database Design
Imagine you are asked to design a system to represent a large, multi-location, apartment rental company.
What are the key objects?
Property. Building. Apartment. Tenant. Manager.
How do they relate to each other?
Many-to-Many:
»»A property could have multiple managers, and a manager could manage multiple properties.
One-to-Many:
»»A building can only be part of one property.
»»An apartment can only be part of one building.
What is the relationship between Tenant and Apartment? An apartment can obviously have multiple tenants. Can a tenant rent multiple apartments? It would be very unusual to, but this could actually happen (particularly if it’s a national company). Talk to your interviewer about this. There is a trade-off between simplifying your database and designing it to be flexible. If you do assume that a Tenant can only rent one Apartment, what do you have to do if this situation occurs?
Large Database Design
When designing a large, scalable database, joins (which are required in the above examples), are generally very slow. Thus, you must denormalize your data. Think carefully about how data will be used—you’ll probably need to duplicate it in multiple tables.
Chapter 15 | Databases
80
CareerCup.com
15.1 Write a method to find the number of employees in each department.
_
________________________________________________________________pg 231
15.2 What are the different types of joins? Please explain how they differ and why certain types are better in certain situations.
_
________________________________________________________________pg 232
15.3 What is denormalization? Explain the pros and cons.
_
________________________________________________________________pg 234
15.4 Draw an entity-relationship diagram for a database with companies, people, and professionals (people who work for companies).
_
________________________________________________________________pg 235
15.5 Imagine a simple database storing information for students’ grades. Design what this database might look like, and provide a SQL query to return a list of the honor roll students (top 10%), sorted by their grade point average.
_
________________________________________________________________pg 236
Chapter 16 | Low Level
Cracking the Coding Interview | Knowledge Based
81
How to Approach:
Many candidates find low level problems to be some of the most challenging. Low level questions require a large amount of knowledge about the underlying architecture of a system. But just how much do you need to know? The answer to that depends, of course, on the company. At a typical large software company where you’d be working on desktop or web applications, you usually only need a minimum amount of knowledge. However, you should understand the concepts below very well, as many interview questions are based off this information.
Big vs Little Endian:
In big endian, the most significant byte is stored at the memory address location with the lowest address. This is akin to left-to-right reading order. Little endian is the reverse: the most significant byte is stored at the address with the highest address.
Stack (Memory)
When a function calls another function which calls another function, this memory goes onto the stack. An int (not a pointer to an int) that is created in a function is stored on the stack.
Heap (Memory)
When you allocate data with new() or malloc(), this data gets stored on the heap.
Malloc
Memory allocated using malloc is persistent—i.e., it will exist until either the programmer frees the memory or the program is terminated.
void *malloc(size_t sz)
Malloc takes as input sz bytes of memory and, if it is successful, returns a void pointer which indicates that it is a pointer to an unknown data type.
void free(void * p)
Free releases a block of memory previously allocated with malloc, calloc, or realloc.
Chapter 16 | Low Level
82
CareerCup.com
16.1 Explain the following terms: virtual memory, page fault, thrashing.
_
________________________________________________________________pg 237
16.2 What is a Branch Target buffer? Explain how it can be used in reducing bubble cycles in cases of branch misprediction.
_
________________________________________________________________pg 238
16.3 Describe direct memory access (DMA). Can a user level buffer / pointer be used by kernel or drivers?
_
________________________________________________________________pg 239
16.4 Write a step by step execution of things that happen after a user presses a key on the keyboard. Use as much detail as possible.
_
________________________________________________________________pg 237
16.5 Write a program to find whether a machine is big endian or little endian.
_
________________________________________________________________pg 241
16.6 Discuss how would you make sure that a process doesn’t access an unauthorized part of the stack.
_
________________________________________________________________pg 242
16.7 What are the best practices to prevent reverse engineering of DLLs?
_
________________________________________________________________pg 244
16.8 A device boots with an empty FIFO queue. In the first 400 ns period after startup, and in each subsequent 400 ns period, a maximum of 80 words will be written to the queue. Each write takes 4 ns. A worker thread requires 3 ns to read a word, and 2 ns to process it before reading the next word. What is the shortest depth of the FIFO such that no data is lost?
_
________________________________________________________________pg 245
16.9 Write an aligned malloc & free function that takes number of bytes and aligned byte (which is always power of 2)
EXAMPLE
align_malloc (1000,128) will return a memory address that is a multiple of 128 and that points to memory of size 1000 bytes.
aligned_free() will free memory allocated by align_malloc.
_
________________________________________________________________pg 247
16.10 Write a function called my2DAlloc which allocates a two dimensional array. Minimize the number of calls to malloc and make sure that the memory is accessible by the notation arr[i][j].
_
________________________________________________________________pg 248
Chapter 17 | Networking
Cracking the Coding Interview | Knowledge Based
83
How to Approach
While the big software houses probably won’t ask you many detailed networking questions in general, some interviewers will attempt to assess your understanding of networking as far as it relates to software and system design. Thus, you should have an understanding of http post and get requests, tcp, etc.
For a more networking based company (Qualcomm, CISCO, etc), we recommend a more thorough understanding. A good way to study is to read the material below, and delve further into it on Wikipedia. When Wikipedia discusses a concept that you are unfamiliar with, click on the concept to read more.
OSI 7 Layer Model
Networking architecture can be divided into seven layers. Each layer provides services to the layer above it and receives services from the layer below it. The seven layers, from top to bottom, are:
OSI 7 Layer Model
Level 7
Application Layer
Level 6
Presentation Layer
Level 5
Session Layer
Level 4
Transport Layer
Level 3
Network Layer
Level 2
Data Link Layer
Level 1
Physical Layer
For a networking focused interview, we suggest reviewing and understanding these concepts and their implications in detail.
Chapter 17 | Networking
84
CareerCup.com
17.1 Explain what happens, step by step, after you type a URL into a browser. Use as much detail as possible.
_
________________________________________________________________pg 249
17.2 Explain any common routing protocol in detail. For example: BGP, OSPF, RIP.
_
________________________________________________________________pg 250
17.3 Compare and contrast the IPv4 and IPv6 protocols.
_
________________________________________________________________pg 252
17.4 What is a network / subnet mask? Explain how host A sends a message / packet to host B when: (a) both are on same network and (b) both are on different networks. Explain which layer makes the routing decision and how.
_
________________________________________________________________pg 254
17.5 What are the differences between TCP and UDP? Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s / receiver’s window) and congestion control.
_
________________________________________________________________pg 255
Chapter 18 | Threads and Locks
Cracking the Coding Interview | Knowledge Based
85
How to Approach:
In a Microsoft, Google or Amazon interview, it’s not terribly common to be asked to implement an algorithm with threads (unless you’re working in a team for which this is a particularly important skill). It is, however, relatively common for interviewers at any company to assess your general understanding of threads, particularly your understanding of deadlocks
Deadlock Conditions
In order for a deadlock to occur, you must have the following four conditions met:
1. Mutual Exclusion: Only one process can use a resource at a given time.
2. Hold and Wait: Processes already holding a resource can request new ones.
3. No Preemption: One process cannot forcibly remove another process’ resource.
4. Circular Wait: Two or more processes form a circular chain where each process is waiting on another resource in the chain.
Deadlock Prevention
Deadlock prevention essentially entails removing one of the above conditions, but many of these conditions are difficult to satisfy. For instance, removing #1 is difficult because many resources can only be used by one process at a time (printers, etc). Most deadlock prevention algorithms focus on avoiding condition #4: circular wait.
If you aren’t familiar with these concepts, please read http://en.wikipedia.org/wiki/Deadlock.
A Simple Java Thread
1 class Foo implements Runnable {
2 public void run() {
3 while (true) beep();
4 }
5 }
6 Foo foo = new Foo ();
7 Thread myThread = new Thread(foo);
8 myThread.start();
Chapter 18 | Threads and Locks
86
CareerCup.com
18.1 What’s the difference between a thread and a process?
_
________________________________________________________________pg 257
18.2 How can you measure the time spent in a context switch?
_
________________________________________________________________pg 258
18.3 Implement a singleton design pattern as a template such that, for any given class Foo, you can call Singleton::instance() and get a pointer to an instance of a singleton of type Foo. Assume the existence of a class Lock which has acquire() and release() methods. How could you make your implementation thread safe and exception safe?
_
________________________________________________________________pg 259
18.4 Design a class which provides a lock only if there are no possible deadlocks.
_
________________________________________________________________pg 261
18.5 Suppose we have the following code:
class Foo {
public:
A
(.....); /* If A is called, a new thread will be created and
* the corresponding function will be executed. */
B(.....); /* same as above */
C(.....); /* same as above */
}
Foo f;
f.A(.....);
f.B(.....);
f.C(.....);
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
iii) Suppose we have the following code to use class Foo. We do not know how the threads will be scheduled in the OS.
Foo f;
f.A(.....); f.B(.....); f.C(.....);
f.A(.....); f.B(.....); f.C(.....);
Can you design a mechanism to make sure that all the methods will be executed in sequence?
_
________________________________________________________________pg 262
18.6 You are given a class with synchronized method A, and a normal method C. If you have two threads in one instance of a program, can they call A at the same time? Can they call A and C at the same time?
_
________________________________________________________________pg 264

Part 4
Additional Review Problems
Chapter 19 | Moderate
Cracking the Coding Interview | Additional Review Problems
89
19.1 Write a function to swap a number in place without temporary variables.
_
________________________________________________________________pg 265
19.2 Design an algorithm to figure out if someone has won in a game of tic-tac-toe.
_
________________________________________________________________pg 266
19.3 Write an algorithm which computes the number of trailing zeros in n factorial.
_
________________________________________________________________pg 268
19.4 Write a method which finds the maximum of two numbers. You should not use if-else or any other comparison operator.
EXAMPLE
Input: 5, 10
Output: 10
_
________________________________________________________________pg 269
19.5 The Game of Master Mind is played as follows:
The computer has four slots containing balls that are red (R), yellow (Y), green (G) or blue (B). For example, the computer might have RGGB (e.g., Slot #1 is red, Slots #2 and #3 are green, Slot #4 is blue).
You, the user, are trying to guess the solution. You might, for example, guess YRGB.
When you guess the correct color for the correct slot, you get a “hit”. If you guess a color that exists but is in the wrong slot, you get a “pseudo-hit”. For example, the guess YRGB has 2 hits and one pseudo hit.
For each guess, you are told the number of hits and pseudo-hits.
Write a method that, given a guess and a solution, returns the number of hits and pseudo hits.
_
________________________________________________________________pg 270
19.6 Given an integer between 0 and 999,999, print an English phrase that describes the integer (eg, “One Thousand, Two Hundred and Thirty Four”).
_
________________________________________________________________pg 271
19.7 You are given an array of integers (both positive and negative). Find the continuous sequence with the largest sum. Return the sum.
EXAMPLE
Input: {2, -8, 3, -2, 4, -10}
Output: 5 (i.e., {3, -2, 4} )
_
________________________________________________________________pg 273
19.8 Design a method to find the frequency of occurrences of any given word in a book.
Chapter 19 | Moderate
90
CareerCup.com
_
________________________________________________________________pg 273
19.9 Since XML is very verbose, you are given a way of encoding it where each tag gets mapped to a pre-defined integer value. The language/grammar is as follows:
Element --> Element Attr* END Element END [aka, encode the element
t
ag, then its attributes, then tack on an END character, then
e
ncode its children, then another end tag]
Attr --> Tag Value [assume all values are strings]
END --> 01
Tag --> some predefined mapping to int
Value --> string value END
Write code to print the encoded version of an xml element (passed in as string).
FOLLOW UP
Is there anything else you could do to (in many cases) compress this even further?
_
________________________________________________________________pg 275
19.10 Write a method to generate a random number between 1 and 7, given a method that generates a random number between 1 and 5 (i.e., implement rand7() using rand5()).
_
________________________________________________________________pg 277
19.11 Design an algorithm to find all pairs of integers within an array which sum to a specified value.
_
________________________________________________________________pg 278
Chapter 20 | Hard
Cracking the Coding Interview | Additional Review Problems
91
20.1 Write a function that adds two numbers. You should not use + or any arithmetic operators.
_
________________________________________________________________pg 279
20.2 Write a method to shuffle a deck of cards. It must be a perfect shuffle - in other words, each 52! permutations of the deck has to be equally likely. Assume that you are given a random number generator which is perfect.
_
________________________________________________________________pg 281
20.3 Write a method to randomly generate a set of m integers from an array of size n. Each element must have equal probability of being chosen.
_
________________________________________________________________pg 282
20.4 Write a method to count the number of 2s between 0 and n.
_
________________________________________________________________pg 283
20.5 You have a large text file containing words. Given any two words, find the shortest distance (in terms of number of words) between them in the file. Can you make the searching operation in O(1) time? What about the space complexity for your solution?
_
________________________________________________________________pg 285
20.6 Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. Assume that the computer memory can hold all one billion numbers.
_
________________________________________________________________pg 286
20.7 Write a program to find the longest word made of other words in a list of words.
EXAMPLE
Input: test, tester, testertest, testing, testingtester
Output: testingtester
_
________________________________________________________________pg 287
20.8 Given a string s and an array of smaller strings T, design a method to search s for each small string in T.
_
________________________________________________________________pg 288
20.9 Numbers are randomly generated and passed to a method. Write a program to find and maintain the median value as new values are generated.
_
________________________________________________________________pg 290
20.10 Given two words of equal length that are in a dictionary, write a method to transform one word into another word by changing only one letter at a time. The new word you get in each step must be in the dictionary.
EXAMPLE
Chapter 20 | Hard
92
CareerCup.com
Input: DAMP, LIKE
Output: DAMP -> LAMP -> LIMP -> LIME -> LIKE
_
________________________________________________________________pg 291
20.11 Imagine you have a square matrix, where each cell is filled with either black or white. Design an algorithm to find the maximum subsquare such that all four borders are filled with black pixels.
_
________________________________________________________________pg 293
20.12 Given an NxN matrix of positive and negative integers, write code to find the sub-matrix with the largest possible sum.
_
________________________________________________________________pg 295
20.13 Given a dictionary of millions of words, give an algorithm to find the largest possible rectangle of letters such that every row forms a word (reading left to right) and every column forms a word (reading top to bottom).
_
________________________________________________________________pg 298
Each problem may have many 'optimal' solutions that differ in runtime, space, clarity, extensibility, etc. We have provided one (or more) optimal solutions. If you have additional solutions you would like to contribute, please contact us at
http://www.xrl.us/ccbook or support@careercup.com.
We welcome all feedback and suggestions. Contact us at
http://www.xrl.us/ccbook or support@careercup.com.
Solutions
Solutions to Chapter 1 | Arrays and Strings
Cracking the Coding Interview | Data Structures
95
1.1 Implement an algorithm to determine if a string has all unique characters. What if you can not use additional data structures?
pg 48
SOLUTION
For simplicity, assume char set is ASCII (if not, we need to increase the storage size. The rest of the logic would be the same). NOTE: This is a great thing to point out to your interviewer!
1 public static boolean isUniqueChars2(String str) {
2 boolean[] char_set = new boolean[256];
3 for (int i = 0; i < str.length(); i++) {
4 int val = str.charAt(i);
5 if (char_set[val]) return false;
6 char_set[val] = true;
7 }
8 return true;
9 }
Time complexity is O(n), where n is the length of the string, and space complexity is O(n).
We can reduce our space usage a little bit by using a bit vector. We will assume, in the below code, that the string is only lower case ‘a’ through ‘z’. This will allow us to use just a single int
1 public static boolean isUniqueChars(String str) {
2 int checker = 0;
3 for (int i = 0; i < str.length(); ++i) {
4 int val = str.charAt(i) - ‘a’;
5 if ((checker & (1 << val)) > 0) return false;
6 checker |= (1 << val);
7 }
8 return true;
9 }
Alternatively, we could do the following:
1. Check every char of the string with every other char of the string for duplicate occurrences. This will take O(n^2) time and no space.
2. If we are allowed to destroy the input string, we could sort the string in O(n log n) time and then linearly check the string for neighboring characters that are identical. Careful, though - many sorting algorithms take up extra space.
Solutions to Chapter 1 | Arrays and Strings
96
CareerCup.com
1.2 Write code to reverse a C-Style String. (C-String means that “abcd” is represented as five characters, including the null character.)
pg 48
SOLUTION
This is a classic interview question. The only “gotcha” is to try to do it in place, and to be careful for the null character.
1 void reverse(char *str) {
2 char * end = str;
3 char tmp;
4 if (str) {
5 while (*end) {
6 ++end;
7 }
8 --end;
9 while (str < end) {
10 tmp = *str;
11 *str++ = *end;
12 *end-- = tmp;
13 }
14 }
15 }
Solutions to Chapter 1 | Arrays and Strings
97 Cracking the Coding Interview | Data Structures
1.3 Design an algorithm and write code to remove the duplicate characters in a string without using any additional buffer. NOTE: One or two additional variables are fine. An extra copy of the array is not.
FOLLOW UP
Write the test cases for this method.
pg 48
SOLUTION
First, ask yourself, what does the interviewer mean by an additional buffer? Can we use an additional array of constant size?
Algorithm—No (Large) Additional Memory:
1. For each character, check if it is a duplicate of already found characters.
2. Skip duplicate characters and update the non duplicate characters.
Time complexity is O(N2).
1 public static void removeDuplicates(char[] str) {
2 if (str == null) return;
3 int len = str.length;
4 if (len < 2) return;
5
6 int tail = 1;
7
8 for (int i = 1; i < len; ++i) {
9 int j;
10 for (j = 0; j < tail; ++j) {
11 if (str[i] == str[j]) break;
12 }
13 if (j == tail) {
14 str[tail] = str[i];
15 ++tail;
16 }
17 }
18 str[tail] = 0;
19 }
Test Cases:
1. String does not contain any duplicates, e.g.: abcd
2. String contains all duplicates, e.g.: aaaa
3. Null string
4. String with all continuous duplicates, e.g.: aaabbb
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 98
5. String with non-contiguous duplicate, e.g.: abababa
Algorithm—With Additional Memory of Constant Size
1 public static void removeDuplicatesEff(char[] str) {
2 if (str == null) return;
3 int len = str.length;
4 if (len < 2) return;
5 boolean[] hit = new boolean[256];
6 for (int i = 0; i < 256; ++i) {
7 hit[i] = false;
8 }
9 hit[str[0]] = true;
10 int tail = 1;
11 for (int i = 1; i < len; ++i) {
12 if (!hit[str[i]]) {
13 str[tail] = str[i];
14 ++tail;
15 hit[str[i]] = true;
16 }
17 }
18 str[tail] = 0;
19 }
Test Cases:
1. String does not contain any duplicates, e.g.: abcd
2. String contains all duplicates, e.g.: aaaa
3. Null string
4. Empty string
5. String with all continuous duplicates, e.g.: aaabbb
6. String with non-contiguous duplicates, e.g.: abababa
Solutions to Chapter 1 | Arrays and Strings
99 Cracking the Coding Interview | Data Structures
1.4 Write a method to decide if two strings are anagrams or not.
pg 48
SOLUTION
There are two easy ways to solve this problem:
Solution #1: Sort the strings
1 boolean anagram(String s, String t) {
2 return sort(s) == sort(t);
3 }
Solution #2: Check if the two strings have identical counts for each unique char.
1 public static boolean anagram(String s, String t) {
2 if (s.length() != t.length()) return false;
3 int[] letters = new int[256];
4 int num_unique_chars = 0;
5 int num_completed_t = 0;
6 char[] s_array = s.toCharArray();
7 for (char c : s_array) { // count number of each char in s.
8 if (letters[c] == 0) ++num_unique_chars;
9 ++letters[c];
10 }
11 for (int i = 0; i < t.length(); ++i) {
12 int c = (int) t.charAt(i);
13 if (letters[c] == 0) { // Found more of char c in t than in s.
14 return false;
15 }
16 --letters[c];
17 if (letters[c] == 0) {
18 ++num_completed_t;
19 if (num_completed_t == num_unique_chars) {
20 // it’s a match if t has been processed completely
21 return i == t.length() - 1;
22 }
23 }
24 }
25 return false;
26 }
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 100
1.5 Write a method to replace all spaces in a string with ‘%20’.
pg 48
SOLUTION
The algorithm is as follows:
1. Count the number of spaces during the first scan of the string.
2. Parse the string again from the end and for each character:
»»If a space is encountered, store “%20”.
»»Else, store the character as it is in the newly shifted location.
1 public static void ReplaceFun(char[] str, int length) {
2 int spaceCount = 0, newLength, i = 0;
3 for (i = 0; i < length; i++) {
4 if (str[i] == ‘ ‘) {
5 spaceCount++;
6 }
7 }
8 newLength = length + spaceCount * 2;
9 str[newLength] = ‘\0’;
10 for (i = length - 1; i >= 0; i--) {
11 if (str[i] == ‘ ‘) {
12 str[newLength - 1] = ‘0’;
13 str[newLength - 2] = ‘2’;
14 str[newLength - 3] = ‘%’;
15 newLength = newLength - 3;
16 } else {
17 str[newLength - 1] = str[i];
18 newLength = newLength - 1;
19 }
20 }
21 }
Solutions to Chapter 1 | Arrays and Strings
101 Cracking the Coding Interview | Data Structures
1.6 Given an image represented by an NxN matrix, where each pixel in the image is 4 bytes, write a method to rotate the image by 90 degrees. Can you do this in place?
pg 48
SOLUTION
The rotation can be performed in layers, where you perform a cyclic swap on the edges on each layer. In the first for loop, we rotate the first layer (outermost edges). We rotate the edges by doing a four-way swap first on the corners, then on the element clockwise from the edges, then on the element three steps away.
Once the exterior elements are rotated, we then rotate the interior region’s edges.
1 public static void rotate(int[][] matrix, int n) {
2 for (int layer = 0; layer < n / 2; ++layer) {
3 int first = layer;
4 int last = n - 1 - layer;
5 for(int i = first; i < last; ++i) {
6 int offset = i - first;
7 int top = matrix[first][i]; // save top
8 // left -> top
9 matrix[first][i] = matrix[last-offset][first];
10
11 // bottom -> left
12 matrix[last-offset][first] = matrix[last][last - offset];
13
14 // right -> bottom
15 matrix[last][last - offset] = matrix[i][last];
16
17 // top -> right
18 matrix[i][last] = top; // right <- saved top
19 }
20 }
21 }
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 102
1.7 Write an algorithm such that if an element in an MxN matrix is 0, its entire row and column is set to 0.
pg 48
SOLUTION
At first glance, this problem seems easy: just iterate through the matrix and every time we see a 0, set that row and column to 0. There’s one problem with that solution though: we will “recognize” those 0s later on in our iteration and then set their row and column to zero. Pretty soon, our entire matrix is 0s!
One way around this is to keep a second matrix which flags the 0 locations. We would then do a second pass through the matrix to set the zeros. This would take O(MN) space.
Do we really need O(MN) space? No. Since we’re going to set the entire row and column to zero, do we really need to track which cell in a row is zero? No. We only need to know that row 2, for example, has a zero.
The code below implement this algorithm. We keep track in two arrays all the rows with zeros and all the columns with zeros. We then make a second pass of the matrix and set a cell to zero if its row or column is zero.
1 public static void setZeros(int[][] matrix) {
2 int[] row = new int[matrix.length];
3 int[] column = new int[matrix[0].length];
4 // Store the row and column index with value 0
5 for (int i = 0; i < matrix.length; i++) {
6 for (int j = 0; j < matrix[0].length;j++) {
7 if (matrix[i][j] == 0) {
8 row[i] = 1;
9 column[j] = 1;
10 }
11 }
12 }
13
14 // Set arr[i][j] to 0 if either row i or column j has a 0
15 for (int i = 0; i < matrix.length; i++) {
16 for (int j = 0; j < matrix[0].length; j++) {
17 if ((row[i] == 1 || column[j] == 1)) {
18 matrix[i][j] = 0;
19 }
20 }
21 }
22 }
Solutions to Chapter 1 | Arrays and Strings
103 Cracking the Coding Interview | Data Structures
1.8 Assume you have a method isSubstring which checks if one word is a substring of another. Given two strings, s1 and s2, write code to check if s2 is a rotation of s1 using only one call to isSubstring (i.e., “waterbottle” is a rotation of “erbottlewat”).
pg 48
SOLUTION
Just do the following checks
1. Check if length(s1) == length(s2). If not, return false.
2. Else, concatenate s1 with itself and see whether s2 is substring of the result.
input: s1 = apple, s2 = pleap ==> apple is a substring of pleappleap
input: s1 = apple, s2 = ppale ==> apple is not a substring of ppaleppale
1 public static boolean isRotation(String s1, String s2) {
2 int len = s1.length();
3 /* check that s1 and s2 are equal length and not empty */
4 if (len == s2.length() && len > 0) {
5 /* concatenate s1 and s1 within new buffer */
6 String s1s1 = s1 + s1;
7 return isSubstring(s1s1, s2);
8 }
9 return false;
10 }

Solutions to Chapter 2 | Linked Lists
Cracking the Coding Interview | Data Structures
105
2.1 Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
pg 50
SOLUTION
If we can use a buffer, we can keep track of elements in a hashtable and remove any dups:
1 public static void deleteDups(LinkedListNode n) {
2 Hashtable table = new Hashtable();
3 LinkedListNode previous = null;
4 while (n != null) {
5 if (table.containsKey(n.data)) previous.next = n.next;
6 else {
7 table.put(n.data, true);
8 previous = n;
9 }
10 n = n.next;
11 }
12 }
Without a buffer, we can iterate with two pointers: “current” does a normal iteration, while “runner” iterates through all prior nodes to check for dups. Runner will only see one dup per node, because if there were multiple duplicates they would have been removed already.
1 public static void deleteDups2(LinkedListNode head) {
2 if (head == null) return;
3 LinkedListNode previous = head;
4 LinkedListNode current = previous.next;
5 while (current != null) {
6 LinkedListNode runner = head;
7 while (runner != current) { // Check for earlier dups
8 if (runner.data == current.data) {
9 LinkedListNode tmp = current.next; // remove current
10 previous.next = tmp;
11 current = tmp; // update current to next node
12 break; // all other dups have already been removed
13 }
14 runner = runner.next;
15 }
16 if (runner == current) { // current not updated - update now
17 previous = current;
18 current = current.next;
19 }
20 }
21 }
Solutions to Chapter 2 | Linked Lists
106
CareerCup.com
2.2 Implement an algorithm to find the nth to last element of a singly linked list.
pg 50
SOLUTION
Note: This problem screams recursion, but we’ll do it a different way because it’s trickier. In a question like this, expect follow up questions about the advantages of recursion vs iteration.
Assumption: The minimum number of nodes in list is n.
Algorithm:
1. Create two pointers, p1 and p2, that point to the beginning of the node.
2. Increment p2 by n-1 positions, to make it point to the nth node from the beginning (to make the distance of n between p1 and p2).
3. Check for p2->next == null if yes return value of p1, otherwise increment p1 and p2. If next of p2 is null it means p1 points to the nth node from the last as the distance between the two is n.
4. Repeat Step 3.
1 LinkedListNode nthToLast(LinkedListNode head, int n) {
2 if (head == null || n < 1) {
3 return null;
4 }
5 LinkedListNode p1 = head;
6 LinkedListNode p2 = head;
7 for (int j = 0; j < n - 1; ++j) { // skip n-1 steps ahead
8 if (p2 == null) {
9 return null; // not found since list size < n
10 }
11 p2 = p2.next;
12 }
13 while (p2.next != null) {
14 p1 = p1.next;
15 p2 = p2.next;
16 }
17 return p1;
18 }
Solutions to Chapter 2 | Linked Lists
107 Cracking the Coding Interview | Data Structures
2.3 Implement an algorithm to delete a node in the middle of a single linked list, given only access to that node.
EXAMPLE
Input: the node ‘c’ from the linked list a->b->c->d->e
Result: nothing is returned, but the new linked list looks like a->b->d->e
pg 50
SOLUTION
The solution to this is to simply copy the data from the next node into this node and then delete the next node.
NOTE: This problem can not be solved if the node to be deleted is the last node in the linked list. That’s ok—your interviewer wants to see you point that out. You could consider marking it as dummy in that case. This is an issue you should discuss with your interviewer.
1 public static boolean deleteNode(LinkedListNode n) {
2 if (n == null || n.next == null) {
3 return false; // Failure
4 }
5 LinkedListNode next = n.next;
6 n.data = next.data;
7 n.next = next.next;
8 return true;
9 }
Solutions to Chapter 2 | Linked Lists
. CareerCup com 108
2.4 You have two numbers represented by a linked list, where each node contains a single digit. The digits are stored in reverse order, such that the 1’s digit is at the head of the list. Write a function that adds the two numbers and returns the sum as a linked list.
EXAMPLE
Input: (3 -> 1 -> 5), (5 -> 9 -> 2)
Output: 8 -> 0 -> 8
pg 50
SOLUTION
We can implement this recursively by adding node by node, just as we would digit by digit.
1. result.data = (node1 + node2 + any earlier carry) % 10
2. if node1 + node2 > 10, then carry a 1 to the next addition.
3. add the tails of the two nodes, passing along the carry.
1 LinkedListNode addLists(LinkedListNode l1, LinkedListNode l2,
2 int carry) {
3 if (l1 == null && l2 == null) {
4 return null;
5 }
6 LinkedListNode result = new LinkedListNode(carry, null, null);
7 int value = carry;
8 if (l1 != null) {
9 value += l1.data;
10 }
11 if (l2 != null) {
12 value += l2.data;
13 }
14 result.data = value % 10;
15 LinkedListNode more = addLists(l1 == null ? null : l1.next,
16 l2 == null ? null : l2.next,
17 value > 10 ? 1 : 1);
18 result.setNext(more);
19 return result;
20 }
Solutions to Chapter 2 | Linked Lists
109 Cracking the Coding Interview | Data Structures
2.5 Given a circular linked list, implement an algorithm which returns node at the beginning of the loop.
DEFINITION
Circular linked list: A (corrupt) linked list in which a node’s next pointer points to an earlier node, so as to make a loop in the linked list.
EXAMPLE
Input: A -> B -> C -> D -> E -> C [the same C as earlier]
Output: C
pg 50
SOLUTION
If we move two pointers, one with speed 1 and another with speed 2, they will end up meeting if the linked list has a loop. Why? Think about two cars driving on a track—the faster car will always pass the slower one!
The tricky part here is finding the start of the loop. Imagine, as an analogy, two people racing around a track, one running twice as fast as the other. If they start off at the same place, when will they next meet? They will next meet at the start of the next lap.
Now, let’s suppose Fast Runner had a head start of k meters on an n step lap. When will they next meet? They will meet k meters before the start of the next lap. (Why? Fast Runner would have made k + 2(n - k) steps, including its head start, and Slow Runner would have made n - k steps. Both will be k steps before the start of the loop.)
Now, going back to the problem, when Fast Runner (n2) and Slow Runner (n1) are moving around our circular linked list, n2 will have a head start on the loop when n1 enters. Specifically, it will have a head start of k, where k is the number of nodes before the loop. Since n2 has a head start of k nodes, n1 and n2 will meet k nodes before the start of the loop.
So, we now know the following:
1. Head is k nodes from LoopStart (by definition).
2. MeetingPoint for n1 and n2 is k nodes from LoopStart (as shown above).
Thus, if we move n1 back to Head and keep n2 at MeetingPoint, and move them both at the same pace, they will meet at LoopStart.
Solutions to Chapter 2 | Linked Lists
. CareerCup com 110
1 LinkedListNode FindBeginning(LinkedListNode head) {
2 LinkedListNode n1 = head;
3 LinkedListNode n2 = head;
4
5 // Find meeting point
6 while (n2.next != null) {
7 n1 = n1.next;
8 n2 = n2.next.next;
9 if (n1 == n2) {
10 break;
11 }
12 }
13
14 // Error check - there is no meeting point, and therefore no loop
15 if (n2.next == null) {
16 return null;
17 }
18
19 /* Move n1 to Head. Keep n2 at Meeting Point. Each are k steps
20 /* from the Loop Start. If they move at the same pace, they must
21 * meet at Loop Start. */
22 n1 = head;
23 while (n1 != n2) {
24 n1 = n1.next;
25 n2 = n2.next;
26 }
27 // Now n2 points to the start of the loop.
28 return n2;
29 }
n1 and n2 will meet here, 3 nodes from start of loop
Solutions to Chapter 3 | Stacks and Queues
Cracking the Coding Interview | Data Structures
111
3.1 Describe how you could use a single array to implement three stacks.
pg 52
SOLUTION
Approach 1:
Divide the array in three equal parts and allow the individual stack to grow in that limited space (note: “[“ means inclusive, while “(“ means exclusive of the end point).
»»for stack 1, we will use [0, n/3)
»»for stack 2, we will use [n/3, 2n/3)
»»for stack 3, we will use [2n/3, n)
This solution is based on the assumption that we do not have any extra information about the usage of space by individual stacks and that we can’t either modify or use any extra space. With these constraints, we are left with no other choice but to divide equally.
1 int stackSize = 300;
2 int[] buffer = new int [stackSize * 3];
3 int[] stackPointer = {0, 0, 0}; // stack pointers to track top elem
4
5 void push(int stackNum, int value) {
6 /* Find the index of the top element in the array + 1, and
7 * increment the stack pointer */
8 int index = stackNum * stackSize + stackPointer[stackNum] + 1;
9 stackPointer[stackNum]++;
10 buffer[index] = value;
11 }
12
13 int pop(int stackNum) {
14 int index = stackNum * stackSize + stackPointer[stackNum];
15 stackPointer[stackNum]--;
16 int value = buffer[index];
17 buffer[index]=0;
18 return value;
19 }
20
21 int peek(int stackNum) {
22 int index = stackNum * stackSize + stackPointer[stackNum];
23 return buffer[index];
24 }
25
26 boolean isEmpty(int stackNum) {
27 return stackPointer[stackNum] == stackNum*stackSize;
28 }
Solutions to Chapter 3 | Stacks and Queues
112
CareerCup.com
Approach 2:
In this approach, any stack can grow as long as there is any free space in the array.
We sequentially allocate space to the stacks and we link new blocks to the previous block. This means any new element in a stack keeps a pointer to the previous top element of that particular stack.
In this implementation, we face a problem of unused space. For example, if a stack deletes some of its elements, the deleted elements may not necessarily appear at the end of the array. So, in that case, we would not be able to use those newly freed spaces.
To overcome this deficiency, we can maintain a free list and the whole array space would be given initially to the free list. For every insertion, we would delete an entry from the free list. In case of deletion, we would simply add the index of the free cell to the free list.
In this implementation we would be able to have flexibility in terms of variable space utilization but we would need to increase the space complexity.
1 int stackSize = 300;
2 int indexUsed = 0;
3 int[] stackPointer = {-1,-1,-1};
4 StackNode[] buffer = new StackNode[stackSize * 3];
5 void push(int stackNum, int value) {
6 int lastIndex = stackPointer[stackNum];
7 stackPointer[stackNum] = indexUsed;
8 indexUsed++;
9 buffer[stackPointer[stackNum]]=new StackNode(lastIndex,value);
10 }
11 int pop(int stackNum) {
12 int value = buffer[stackPointer[stackNum]].value;
13 int lastIndex = stackPointer[stackNum];
14 stackPointer[stackNum] = buffer[stackPointer[stackNum]].previous;
15 buffer[lastIndex] = null;
16 indexUsed--;
17 return value;
18 }
19 int peek(int stack) { return buffer[stackPointer[stack]].value; }
20 boolean isEmpty(int stackNum) { return stackPointer[stackNum] == -1; }
21
22 class StackNode {
23 public int previous;
24 public int value;
25 public StackNode(int p, int v){
26 value = v;
27 previous = p;
28 }
29 }
Solutions to Chapter 3 | Stacks and Queues
113 Cracking the Coding Interview | Data Structures
3.2 How would you design a stack which, in addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time.
pg 52
SOLUTION
You can implement this by having each node in the stack keep track of the minimum beneath itself. Then, to find the min, you just look at what the top element thinks is the min.
When you push an element onto the stack, the element is given the current minimum. It sets its “local min” to be the min.
1 public class StackWithMin extends Stack<NodeWithMin> {
2 public void push(int value) {
3 int newMin = Math.min(value, min());
4 super.push(new NodeWithMin(value, newMin));
5 }
6
7 public int min() {
8 if (this.isEmpty()) {
9 return Integer.MAX_VALUE;
10 } else {
11 return peek().min;
12 }
13 }
14 }
15
16 class NodeWithMin {
17 public int value;
18 public int min;
19 public NodeWithMin(int v, int min){
20 value = v;
21 this.min = min;
22 }
23 }
There’s just one issue with this: if we have a large stack, we waste a lot of space by keeping track of the min for every single element. Can we do better?
We can (maybe) do a bit better than this by using an additional stack which keeps track of the mins.
1 public class StackWithMin2 extends Stack<Integer> {
2 Stack<Integer> s2;
3 public StackWithMin2() {
4 s2 = new Stack<Integer>();
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 114
5 }
6 public void push(int value){
7 if (value <= min()) {
8 s2.push(value);
9 }
10 super.push(value);
11 }
12 public Integer pop() {
13 int value = super.pop();
14 if (value == min()) {
15 s2.pop();
16 }
17 return value;
18 }
19 public int min() {
20 if (s2.isEmpty()) {
21 return Integer.MAX_VALUE;
22 } else {
23 return s2.peek();
24 }
25 }
26 }
Why might this be more space efficient? If many elements have the same local min, then we’re keeping a lot of duplicate data. By having the mins kept in a separate stack, we don’t have this duplicate data (although we do use up a lot of extra space because we have a stack node instead of a single int).
Solutions to Chapter 3 | Stacks and Queues
115 Cracking the Coding Interview | Data Structures
3.3 Imagine a (literal) stack of plates. If the stack gets too high, it might topple. Therefore, in real life, we would likely start a new stack when the previous stack exceeds some threshold. Implement a data structure SetOfStacks that mimics this. SetOfStacks should be composed of several stacks, and should create a new stack once the previous one exceeds capacity. SetOfStacks.push() and SetOfStacks.pop() should behave identically to a single stack (that is, pop() should return the same values as it would if there were just a single stack).
FOLLOW UP
Implement a function popAt(int index) which performs a pop operation on a specific sub-stack.
pg 52
SOLUTION
In this problem, we’ve been told what our data structure should look like:
1 class SetOfStacks {
2 ArrayList<Stack> stacks = new ArrayList<Stack>();
3 public void push(int v) { ... }
4 public int pop() { ... }
5 }
We know that push() should behave identically to a single stack, which means that we need push() to call push on the last stack. We have to be a bit careful here though: if the last stack is at capacity, we need to create a new stack. Our code should look something like this:
1 public void push(int v) {
2 Stack last = getLastStack();
3 if (last != null && !last.isAtCapacity()) { // add to last stack
4 last.push(v);
5 } else { // must create new stack
6 Stack stack = new Stack(capacity);
7 stack.push(v);
8 stacks.add(stack);
9 }
10 }
What should pop() do? It should behave similarly to push(), in that it should operate on the last stack. If the last stack is empty (after popping), then we should remove it from the list of stacks.
1 public int pop() {
2 Stack last = getLastStack();
3 int v = last.pop();
4 if (last.size == 0) stacks.remove(stacks.size() - 1);
5 return v;
6 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 116
What about the follow up question? This is a bit trickier to implement, but essentially we should imagine a “rollover” system. If we pop an element from stack 1, we need to remove the bottom of stack 2 and push it onto stack 1. We then need to rollover from stack 3 to stack 2, stack 4 to stack 3, etc.
NOTE: You could make an argument that, rather than “rolling over,” we should be OK with some stacks not being at full capacity. This would improve the time complexity (by a fair amount, with a large number of elements), but it might get us into tricky situations later on if someone assumes that all stacks (other than the last) operate at full capacity. There’s no “right answer” here; discuss this trade-off with your interviewer!
1 public class SetOfStacks {
2 ArrayList<Stack> stacks = new ArrayList<Stack>();
3 public int capacity;
4 public SetOfStacks(int capacity) { this.capacity = capacity; }
5
6 public Stack getLastStack() {
7 if (stacks.size() == 0) return null;
8 return stacks.get(stacks.size() - 1);
9 }
10
11 public void push(int v) { /* see earlier code */ }
12 public int pop() {
13 Stack last = getLastStack();
14 System.out.println(stacks.size());
15 int v = last.pop();
16 if (last.size == 0) stacks.remove(stacks.size() - 1);
17 return v;
18 }
19
20 public int popAt(int index) {
21 return leftShift(index, true);
22 }
23
24 public int leftShift(int index, boolean removeTop) {
25 Stack stack = stacks.get(index);
26 int removed_item;
27 if (removeTop) removed_item = stack.pop();
28 else removed_item = stack.removeBottom();
29 if (stack.isEmpty()) {
30 stacks.remove(index);
31 } else if (stacks.size() > index + 1) {
32 int v = leftShift(index + 1, false);
Solutions to Chapter 3 | Stacks and Queues
117 Cracking the Coding Interview | Data Structures
33 stack.push(v);
34 }
35 return removed_item;
36 }
37 }
38
39 public class Stack {
40 private int capacity;
41 public Node top, bottom;
42 public int size = 0;
43
44 public Stack(int capacity) { this.capacity = capacity; }
45 public boolean isAtCapacity() { return capacity == size; }
46
47 public void join(Node above, Node below) {
48 if (below != null) below.above = above;
49 if (above != null) above.below = below;
50 }
51
52 public boolean push(int v) {
53 if (size >= capacity) return false;
54 size++;
55 Node n = new Node(v);
56 if (size == 1) bottom = n;
57 join(n, top);
58 top = n;
59 return true;
60 }
61
62 public int pop() {
63 Node t = top;
64 top = top.below;
65 size--;
66 return t.value;
67 }
68
69 public boolean isEmpty() { return size == 0; }
70 public int removeBottom() {
71 Node b = bottom;
72 bottom = bottom.above;
73 if (bottom != null) bottom.below = null;
74 size--;
75 return b.value;
76 }
77 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 118
3.4 In the classic problem of the Towers of Hanoi, you have 3 rods and N disks of different sizes which can slide onto any tower. The puzzle starts with disks sorted in ascending order of size from top to bottom (e.g., each disk sits on top of an even larger one). You have the following constraints:
(A) Only one disk can be moved at a time.
(B) A disk is slid off the top of one rod onto the next rod.
(C) A disk can only be placed on top of a larger disk.
Write a program to move the disks from the first rod to the last using Stacks.
pg 52
SOLUTION
We need to move N disks from Rod 1 to Rod 3, but let’s start from the beginning. Moving the top disk is easy - we just move it to Disk 3.
Can we move the top two disks? Yes:
1. Move Disk 1 from Rod 1 to Rod 2
2. Move Disk 2 from Rod 1 to Rod 3
3. Move Disk 1 from Rod 2 to Rod 3
Can we move the top three disks?
1. We know we can move the top two disks around from one Rod to another (as shown earlier), so let’s assume we have moved Disk 1 and 2 to Rod 2.
2. Move Disk 3 to Rod 3
3. Again we know we can move the top two disks around, so let’s move them from Rod 2 to Rod 3.
This approach leads to a natural recursive algorithm:
1 public static void main(String[] args)
2 int n = 5;
3 Tower[] towers = new Tower[n];
4 for (int i = 0; i < 3; i++) towers[i] = new Tower(i);
5 for (int i = n - 1; i >= 0; i--) towers[0].add(i);
6 towers[0].moveDisks(n, towers[2], towers[1]);
7 }
8
9 public class Tower {
10 private Stack<Integer> disks;
11 private int index;
12 public Tower(int i) {
Solutions to Chapter 3 | Stacks and Queues
119 Cracking the Coding Interview | Data Structures
13 disks = new Stack<Integer>();
14 index = i;
15 }
16
17 public int index() {
18 return index;
19 }
20
21 public void add(int d) {
22 if (!disks.isEmpty() && disks.peek() <= d) {
23 System.out.println(“Error placing disk ” + d);
24 } else {
25 disks.push(d);
26 }
27 }
28
29 public void moveTopTo(Tower t) {
30 int top = disks.pop();
31 t.add(top);
32 System.out.println(“Move disk ” + top + “ from ” + index() +
33 “ to ” + t.index());
34 }
35
36 public void print() {
37 System.out.println(“Contents of Tower “ + index());
38 for (int i = disks.size() - 1; i >= 0; i--) {
39 System.out.println(“ “ + disks.get(i));
40 }
41 }
42
43 public void moveDisks(int n, Tower destination, Tower buffer) {
44 if (n > 0) {
45 moveDisks(n - 1, buffer, destination);
46 moveTopTo(destination);
47 buffer.moveDisks(n - 1, destination, this);
48 }
49 }
50 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 120
3.5 Implement a MyQueue class which implements a queue using two stacks.
pg 52
SOLUTION
Since the major difference between a queue and a stack is the order (first-in-first-out vs. last-in-first-out), we know that we need to modify peek() and pop() to go in reverse order. We can use our second stack to reverse the order of the elements (by popping s1 and pushing the elements on to s2). In such an implementation, on each peek() and pop() operation, we would pop everything from s1 onto s2, perform the peek / pop operation, and then push everything back.
This will work, but if two pop / peeks are performed back-to-back, we’re needlessly moving elements. We can implement a “lazy” approach where we let the elements sit in s2.
s1 will thus be ordered with the newest elements on the top, while s2 will have the oldest elements on the top. We push the new elements onto s1, and peek and pop from s2. When s2 is empty, we’ll transfer all the elements from s1 onto s2, in reverse order.
1 public class MyQueue<T> {
2 Stack<T> s1, s2;
3 public MyQueue() {
4 s1 = new Stack<T>();
5 s2 = new Stack<T>();
6 }
7
8 public int size() {
9 return s1.size() + s2.size();
10 }
11
12 public void add(T value) {
13 s1.push(value);
14 }
15
16 public T peek() {
17 if (!s2.empty()) return s2.peek();
18 while (!s1.empty()) s2.push(s1.pop());
19 return s2.peek();
20 }
21
22 public T remove() {
23 if (!s2.empty()) return s2.pop();
24 while (!s1.empty()) s2.push(s1.pop());
25 return s2.pop();
26 }
27 }
Solutions to Chapter 3 | Stacks and Queues
121 Cracking the Coding Interview | Data Structures
3.6 Write a program to sort a stack in ascending order. You should not make any assumptions about how the stack is implemented. The following are the only functions that should be used to write this program: push | pop | peek | isEmpty.
pg 52
SOLUTION
Sorting can be performed with one more stack. The idea is to pull an item from the original stack and push it on the other stack. If pushing this item would violate the sort order of the new stack, we need to remove enough items from it so that it’s possible to push the new item. Since the items we removed are on the original stack, we’re back where we started. The algorithm is O(N^2) and appears below.
1 public static Stack<Integer> sort(Stack<Integer> s) {
2 Stack<Integer> r = new Stack<Integer>();
3 while(!s.isEmpty()) {
4 int tmp = s.pop();
5 while(!r.isEmpty() && r.peek() > tmp) {
6 s.push(r.pop());
7 }
8 r.push(tmp);
9 }
10 return r;
11 }

Solutions to Chapter 4 | Trees and Graphs
Cracking the Coding Interview | Data Structures
123
4.1 Implement a function to check if a tree is balanced. For the purposes of this question, a balanced tree is defined to be a tree such that no two leaf nodes differ in distance from the root by more than one.
pg 54
SOLUTION
The idea is very simple: the difference of min depth and max depth should not exceed 1, since the difference of the min and the max depth is the maximum distance difference possible in the tree.
1 public static int maxDepth(TreeNode root) {
2 if (root == null) {
3 return 0;
4 }
5 return 1 + Math.max(maxDepth(root.left), maxDepth(root.right));
6 }
7
8 public static int minDepth(TreeNode root) {
9 if (root == null) {
10 return 0;
11 }
12 return 1 + Math.min(minDepth(root.left), minDepth(root.right));
13 }
14
15 public static boolean isBalanced(TreeNode root){
16 return (maxDepth(root) - minDepth(root) <= 1);
17 }
Solutions to Chapter 4 | Trees and Graphs
124
CareerCup.com
4.2 Given a directed graph, design an algorithm to find out whether there is a route between two nodes.
pg 54
SOLUTION
This problem can be solved by just simple graph traversal, such as depth first search or breadth first search. We start with one of the two nodes and, during traversal, check if the other node is found. We should mark any node found in the course of the algorithm as ‘already visited’ to avoid cycles and repetition of the nodes.
1 public enum State {
2 Unvisited, Visited, Visiting;
3 }
4
5 public static boolean search(Graph g, Node start, Node end) {
6 LinkedList<Node> q = new LinkedList<Node>(); // operates as Stack
7 for (Node u : g.getNodes()) {
8 u.state = State.Unvisited;
9 }
10 start.state = State.Visiting;
11 q.add(start);
12 Node u;
13 while(!q.isEmpty()) {
14 u = q.removeFirst(); // i.e., pop()
15 if (u != null) {
16 for (Node v : u.getAdjacent()) {
17 if (v.state == State.Unvisited) {
18 if (v == end) {
19 return true;
20 } else {
21 v.state = State.Visiting;
22 q.add(v);
23 }
24 }
25 }
26 u.state = State.Visited;
27 }
28 }
29 return false;
30 }
Solutions to Chapter 4 | Trees and Graphs
125 Cracking the Coding Interview | Data Structures
4.3 Given a sorted (increasing order) array, write an algorithm to create a binary tree with minimal height.
pg 54
SOLUTION
We will try to create a binary tree such that for each node, the number of nodes in the left subtree and the right subtree are equal, if possible.
Algorithm:
1. Insert into the tree the middle element of the array.
2. Insert (into the left subtree) the left subarray elements
3. Insert (into the right subtree) the right subarray elements
4. Recurse
1 public static TreeNode addToTree(int arr[], int start, int end){
2 if (end < start) {
3 return null;
4 }
5 int mid = (start + end) / 2;
6 TreeNode n = new TreeNode(arr[mid]);
7 n.left = addToTree(arr, start, mid - 1);
8 n.right = addToTree(arr, mid + 1, end);
9 return n;
10 }
11
12 public static TreeNode createMinimalBST(int array[]) {
13 return addToTree(array, 0, array.length - 1);
14 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 126
4.4 Given a binary search tree, design an algorithm which creates a linked list of all the nodes at each depth (eg, if you have a tree with depth D, you’ll have D linked lists).
pg 54
SOLUTION
We can do a simple level by level traversal of the tree, with a slight modification of the breath-first traversal of the tree.
In a usual breath first search traversal, we simply traverse the nodes without caring which level we are on. In this case, it is critical to know the level. We thus use a dummy node to indicate when we have finished one level and are starting on the next.
1 ArrayList<LinkedList<TreeNode>> findLevelLinkList(TreeNode root) {
2 int level = 0;
3 ArrayList<LinkedList<TreeNode>> result =
4 new ArrayList<LinkedList<TreeNode>>();
5 LinkedList<TreeNode> list = new LinkedList<TreeNode>();
6 list.add(root);
7 result.add(level, list);
8 while (true) {
9 list = new LinkedList<TreeNode>();
10 for (int i = 0; i < result.get(level).size(); i++) {
11 TreeNode n = (TreeNode) result.get(level).get(i);
12 if (n != null) {
13 if(n.left != null) list.add(n.left);
14 if(n.right!= null) list.add(n.right);
15 }
16 }
17 if (list.size() > 0) {
18 result.add(level + 1, list);
19 } else {
20 break;
21 }
22 level++;
23 }
24 return result;
25 }
Solutions to Chapter 4 | Trees and Graphs
127 Cracking the Coding Interview | Data Structures
4.5 Write an algorithm to find the ‘next’ node (e.g., in-order successor) of a given node in a binary search tree where each node has a link to its parent.
pg 54
SOLUTION
We approach this problem by thinking very, very carefully about what happens on an in-order traversal. On an in-order traversal, we visit X.left, then X, then X.right.
So, if we want to find X.successor(), we do the following:
1. If X has a right child, then the successor must be on the right side of X (because of the order in which we visit nodes). Specifically, the left-most child must be the first node visited in that subtree.
2. Else, we go to X’s parent (call it P).
2.a. If X was a left child (P.left = X), then P is the successor of X
2.b. If X was a right child (P.right = X), then we have fully visited P, so we call successor(P).
1 public static TreeNode inorderSucc(TreeNode e) {
2 if (e != null) {
3 TreeNode p;
4 // Found right children -> return 1st inorder node on right
5 if (e.parent == null || e.right != null) {
6 p = leftMostChild(e.right);
7 } else {
8 // Go up until we’re on left instead of right (case 2b)
9 while ((p = e.parent) != null) {
10 if (p.left == e) {
11 break;
12 }
13 e = p;
14 }
15 }
16 return p;
17 }
18 return null;
19 }
20
21 public static TreeNode leftMostChild(TreeNode e) {
22 if (e == null) return null;
23 while (e.left != null) e = e.left;
24 return e;
25 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 128
4.6 Design an algorithm and write code to find the first common ancestor of two nodes in a binary tree. Avoid storing additional nodes in a data structure. NOTE: This is not necessarily a binary search tree.
pg 54
SOLUTION
If this were a binary search tree, we could do a modified find on the two nodes and see where the paths diverge. Unfortunately, this is not a binary search tree, so we much try other approaches.
Attempt #1:
If each node has a link to its parent, we could trace p and q’s paths up until they intersect.
Attempt #2:
Alternatively, you could follow a chain in which p and q are on the same side. That is, if p and q are both on the left of the node, branch left to look for the common ancestor. When p and q are no longer on the same side, you must have found the first common ancestor.
1 public Tree commonAncestor(Tree root, Tree p, Tree q) {
2 if (covers(root.left, p) && covers(root.left, q))
3 return commonAncestor(root.left, p, q);
4 if (covers(root.right, p) && covers(root.right, q))
5 return commonAncestor(root.right, p, q);
6 return root;
7 }
8 private boolean covers(Tree root, Tree p) { /* is p a child of root? */
9 if (root == null) return false;
10 if (root == p) return true;
11 return covers(root.left, p) || covers(root.right, p);
12 }
What is the running time of this algorithm? One way of looking at this is to see how many times each node is touched. Covers touches every child node, so we know that every single node in the tree must be touched at least once, and many nodes are touched multiple times.
Attempt #3:
For any node r, we know the following:
1. If p is on one side and q is on the other, r is the first common ancestor.
2. Else, the first common ancestor is on the left or the right side.
So, we can create a simple recursive algorithm called search that calls search(left side) and search(right side) looking at how many nodes (p or q) are placed from the left side and from the right side of the current node. If there are two nodes on one of the sides, then we have
Solutions to Chapter 4 | Trees and Graphs
129 Cracking the Coding Interview | Data Structures
to check if the child node on this side is p or q (because in this case the current node is the common ancestor). If the child node is neither p nor q, we should continue to search further (starting from the child).
If one of the searched nodes (p or q) is located on the right side of the current node, then the other node is located on the other side. Thus the current node is the common ancestor.
1 static int TWO_NODES_FOUND = 2;
2 static int ONE_NODE_FOUND = 1;
3 static int NO_NODES_FOUND = 0;
4
5 // Checks how many “special” nodes are located under this root
6 int covers(TreeNode root, TreeNode p, TreeNode q) {
7 int ret = NO_NODES_FOUND;
8 if (root == null) return ret;
9 if (root == p || root == q) ret += 1;
10 ret += covers(root.left, p, q);
11 if(ret == TWO_NODES_FOUND) // Found p and q
12 return ret;
13 return ret + covers(root.right, p, q);
14 }
15
16 TreeNode commonAncestor(TreeNode root, TreeNode p, TreeNode q) {
17 if (q == p && (root.left == q || root.right == q)) return root;
18 int nodesFromLeft = covers(root.left, p, q); // Check left side
19 if (nodesFromLeft == TWO_NODES_FOUND) {
20 if(root.left == p || root.left == q) return root.left;
21 else return commonAncestor(root.left, p, q);
22 } else if (nodesFromLeft == ONE_NODE_FOUND) {
23 if (root == p) return p;
24 else if (root == q) return q;
25 }
26 int nodesFromRight = covers(root.right, p, q); // Check right side
27 if(nodesFromRight == TWO_NODES_FOUND) {
28 if(root.right == p || root.right == q) return root.right;
29 else return commonAncestor(root.right, p, q);
30 } else if (nodesFromRight == ONE_NODE_FOUND) {
31 if (root == p) return p;
32 else if (root == q) return q;
33 }
34 if (nodesFromLeft == ONE_NODE_FOUND &&
35 nodesFromRight == ONE_NODE_FOUND) return root;
36 else return null;
37 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 130
4.7 You have two very large binary trees: T1, with millions of nodes, and T2, with hundreds of nodes. Create an algorithm to decide if T2 is a subtree of T1.
pg 54
SOLUTION
Note that the problem here specifies that T1 has millions of nodes—this means that we should be careful of how much space we use. Let’s say, for example, T1 has 10 million nodes—this means that the data alone is about 40 mb. We could create a string representing the inorder and preorder traversals. If T2’s preorder traversal is a substring of T1’s preorder traversal, and T2’s inorder traversal is a substring of T1’s inorder traversal, then T2 is a substring of T1. We can check this using a suffix tree. However, we may hit memory limitations because suffix trees are extremely memory intensive. If this become an issue, we can use an alternative approach.
Alternative Approach: The treeMatch procedure visits each node in the small tree at most once and is called no more than once per node of the large tree. Worst case runtime is at most O(n * m), where n and m are the sizes of trees T1 and T2, respectively. If k is the number of occurrences of T2’s root in T1, the worst case runtime can be characterized as O(n + k * m).
1 boolean containsTree(TreeNode t1, TreeNode t2) {
2 if (t2 == null) return true; // The empty tree is always a subtree
3 else return subTree(t1, t2);
4 }
5
6 boolean subTree(TreeNode r1, TreeNode r2) {
7 if (r1 == null)
8 return false; // big tree empty & subtree still not found.
9 if (r1.data == r2.data) {
10 if (matchTree(r1,r2)) return true;
11 }
12 return (subTree(r1.left, r2) || subTree(r1.right, r2));
13 }
14
15 boolean matchTree(TreeNode r1, TreeNode r2) {
16 if (r2 == null && r1 == null)
17 return true; // nothing left in the subtree
18 if (r1 == null || r2 == null)
19 return false; // big tree empty & subtree still not found
20 if (r1.data != r2.data)
21 return false; // data doesn’t match
22 return (matchTree(r1.left, r2.left) &&
23 matchTree(r1.right, r2.right));
24 }
25 }
Solutions to Chapter 4 | Trees and Graphs
131 Cracking the Coding Interview | Data Structures
4.8 You are given a binary tree in which each node contains a value. Design an algorithm to print all paths which sum up to that value. Note that it can be any path in the tree - it does not have to start at the root.
pg 54
SOLUTION
Let’s approach this problem by simplifying it. What if the path had to start at the root? In that case, we would have a much easier problem:
Start from the root and branch left and right, computing the sum thus far on each path. When we find the sum, we print the current path. Note that we don’t stop just because we found the sum. Why? Because we could have the following path (assume we are looking for the sum 5): 2 + 3 + –4 + 3 + 1 + 2. If we stopped once we hit 2 + 3, we’d miss several paths (2 + 3 + -4 + 3 + 1 and 3 + -4 + 3 + 1 + 2). So, we keep going along every possible path.
Now, what if the path can start anywhere? In that case, we make a small modification. On every node, we look “up” to see if we’ve found the sum. That is—rather than asking “does this node start a path with the sum?,” we ask “does this node complete a path with the sum?”
1 void findSum(TreeNode head, int sum, ArrayList<Integer> buffer,
2 int level) {
3 if (head == null) return;
4 int tmp = sum;
5 buffer.add(head.data);
6 for (int i = level;i >- 1; i--){
7 tmp -= buffer.get(i);
8 if (tmp == 0) print(buffer, i, level);
9 }
10 ArrayList<Integer> c1 = (ArrayList<Integer>) buffer.clone();
11 ArrayList<Integer> c2 = (ArrayList<Integer>) buffer.clone();
12 findSum(head.left, sum, c1, level + 1);
13 findSum(head.right, sum, c2, level + 1);
14 }
15
16 void print(ArrayList<Integer> buffer, int level, int i2) {
17 for (int i = level; i <= i2; i++) {
18 System.out.print(buffer.get(i) + “ ”);
19 }
20 System.out.println();
21 }
What is the time complexity of this algorithm? Well, if a node is at level r, we do r amount of work (that’s in the looking “up” step). We can take a guess at O(n lg n) (n nodes, doing an
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 132
average of lg n amount of work on each step), or we can be super mathematical:
There are 2^r nodes at level r.
1*2^1 + 2*2^2 + 3*2^3 + 4*2^4 + ... d * 2^d
= sum(r * 2^r, r from 0 to depth)
= 2 (d-1) * 2^d + 2
n = 2^d ==> d = lg n
NOTE: 2^lg(x) = x
O(2 (lg n - 1) * 2^(lg n) + 2) = O(2 (lg n - 1) * n ) = O(n lg n)
Following similar logic, our space complexity is O(n lg n).
Solutions to Chapter 5 | Bit Manipulation
Cracking the Coding Interview | Concepts and Algorithms
133
5.1 You are given two 32-bit numbers, N and M, and two bit positions, i and j. Write a method to set all bits between i and j in N equal to M (e.g., M becomes a substring of N located at i and starting at j).
EXAMPLE:
Input: N = 10000000000, M = 10101, i = 2, j = 6
Output: N = 10001010100
pg 58
SOLUTION
This code operates by clearing all bits in N between position i and j, and then ORing to put M in there.
1 public static int updateBits(int n, int m, int i, int j) {
2 int max = ~0; /* All 1’s */
3
4 // 1’s through position j, then 0’s
5 int left = max - ((1 << j) - 1);
6
7 // 1’s after position i
8 int right = ((1 << i) - 1);
9
10 // 1’s, with 0s between i and j
11 int mask = left | right;
12
13 // Clear i through j, then put m in there
14 return (n & mask) | (m << i);
15 }
Solutions to Chapter 5 | Bit Manipulation
134
CareerCup.com
5.2 Given a (decimal - e.g. 3.72) number that is passed in as a string, print the binary representation. If the number can not be represented accurately in binary, print “ERROR”
pg 58
SOLUTION
First, let’s start off by asking ourselves what a non-integer number in binary looks like. By analogy to a decimal number, the number n = 0.101 = 1 * (1/2^1) + 0 * (1/2^2) + 1 * (1/2^3).
Printing the int part of n is straight-forward (see below). To print the decimal part, we can multiply by 2 and check if the 2*n is greater than or equal to one. This is essentially “shifting” the fractional sum. That is:
r = 2*n = 2*0.101 = 1*(1 / 2^0) + 0*(1 / 2^1) + 1*(1 / 2^2) = 1.01
If r >= 1, then we know that n had a 1 right after the decimal point. By doing this continuously, we can check every digit.
1 public static String printBinary(String n) {
2 int intPart = Integer.parseInt(n.substring(0, n.indexOf(‘.’)));
3 double decPart = Double.parseDouble(
4 n.substring(n.indexOf(‘.’), n.length()));
5 String int_string = “”;
6 while (intPart > 0) {
7 int r = intPart % 2;
8 intPart >>= 1;
9 int_string = r + int_string;
10 }
11 StringBuffer dec_string = new StringBuffer();
12 while (decPart > 0) {
13 if (dec_string.length() > 32) return “ERROR”;
14 if (decPart == 1) {
15 dec_string.append((int)decPart);
16 break;
17 }
18 double r = decPart * 2;
19 if (r >= 1) {
20 dec_string.append(1);
21 decPart = r - 1;
22 } else {
23 dec_string.append(0);
24 decPart = r;
25 }
26 }
27 return int_string + “.” + dec_string.toString();
28 }
Solutions to Chapter 5 | Bit Manipulation
135 Cracking the Coding Interview | Concepts and Algorithms
5.3 Given an integer, print the next smallest and next largest number that have the same number of 1 bits in their binary representation.
pg 58
SOLUTION
The Brute Force Approach:
An easy approach is simply brute force: count the number of 1’s in n, and then increment (or decrement) until you find a number with the same number of 1’s. Easy - but not terribly interesting. Can we do something a bit more optimal? Yes!
Number Properties Approach for Next Number
Observations:
»»If we “turn on” a 0, we need to “turn off” a 1
»»If we turn on a 0 at bit i and turn off a 1 at bit j, the number changes by 2^i - 2^j.
»»If we want to get a bigger number with the same number of 1s and 0s, i must be bigger than j.
Solution:
1. Traverse from right to left. Once we’ve passed a 1, turn on the next 0. We’ve now increased the number by 2^i. Yikes! Example: xxxxx011100 becomes xxxxx111100
2. Turn off the one that’s just to the right side of that. We’re now bigger by 2^i - 2^(i-1) Example: xxxxx111100 becomes xxxxx101100
3. Make the number as small as possible by rearranging all the 1s to be as far right as possible: Example: xxxxx101100 becomes xxxxx100011
To get the previous number, we do the reverse.
1. Traverse from right to left. Once we’ve passed a zero, turn off the next 1. Example: xxxxx100011 becomes xxxxx000011.
2. Turn on the 0 that is directly to the right. Example: xxxxx000011 becomes xxxxx010011.
3. Make the number as big as possible by shifting all the ones as far to the left as possible. Example: xxxxx010011 becomes xxxxx011100 .
And now, for the code. Note the emphasis on pulling common code out into a reusable function. Your interviewer will look for “clean code” like this.
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 136
1 public static boolean GetBit(int n, int index) {
2 return ((n & (1 << index)) > 0);
3 }
4
5 public static int SetBit(int n, int index, boolean b) {
6 if (b) {
7 return n | (1 << index);
8 } else {
9 int mask = ~(1 << index);
10 return n & mask;
11 }
12 }
13
14 public static int GetNext_NP(int n) {
15 if (n <= 0) return -1;
16
17 int index = 0;
18 int countOnes = 0;
19
20 // Find first one.
21 while (!GetBit(n, index)) index++;
22
23 // Turn on next zero.
24 while (GetBit(n, index)) {
25 index++;
26 countOnes++;
27 }
28 n = SetBit(n, index, true);
29
30 // Turn off previous one
31 index--;
32 n = SetBit(n, index, false);
33 countOnes--;
34
35 // Set zeros
36 for (int i = index - 1; i >= countOnes; i--) {
37 n = SetBit(n, i, false);
38 }
39
40 // Set ones
41 for (int i = countOnes - 1; i >= 0; i--) {
42 n = SetBit(n, i, true);
43 }
44
45 return n;
Solutions to Chapter 5 | Bit Manipulation
137 Cracking the Coding Interview | Concepts and Algorithms
46 }
47
48 public static int GetPrevious_NP(int n) {
49 if (n <= 0) return -1; // Error
50
51 int index = 0;
52 int countZeros = 0;
53
54 // Find first zero.
55 while (GetBit(n, index)) index++;
56
57 // Turn off next 1.
58 while (!GetBit(n, index)) {
59 index++;
60 countZeros++;
61 }
62 n = SetBit(n, index, false);
63
64 // Turn on previous zero
65 index--;
66 n = SetBit(n, index, true);
67 countZeros--;
68
69 // Set ones
70 for (int i = index - 1; i >= countZeros; i--) {
71 n = SetBit(n, i, true);
72 }
73
74 // Set zeros
75 for (int i = countZeros - 1; i >= 0; i--) {
76 n = SetBit(n, i, false);
77 }
78
79 return n;
80 }
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 138
5.4 Explain what the following code does: ((n & (n-1)) == 0).
pg 58
SOLUTION
We can work backwards to solve this question.
What does it mean if A & B == 0?
It means that A and B never have a 1 bit in the same place. So if n & (n-1) == 0, then n and n-1 never share a 1.
What does n-1 look like (as compared with n)?
Try doing subtraction by hand (in base 2 or 10). What happens?
1101011000 [base 2]
- 1
= 1101010111 [base 2]
593100 [base 10]
- 1
= 593099 [base 10]
When you subtract 1 from a number, you look at the least significant bit. If it’s a 1 you change it to zero and you are done. If it’s a zero, you must “borrow” from a larger bit. So, you go to increasingly larger bits, changing each bit from a 0 to a 1, until you find a 1. You flip that one to a 0 and you are done.
Thus, n-1 will look like n, except that n’s initial 0s will be 1’s in n-1, and n’s least significant 1 will be a 0 in (n-1). That is:
if n = abcde1000
then n-1 = abcde0111
So what does n & (n-1) == 0 indicate?
n and (n-1) must have no 1s in common. Given that they look like this:
if n = abcde1000
then n-1 = abcde0111
abcde must be all 0s, which means that n must look like this: 00001000. n is therefore a power of two.
So, we have our answer: ((n & (n-1)) == 0) checks if n is a power of 2 (or 0).
Solutions to Chapter 5 | Bit Manipulation
139 Cracking the Coding Interview | Concepts and Algorithms
5.5 Write a function to determine the number of bits required to convert integer A to integer B.
Input: 31, 14
Output: 2
pg 58
SOLUTION
This seemingly complex problem is actually rather straightforward. To approach this, ask yourself how you would figure out which bits in two numbers are different. Simple: with an xor.
Each 1 in the xor will represent one different bit between A and B. We then simply need to count the number of bits that are 1.
1 public static int bitSwapRequired(int a, int b) {
2 int count = 0;
3 for (int c = a ^ b; c != 0; c = c >> 1) {
4 count += c & 1;
5 }
6 return count;
7 }
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 140
5.6 Write a program to swap odd and even bits in an integer with as few instructions as possible (e.g., bit 0 and bit 1 are swapped, bit 2 and bit 3 are swapped, etc).
pg 58
SOLUTION
Mask all odd bits with 10101010 in binary (which is 0xAA), then shift them left to put them in the even bits. Then, perform a similar operation for even bits. This takes a total 5 instructions.
1 public static int swapOddEvenBits(int x) {
2 return ( ((x & 0xaaaaaaaa) >> 1) | ((x & 0x55555555) << 1) );
3 }
Solutions to Chapter 5 | Bit Manipulation
141 Cracking the Coding Interview | Concepts and Algorithms
5.7 An array A[1... n] contains all the integers from 0 to n except for one number which is missing. In this problem, we cannot access an entire integer in A with a single operation. The elements of A are represented in binary, and the only operation we can use to access them is “fetch the jth bit of A[i]”, which takes constant time. Write code to find the missing integer. Can you do it in O(n) time?
pg 58
SOLUTION
Picture a list of binary numbers between 0 to n. What will change when we remove one number? We’ll get an imbalance of 1s and 0s in the least significant bit. That is, before removing the number k, we have this list of least significant bits (in some order):
0 0 0 0 0 1 1 1 1 1 OR 0 0 0 0 0 1 1 1 1
Suppose we secretly removed either a 1 or a 0 from this list. Could we tell which one was removed?
remove(0 from 0 0 0 0 0 1 1 1 1 1) --> 0 0 0 0 1 1 1 1 1
remove(1 from 0 0 0 0 0 1 1 1 1 1) --> 0 0 0 0 0 1 1 1 1
remove(0 from 0 0 0 0 0 1 1 1 1) --> 0 0 0 0 1 1 1 1
remove(1 from 0 0 0 0 0 1 1 1 1) --> 0 0 0 0 0 1 1 1
Note that if 0 is removed, we always wind up with count(1) >= count(0). If 1 is removed, we wind up with count(1) < count(0). Therefore, we can look at the least significant bit to figure out in O(N) time whether the missing number has a 0 or a 1 in the least significant bit (LSB). If LSB(missing) == 0, then we can discard all numbers with LSB = 1. If LSB(missing) == 1, we can discard all numbers with LSB = 0.
What about the next iteration, with the second least significant bit (SLSB)? We’ve discarded all the numbers with LSB = 1, so our list looks something like this (if n = 5, and missing = 3):
00000
00001
00010
-----
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
Our SLSBs now look like 0 0 1 0 1 0. Using the same logic as we applied for LSB, we can figure out that the missing number must have SLSB = 1. Our number must look like xxxx11.
Third iteration, discarding numbers with SLSB = 0:
00000
00001
00010
-----
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
We can now compute that count(TLSB = 1) = 1 and count(TLSB = 1) = 1. Therefore, TLSB = 0. We can recurse repeatedly, building our number bit by bit:
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 142
1 int findMissing(ArrayList<BitInteger> array) {
2 return findMissing(array, BitInteger.INTEGER_SIZE - 1);
3 }
4
5 int findMissing(ArrayList<BitInteger> input, int column) {
6 if (column < 0) { // Base case and error condition
7 return 0;
8 }
9 ArrayList<BitInteger> oddIndices = new ArrayList<BitInteger>();
10 ArrayList<BitInteger> evenIndices = new ArrayList<BitInteger>();
11 for (BitInteger t : input) {
12 if (t.fetch(column) == 0) {
13 evenIndices.add(t);
14 } else {
15 oddIndices.add(t);
16 }
17 }
18 if (oddIndices.size() >= evenIndices.size()) {
19 return (findMissing(evenIndices, column - 1)) << 1 | 0;
20 } else {
21 return (findMissing(oddIndices, column - 1)) << 1 | 1;
22 }
23 }
24
What is the run-time of this algorithm? On the first pass, we look at O(N) bits. On the second pass, we’ve eliminated N/2 numbers, so we then look at O(N/2) bits. On the third pass, we have eliminated another half of the numbers, so we then look at O(N/4) bits. If we keep going, we get an equation that looks like:
O(N) + O(N/2) + O(N/4) + O(N/8) + ... = O(2N) = O(N)
Our run-time is O(N).
Solutions to Chapter 6 | Brain Teasers
Cracking the Coding Interview | Concepts and Algorithms
143
6.1 Add arithmetic operators (plus, minus, times, divide) to make the following expression true: 3 1 3 6 = 8. You can use any parentheses you’d like.
pg 60
SOLUTION
An interviewer is asking this problem to see how you think and approach problems—so don’t just guess randomly.
Try approaching this the following way: What sorts of operations would get us to 8? I can think of a few:
4 * 2 = 8
16 / 2 = 8
4 + 4 = 8
Let’s start with the first one. Is there any way to make 3 1 3 6 produce 4 * 2? We can easily notice that 3 + 1 = 4 (the first two numbers). We can also notice that 6 / 3 = 2. If we had “3 1 6 3”, we’d be done, since (3 + 1)*(6 / 3) = 8. Although it seems a little unconventional to do this, we can, in fact, just flip the 6 and the 3 to get the solution:
(( 3 + 1 ) / 3) * 6 = 8
Solutions to Chapter 6 | Brain Teasers
144
CareerCup.com
6.2 There is an 8x8 chess board in which two diagonally opposite corners have been cut off. You are given 31 dominos, and a single domino can cover exactly two squares. Can you use the 31 dominos to cover the entire board? Prove your answer (by providing an example, or showing why it’s impossible).
pg 60
SOLUTION
Impossible. Here’s why: The chess board initially has 32 black and 32 white squares. By removing opposite corners (which must be the same color), we’re left with 30 of one color and 32 of the other color. Let’s say, for the sake of argument, that we have 30 black and 32 white squares.
When we lay down each domino, we’re taking up one white and one black square. Therefore, 31 dominos will take up 31 white squares and 31 black squares exactly. On this board, however, we must have 30 black squares and 32 white squares. Hence, it is impossible.
Solutions to Chapter 6 | Brain Teasers
145 Cracking the Coding Interview | Concepts and Algorithms
6.3 You have a five quart jug and a three quart jug, and an unlimited supply of water (but no measuring cups). How would you come up with exactly four quarts of water?
NOTE: The jugs are oddly shaped, such that filling up exactly ‘half’ of the jug would be impossible.
pg 60
SOLUTION
We can pour water back and forth between the two jugs as follows:
5 Quart Contents
3 Quart Contents
Note
5
0
Filled 5 quart jug
2
3
Filled 3Q with 5Q’s contents
0
2
Dumped 3Q
5
2
Filled 5Q
4
3
Fill remainder of 3Q with 5Q
4
Done! We have four quarts.
OBSERVATIONS AND SUGGESTIONS:
»»Many brain teasers have a math / CS root to them—this is one of them! Note that as long as the two jug sizes are relatively prime (i.e., have no common prime factors), you can find a pour sequence for any value between 1 and the sum of the jug sizes.
Solutions to Chapter 6 | Brain Teasers
. CareerCup com 146
6.4 A bunch of men are on an island. A genie comes down and gathers everyone together and places a magical hat on some people’s heads (i.e., at least one person has a hat). The hat is magical: it can be seen by other people, but not by the wearer of the hat himself. To remove the hat, those (and only those who have a hat) must dunk themselves underwater at exactly midnight. If there are n people and c hats, how long does it take the men to remove the hats? The men cannot tell each other (in any way) that they have a hat.
FOLLOW UP
Prove that your solution is correct.
pg 60
SOLUTION
This problem seems hard, so let’s simplify it by looking at specific cases.
Case c = 1: Exactly one man is wearing a hat.
Assuming all the men are intelligent, the man with the hat should look around and realize that no one else is wearing a hat. Since the genie said that at least one person is wearing a hat, he must conclude that he is wearing a hat. Therefore, he would be able to remove it that night.
Case c = 2: Exactly two men are wearing hats.
The two men with hats see one hat, and are unsure whether c = 1 or c = 2. They know, from the previous case, that if c = 1, the hats would be removed on Night #1. Therefore, if the other man still has a hat, he must deduce that c = 2, which means that he has a hat. Both men would then remove the hats on Night #2
Case General: If c = 3, then each man is unsure whether c = 2 or 3. If it were 2, the hats would be removed on Night #2. If they are not, they must deduce that c = 3, and therefore they have a hat. We can follow this logic for c = 4, 5, …
Proof by Induction
Using induction to prove a statement P(n)
If (1) P(1) = TRUE (e.g., the statement is true when n = 1)
AND (2) if P(n) = TRUE -> P(n+1) = TRUE (e.g., P(n+1) is true whenever P(2) is true).
THEN P(n) = TRUE for all n >= 1.
Explanation
»»Condition 2 sets up an infinite deduction chain: P(1) implies P(2) implies P(3) implies ... P(n) implies P(n+1) implies ...
Solutions to Chapter 6 | Brain Teasers
147 Cracking the Coding Interview | Concepts and Algorithms
»»Condition one (P(1) is true) ignites this chain, with truth cascading off into infinity.
Base Case: c = 1 (See previous page).
Assume true for c hats. i.e., if there are c hats, it will take c nights to remove all of them.
Prove true for c+1 hats.
Each man with a hat sees c hat, and can not be immediately sure whether there are c hats or c+1 hats. However, he knows that if there are c hats, it will take exactly c nights to remove them. Therefore, when c nights have passed and everyone still has a hat, he can only conclude that there are c+1 hats. He must know that he is wearing a hat. Each man makes the same conclusion and simultaneously removes the hats on night c+1.
Therefore, we have met the principles of induction. We have proven that it will take c nights to remove c hats.
Solutions to Chapter 6 | Brain Teasers
. CareerCup com 148
6.5 There is a building of 100 floors. If an egg drops from the Nth floor or above it will break. If it’s dropped from any floor below, it will not break. You’re given 2 eggs. Find N, while minimizing the number of drops for the worst case.
pg 60
SOLUTION
Observation: Regardless of how we drop Egg1, Egg2 must do a linear search. i.e., if Egg1 breaks between floor 10 and 15, we have to check every floor in between with the Egg2
The Approach:
A First Try: Suppose we drop an egg from the 10th floor, then the 20th, …
»»If the first egg breaks on the first drop (Floor 10), then we have at most 10 drops total.
»»If the first egg breaks on the last drop (Floor 100), then we have at most 19 drops total (floors 10, 20, ... ,90, 100, then 91 through 99).
»»That’s pretty good, but all we’ve considered is the absolute worst case. We should do some “load balancing” to make those two cases more even.
Goal: Create a system for dropping Egg1 so that the most drops required is consistent, whether Egg1 breaks on the first drop or the last drop.
1. A perfectly load balanced system would be one in which Drops of Egg1 + Drops of Egg2 is always the same, regardless of where Egg1 broke.
2. For that to be the case, since each drop of Egg1 takes one more step, Egg2 is allowed one fewer step.
3. We must, therefore, reduce the number of steps potentially required by Egg2 by one drop each time. For example, if Egg1 is dropped on Floor 20 and then Floor 30, Egg2 is potentially required to take 9 steps. When we drop Egg1 again, we must reduce potential Egg2 steps to only 8. That is, we must drop Egg1 at floor 39.
4. We know, therefore, Egg1 must start at Floor X, then go up by X-1 floors, then X-2, …, until it gets to 100.
5. Solve for X+(X-1)+(X-2)+…+1 = 100. X(X+1)/2 = 100 -> X = 14
We go to Floor 14, then 27, then 39, … This takes 14 steps maximum.
Solutions to Chapter 6 | Brain Teasers
149 Cracking the Coding Interview | Concepts and Algorithms
6.6 There are one hundred closed lockers in a hallway. A man begins by opening all one hundred lockers. Next, he closes every second locker. Then he goes to every third locker and closes it if it is open or opens it if it is closed (e.g., he toggles every third locker). After his one hundredth pass in the hallway, in which he toggles only locker number one hundred, how many lockers are open?
pg 60
SOLUTION
Question: For which rounds is a door toggled (open or closed)?
A door n is toggled once for each factor of n, including itself and 1. That is, door 15 is toggled on round 1, 3, 5, and 15.
Question: When would a door be left open?
Answer: A door is left open if the number of factors (x) is odd. You can think about this by pairing factors off as an open and a close. If there’s one remaining, the door will be open.
Question: When would x be odd?
Answer: x is odd if n is a perfect square. Here’s why: pair n’s factors by their complements. For example, if n is 36, the factors are (1, 36), (2, 18), (3, 12), (4, 9), (6, 6). Note that (6, 6) only contributes 1 factor, thus giving n an odd number of factors.
Question: How many perfect squares are there?
Answer: There are 10 perfect squares. You could count them (1, 4, 9, 16, 25, 36, 49, 64, 81, 100), or you could simply realize that you can take the numbers 1 through 10 and square them (1*1, 2*2, 3*3, ... , 10*10).
Therefore, there are 10 lockers open.

Solutions to Chapter 7 | Object Oriented Design
Cracking the Coding Interview | Concepts and Algorithms
151
7.1 Design the data structures for a generic deck of cards. Explain how you would subclass it to implement particular card games.
pg 62
SOLUTION
1 public class Card {
2 public enum Suit {
3 CLUBS (1), SPADES (2), HEARTS (3), DIAMONDS (4);
4 int value;
5 private Suit(int v) { value = v; }
6 };
7
8 private int card;
9 private Suit suit;
10
11 public Card(int r, Suit s) {
12 card = r;
13 suit = s;
14 }
15
16 public int value() { return card; }
17 public Suit suit() { return suit; }
18 }
Assume that we’re building a blackjack game, so we need to know the value of the cards. Face cards are ten and an ace is 11 (most of the time, but that’s the job of the Hand class, not the following class).
1 public class BlackJackCard extends Card {
2 public BlackJackCard(int r, Suit s) { super(r, s); }
3
4 public int value() {
5 int r = super.value();
6 if (r == 1) return 11; // aces are 11
7 if (r < 10) return r;
8 return 10;
9 }
10
11 boolean isAce() {
12 return super.value() == 1;
13 }
14 }
Solutions to Chapter 7 | Object Oriented Design
152
CareerCup.com
7.2 Imagine you have a call center with three levels of employees: fresher, technical lead (TL), product manager (PM). There can be multiple employees, but only one TL or PM. An incoming telephone call must be allocated to a fresher who is free. If a fresher can’t handle the call, he or she must escalate the call to technical lead. If the TL is not free or not able to handle it, then the call should be escalated to PM. Design the classes and data structures for this problem. Implement a method getCallHandler().
pg 62
SOLUTION
All three ranks of employees have different work to be done, so those specific functions are profile specific. We should keep these specific things within their respective class.
There are a few things which are common to them, like address, name, job title, age, etc. These things can be kept in one class and can be extended / inherited by others.
Finally, there should be one CallHandler class which would route the calls to the concerned person.
NOTE: On any object oriented design question, there are many ways to design the objects. Discuss the trade-offs of different solutions with your interviewer. You should usually design for long term code flexibility and maintenance.
1 public class CallHandler {
2 static final int LEVELS = 3; // we have 3 levels of employees
3 static final int NUM_FRESHERS = 5; // we have 5 freshers
4 ArrayList<Employee>[] employeeLevels = new ArrayList[LEVELS];
5 // queues for each call’s rank
6 Queue<Call>[] callQueues = new LinkedList[LEVELS];
7
8 public CallHandler() { ... }
9
10 Employee getCallHandler(Call call) {
11 for (int level = call.rank; level < LEVELS - 1; level++) {
12 ArrayList<Employee> employeeLevel = employeeLevels[level];
13 for (Employee emp : employeeLevel) {
14 if (emp.free) {
15 return emp;
16 }
17 }
18 }
19 return null;
20 }
21
22 // routes the call to an available employee, or adds to a queue
Solutions to Chapter 7 | Object Oriented Design
153 Cracking the Coding Interview | Concepts and Algorithms
23 void dispatchCall(Call call) {
24 // try to route the call to an employee with minimal rank
25 Employee emp = getCallHandler(call);
26 if (emp != null) {
27 emp.ReceiveCall(call);
28 } else {
29 // place the call into queue according to its rank
30 callQueues[call.rank].add(call);
31 }
32 }
33 void getNextCall(Employee e) {...} // look for call for e’s rank
34 }
35
36 class Call {
37 int rank = 0; // minimal rank of employee who can handle this call
38 public void reply(String message) { ... }
39 public void disconnect() { ... }
40 }
41
42 class Employee {
43 CallHandler callHandler;
44 int rank; // 0- fresher, 1 - technical lead, 2 - product manager
45 boolean free;
46 Employee(int rank) { this.rank = rank; }
47 void ReceiveCall(Call call) { ... }
48 void CallHandled(Call call) { ... } // call is complete
49 void CannotHandle(Call call) { // escalate call
50 call.rank = rank + 1;
51 callHandler.dispatchCall(call);
52 free = true;
53 callHandler.getNextCall(this); // look for waiting call
54 }
55 }
56
57 class Fresher extends Employee {
58 public Fresher() { super(0); }
59 }
60 class TechLead extends Employee {
61 public TechLead() { super(1); }
62 }
63 class ProductManager extends Employee {
64 public ProductManager() { super(2); }
65 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 154
7.3 Design a musical juke box using object oriented principles.
pg 62
SOLUTION
Let’s first understand the basic system components:
»»CD player
»»CD
»»Display () (displays length of song, remaining time and playlist)
Now, let’s break this down further:
»»Playlist creation (includes add, delete, shuffle etc sub functionalities)
»»CD selector
»»Track selector
»»Queueing up a song
»»Get next song from playlist
A user also can be introduced:
»»Adding
»»Deleting
»»Credit information
How do we group this functionality based on Objects (data + functions which go together)?
Object oriented design suggests wrapping up data with their operating functions in a single entity class.
1 public class CD { }
2 public class CDPlayer {
3 private Playlist p;
4 private CD c;
5 public Playlist getPlaylist() { return p; }
6 public void setPlaylist(Playlist p) { this.p = p; }
7 public CD getCD() { return c; }
8 public void setCD(CD c) { this.c = c; }
9 public CDPlayer(Playlist p) { this.p = p; }
10 public CDPlayer(CD c, Playlist p) { ... }
11 public CDPlayer(CD c) { this.c = c; }
12 public void playTrack(Song s) { ... }
13 }
14
15 public class JukeBox {
Solutions to Chapter 7 | Object Oriented Design
155 Cracking the Coding Interview | Concepts and Algorithms
16 private CDPlayer cdPlayer;
17 private User user;
18 private Set<CD> cdCollection;
19 private TrackSelector ts;
20
21 public JukeBox(CDPlayer cdPlayer, User user, Set<CD> cdCollection,
22 TrackSelector ts) { ... }
23 public Song getCurrentTrack() { return ts.getCurrentSong(); }
24 public void processOneUser(User u) { this.user = u; }
25 }
26
27 public class Playlist {
28 private Song track;
29 private Queue<Song> queue;
30 public Playlist(Song track, Queue<Song> queue) { ... }
31 public Song getNextTrackToPlay(){ return queue.peek(); }
32 public void queueUpTrack(Song s){ queue.add(s); }
33 }
34
35 public class Song {
36 private String songName;
37 }
38
39 public class TrackSelector {
40 private Song currentSong;
41 public TrackSelector(Song s) { currentSong=s; }
42 public void setTrack(Song s) { currentSong = s; }
43 public Song getCurrentSong() { return currentSong; }
44 }
45
46 public class User {
47 private String name;
48 public String getName() { return name; }
49 public void setName(String name) { this.name = name; }
50 public long getID() { return ID; }
51 public void setID(long iD) { ID = iD; }
52 private long ID;
53 public User(String name, long iD) { ... }
54 public User getUser() { return this; }
55 public static User addUser(String name, long iD) { ... }
56 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 156
7.4 Design a chess game using object oriented principles.
pg 62
SOLUTION
1 public class ChessPieceTurn { };
2 public class GameManager {
3 void processTurn(PlayerBase player) { };
4 boolean acceptTurn(ChessPieceTurn turn) { return true; };
5 Position currentPosition;
6 }
7
8 public abstract class PlayerBase {
9 public abstract ChessPieceTurn getTurn(Position p);
10 }
11 class ComputerPlayer extends PlayerBase {
12 public ChessPieceTurn getTurn(Position p) { return null; }
13 public void setDifficulty() { };
14 public PositionEstimator estimater;
15 public PositionBackTracker backtracter;
16 }
17 public class HumanPlayer extends PlayerBase {
18 public ChessPieceTurn getTurn(Position p) { return null; }
19 }
20
21 public abstract class ChessPieceBase {
22 abstract boolean canBeChecked();
23 abstract boolean isSupportCastle();
24 }
25 public class King extends ChessPieceBase { ... }
26 public class Queen extends ChessPieceBase { ... }
27
28 public class Position { // represents chess positions in compact form
29 ArrayList<ChessPieceBase> black;
30 ArrayList<ChessPieceBase> white;
31 }
32
33 public class PositionBackTracker {
34 public static Position getNext(Position p) { return null; }
35 }
36 public class PositionEstimator {
37 public static PositionPotentialValue estimate(Position p) { ... }
38 }
39 public abstract class PositionPotentialValue {
40 abstract boolean lessThan(PositionPotentialValue pv);
41 }
Solutions to Chapter 7 | Object Oriented Design
157 Cracking the Coding Interview | Concepts and Algorithms
7.5 Design the data structures for an online book reader system.
pg 62
SOLUTION
Since the problem doesn’t describe much about the functionality, let’s assume we want to design a basic online reading system which provides the following functionality:
»»User membership creation and extension.
»»Searching the database of books
»»Reading the books
To implement these we may require many other functions, like get, set, update, etc. Objects required would likely include User, Book, and Library.
The following code / object oriented design describes this functionality:
1 public class Book {
2 private long ID;
3 private String details;
4 private static Set<Book> books;
5
6 public Book(long iD, String details) { ... }
7 public static void addBook(long iD, String details){
8 books.add(new Book(iD, details));
9 }
10
11 public void update() { }
12 public static void delete(Book b) { books.remove(b); }
13 public static Book find(long id){
14 for (Book b : books)
15 if(b.getID() == id) return b;
16 return null;
17 }
18 }
19
20 public class User {
21 private long ID;
22 private String details;
23 private int accountType;
24 private static Set<User> users;
25
26 public Book searchLibrary(long id) { return Book.find(id); }
27 public void renewMembership() { ... }
28
29 public static User find(long ID) {
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 158
30 for (User u : users) {
31 if (u.getID() == ID) return u;
32 }
33 return null;
34 }
35
36 public static void addUser(long ID, String details,
37 int accountType) {
38 users.add(new User(ID, details, accountType));
39 }
40
41 public User(long iD, String details, int accountType) { ... }
42 }
43
44 public class OnlineReaderSystem {
45 private Book b;
46 private User u;
47 public OnlineReaderSystem(Book b, User u) { ... }
48 public void listenRequest() { }
49 public Book searchBook(long ID) { return Book.find(ID); }
50 public User searchUser(long ID){ return User.find(ID); }
51 public void display() { }
52 }
This design is a very simplistic implementation of such a system. We have a class for User to keep all the information regarding the user, and an identifier to identify each user uniquely. We can add functionality like registering the user, charging a membership amount and monthly / daily quota, etc.
Next, we have book class where we will keep all the book’s information. We would also implement functions like add / delete / update books.
Finally, we have a manager class for managing the online book reader system which would have a listen function to listen for any incoming requests to log in. It also provides book search functionality and display functionality. Because the end user interacts through this class, search must be implemented here.
Solutions to Chapter 7 | Object Oriented Design
159 Cracking the Coding Interview | Concepts and Algorithms
7.6 Implement a jigsaw puzzle. Design the data structures and explain an algorithm to solve the puzzle.
pg 62
SOLUTION
1 class Edge {
2 enum Type { inner, outer, flat }
3 Piece parent;
4 Type type;
5 bool fitsWith(Edge type) { ... }; // Inners & outer fit together.
6 }
7 class Piece {
8 Edge left, right, top, bottom;
9 Orientation solvedOrientation = ...; // 90, 180, etc
10 }
11 class Puzzle {
12 Piece[][] pieces; /* Remaining pieces left to put away. */
13 Piece[][] solution;
14 Edge[] inners, outers, flats;
15 /* We’re going to solve this by working our way in-wards, starting
16 * with the corners. This is a list of the inside edges. */
17 Edge[] exposed_edges;
18
19 void sort() {
20 /* Iterate through all edges, adding each to inners, outers,
21 * etc, as appropriate. Look for the corners—add those to
22 * solution. Add each non-flat edge of the corner to
23 * exposed_edges. */
24 }
25
26 void solve() {
27 foreach edge1 in exposed_edges {
28 /* Look for a match to edge1 */
29 if (edge1.type == Edge.Type.inner) {
30 foreach edge2 in outers {
31 if edge1.fitsWith(edge2) {
32 /* We found a match! Remove edge1 from
33 * exposed_edges. Add edge2’s piece to
34 * solution. Check which edges of edge2 are
35 * exposed, and add those to exposed_edges. */
36 }
37 }
38 /* Do the same thing, swapping inner & outer. */
39 }
40 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 160
41 }
42 }
Overview:
1. We grouped the edges by their type. Because inners go with outers, and vice versa, this enables us to go straight to the potential matches.
We keep track of the inner perimeter of the puzzle (exposed_edges) as we work our way inwards. exposed_edges is initialized to be the corner’s edges.
Solutions to Chapter 7 | Object Oriented Design
161 Cracking the Coding Interview | Concepts and Algorithms
7.7 Explain how you would design a chat server. In particular, provide details about the various backend components, classes, and methods. What would be the hardest problems to solve?
pg 62
SOLUTION
What is our chat server?
This is something you should discuss with your interviewer, but let’s make a couple of assumptions: imagine we’re designing a basic chat server that needs to support a small number of users. People have a contact list, they see who is online vs offline, and they can send text-based messages to them. We will not worry about supporting group chat, voice chat, etc. We will also assume that contact lists are mutual: I can only talk to you if you can talk to me. Let’s keep it simple.
What specific actions does it need to support?
»»User A signs online
»»User A asks for their contact list, with each person’s current status.
»»Friends of User A now see User A as online
»»User A adds User B to contact list
»»User A sends text-based message to User B
»»User A changes status message and/or status type
»»User A removes User B
»»User A signs offline
What can we learn about these requirements?
We must have a concept of users, add request status, online status, and messages.
What are the core components? We’ll need a database to store items and an “always online” application as the server. We might recommend using XML for the communication between the chat server and the clients, as it’s easy for a person and a machine to read.
What are the key objects and methods?
We have listed the key objects and methods below. Note that we have hidden many of the details, such as how to actually push the data out to a client.
1 enum StatusType {
2 online, offline, away;
3 }
4
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 162
5 class Status {
6 StatusType status_type;
7 String status_message;
8 }
9
10 class User {
11 String username;
12 String display_name;
13 User[] contact_list;
14 AddRequest[] requests;
15 boolean updateStatus(StatusType stype, String message) { … };
16 boolean addUserWithUsername(String name);
17 boolean approveRequest(String username);
18 boolean denyRequest(String username);
19 boolean removeContact(String username);
20 boolean sendMessage(String username, String message);
21 }
22 /* Holds data that from_user would like to add to_user */
23 class AddRequest {
24 User from_user;
25 User to_user;
26 }
27 class Server {
28 User getUserByUsername(String username);
29 }
What problems would be the hardest to solve (or the most interesting)?
Q1 How do we know if someone is online—I mean, really, really know?
While we would like users to tell us when they sign off, we can’t know for sure. A user’s connection might have died, for example. To make sure that we know when a user has signed off, we might try regularly pinging the client to make sure it’s still there.
Q2 How do we deal with conflicting information?
We have some information stored in the computer’s memory and some in the database. What happens if they get out of sync? Which one is “right”?
Q3 How do we make our server scale?
While we designed out chat server without worrying—too much– about scalability, in real life this would be a concern. We’d need to split our data across many servers, which would increase our concern about out of sync data.
Q4 How we do prevent denial of service attacks?
Clients can push data to us—what if they try to DOS us? How do we prevent that?
Solutions to Chapter 7 | Object Oriented Design
163 Cracking the Coding Interview | Concepts and Algorithms
7.8 Othello is played as follows: Each Othello piece is white on one side and black on the other. When a piece is surrounded by its opponents on both the left and right sides, or both the top and bottom, it is said to be captured and its color is flipped. On your turn, you must capture at least one of your opponent’s pieces. The game ends when either user has no more valid moves, and the win is assigned to the person with the most pieces. Implement the object oriented design for Othello.
pg 62
SOLUTION
Othello has these major steps:
2. Game () which would be the main function to manage all the activity in the game:
3. Initialize the game which will be done by constructor
4. Get first user input
5. Validate the input
6. Change board configuration
7. Check if someone has won the game
8. Get second user input
9. Validate the input
10. Change the board configuration
11. Check if someone has won the game...
NOTE: The full code for Othello is contained in the code attachment.
1 public class Question {
2 private final int white = 1;
3 private final int black = 2;
4 private int[][] board;
5
6 /* Sets up the board in the standard othello starting positions,
7 * and starts the game */
8 public void start () { ... }
9
10 /* Returns the winner, if any. If there are no winners, returns
11 * 0 */
12 private int won() {
13 if (!canGo (white) && !canGo (black)) {
14 int count = 0;
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 164
15 for (int i = 0; i < 8; i++) {
16 for (int j = 0; j < 8; j++) {
17 if (board [i] [j] == white) {
18 count++;
19 }
20 if (board [i] [j] == black) {
21 count--;
22 }
23 }
24 }
25 if (count > 0) return white;
26 if (count < 0) return black;
27 return 3;
28 }
29 return 0;
30 }
31
32 /* Returns whether the player of the specified color has a valid
33 * move in his turn. This will return false when
34 * 1. none of his pieces are present
35 * 2. none of his moves result in him gaining new pieces
36 * 3. the board is filled up
37 */
38 private boolean canGo(int color) { ... }
39
40 /* Returns if a move at coordinate (x,y) is a valid move for the
41 * specified player */
42 private boolean isValid(int color, int x, int y) { ... }
43
44 /* Prompts the player for a move and the coordinates for the move.
45 * Throws an exception if the input is not valid or if the entered
46 * coordinates do not make a valid move. */
47 private void getMove (int color) throws Exception { ... }
48
49 /* Adds the move onto the board, and the pieces gained from that
50 * move. Assumes the move is valid. */
51 private void add (int x, int y, int color) { ... }
52
53 /* The actual game: runs continuously until a player wins */
54 private void game() {
55 printBoard();
56 while (won() == 0) {
57 boolean valid = false;
58 while (!valid) {
59 try {
60 getMove(black);
Solutions to Chapter 7 | Object Oriented Design
165 Cracking the Coding Interview | Concepts and Algorithms
61 valid = true;
62 } catch (Exception e) {
63 System.out.println (“Enter a valid coordinate!”);
64 }
65 }
66 valid = false;
67 printBoard();
68 while (!valid) {
69 try {
70 getMove(white);
71 valid = true;
72 } catch (Exception e) {
73 System.out.println (“Enter a valid coordinate!”);
74 }
75 }
76 printBoard ();
77 }
78
79 if (won()!=3) {
80 System.out.println (won () == 1 ? “white” : “black” +
81 “ won!”);
82 } else {
83 System.out.println(“It’s a draw!”);
84 }
85 }
86 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 166
7.9 Explain the data structures and algorithms that you would use to design an in-memory file system. Illustrate with an example in code where possible.
pg 62
SOLUTION
For data block allocation, we can use bitmask vector and linear search (see “Practical File System Design”) or B+ trees (see Wikipedia).
1 struct DataBlock { char data[DATA_BLOCK_SIZE]; };
2 DataBlock dataBlocks[NUM_DATA_BLOCKS];
3 struct INode { std::vector<int> datablocks; };
4 struct MetaData {
5 int size;
6 Date last_modifed, created;
7 char extra_attributes;
8 };
9 std::vector<bool> dataBlockUsed(NUM_DATA_BLOCKS);
10 std::map<string, INode *> mapFromName;
11 struct FSBase;
12 struct File : public FSBase {
13 private:
14 std::vector<INode> * nodes;
15 MetaData metaData;
16 };
17
18 struct Directory : pubic FSBase { std::vector<FSBase* > content; };
19 struct FileSystem {
20 init();
21 mount(FileSystem*);
22 unmount(FileSystem*);
23 File createFile(cosnt char* name) { ... }
24 Directory createDirectory(const char* name) { ... }
25 // mapFromName to find INode corresponding to file
26 void openFile(File * file, FileMode mode) { ... }
27 void closeFile(File * file) { ... }
28 void writeToFile(File * file, void * data, int num) { ... }
29 void readFromFile(File* file, void* res, int numbutes,
30 int position) { ... }
31 };
Solutions to Chapter 7 | Object Oriented Design
167 Cracking the Coding Interview | Concepts and Algorithms
7.10 Describe the data structures and algorithms that you would use to implement a garbage collector in C++.
pg 62
SOLUTION
In C++, garbage collection with reference counting is almost always implemented with smart pointers, which perform reference counting. The main reason for using smart pointers over raw ordinary pointers is the conceptual simplicity of implementation and usage.
With smart pointers, everything related to garbage collection is performed behind the scenes - typically in constructors / destructors / assignment operator / explicit object management functions.
There are two types of functions, both of which are very simple:
1 RefCountPointer::type1() {
2 /* implementation depends on reference counting organisation.
3 * There can also be no ref. counter at all (see approach #4) */
4 incrementRefCounter(); }
5
6 RefCountPointer::type2() {
7 /* Implementation depends on reference counting organisation.
8 * There can also be no ref. counter at all (see approach #4). */
9 decrementRefCounter();
10 if (referenceCounterIsZero()) {
11 destructObject();
12 }
13 }
There are several approaches for reference counting implementation in C++:
1. Simple reference counting.
1 struct Object { };
2 struct RefCount {
3 int count;
4 };
5 struct RefCountPtr {
6 Object * pointee;
7 RefCount * refCount;
8 };
Advantages: performance.
Disadvantages: memory overhead because of two pointers.
2. Alternative reference counting.
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 168
1 struct Object { … };
2 struct RefCountPtrImpl {
3 int count;
4 Object * object;
5 };
6 struct RefCountPtr {
7 RefCountPtrImpl * pointee;
8 };
Advantages: no memory overhead because of two pointers.
Disadvantages: performance penalty because of extra level of indirection.
3. Intrusive reference counting.
1 struct Object { … };
2 struct ObjectIntrusiveReferenceCounting {
3 Object object;
4 int count;
5 };
6 struct RefCountPtr {
7 ObjectIntrusiveReferenceCounting * pointee;
8 };
Advantages: no previous disadvantages.
Disadvantages: class for intrusive reference counting should be modified.
4. Ownership list reference counting. It is an alternative for approach 1-3. For 1-3 it is only important to determine that counter is zero—its actual value is not important. This is the main idea of approach # 4.
All Smart-Pointers for given objects are stored in doubly-linked lists. The constructor of a smart pointer adds the new node to a list, and the destructor removes a node from the list and checks if the list is empty or not. If it is empty, the object is deleted.
1 struct Object { };
2 struct ListNode {
3 Object * pointee;
4 ListNode * next;
5 }
Solutions to Chapter 8 | Recursion
Cracking the Coding Interview | Concepts and Algorithms
169
8.1 Write a method to generate the nth Fibonacci number.
pg 64
SOLUTION
There are three potential approaches: (1) recursive approach (2) iterative approach (3) using matrix math. We have described the recursive and iterative approach below, as you would not be expected to be able to derive the matrix-based approach in an interview. For the interested math-geeks, you may read about the (most efficient) matrix-based algorithm at http://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form.
Recursive Solution:
1 int fibo(int n) {
2 if (n == 0) {
3 return 0; // f(0) = 0
4 } else if (n == 1) {
5 return 1; // f(1) = 1
6 } else if (n > 1) {
7 return fibo(n-1) + fibo(n-2); // f(n) = f(n—1) + f(n-2)
8 } else {
9 return –1; // Error condition
10 }
11 }
Iterative Solution:
1 int fibo(int n) {
2 if (n < 0) return -1; // Error condition.
3 if (n == 0) return 0;
4 int a = 1, b = 1;
5 for (int i = 3; i <= n; i++) {
6 int c = a + b;
7 a = b;
8 b = c;
9 }
10 return b;
11 }
Solutions to Chapter 8 | Recursion
170
CareerCup.com
8.2 Imagine a robot sitting on the upper left hand corner of an NxN grid. The robot can only move in two directions: right and down. How many possible paths are there for the robot?
FOLLOW UP
Imagine certain squares are “off limits”, such that the robot can not step on them. Design an algorithm to get all possible paths for the robot.
pg 64
SOLUTION
Part 1: (For clarity, we will solve this part assuming an X by Y grid)
Each path has (X-1)+(Y-1) steps. Imagine the following paths:
X X Y Y X (move right -> right -> down -> down -> right)
X Y X Y X (move right -> down -> right -> down -> right)
...
Each path can be fully represented by the moves at which we move right. That is, if I were to ask you which path you took, you could simply say “I moved right on step 3 and 4.”
Since you must always move right X-1 times, and you have X-1 + Y-1 total steps, you have to pick X-1 times to move right out of X-1+Y-1 choices. Thus, there are C(X-1, X-1+Y-1) paths (e.g., X-1+Y-1 choose X-1):
(X-1 + Y-1)! / ((X-1)! * (Y-1)!)
Part 2: Code
We can implement a simple recursive algorithm with backtracking:
1 ArrayList<Point> current_path = new ArrayList<Point>();
2 public static boolean getPaths(int x, int y) {
3 Point p = new Point(x, y);
4 current_path.add(p);
5 if (0 == x && 0 == y) return true; // current_path
6 boolean success = false;
7 if (x >= 1 && is_free(x - 1, y)) { // Try right
8 success = getPaths(x - 1, y); // Free! Go right
9 }
10 if (!success && y >= 1 && is_free(x, y - 1)) { // Try down
11 success = getPaths(x, y - 1); // Free! Go down
12 }
13 if (!success) {
14 current_path.remove(p); // Wrong way!
15 }
16 return success;
17 }
Solutions to Chapter 8 | Recursion
171 Cracking the Coding Interview | Concepts and Algorithms
8.3 Write a method that returns all subsets of a set.
pg 64
SOLUTION
We should first have some reasonable expectations of our time and space complexity. How many subsets of a set are there? We can compute this by realizing that when we generate a subset, each element has the “choice” of either being in there or not. That is, for the first element, there are 2 choices. For the second, there are two, etc. So, doing 2 * 2 * ... * 2 n times gives us 2^n subsets. We will not be able to do better than this in time or space complexity.
Approach #1: Recursion
This is a great problem to implement with recursion since we can build all subsets of a set using all subsets of a smaller set. Specifically, given a set S, we can do the following recursively:
»»Let first = S[0]. Let smallerSet = S[1, ... , n].
»»Compute all subsets of smallerSet and put them in allsubsets.
»»For each subset in allsubsets, clone it and add first to the subset.
The following code implements this algorithm:
1 ArrayList<ArrayList<Integer>> getSubsets(ArrayList<Integer> set,
2 int index) {
3 ArrayList<ArrayList<Integer>> allsubsets;
4 if (set.size() == index) {
5 allsubsets = new ArrayList<ArrayList<Integer>>();
6 allsubsets.add(new ArrayList<Integer>()); // Empty set
7 } else {
8 allsubsets = getSubsets(set, index + 1);
9 int item = set.get(index);
10 ArrayList<ArrayList<Integer>> moresubsets =
11 new ArrayList<ArrayList<Integer>>();
12 for (ArrayList<Integer> subset : allsubsets) {
13 ArrayList<Integer> newsubset = new ArrayList<Integer>();
14 newsubset.addAll(subset); //
15 newsubset.add(item);
16 moresubsets.add(newsubset);
17 }
18 allsubsets.addAll(moresubsets);
19 }
20 return allsubsets;
21 }
Approach #2: Combinatorics
»»When we’re generating a set, we have two choices for each element: (1) the element is
Solutions to Chapter 8 | Recursion
. CareerCup com 172
in the set (the “yes” state) or (2) the element is not in the set (the “no” state). This means that each subset is a sequence of yesses / nos—e.g., “yes, yes, no, no, yes, no”
»»This gives us 2^n possible subsets. How can we iterate through all possible sequences of “yes” / “no” states for all elements? If each “yes” can be treated as a 1 and each “no” can be treated as a 0, then each subset can be represented as a binary string.
»»Generating all subsets then really just comes down to generating all binary numbers (that is, all integers). Easy!
1 ArrayList<ArrayList<Integer>> getSubsets2(ArrayList<Integer> set) {
2 ArrayList<ArrayList<Integer>> allsubsets =
3 new ArrayList<ArrayList<Integer>>();
4 int max = 1 << set.size();
5 for (int i = 0; i < max; i++) {
6 ArrayList<Integer> subset = new ArrayList<Integer>();
7 int k = i;
8 int index = 0;
9 while (k > 0) {
10 if ((k & 1) > 0) {
11 subset.add(set.get(index));
12 }
13 k >>= 1;
14 index++;
15 }
16 allsubsets.add(subset);
17 }
18 return allsubsets;
19 }
Solutions to Chapter 8 | Recursion
173 Cracking the Coding Interview | Concepts and Algorithms
8.4 Write a method to compute all permutations of a string
pg 64
SOLUTION
Let’s assume a given string S represented by the letters A1, A2, A3, ... , An
To permute set S, we can select the first character, A1, permute the remainder of the string to get a new list. Then, with that new list, we can “push” A1 into each possible position.
For example, if our string is “abc”, we would do the following:
1. Let first = “a” and let remainder = “bc”
2. Let list = permute(bc) = {“bc”, “cd”}
3. Push “a” into each location of “bc” (--> “abc”, “bac”, “bca”) and “cb” (--> “acb”, “cab”, “cba”)
4. Return our new list
Now, the code to do this:
1 public static ArrayList<String> getPerms(String s) {
2 ArrayList<String> permutations = new ArrayList<String>();
3 if (s == null) { // error case
4 return null;
5 } else if (s.length() == 0) { // base case
6 permutations.add(“”);
7 return permutations;
8 }
9
10 char first = s.charAt(0); // get the first character
11 String remainder = s.substring(1); // remove the first character
12 ArrayList<String> words = getPerms(remainder);
13 for (String word : words) {
14 for (int j = 0; j <= word.length(); j++) {
15 permutations.add(insertCharAt(word, first, j));
16 }
17 }
18 return permutations;
19 }
20
21 public static String insertCharAt(String word, char c, int i) {
22 String start = word.substring(0, i);
23 String end = word.substring(i);
24 return start + c + end;
25 }
This solution takes O(n!) time, since there are n! permutations.
Solutions to Chapter 8 | Recursion
. CareerCup com 174
8.5 Implement an algorithm to print all valid (e.g., properly opened and closed) combinations of n-pairs of parentheses.
EXAMPLE:
input: 3 (e.g., 3 pairs of parentheses)
output: ()()(), ()(()), (())(), ((()))
pg 64
SOLUTION
We can solve this problem recursively by recursing through the string. On each iteration, we have the index for a particular character in the string. We need to select either a left or a right paren. When can we use left, and when can we use a right paren?
»»Left: As long as we haven’t used up all the left parentheses, we can always insert a left paren.
»»Right: We can insert a right paren as long as it won’t lead to a syntax error. When will we get a syntax error? We will get a syntax error if there are more right parentheses than left.
So, we simply keep track of the number of left and right parentheses allowed. If there are left parens remaining, we’ll insert a left paren and recurse. If there are more right parens remaining than left (eg, if there are more left parens used), then we’ll insert a right paren and recurse.
1 public static void printPar(int l, int r, char[] str, int count) {
2 if (l < 0 || r < l) return; // invalid state
3 if (l == 0 && r == 0) {
4 System.out.println(str); // found one, so print it
5 } else {
6 if (l > 0) { // try a left paren, if there are some available
7 str[count] = ‘(‘;
8 printPar(l - 1, r, str, count + 1);
9 }
10 if (r > l) { // try a right paren, if there’s a matching left
11 str[count] = ‘)’;
12 printPar(l, r - 1, str, count + 1);
13 }
14 }
15 }
16
17 public static void printPar(int count) {
18 char[] str = new char[count*2];
19 printPar(count, count, str, 0);
20 }
Solutions to Chapter 8 | Recursion
175 Cracking the Coding Interview | Concepts and Algorithms
8.6 Implement the “paint fill” function that one might see on many image editing programs. That is, given a screen (represented by a 2-dimensional array of Colors), a point, and a new color, fill in the surrounding area until you hit a border of that color.
pg 64
SOLUTION
First, let’s visualize how this method works. When we call Paint Fill (eg, “click” paint fill in the image editing application) on, say, a green pixel, we want to “bleed” outwards. Pixel by pixel, we expand outwards calling PaintFill on the surrounding pixel. When we hit a pixel that is not green, we stop. Surrounding green pixels may still be painted if they are touched by another Paint Fill operation.
We can implement this algorithm recursively:
1 enum Color {
2 Black, White, Red, Yellow, Green
3 }
4 boolean PaintFill(Color[][] screen, int x, int y, Color ocolor,
5 Color ncolor) {
6 if (x < 0 || x >= screen[0].length ||
7 y < 0 || y >= screen.length) {
8 return false;
9 }
10 if (screen[y][x] == ocolor) {
11 screen[y][x] = ncolor;
12 PaintFill(screen, x - 1, y, ocolor, ncolor); // left
13 PaintFill(screen, x + 1, y, ocolor, ncolor); // right
14 PaintFill(screen, x, y - 1, ocolor, ncolor); // top
15 PaintFill(screen, x, y + 1, ocolor, ncolor); // bottom
16 }
17 return true;
18 }
19
20 boolean PaintFill(Color[][] screen, int x, int y, Color ncolor) {
21 return PaintFill(screen, x, y, screen[y][x], ncolor);
22 }
Solutions to Chapter 8 | Recursion
. CareerCup com 176
8.7 Given an infinite number of quarters (25 cents), dimes (10 cents), nickels (5 cents) and pennies (1 cent), write code to calculate the number of ways of representing n cents.
pg 64
SOLUTION
This is a recursive problem, so let’s figure out how to do makeChange(n) using prior solutions (i.e., sub-problems). Let’s say n = 100, so we want to compute the number of ways of making change of 100 cents. What’s the relationship to its sub-problems?
We know that makeChange(100):
= makeChange(100 using 0 quarters) + makeChange(100 using 1 quarter) + makeChange(100 using 2 quarter) + makeChange(100 using 3 quarter) + makeChange(100 using 4 quarter)
Can we reduce this further? Yes!
= makeChange(100 using 0 quarters) + makeChange(75 using 0 quarter) + makeChange(50 using 0 quarters) + makeChange(25 using 0 quarters) + 1
Now what? We’ve used up all our quarters, so now we can start applying our next biggest denomination: dimes.
This leads to a recursive algorithm that looks like this:
1 public static int makeChange(int n, int denom) {
2 int next_denom = 0;
3 switch (denom) {
4 case 25:
5 next_denom = 10;
6 break;
7 case 10:
8 next_denom = 5;
9 break;
10 case 5:
11 next_denom = 1;
12 break;
13 case 1:
14 return 1;
15 }
16 int ways = 0;
17 for (int i = 0; i * denom <= n; i++) {
18 ways += makeChange(n - i * denom, next_denom);
19 }
20 return ways;
21 }
22
23 System.out.writeln(makeChange(n, 25));
Solutions to Chapter 8 | Recursion
177 Cracking the Coding Interview | Concepts and Algorithms
8.8 Write an algorithm to print all ways of arranging eight queens on a chess board so that none of them share the same row, column or diagonal.
pg 64
SOLUTION
We will use a backtracking algorithm. For each row, the column where we want to put the queen is based on checking that it does not violate the required condition.
1. For this, we need to store the column of the queen in each row as soon as we have finalized it. Let ColumnForRow[] be the array which stores the column number for each row.
2. The checks that are required for the three given conditions are:
»»On same Column : ColumnForRow[i] == ColumnForRow[j]
»»On same Diagonal: (ColumnForRow[i] - ColumnForRow[j] ) == ( i- j) or
(ColumnForRow[j] - ColumnForRow[i]) == (i - j)
1 int columnForRow[] = new int [8];
2 boolean check(int row) {
3 for (int i = 0; i < row; i++) {
4 int diff = Math.abs(columnForRow[i] - columnForRow[row]);
5 if (diff == 0 || diff == row - i) return false;
6 }
7 return true;
8 }
9
10 void PlaceQueen(int row){
11 if (row == 8) {
12 printBoard();
13 return;
14 }
15 for (int i = 0; i < 8; i++) {
16 columnForRow[row]=i;
17 if(check(row)){
18 PlaceQueen(row+1);
19 }
20 }
21 }

Solutions to Chapter 9 | Sorting and Searching
Cracking the Coding Interview | Concepts and Algorithms
179
9.1 You are given two sorted arrays, A and B, and A has a large enough buffer at the end to hold B. Write a method to merge B into A in sorted order.
pg 66
SOLUTION
This code is a part of the standard merge-sort code. We merge A and B from the back, by comparing each element.
1 public static void merge(int[] a, int[] b, int n, int m) {
2 int k = m + n - 1; // Index of last location of array b
3 int i = n - 1; // Index of last element in array b
4 int j = m - 1; // Index of last element in array a
5
6 // Start comparing from the last element and merge a and b
7 while (i >= 0 && j >= 0) {
8 if (a[i] > b[j]) {
9 a[k--] = a[i--];
10 } else {
11 a[k--] = b[j--];
12 }
13 }
14 while (j >= 0) {
15 a[k--] = b[j--];
16 }
17 }
Note: You don’t need to copy the contents of a after running out of b’s. They are already in place.
Solutions to Chapter 9 | Sorting and Searching
180
CareerCup.com
9.2 Write a method to sort an array of strings so that all the anagrams are next to each other.
pg 66
SOLUTION
The basic idea is to implement a normal sorting algorithm where you override the compareTo method to compare the “signature” of each string. In this case, the signature is the alphabetically sorted string.
1 public class AnagramComparator implements Comparator<String> {
2 public String sortChars(String s) {
3 char[] content = s.toCharArray();
4 Arrays.sort(content);
5 return new String(content);
6 }
7
8 public int compare(String s1, String s2) {
9 return sortChars(s1).compareTo(sortChars(s2));
10 }
11 }
Now, just sort the arrays, using this compareTo method instead of the usual one.
12 Arrays.sort(array, new AnagramComparator());
Solutions to Chapter 9 | Sorting and Searching
181 Cracking the Coding Interview | Concepts and Algorithms
9.3 Given a sorted array of n integers that has been rotated an unknown number of times, give an O(log n) algorithm that finds an element in the array. You may assume that the array was originally sorted in increasing order.
EXAMPLE:
Input: find 5 in array (15 16 19 20 25 1 3 4 5 7 10 14)
Output: 8 (the index of 5 in the array)
pg 66
SOLUTION
We can do this with a modification of binary search.
1 public static int search(int a[], int l, int u, int x) {
2 while (l <= u) {
3 int m = (l + u) / 2;
4 if (x == a[m]) {
5 return m;
6 } else if (a[l] <= a[m]) {
7 if (x > a[m]) {
8 l = m+1;
9 } else if (x >=a [l]) {
10 u = m-1;
11 } else {
12 l = m+1;
13 }
14 }
15 else if (x < a[m]) u = m-1;
16 else if (x <= a[u]) l = m+1;
17 else u = m - 1;
18 }
19 return -1;
20 }
21
22 public static int search(int a[], int x) {
23 return search(a, 0, a.length - 1, x);
24 }
What about duplicates? You may observe that the above function doesn’t give you an efficient result in case of duplicate elements. However, if your array has duplicate entries then we can’t do better than O(n) which is as good as linear search.
For example, if the array is [2,2,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,2], there is no way to find element 3 until you do a linear search.
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 182
9.4 If you have a 2 GB file with one string per line, which sorting algorithm would you use to sort the file and why?
pg 66
SOLUTION
When an interviewer gives a size limit of 2GB, it should tell you something - in this case, it suggests that they don’t want you to bring all the data into memory.
So what do we do? We only bring part of the data into memory..
Algorithm:
How much memory do we have available? Let’s assume we have X MB of memory available.
1. Divide the file into K chunks, where X * K = 2 GB. Bring each chunk into memory and sort the lines as usual using any O(n log n) algorithm. Save the lines back to the file.
2. Now bring the next chunk into memory and sort.
3. Once we’re done, merge them one by one.
The above algorithm is also known as external sort. Step 3 is known as N-way merge
The rationale behind using external sort is the size of data. Since the data is too huge and we can’t bring it all into memory, we need to go for a disk based sorting algorithm.
Solutions to Chapter 9 | Sorting and Searching
183 Cracking the Coding Interview | Concepts and Algorithms
9.5 Given a sorted array of strings which is interspersed with empty strings, write a method to find the location of a given string.
Example: find “ball” in [“at”, “”, “”, “”, “ball”, “”, “”, “car”, “”, “”, “dad”, “”, “”] will return 4
Example: find “ballcar” in [“at”, “”, “”, “”, “”, “ball”, “car”, “”, “”, “dad”, “”, “”] will return -1
pg 66
SOLUTION
Use ordinary binary search, but when you hit an empty string, advance to the next non-empty string; if there is no next non-empty string, search the left half.
1 public int search(String[] strings, String str, int first, int last) {
2 while (first <= last) {
3 // Ensure there is something at the end
4 while (first <= last && strings[last] == “”) {
5 --last;
6 }
7 if (last < first) {
8 return -1; // this block was empty, so fail
9 }
10 int mid = (last + first) >> 1;
11 while (strings[mid] == “”) {
12 ++mid; // will always find one
13 }
14 int r = strings[mid].compareTo(str);
15 if (r == 0) return mid;
16 if (r < 0) {
17 first = mid + 1;
18 } else {
19 last = mid - 1;
20 }
21 }
22 return -1;
23 }
24
25 public int search(String[] strings, String str) {
26 if (strings == null || str == null) return -1;
27 if (str == “”) {
28 for (int i = 0; i < strings.length; i++) {
29 if (strings[i] == “”) return i;
30 }
31 return -1;
32 }
33 return search(strings, str, 0, strings.length - 1);
34 }
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 184
9.6 Given a matrix in which each row and each column is sorted, write a method to find an element in it.
pg 66
SOLUTION
Assumptions:
»»Rows are sorted left to right in ascending order. Columns are sorted top to bottom in ascending order.
»»Matrix is of size MxN.
This algorithm works by elimination. Every move to the left (--col) eliminates all the elements below the current cell in that column. Likewise, every move down eliminates all the elements to the left of the cell in that row.
1 boolean FindElem(int[][] mat, int elem, int M, int N) {
2 int row = 0;
3 int col = N-1;
4 while (row < M && col >= 0) {
5 if (mat[row][col] == elem) {
6 return true;
7 } else if (mat[row][col] > elem) {
8 col--;
9 } else {
10 row++;
11 }
12 }
13 return false;
14 }
Solutions to Chapter 9 | Sorting and Searching
185 Cracking the Coding Interview | Concepts and Algorithms
9.7 A circus is designing a tower routine consisting of people standing atop one another’s shoulders. For practical and aesthetic reasons, each person must be both shorter and lighter than the person below him or her. Given the heights and weights of each person in the circus, write a method to compute the largest possible number of people in such a tower.
EXAMPLE:
Input (ht, wt): (65, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68, 110)
Output: The longest tower is length 6 and includes from top to bottom: (56, 90) (60,95) (65,100) (68,110) (70,150) (75,190)
pg 66
SOLUTION
Step 1. Sort all items by height first, and then by weight. This means that if all the heights are unique, then the items will be sorted by their height. If heights are the same, items will be sorted by their weight.
Example:
»»Before sorting: (60, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68,110).
»»After sorting: (56, 90), (60, 95), (60,100), (68, 110), (70,150), (75,190).
Step 2. Find the longest sequence which contains increasing heights and increasing weights.
To do this, we:
a) Start at the beginning of the sequence. Currently, max_sequence is empty.
b) If, for the next item, the height and the weight is not greater than those of the previous item, we mark this item as “unfit” .
(60,95)
(65,100)
(75,80)
(80, 100)
(unfit item)
c) If the sequence found has more items than “max sequence”, it becomes “max sequence”.
d) After that the search is repeated from the “unfit item”, until we reach the end of the original sequence.
1 public class Question {
2 ArrayList<HtWt> items;
3 ArrayList<HtWt> lastFoundSeq;
4 ArrayList<HtWt> maxSeq;
5
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 186
6 // Returns longer sequence
7 ArrayList<HtWt> seqWithMaxLength(ArrayList<HtWt> seq1,
8 ArrayList<HtWt> seq2) {
9 return seq1.size() > seq2.size() ? seq1 : seq2;
10 }
11
12 // Fills next seq w decreased wts&returns index of 1st unfit item.
13 int fillNextSeq(int startFrom, ArrayList<HtWt> seq) {
14 int firstUnfitItem = startFrom;
15 if (startFrom < items.size()) {
16 for (int i = 0; i < items.size(); i++) {
17 HtWt item = items.get(i);
18 if (i == 0 || items.get(i-1).isBefore(item)) {
19 seq.add(item);
20 } else {
21 firstUnfitItem = i;
22 }
23 }
24 }
25 return firstUnfitItem;
26 }
27
28 // Find the maximum length sequence
29 void findMaxSeq() {
30 Collections.sort(items);
31 int currentUnfit = 0;
32 while (currentUnfit < items.size()) {
33 ArrayList<HtWt> nextSeq = new ArrayList<HtWt>();
34 int nextUnfit = fillNextSeq(currentUnfit, nextSeq);
35 maxSeq = seqWithMaxLength(maxSeq, nextSeq);
36 if (nextUnfit == currentUnfit) break;
37 else currentUnfit = nextUnfit;
38 }
39 }
40 }
Solutions to Chapter 10 | Mathematical
Cracking the Coding Interview | Concepts and Algorithms
187
10.1 You have a basketball hoop and someone says that you can play 1 of 2 games.
Game #1: You get one shot to make the hoop.
Game #2: You get three shots and you have to make 2 of 3 shots.
If p is the probability of making a particular shot, for which values of p should you pick one game or the other?
pg 68
SOLUTION
Probability of winning Game 1: p
Probability of winning Game 2:
Let s(k,n) be the probability of making exactly k shots out of n. The probability of winning game 2 is s(2, 3)+s(3, 3). Since, s(k, n) = C(n, k) ( 1- p)^(n - k) p^k, the probability of winning is 3 * (1 - p) * p^2 + p^3.
Simplified, it becomes 3 * p^2 - 2 * p^3.
You should play Game1 if P(Game1) > P(Game2):
p > 3*p^2 - 2*p^3.
1 > 3*p - 2*p^2
2*p^2 - 3*p + 1 > 0
(2p - 1)(p - 1) > 0
Both terms must be positive or both must be negative. But we know p < 1, so (p - 1) < 0. This means both terms must be negative.
(2p - 1) < 0
2p < 1
p < .5
So, we should play Game1 if p < .5.
Solutions to Chapter 10 | Mathematical
188
CareerCup.com
10.2 There are three ants on different vertices of a triangle. What is the probability of collision (between any two or all of them) if they start walking on the sides of the triangle?
Similarly find the probability of collision with ‘n’ ants on an ‘n’ vertex polygon.
pg 68
SOLUTION
None of the three ants will collide if all three are moving in clockwise direction, or all three are moving in a counter-clockwise direction. Otherwise, there will definitely be a collision.
How many ways are there for the three ants to move? Each ant can move in 2 directions, so there are 2^3 ways the ant can move. There are only two ways which will avoid a collision, therefore the probability of collision is (2^3 – 2) / (2^3) = 6 / 8 = 3 / 4.
To generalize this to an n-vertex polygon: there are still only 2 ways in which the ants can move to avoid a collision, but there are 2^n ways they can move total. Therefore, in general, probability of collision is (2^n – 2) / 2^n = 1 – 1/2^(n-1).
Solutions to Chapter 10 | Mathematical
189 Cracking the Coding Interview | Concepts and Algorithms
10.3 Given two lines on a Cartesian plane, determine whether the two lines would intersect.
pg 68
SOLUTION
There are a lot of unknowns in this problem (what format are the lines in? What if they are the same line?), but let’s assume:
»»If two lines are the same (same line = same slope and y-intercept), they are considered to intersect.
»»We get to decide the data structure.
1 public class Line {
2 static double epsilon = 0.000001;
3 public double slope;
4 public double yintercept;
5
6 public Line(double s, double y) {
7 slope = s;
8 yintercept = y;
9 }
10
11 public boolean intersect(Line line2) {
12 return Math.abs(slope - line2.slope) > epsilon ||
13 Math.abs(yintercept - line2.yintercept) < epsilon;
14 }
15 }
OBSERVATIONS AND SUGGESTIONS:
»»Ask questions. This question has a lot of unknowns—ask questions to clarify them. Many interviewers intentionally ask vague questions to see if you’ll clarify your assumptions.
»»When possible, design and use data structures. It shows that you understand and care about object oriented design.
»»Think through which data structures you design to represent a line. There are a lot of options, with lots of trade offs. Pick one and explain your choice.
»»Don’t assume that the slope and y-intercept are integers.
»»Understand limitations of floating point representations. Never check for equality with ==.
Solutions to Chapter 10 | Mathematical
. CareerCup com 190
10.4 Write a method to implement *, - , / operations. You should use only the + operator.
pg 68
SOLUTION
With an understanding of what each operation (minus, times, divide) does, this problem can be approached logically.
»»Subtraction should be relatively straightforward, as we all know that a - b is the same thing as a + (-1)*b.
»»Multiplication: we have to go back to what we learned in grade school: 21 * 3 = 21 + 21 + 21. It’s slow, but it works.
»»Division is the trickiest, because we usually think of 21 / 3 as something like “if you divide a 21 foot board into 3 pieces, how big is each piece?” If we think about it the other way around, it’s a little easier: “I divided a 21 foot board in x pieces and got pieces of 3 feet each, how many pieces were there?” From here, we can see that if we continuously subtract 3 feet from 21 feet, we’ll know how many pieces there are. That is, we continuously subtract b from a and count how many times we can do that.
1 /* Flip a positive sign to negative, or a negative sign to pos */
2 public static int FnNegate(int a) {
3 int neg = 0;
4 int d = a < 0 ? 1 : -1;
5 while (a != 0) {
6 neg += d;
7 a += d;
8 }
9 return neg;
10 }
11
12 /* Subtract two numbers by negating b and adding them */
13 public static int FnMinus(int a, int b) {
14 return a + FnNegate(b);
15 }
16
17 /* Check if a and b are different signs */
18 public static boolean DifferentSigns(int a, int b) {
19 return ((a < 0 && b > 0) || (a > 0 && b < 0)) ? true : false;
20 }
21
22 /* Return absolute value */
23 public static int abs(int a) {
24 if (a < 0) return FnNegate(a);
25 else return a;
26 }
Solutions to Chapter 10 | Mathematical
191 Cracking the Coding Interview | Concepts and Algorithms
27
28 /* Multiply a by b by adding a to itself b times */
29 public static int FnTimes(int a, int b) {
30 if (a < b) return FnTimes(b, a); // algo is faster if b < a
31 int sum = 0;
32 for (int iter = abs(b); iter > 0; --iter) sum += a;
33 if (b < 0) sum = FnNegate(sum);
34 return sum;
35 }
36
37 /* Divide a by b by literally counting how many times does b go into
38 * a. That is, count how many times you can subtract b from a until
39 * you hit 0. */
40 public static int FnDivide(int a, int b) throws
41 java.lang.ArithmeticException {
42 if (b == 0) {
43 throw new java.lang.ArithmeticException(“Divide by 0.”);
44 }
45 int quotient = 0;
46 int divisor = FnNegate(abs(b));
47 int divend; /* dividend */
48 for (divend = abs(a); divend >= abs(divisor); divend += divisor) {
49 ++quotient;
50 }
51 if (DifferentSigns(a, b)) quotient = FnNegate(quotient);
52 return quotient;
53 }
OBSERVATIONS AND SUGGESTIONS
»»A logical approach of going back to what exactly multiplication and division do comes in handy. Remember that. All (good) interview problems can be approached in a logical, methodical way!
»»The interviewer is looking for this sort of logical work-your-way-through-it approach.
»»This is a great problem to demonstrate your ability to write clean code—specifically, to show your ability to re-use code. For example, if you were writing this solution and didn’t put FnNegate in its own method, you should move it out once you see that you’ll use it multiple times.
»»Be careful about making assumptions while coding. Don’t assume that the numbers are all positive, or that a is bigger than b.
Solutions to Chapter 10 | Mathematical
. CareerCup com 192
10.5 Given two squares on a two dimensional plane, find a line that would cut these two squares in half.
pg 68
SOLUTION
Any line that goes through the center of a rectangle must cut it in half. Therefore, if you drew a line connecting the centers of the two squares, it would cut both in half.
1 public class Square {
2 public double left;
3 public double top;
4 public double bottom;
5 public double right;
6 public Square(double left, double top, double size) {
7 this.left = left;
8 this.top = top;
9 this.bottom = top + size;
10 this.right = left + size;
11 }
12
13 public Point middle() {
14 return new Point((this.left + this.right) / 2,
15 (this.top + this.bottom) / 2);
16 }
17
18 public Line cut(Square other) {
19 Point middle_s = this.middle();
20 Point middle_t = other.middle();
21 if (middle_s == middle_t) {
22 return new Line(new Point(left, top),
23 new Point(right, bottom));
24 } else {
25 return new Line(middle_s, middle_t);
26 }
27 }
28 }
SUGGESTIONS AND OBSERVATIONS
The main point of this problem is to see how careful you are about coding. It’s easy to glance over the special cases (e.g., the two squares having the same middle). Make a list of these special cases before you start the problem and make sure to handle them appropriately.
Solutions to Chapter 10 | Mathematical
193 Cracking the Coding Interview | Concepts and Algorithms
10.6 Given a two dimensional graph with points on it, find a line which passes the most number of points.
pg 68
SOLUTION
If we draw a line between every two points, we can check to see which line is the most common. A brute force approach would be to simply iterate through each line segment (formed by pairs of points) and count how many points fall on it. This would take O(N^3) time.
Before we discuss if we can do better, let’s figure out how we can represent a line. A line can be represented in (at least) two different ways: (1) as a pairing of points or (2) as a slope and a y-intercept.
Because our line is infinite, the slope and y-intercept approach seems more appropriate. The slope and y-intercept approach has an additional advantage: every line segment on the same greater line will have identical slopes and y-intercepts.
Let’s re-think our solution. We have a bunch of line segments, represented as a slope and y-intercept, and we want to find the most common slope and y-intercept. How can we find the most common one?
This is really no different than the old “find the most common number in a list of numbers” problem. We just iterate through the lines segments and use a hash table to count the number of times we’ve seen each line.
1 public static Line findBestLine(GraphPoint[] points) {
2 Line bestLine = null;
3 HashMap<Line, Integer> line_count = new HashMap<Line, Integer>();
4 for (int i = 0; i < points.length; i++) {
5 for (int j = i + 1; j < points.length; j++) {
6 Line line = new Line(points[i], points[j]);
7 if (!line_count.containsKey(line)) {
8 line_count.put(line, 0);
9 }
10 line_count.put(line, line_count.get(line) + 1);
11 if (bestLine == null ||
12 line_count.get(line) > line_count.get(bestLine)) {
13 bestLine = line;
14 }
15 }
16 }
17 return bestLine;
18 }
19
20 public class Line {
21 private static double epsilon = .0001;
Solutions to Chapter 10 | Mathematical
. CareerCup com 194
22 public double slope;
23 public double intercept;
24 private boolean infinite_slope = false;
25 public Line(GraphPoint p, GraphPoint q) {
26 if (Math.abs(p.x - q.x) > epsilon) { // if x’s are different
27 slope = (p.y - q.y) / (p.x - q.x); // compute slope
28 intercept = p.y - slope * p.x; // y intercept from y=mx+b
29 } else {
30 infinite_slope = true;
31 intercept = p.x; // x-intercept, since slope is infinite
32 }
33 }
34
35 public boolean isEqual(double a, double b) {
36 return (Math.abs(a - b) < epsilon);
37 }
38
39 @Override
40 public int hashCode() {
41 int sl = (int)(slope * 1000);
42 int in = (int)(intercept * 1000);
43 return sl | in;
44 }
45
46 @Override
47 public boolean equals(Object o) {
48 Line l = (Line) o;
49 if (isEqual(l.slope, slope) && isEqual(l.intercept, intercept)
50 && (infinite_slope == l.infinite_slope)) {
51 return true;
52 }
53 return false;
54 }
55 }
OBSERVATIONS AND SUGGESTIONS
»»Be careful about the calculation of the slope of a line. The line might be completely vertical. We can keep track of this in a separate flag (infinite_slope). We need to check this condition in the equals method.
»»Remember that when we perform division to calculate the slope, division is not exact. Therefore, rather than checking to see if two slopes are exactly equal, we need to check if they’re different by greater than epsilon.
Solutions to Chapter 10 | Mathematical
195 Cracking the Coding Interview | Concepts and Algorithms
10.7 Design an algorithm to find the kth number such that the only prime factors are 3, 5, and 7.
pg 68
SOLUTION
Any such number will look like (3^i)*(5^j)*(7^k). Here are the first 13 numbers:
1
-
3^0 * 5^0 * 7 ^ 0
3
3
3^1 * 5^0 * 7 ^ 0
5
5
3^0 * 5^1 * 7 ^ 0
7
7
3^0 * 5^0 * 7 ^ 1
9
3*3
3^2 * 5^0 * 7 ^ 0
15
3*5
3^1 * 5^1 * 7 ^ 0
21
3*7
3^1 * 5^0 * 7 ^ 1
25
5*5
3^0 * 5^2 * 7 ^ 0
27
3*9
3^3 * 5^0 * 7 ^ 0
35
5*7
3^0 * 5^1 * 7 ^1
45
5*9
3^2 * 5^1 * 7 ^0
49
7*7
3^0 * 5^0 * 7 ^2
63
3*21
3^2 * 5^0 * 7 ^1
»»3 * (previous number in list)
»»5 * (previous number in list)
»»7 * (previous number in list)
How would we find the next number in the list? Well, we could multiply 3, 5 and 7 times each number in the list and find the smallest element that has not yet been added to our list. This solution is O(n^2). Not bad, but I think we can do better.
In our current algorithm, we’re doing 3*1, 3*3, 3*5, 3*7, 3*9, 3*15, 3*21, 3*25 …, and the same for 5 and 7. We’ve already done almost all this work before—why are we doing it again?
We can fix this by multiplying each number we add to our list by 3, 5, 7 and putting the results in one of the three first-in-first-out queues. To look for the next “magic” number, we pick the smallest element in the three queues. Here is the algorithm:
1. Initialize array magic and queues Q3, Q5 and Q7
2. Insert 1 into magic.
3. Insert 1*3, 1*5 and 1*7 into Q3, Q5 and Q7 respectively.
4. Let x be the minimum element in Q3, Q5 and Q7. Append x to magic.
5. If x was found in:
Solutions to Chapter 10 | Mathematical
. CareerCup com 196
Q3 -> append x*3, x*5 and x*7 to Q3, Q5 and Q7. Remove x from Q3.
Q5 -> append x*5 and x*7 to Q5 and Q7. Remove x from Q5.
Q7 -> only append x*7 to Q7. Remove x from Q7.
Note: we do not need to append x*3 and x*5 to all lists because they will already be found in another list.
6. Repeat steps 4 - 6 until we’ve found k elements.
1 public static int getKthMagicNumber(int k) {
2 if (k <= 0) return 0;
3 int val = 1;
4 Queue<Integer> Q3 = new LinkedList<Integer>();
5 Queue<Integer> Q5 = new LinkedList<Integer>();
6 Queue<Integer> Q7 = new LinkedList<Integer>();
7 Q3.add(3);
8 Q5.add(5);
9 Q7.add(7);
10 for (--k; k > 0; --k) { // We’ve done one iteration already.
11 val = Math.min(Q3.peek().intValue(),
12 Math.min(Q5.peek().inValue(), Q7.peek().intValue()));
13 if (val == Q7.peek()) {
14 Q7.remove();
15 } else {
16 if (val == Q5.peek()) {
17 Q5.remove();
18 } else { // must be from Q3
19 Q3.remove();
20 Q3.add(val * 3);
21 }
22 Q5.add(val * 5);
23 }
24 Q7.add(val * 7);
25 }
26 return val;
27 }
OBSERVATIONS AND SUGGESTIONS:
When you get this question, do your best to solve it—even though it’s really difficult. Explain a brute force approach (not as tricky) and then start thinking about how you can optimize it. Or, try to find a pattern in the numbers.
Chances are, your interviewer will help you along when you get stuck. Whatever you do, don’t give up! Think out loud, wonder aloud, explain your thought process. Your interviewer will probably jump in to guide you.
Solutions to Chapter 11 | System Design and Memory Limits
Cracking the Coding Interview | Concepts and Algorithms
197
11.1 If you were integrating a feed of end of day stock price information (open, high, low, and closing price) for 5,000 companies, how would you do it? You are responsible for the development, rollout and ongoing monitoring and maintenance of the feed. Describe the different methods you considered and why you would recommend your approach. The feed is delivered once per trading day in a comma-separated format via an FTP site. The feed will be used by 1000 daily users in a web application.
pg 72
SOLUTION
Let’s assume we have some scripts which are scheduled to get the data via FTP at the end of the day. Where do we store the data? How do we store the data in such a way that we can do various analyses of it?
Proposal #1
Keep the data in text files. This would be very difficult to manage and update, as well as very hard to query. Keeping unorganized text files would lead to a very inefficient data model.
Proposal #2
We could use a database. This provides the following benefits:
»»Logical storage of data.
»»Facilitates an easy way of doing query processing over the data.
Example: return all stocks having open > N AND closing price < M
Advantages:
»»Makes the maintenance easy once installed properly.
»»Roll back, backing up data, and security could be provided using standard database features. We don’t have to “reinvent the wheel.”
Proposal #3
If requirements are not that broad and we just want to do a simple analysis and distribute the data, then XML could be another good option.
Our data has fixed format and fixed size: company_name, open, high, low, closing price. The XML could look like this:
<root>
<date value=“2008-10-12”>
<company name=“foo”>
<open>126.23</open>
<high>130.27</high>
<low>122.83</low>
Solutions to Chapter 11 | System Design and Memory Limits
198
CareerCup.com
<closingPrice>127.30</closingPrice>
</company>
<company name=“bar”>
<open>52.73</open>
<high>60.27</high>
<low>50.29</low>
<closingPrice>54.91</closingPrice>
</company>
</date>
<date value=“2008-10-11”> . . . </date>
</root>
Benefits:
»»Very easy to distribute. This is one reason that XML is a standard data model to share /distribute data.
»»Efficient parsers are available to parse the data and extract out only desired data.
»»We can add new data to the XML file by carefully appending data. We would not have to re-query the database.
However, querying the data could be difficult.
Solutions to Chapter 11 | System Design and Memory Limits
199 Cracking the Coding Interview | Concepts and Algorithms
11.2 How would you design the data structures for a very large social network (Facebook, LinkedIn, etc)? Describe how you would design an algorithm to show the connection, or path, between two people (e.g., Me -> Bob -> Susan -> Jason -> You).
pg 72
SOLUTION
Approach:
Forget that we’re dealing with millions of users at first. Design this for the simple case.
We can construct a graph by assuming every person is a node and if there is an edge between two nodes, then the two people are friends with each other.
class Person {
Person[] friends;
// Other info
}
If I want to find the connection between two people, I would start with one person and do a simple breadth first search.
But... oh no! Millions of users!
When we deal with a service the size of Orkut or Facebook, we cannot possibly keep all of our data on one machine. That means that our simple Person data structure from above doesn’t quite work—our friends may not live on the same machine as us. Instead, we can replace our list of friends with a list of their IDs, and traverse as follows:
1. For each friend ID: int machine_index = lookupMachineForUserID(id);
2. Go to machine machine_index
3. Person friend = lookupFriend(machine_index);
There are more optimizations and follow up questions here than we could possibly discuss, but here are just a few thoughts.
Optimization: Reduce Machine Jumps
Jumping from one machine to another is expensive. Instead of randomly jumping from machine to machine with each friend, try to batch these jumps—e.g., if 5 of my friends live on one machine, I should look them up all at once.
Optimization: Smart Division of People and Machines
People are much more likely to be friends with people who live in the same country as them. Rather than randomly dividing people up across machines, try to divvy them up by country, city, state, etc. This will reduce the number of jumps.
Question: Breadth First Search usually requires “marking” a node as visited. How do you do that in
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 200
this case?
Usually, in BFS, we mark a node as visited by setting a flag visited in its node class. Here, we don’t want to do that (there could be multiple searches going on at the same time, so it’s bad to just edit our data). In this case, we could mimic the marking of nodes with a hash table to lookup a node id and whether or not it’s been visited.
Other Follow-Up Questions:
»»In the real world, servers fail. How does this affect you?
»»How could you take advantage of caching?
»»Do you search until the end of the graph (infinite)? How do you decide when to give up?
»»In real life, some people have more friends of friends than others, and are therefore more likely to make a path between you and someone else. How could you use this data to pick where you start traversing?
The following code demonstrates our algorithm:
1 public class Server {
2 ArrayList<Machine> machines = new ArrayList<Machine>();
3 }
4
5 public class Machine {
6 public ArrayList<Person> persons = new ArrayList<Person>();
7 public int machineID;
8 }
9
10 public class Person {
11 private ArrayList<Integer> friends;
12 private int ID;
13 private int machineID;
14 private String info;
15 private Server server = new Server();
16
17 public String getInfo() { return info; }
18 public void setInfo(String info) {
19 this.info = info;
20 }
21
22 public int[] getFriends() {
23 int[] temp = new int[friends.size()];
24 for (int i = 0; i < temp.length; i++) {
25 temp[i] = friends.get(i);
26 }
27 return temp;
28 }
Solutions to Chapter 11 | System Design and Memory Limits
201 Cracking the Coding Interview | Concepts and Algorithms
29 public int getID() { return ID; }
30 public int getMachineID() { return machineID; }
31 public void addFriend(int id) { friends.add(id); }
32
33 // Look up a person given their ID and Machine ID
34 public Person lookUpFriend(int machineID, int ID) {
35 for (Machine m : server.machines) {
36 if (m.machineID == machineID) {
37 for (Person p : m.persons) {
38 if (p.ID == ID){
39 return p;
40 }
41 }
42 }
43 }
44 return null;
45 }
46
47 // Look up a machine given the machine ID
48 public Machine lookUpMachine(int machineID) {
49 for (Machine m:server.machines) {
50 if (m.machineID == machineID)
51 return m;
52 }
53 return null;
54 }
55
56 public Person(int iD, int machineID) {
57 ID = iD;
58 this.machineID = machineID;
59 }
60 }
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 202
11.3 Given an input file with four billion integers, provide an algorithm to generate an integer which is not contained in the file. Assume you have 1 GB of memory.
FOLLOW UP
What if you have only 10 MB of memory?
pg 72
SOLUTION
There are a total of 2^32, or 4 billion, distinct integers possible. We have 1 GB of memory, or 8 billion bits.
Thus, with 8 billion bits, we can map all possible integers to a distinct bit with the available memory. The logic is as follows:
1. Create a bit vector (BV) of size 4 billion.
2. Initialize BV with all 0’s
3. Scan all numbers (num) from the file and write BV[num] = 1;
4. Now scan again BV from 0th index
5. Return the first index which has 0 value.
1 byte[] bitfield = new byte [0xFFFFFFF/8];
2 void findOpenNumber2() throws FileNotFoundException {
3 Scanner in = new Scanner(new FileReader(“input_file_q11_4.txt”));
4 while (in.hasNextInt()) {
5 int n = in.nextInt ();
6 /* Finds the corresponding number in the bitfield by using the
7 * OR operator to set the nth bit of a byte (e.g.. 10 would
8 * correspond to the 2nd bit of index 2 in the byte array). */
9 bitfield [n / 8] |= 1 << (n % 8);
10 }
11
12 for (int i = 0 ; i < bitfield.length; i++) {
13 for (int j = 0; j < 8; j++) {
14 /* Retrieves the individual bits of each byte. When 0 bit
15 * is found, finds the corresponding value. */
16 if ((bitfield[i] & (1 << j)) == 0) {
17 System.out.println (i * 8 + j);
18 return;
19 }
20 }
21 }
22 }
Solutions to Chapter 11 | System Design and Memory Limits
203 Cracking the Coding Interview | Concepts and Algorithms
Follow Up: What if we have only 10 MB memory?
It’s possible to find a missing integer with just two passes of the data set. We can divide up the integers into blocks of some size (we’ll discuss how to decide on a size later). Let’s just assume that we divide up the integers into blocks of 1000. So, block 0 represents the numbers 0 through 999, block 1 represents blocks 1000 - 1999, etc. Since the range of ints is finite, we know that the number of blocks needed is finite.
In the first pass, we count how many ints are in each block. That is, if we see 552, we know that that is in block 0, we increment counter[0]. If we see 1425, we know that that is in block 1, so we increment counter[1].
At the end of the first pass, we’ll be able to quickly spot a block that is missing a number. If our block size is 1000, then any block which has fewer than 1000 numbers must be missing a number. Pick any one of those blocks.
In the second pass, we’ll actually look for which number is missing. We can do this by creating a simple bit vector of size 1000. We iterate through the file, and for each number that should be in our block, we set the appropriate bit in the bit vector. By the end, we’ll know which number (or numbers) is missing.
Now we just have to decide what the block size is.
A quick answer is 2^20 values per block. We will need an array with 2^12 block counters and a bit vector in 2^17 bytes. Both of these can comfortably fit in 10*2^20 bytes.
What’s the smallest footprint? When the array of block counters occupies the same memory as the bit vector. Let N = 2^32.
counters (bytes): blocks * 4
bit vector (bytes): (N / blocks) / 8
blocks * 4 = (N / blocks) / 8
blocks^2 = N / 32
blocks = sqrt(N/2)/4
It’s possible to find a missing integer with just under 65KB (or, more exactly, sqrt(2)*2^15 bytes).
1 int bitsize = 1048576; // 2^20 bits (2^17 bytes)
2 int blockNum = 4096; // 2^12
3 byte[] bitfield = new byte[bitsize/8];
4 int[] blocks = new int[blockNum];
5
6 void findOpenNumber() throws FileNotFoundException {
7 int starting = -1;
8 Scanner in = new Scanner (new FileReader (“input_file_q11_4.txt”));
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 204
9 while (in.hasNextInt()) {
10 int n = in.nextInt();
11 blocks[n / (bitfield.length * 8)]++;
12 }
13
14 for (int i = 0; i < blocks.length; i++) {
15 if (blocks[i] < bitfield.length * 8){
16 /* if value < 2^20, then at least 1 number is missing in
17 * that section. */
18 starting = i * bitfield.length * 8;
19 break;
20 }
21 }
22
23 in = new Scanner(new FileReader(“input_file_q11_4.txt”));
24 while (in.hasNextInt()) {
25 int n = in.nextInt();
26 /* If the number is inside the block that’s missing numbers,
27 * we record it */
28 if( n >= starting && n < starting + bitfield.length * 8){
29 bitfield [(n-starting) / 8] |= 1 << ((n - starting) % 8);
30 }
31 }
32
33 for (int i = 0 ; i < bitfield.length; i++) {
34 for (int j = 0; j < 8; j++) {
35 /* Retrieves the individual bits of each byte. When 0 bit
36 * is found, finds the corresponding value. */
37 if ((bitfield[i] & (1 << j)) == 0) {
38 System.out.println(i * 8 + j + starting);
39 return;
40 }
41 }
42 }
43 }
Solutions to Chapter 11 | System Design and Memory Limits
205 Cracking the Coding Interview | Concepts and Algorithms
11.4 You have an array with all the numbers from 1 to N, where N is at most 32,000. The array may have duplicate entries and you do not know what N is. With only 4KB of memory available, how would you print all duplicate elements in the array?
pg 72
SOLUTION
We have 4KB of memory which means we can address up to 8 * 4 * (2^10) bits. Note that 32* (2^10) bits is greater than 32000. We can create a bit vector with 32000 bits, where each bit represents one integer.
NOTE: While this isn’t an especially difficult problem, it’s important to implement this cleanly. We will define our own bit vector class to hold a large bit vector.
1 public static void checkDuplicates(int[] array) {
2 BitSet bs = new BitSet(32000);
3 for (int i = 0; i < array.length; i++) {
4 int num = array[i];
5 int num0 = num - 1; // bitset starts at 0, numbers start at 1
6 if (bs.get(num0)) {
7 System.out.println(num);
8 } else {
9 bs.set(num0);
10 }
11 }
12 }
13
14 class BitSet {
15 int[] bitset;
16
17 public BitSet(int size) {
18 bitset = new int[size >> 5]; // divide by 32
19 }
20
21 boolean get(int pos) {
22 int wordNumber = (pos >> 5); // divide by 32
23 int bitNumber = (pos & 0x1F); // mod 32
24 return (bitset[wordNumber] & (1 << bitNumber)) != 0;
25 }
26
27 void set(int pos) {
28 int wordNumber = (pos >> 5); // divide by 32
29 int bitNumber = (pos & 0x1F); // mod 32
30 bitset[wordNumber] |= 1 << bitNumber;
31 }
32 }
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 206
11.5 If you were designing a web crawler, how would you avoid getting into infinite loops?
pg 72
SOLUTION
First, how does the crawler get into a loop? The answer is very simple: when we re-parse an already parsed page. This would mean that we revisit all the links found in that page, and this would continue in a circular fashion.
Be careful about what the interviewer considers the “same” page. Is it URL or content? One could easily get redirected to a previously crawled page.
So how do we stop visiting an already visited page? The web is a graph-based structure, and we commonly use DFS (depth first search) and BFS (breadth first search) for traversing graphs. We can mark already visited pages the same way that we would in a BFS/DFS.
We can easily prove that this algorithm will terminate in any case. We know that each step of the algorithm will parse only new pages, not already visited pages. So, if we assume that we have N number of unvisited pages, then at every step we are reducing N (N-1) by 1. That proves that our algorithm will continue until they are only N steps.
SUGGESTIONS AND OBSERVATIONS
»»This question has a lot of ambiguity. Ask clarifying questions!
»»Be prepared to answer questions about coverage.
»»What kind of pages will you hit with a DFS versus a BFS?
»»What will you do when your crawler runs into a honey pot that generates an infinite subgraph for you to wander about?
Solutions to Chapter 11 | System Design and Memory Limits
207 Cracking the Coding Interview | Concepts and Algorithms
11.6 You have a billion urls, where each is a huge page. How do you detect the duplicate documents?
pg 72
SOLUTION
Observations:
1. Pages are huge, so bringing all of them in memory is a costly affair. We need a shorter representation of pages in memory. A hash is an obvious choice for this.
2. Billions of urls exist so we don’t want to compare every page with every other page (that would be O(n^2)).
Based on the above two observations we can derive an algorithm which is as follows:
1. Iterate through the pages and compute the hash table of each one.
2. Check if the hash value is in the hash table. If it is, throw out the url as a duplicate. If it is not, then keep the url and insert it in into the hash table.
This algorithm will provide us a list of unique urls. But wait, can this fit on one computer?
»»How much space does each page take up in the hash table?
»»Each page hashes to a four byte value.
»»Each url is an average of 30 characters, so that’s another 30 bytes at least.
»»Each url takes up roughly 34 bytes.
»»34 bytes * 1 billion = 31.6 gigabytes. We’re going to have trouble holding that all in memory!
What do we do?
»»We could split this up into files. We’ll have to deal with the file loading / unloading—ugh.
»»We could hash to disk. Size wouldn’t be a problem, but access time might. A hash table on disk would require a random access read for each check and write to store a viewed url. This could take msecs waiting for seek and rotational latencies. Elevator algorithms could elimate random bouncing from track to track.
»»Or, we could split this up across machines, and deal with network latency. Let’s go with this solution, and assume we have n machines.
»»First, we hash the document to get a hash value v
»»v%n tells us which machine this document’s hash table can be found on.
»»v / n is the value in the hash table that is located on its machine.
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 208
11.7 You have to design a database that can store terabytes of data. It should support efficient range queries. How would you do it?
pg 72
SOLUTION
Construct an index for each field that requires range queries. Use a B+ tree to implement the index. A B+ tree organizes sorted data for efficient insertion, retrieval and removal of records. Each record is identified by a key (for this problem, it is the field value). Since it is a dynamic, multilevel index, finding the beginning of the range depends only on the height of the tree, which is usually quite small. Record references are stored in the leaves, sorted by the key. Additional records can be found by following a next block reference. Records will be sequentially available until the key value reaches the maximum value specified in the query. Thus, runtimes will be dominated by the number of elements in a range.
Avoid using trees that store data at interior nodes, as traversing the tree will be expensive since it won’t be resident in memory.
Solutions to Chapter 12 | Testing
Cracking the Coding Interview | Concepts and Algorithms
209
12.1 Find the mistake(s) in the following code:
1 unsigned int i;
2 for (i = 100; i <= 0; --i)
3 printf(“%d\n”, i);
pg 70
SOLUTION
The printf will never get executed, as “i” is initialized to 100, so condition check “i <= 0” will fail.
Suppose the code is changed to “i >= 0.” Then, it will become an infinite loop, because “i” is an unsigned int which can’t be negative.
The correct code to print all numbers from 100 to 1, is “i > 0”.
1 unsigned int i;
2 for (i = 100; i > 0; --i)
3 printf(“%d\n”, i);
One additional correction is to use %u in place of %d, as we are printing unsigned int.
1 unsigned int i;
2 for (i = 100; i > 0; --i)
3 printf(“%u\n”, i);
Solutions to Chapter 12 | Testing
210
CareerCup.com
12.2 You are given the source to an application which crashes when it is run. After running it ten times in a debugger, you find it never crashes in the same place. The application is single threaded, and uses only the C standard library. What programming errors could be causing this crash? How would you test each one?
pg 70
SOLUTION
The question largely depends on the type of application being diagnosed. However, we can give some general causes of random crashes.
1. Random variable: The application uses some random number or variable component which may not be fixed for every execution of the program. Examples include: user input, a random number generated by the program, or the time of day.
2. Memory Leak: The program may have run out of memory. Other culprits are totally random for each run since it depends on the number of processes running at that particular time. This also includes heap overflow or corruption of data on the stack.
It is also possible that the program depends on another application / external module that could lead to the crash. If our application, for example, depends on some system attributes and they are modified by another program, then this interference may lead to a crash. Programs which interact with hardware are more prone to these errors.
In an interview, we should ask about which kind of application is being run. This information may give you some idea about the kind of error the interviewer is looking for. For example, a web server is more prone to memory leakage, whereas a program that runs close to the system level is more prone to crashes due to system dependencies.
Solutions to Chapter 12 | Testing
211 Cracking the Coding Interview | Concepts and Algorithms
12.3 We have the following method used in a chess game: boolean canMoveTo(int x, int y) x and y are the coordinates of the chess board and it returns whether or not the piece can move to that position. Explain how you would test this method.
pg 70
SOLUTION
There are two primary types of testing we should do:
Validation of input/output:
We should validate both the input and output to make sure that each are valid. This might entail:
1. Checking whether input is within the board limit.
»»Attempt to pass in negative numbers
»»Attempt to pass in x which is larger than the width
»»Attempt to pass in y which is larger than the width
Depending on the implementation, these should either return false or throw an exception.
2. Checking if output is within the valid set of return values. (Not an issue in this case, since there are no “invalid” boolean values.)
Functional testing:
Ideally, we would like to test every possible board, but this is far too big. We can do a reasonable coverage of boards however. There are 6 pieces in chess, so we need to do something like this:
1 foreach piece a:
2 for each other type of piece b (6 types + empty space)
3 foreach direction d
4 Create a board with piece a.
5 Place piece b in direction d.
6 Try to move – check return value.
Solutions to Chapter 12 | Testing
. CareerCup com 212
12.4 How would you load test a webpage without using any test tools?
pg 70
SOLUTION
Load testing helps to identify a web application’s maximum operating capacity, as well as any bottlenecks that may interfere with its performance. Similarly, it can check how an application responds to variations in load.
To perform load testing, we must first identify the performance-critical scenarios and the metrics which fulfill our performance objectives. Typical criteria include:
»»response time
»»throughput
»»resource utilization
»»maximum load that the system can bear.
Then, we design tests to simulate the load, taking care to measure each of these criteria.
In the absence of formal testing tools, we can basically create our own. For example, we could simulate concurrent users by creating thousands of virtual users. We would write a multi-threaded program with thousands of threads, where each thread acts as a real-world user loading the page. For each user, we would programmatically measure response time, data I/O, etc.
We would then analyze the results based on the data gathered during the tests and compare it with the accepted values.
Solutions to Chapter 12 | Testing
213 Cracking the Coding Interview | Concepts and Algorithms
12.5 How would you test a pen?
pg 70
SOLUTION
This problem is largely about understand the constraints: what exactly is the pen? You should ask a lot of questions to understand what exactly you are trying to test. To illustrate the technique in this problem, let us guide you through a mock-conversation.
Interviewer: How would you test a pen?
Candidate: Let me find out a bit about the pen. Who is going to use the pen?
Interviewer: Probably children.
Candidate: Ok, that’s interesting. What will they be doing with it? Will they be writing, drawing, or doing something else with it?
Interviewer: Drawing.
Candidate: Ok, great. On what? Paper? Clothing? Walls?
Interviewer: On clothing.
Candidate: Great. What kind of tip does the pen have? Felt? Ball point? Is it intended to wash off, or is it intended to be permanent?
Interviewer: It’s intended to wash off.
…. many questions later ...
Candidate: Ok, so as I understand it, we have a pen that is being targeted at 5—10 year olds. The pen has a felt tip and comes in red, green, blue and black. It’s intended to wash off clothing. Is that correct?
…
The candidate now has a problem that is significantly different from what it initially seemed to be. Thus, the candidate might now want to test:
1. Does the pen wash off with warm water, cold water, and luke warm water?
2. Does the pen wash off after staying on the clothing for several weeks? What happens if you wash the clothing while the pen is still wet?
3. Is the pen safe (e.g.—non-toxic) for children?
and so on...
Solutions to Chapter 12 | Testing
. CareerCup com 214
12.6 How would you test an ATM in a distributed banking system?
pg 70
SOLUTION
The first thing to do on this question is to clarify assumptions. Ask the following questions:
»»Who is going to use the ATM? Answers might be “anyone,” or it might be “blind people” - or any number of other answers.
»»What are they going to use it for? Answers might be “withdrawing money,” “transferring money,” “checking their balance,” or many other answers.
»»What tools do we have to test? Do we have access to the code, or just the ATM machine?
Remember: a good tester makes sure she knows what she’s testing!
Here are a few test cases for how to test just the withdrawing functionality:
»»Withdrawing money less than the account balance
»»Withdrawing money greater than the account balance
»»Withdrawing money equal to the account balance
»»Withdrawing money from an ATM and from the internet at the same time
»»Withdrawing money when the connection to the bank’s network is lost
»»Withdrawing money from multiple ATMs simultaneously
Solutions to Chapter 13 | C++
Cracking the Coding Interview | Knowledge Based
215
13.1 Write a method to print the last K lines of an input file using C++.
pg 76
SOLUTION
One brute force way could be to count the number of lines (N) and then print from N-10 to Nth line. But, this requires two reads of the file – potentially very costly if the file is large.
We need a solution which allows us to read just once and be able to print the last K lines. We can create extra space for K lines and then store each set of K lines in the array. So, initially, our array has lines 0 through 9, then 1 through 10, then 2 through 11, etc (if K = 10). Each time that we read a new line, we purge the oldest line from the array. Instead of shifting the array each time (very inefficient), we will use a circular array. This will allow us to always find the oldest element in O(1) time.
Example of inserting elements into a circular array:
step 1 (initially): array = {a, b, c, d, e, f}. p = 0
step 2 (insert g): array = {g, b, c, d, e, f}. p = 1
step 3 (insert h): array = {g, h, c, d, e, f}. p = 2
step 4 (insert i): array = {g, h, i, d, e, f}. p = 3
Code:
1 string L[K];
2 int lines = 0;
3 while (file.good()) {
4 getline(file, L[lines % K]); // read file line by line
5 ++lines;
6 }
7 // if less than K lines were read, print them all
8 int start, count;
9 if (lines < K) {
10 start = 0;
11 count = lines;
12 } else {
13 start = lines % K;
14 count = K;
15 }
16 for (int i = 0; i < count; ++i) {
17 cout << L[(start + i) % K] << endl;
18 }
OBSERVATIONS AND SUGGESTIONS:
»»Note, if you do printf(L[(index + i) % K]) when there are %’s in the string, bad things will happen.
Solutions to Chapter 13 | C++
216
CareerCup.com
13.2 Compare and contrast a hash table vs. an STL map. How is a hash table implemented? If the number of inputs is small, what data structure options can be used instead of a hash table?
pg 76
SOLUTION
Compare and contrast Hash Table vs. STL map
In a hash table, a value is stored by applying hash function on a key. Thus, values are not stored in a hash table in sorted order. Additionally, since hash tables use the key to find the index that will store the value, an insert/lookup can be done in amortised O(1) time (assuming only a few collisions in the hashtable). One must also handle potential collisions in a hashtable.
In an STL map, insertion of key/value pair is in sorted order of key. It uses a tree to store values, which is why an O(log N) insert/lookup is required. There is also no need to handle collisions. An STL map works well for things like:
»»find min element
»»find max element
»»print elements in sorted order
»»find the exact element or, if the element is not found, find the next smallest number
How is a hash table implemented?
1. A good hash function is required (e.g.: operation % prime number) to ensure that the hash values are uniformly distributed.
2. A collision resolving method is also needed: chaining (good for dense table entries), probing (good for sparse table entries), etc.
3. Implement methods to dynamically increase or decrease the hash table size on a given criterion. For example, when the [number of elements] by [table size] ratio is greater than the fixed threshold, increase the hash table size by creating a new hash table and transfer the entries from the old table to the new table by computing the index using new hash function.
What can be used instead of a hash table, if the number of inputs is small?
You can use an STL map. Although this takes O(log n) time, since the number of inputs is small, this time is negligible.
Solutions to Chapter 13 | C++
217 Cracking the Coding Interview | Knowledge Based
13.3 How do virtual functions work in C++?
pg 76
SOLUTION
A virtual function depends on a “vtable” or “Virtual Table”. If any function of a class is declared as virtual, a v-table is constructed which stores addresses of the virtual functions of this class. The compiler also adds a hidden vptr variable in all such classes which points to the vtable of that class. If a virtual function is not overridden in the derived class, the vtable of the derived class stores the address of the function in his parent class. The v-table is used to resolve the address of the function, for whenever the virtual function is called. Dynamic binding in C++ is therefore performed through the vtable mechanism.
Thus, when we assign the derived class object to the base class pointer, the vptr points to the vtable of the derived class. This assignment ensures that the most derived virtual function gets called.
1 class Shape {
2 public:
3 int edge_length;
4 virtual int circumference () {
5 cout << “Circumference of Base Class\n”;
6 return 0;
7 }
8 };
9 class Triangle: public Shape {
10 public:
11 int circumference () {
12 cout<< “Circumference of Triangle Class\n”;
13 return 3 * edge_length;
14 }
15 };
16 void main() {
17 Shape * x = new Shape();
18 x->circumference(); // prints “Circumference of Base Class”
19 Shape *y = new Triangle();
20 y->circumference(); // prints “Circumference of Triangle Class”
21 }
In the above example, circumference is a virtual function in shape class, so it becomes virtual in each of the derived classes (triangle, rectangle). C++ non-virtual function calls are resolved at compile time with static binding, while virtual function calls are resolved at run time with dynamic binding.
Solutions to Chapter 13 | C++
. CareerCup com 218
13.4 What is the difference between deep copy and shallow copy? Explain how you would use each.
pg 76
SOLUTION
1 struct Test {
2 char * ptr;
3 };
4 void shallow_copy(Test & src, Test & dest) {
5 dest.ptr = src.ptr;
6 }
7 void deep_copy(Test & src, Test & dest) {
8 dest.ptr = malloc(strlen(src.ptr) + 1);
9 memcpy(dest.ptr, src.ptr);
10 }
Note that shallow_copy may cause a lot of programming run-time errors, especially with the creation and deletion of objects. Shallow copy should be used very carefully and only when a programmer really understands what he wants to do. In most cases shallow copy is used when there is a need to pass information about a complex structure without actual duplication of data (e.g., call by reference). One must also be careful with destruction of shallow copy.
In real life, shallow copy is rarely used. There is an important programming concept called “smart pointer” that, in some sense, is an enhancement of the shallow copy concept.
Deep copy should be used in most cases, especially when the size of the copied structure is small.
Solutions to Chapter 13 | C++
219 Cracking the Coding Interview | Knowledge Based
13.5 What is the significance of the keyword “volatile” in C?
pg 76
SOLUTION
Volatile informs the compiler that the value of the variable can change from the outside, without any update done by the code.
Declaring a simple volatile variable:
volatile int x;
int volatile x;
Declaring a pointer variable for a volatile memory (only the pointer address is volatile):
volatile int * x;
int volatile * x;
Declaring a volatile pointer variable for a non-volatile memory (only memory contained is volatile):
int * volatile x;
Declaring a volatile variable pointer for a volatile memory (both pointer address and memory contained are volatile):
volatile int * volatile x;
int volatile * volatile x;
Volatile variables are not optimized, but this can actually be useful. Imagine this function:
1 int opt = 1;
2 void Fn(void) {
3 start:
4 if (opt == 1) goto start;
5 else break;
6 }
At first glance, our code appears to loop infinitely. The compiler will try to optimize it to:
1 void Fn(void) {
2 start:
3 int opt = 1;
4 if (true)
5 goto start;
6 }
This becomes an infinite loop. However, an external program might write ‘0’ to the location of variable opt. Volatile variables are also useful when multi-threaded programs have global variables and any thread can modify these shared variables. Of course, we don’t want optimization on them.
Solutions to Chapter 13 | C++
. CareerCup com 220
13.6 What is name hiding in C++?
pg 76
SOLUTION
Let us explain through an example. In C++, when you have a class with an overloaded method, and you then extend and override that method, you must override all of the overloaded methods.
For example:
1 class FirstClass {
2 public:
3 virtual void MethodA (int);
4 virtual void MethodA (int, int);
5 };
6 void FirstClass::MethodA (int i) {
7 std::cout << “ONE!!\n”;
8 }
9 void FirstClass::MethodA (int i, int j) {
10 std::cout << “TWO!!\n”;
11 }
This is a simple class with two methods (or one overloaded method). If you want to override the one-parameter version, you can do the following:
1 class SecondClass : public FirstClass {
2 public:
3 void MethodA (int);
4 };
5 void SecondClass::MethodA (int i) {
6 std::cout << “THREE!!\n”;
7 }
8 void main () {
9 SecondClass a;
10 a.MethodA (1);
11 a.MethodA (1, 1);
12 }
However, the second call won’t work, since the two-parameter MethodA is not visible. That is name hiding.
Solutions to Chapter 13 | C++
221 Cracking the Coding Interview | Knowledge Based
13.7 Why does a destructor in base class need to be declared virtual?
pg 76
SOLUTION
Calling a method with an object pointer always invokes:
»»the most derived class function, if a method is virtual.
»»the function implementation corresponding to the object pointer type (used to call the method), if a method is non-virtual.
A virtual destructor works in the same way. A destructor gets called when an object goes out of scope or when we call delete on an object pointer.
When any derived class object goes out of scope, the destructor of that derived class gets called first. It then calls its parent class destructor so memory allocated to the object is properly released.
But, if we call delete on a base pointer which points to a derived class object, the base class destructor gets called first (for non-virtual function). For example:
1 class Base {
2 public:
3 Base() { cout << “Base Constructor “ << endl; }
4 ~Base() { cout << “Base Destructor “ << endl; } /* see below */
5 };
6 class Derived: public Base {
7 public:
8 Derived() { cout << ”Derived Constructor “ << endl; }
9 ~Derived() { cout << ”Derived Destructor “ << endl; }
10 };
11 void main() {
12 Base *p = new Derived();
13 delete p;
14 }
Output:
Base Constructor
Derived Constructor
Base Destructor
If we declare the base class destructor as virtual, this makes all the derived class destructors virtual as well.
If we replace the above destructor with:
1 virtual ~Base() {
2 cout << “Base Destructor” << endl;
3 }
Solutions to Chapter 13 | C++
. CareerCup com 222
Then the output becomes:
Base Constructor
Derived Constructor
Derived Destructor
Base Destructor
So we should use virtual destructors if we call delete on a base class pointer which points to a derived class.
Solutions to Chapter 13 | C++
223 Cracking the Coding Interview | Knowledge Based
13.8 Write a method that takes a pointer to a Node structure as a parameter and returns a complete copy of the passed-in data structure. The Node structure contains two pointers to other Node structures.
pg 76
SOLUTION
The algorithm will maintain a mapping from a node address in the original structure to the corresponding node in the new structure. This mapping will allow us to discover previously copied nodes during a traditional depth first traversal of the structure. (Traversals often mark visited nodes--the mark can take many forms and does not necessarily need to be stored in the node.) Thus, we have a simple recursive algorithm:
1 typedef map<Node*, Node*> NodeMap;
2
3 Node * copy_recursive(Node * cur, NodeMap & nodeMap) {
4 if(cur == NULL) {
5 return NULL;
6 }
7 NodeMap::iterator i = nodeMap.find(cur);
8 if (i != nodeMap.end()) {
9 // we’ve been here before, return the copy
10 return i->second;
11 }
12 Node * node = new Node;
13 nodeMap[cur] = node; // map current node before traversing links
14 node->ptr1 = copy_recursive(cur->ptr1, nodeMap);
15 node->ptr2 = copy_recursive(cur->ptr2, nodeMap);
16 return node;
17 }
18 Node * copy_structure(Node * root) {
19 NodeMap nodeMap; // we will need an empty map
20 return copy_recursive(root, nodeMap);
21 }
22
Solutions to Chapter 13 | C++
. CareerCup com 224
13.9 Write a smart pointer (smart_ptr) class.
pg 76
SOLUTION
Smart_ptr is the same as a normal pointer, but it provides safety via automatic memory. It avoids dangling pointers, memory leaks, allocation failures etc. The smart pointer must maintain a single reference count for all instances.
1 template <class T> class SmartPointer {
2 public:
3 SmartPointer(T * ptr) {
4 ref = ptr;
5 ref_count = (unsigned*)malloc(sizeof(unsigned));
6 *ref_count = 1;
7 }
8 SmartPointer(SmartPointer<T> & sptr) {
9 ref = sptr.ref;
10 ref_count = sptr.ref_count;
11 ++*ref_count;
12 }
13 SmartPointer<T> & operator=(SmartPointer<T> & sptr) {
14 if (this != &sptr) {
15 ref = sptr.ref;
16 ref_count = sptr.ref_count;
17 ++*ref_count;
18 }
19 return *this;
20 }
21 ~SmartPointer() {
22 --*ref_count;
23 if (*ref_count == 0) {
24 delete ref;
25 free(ref_count);
26 ref = NULL;
27 ref_count = NULL;
28 }
29 }
30 T getValue() { return *ref; }
31 protected:
32 T * ref;
33 unsigned * ref_count;
34 };
Solutions to Chapter 14 | Java
Cracking the Coding Interview | Knowledge Based
225
14.1 In terms of inheritance, what is the effect of keeping a constructor private?
pg 78
SOLUTION
Declaring the constructor private will ensure that no one outside of the class can directly instantiate the class. In this case, the only way to create an instance of the class is by providing a static public method, as is done when using the Factory Method Pattern.
Additionally, because the constructor is private, the class also cannot be inherited.
Solutions to Chapter 14 | Java
226
CareerCup.com
14.2 In Java, does the finally block gets executed if we insert a return statement inside the try block of a try-catch-finally?
pg 78
SOLUTION
Yes, it will get executed.
The finally block gets executed when the try block exists. However, even when we attempt to exit within the try block (normal exit, return, continue, break or any exception), the finally block will still be executed.
Note: There are some cases in which the finally block will not get executed: if the virtual machine exits in between try/catch block execution, or the thread which is executing try/catch block gets killed.
Solutions to Chapter 14 | Java
227 Cracking the Coding Interview | Knowledge Based
14.3 What is the difference between final, finally, and finalize?
pg 78
SOLUTIONS
Final
When applied to a variable (primitive): The value of the variable cannot change.
When applied to a variable (reference): The reference variable cannot point to any other object on the heap.
When applied to a method: The method cannot be overridden.
When applied to a class: The class cannot be subclassed.
Finally
There is an optional finally block after the try block or after the catch block. Statements in the finally block will always be executed (except if JVM exits from the try block). The finally block is used to write the clean up code.
Finalize
This is the method that the JVM runs before running the garbage collector.
Solutions to Chapter 14 | Java
. CareerCup com 228
14.4 Explain the difference between templates in C++ and generics in Java.
pg 78
SOLUTION
C++ Templates
Java Generics
Classes and functions can be templated.
Classes and methods can be genericized.
Parameters can be any type or integral value.
Parameters can only be reference types (not primitive types).
Separate copies of the class or function are likely to be generated for each type parameter when compiled.
One version of the class or function is compiled, works for all type parameters.
Objects of a class with different type parameters are different types at run time.
Type parameters are erased when compiled; objects of a class with different type parameters are the same type at run time.
Implementation source code of the templated class or function must be included in order to use it (declaration insufficient).
Signature of the class or function from a compiled class file is sufficient to use it.
Templates can be specialized - a separate implementation could be provided for a particular template parameter.
Generics cannot be specialized.
Does not support wildcards. Instead, return types are often available as nested typedefs.
Supports wildcard as type parameter if it is only used once.
Does not directly support bounding of type parameters, but metaprogramming provides this.
Supports bounding of type parameters with "extends" and "super" for upper and lower bounds, respectively; allows enforcement of relationships between type parameters.
Allows instantiation of class of type parameter type.
Does not allow instantiation of class of type parameter type.
Type parameter of templated class can be used for static methods and variables.
Type parameter of templated class cannot be used for static methods and variables.
Static variables are not shared between classes of different type parameters.
Static variables are shared between instances of a classes of different type parameters.
From http://en.wikipedia.org/wiki/Comparison_of_Java_and_C%2B%2B#Templates_vs._Generics
Solutions to Chapter 14 | Java
229 Cracking the Coding Interview | Knowledge Based
14.5 Explain what object reflection is in Java and why it is useful.
pg 78
SOLUTION
Object Reflection is a feature in Java which provides a way to get reflective information about Java classes and objects, such as:
1. Getting information about methods and fields present inside the class at run time.
2. Creating a new instance of a class.
3. Getting and setting the object fields directly by getting field reference, regardless of what the access modifier is.
1 import java.lang.reflect.*;
2
3 public class Sample {
4 public static void main(String args[]) {
5 try {
6 Class c = Class.forName(“java.sql.Connection”);
7 Method m[] = c.getDeclaredMethods();
8 for (int i = 0; i < 3; i++) {
9 System.out.println(m[i].toString());
10 }
11 } catch (Throwable e) {
12 System.err.println(e);
13 }
14 }
15 }
This code’s output is the names of the first 3 methods inside the “java.sql.Connection” class (with fully qualified parameters).
Why it is useful:
1. Helps in observing or manipulating the runtime behavior of applications.
2. Useful while debugging and testing applications, as it allows direct access to methods, constructors, fields, etc.
Solutions to Chapter 14 | Java
. CareerCup com 230
14.6 Suppose you are using a map in your program, how would you count the number of times the program calls the put() and get() functions?
pg 78
SOLUTION
One simple solution is to put count variables for get() and put() methods and, whenever they are called, increment the count. We can also achieve this by extending the existing library map and overriding the get() and put() functions.
At first glance, this seems to work. However, what if we created multiple instances of the map? How would you sum up the total count for each map object?
The simplest solution for this is to keep the count variables static. We know static variables have only one copy for all objects of the class so the total count would be reflected in count variables.
Solutions to Chapter 15 | Databases
Cracking the Coding Interview | Knowledge Based
231
15.1 Write a method to find the number of employees in each department.
pg 80
SOLUTION
This problem uses a straight-forward join of Departments and Employees. Note that we use a left join instead of an inner join because we want to include Departments with 0 employees.
1 select Dept_Name, Departments.Dept_ID, count(*) as ‘num_employees’
2 from Departments
3 left join Employees
4 on Employees.Dept_ID = Departments.Dept_ID
5 group by Departments.Dept_ID, Dept_Name
Solutions to Chapter 15 | Databases
232
CareerCup.com
15.2 What are the different types of joins? Please explain how they differ and why certain types are better in certain situations.
pg 80
SOLUTION
JOIN is used to combine the results of two tables. To perform a join, each of the tables must have at least one field which will be used to find matching records from the other table. The join type defines which records will go into the result set.
Let’s take for example two tables: one table lists “regular” beverages, and another lists the calorie-free beverages. Each table has two fields: the beverage name and its product code. The “code” field will be used to perform the record matching.
Regular Beverages:
Name
Code
Budweiser
BUDWEISER
Coca-Cola
COCACOLA
Pepsi
PEPSI
Calorie-Free Beverages:
Code
Name
COCACOLA
Diet Coca-Cola
FRESCA
Fresca
PEPSI
Diet Pepsi
PEPSI
Pepsi Light
Water
Purified Water
Let’s join this table by the code field. Whereas the order of the joined tables makes sense in some cases, we will consider the following statement:
[Beverage] JOIN [Calorie-Free Beverage]
i.e. [Beverage] is from the left of the join operator, and [Calorie-Free Beverage] is from the right.
1. INNER JOIN: Result set will contain only those data where the criteria match. In our example we will get 3 records: 1 with COCACOLA and 2 with PEPSI codes.
2. OUTER JOIN: OUTER JOIN will always contain the results of INNER JOIN, however it can contain some records that have no matching record in other table. OUTER JOINs are divided to following subtypes:
Solutions to Chapter 15 | Databases
233 Cracking the Coding Interview | Knowledge Based
2.1. LEFT OUTER JOIN, or simply LEFT JOIN: The result will contain all records from the left table. If no matching records were found in the right table, then its fields will contain the NULL values. In our example, we would get 4 records. In addition to INNER JOIN results, BUDWEISER will be listed, because it was in the left table.
2.2. RIGHT OUTER JOIN, or simply RIGHT JOIN: This type of join is the opposite of LEFT JOIN; it will contain all records from the right table, and missing fields from the left table will contain NULL. If we have two tables A and B, then we can say that statement A LEFT JOIN B is equivalent to statement B RIGHT JOIN A.
In our example, we will get 5 records. In addition to INNER JOIN results, FRESCA and WATER records will be listed.
2.3. FULL OUTER JOIN
This type of join combines the results of LEFT and RIGHT joins. All records from both tables will be part of the result set, whether the matching record exists in the other table or not. If no matching record was found then the corresponding result fields will have a NULL value.
In our example, we will get 6 records.
Solutions to Chapter 15 | Databases
. CareerCup com 234
15.3 What is denormalization? Explain the pros and cons.
pg 80
SOLUTION
Denormalization is the process of attempting to optimize the performance of a database by adding redundant data or by grouping data. In some cases, denormalization helps cover up the inefficiencies inherent in relational database software. A relational normalized database imposes a heavy access load over physical storage of data even if it is well tuned for high performance.
A normalized design will often store different but related pieces of information in separate logical tables (called relations). If these relations are stored physically as separate disk files, completing a database query that draws information from several relations (a join operation) can be slow. If many relations are joined, it may be prohibitively slow. There are two strategies for dealing with this. The preferred method is to keep the logical design normalized, but allow the database management system (DBMS) to store additional redundant information on disk to optimize query response. In this case, it is the DBMS software’s responsibility to ensure that any redundant copies are kept consistent. This method is often implemented in SQL as indexed views (Microsoft SQL Server) or materialized views (Oracle). A view represents information in a format convenient for querying, and the index ensures that queries against the view are optimized.
The more usual approach is to denormalize the logical data design. With care, this can achieve a similar improvement in query response, but at a cost—it is now the database designer’s responsibility to ensure that the denormalized database does not become inconsistent. This is done by creating rules in the database called constraints, that specify how the redundant copies of information must be kept synchronized. It is the increase in logical complexity of the database design and the added complexity of the additional constraints that make this approach hazardous. Moreover, constraints introduce a trade-off, speeding up reads (SELECT in SQL) while slowing down writes (INSERT, UPDATE, and DELETE). This means a denormalized database under heavy write load may actually offer worse performance than its functionally equivalent normalized counterpart.
A denormalized data model is not the same as a data model that has not been normalized, and denormalization should only take place after a satisfactory level of normalization has taken place and that any required constraints and/or rules have been created to deal with the inherent anomalies in the design. For example, all the relations are in third normal form and any relations with join and multivalued dependencies are handled appropriately.
From http://en.wikipedia.org/wiki/Denormalization
Solutions to Chapter 15 | Databases
235 Cracking the Coding Interview | Knowledge Based
15.4 Draw an entity-relationship diagram for a database with companies, people, and professionals (people who work for companies).
pg 80
SOLUTION
People who work for companies are Professionals. So there is an ISA (is a) relationship between People and Professionals (or we could say that a Professional is derived from People).
Each Professional has additional information such as degree, work experiences, etc, in addition to the properties derived from People.
A Professional works for one company at a time, but Companies can hire many Professionals, so there is a Many to One relationship between Professionals and Companies. This “Works For” relationship can store attributes such as date of joining the company, salary, etc. These attributes are only defined when we relate a Professional with a Company.
A Person can have multiple phone numbers, which is why Phone is a multi-valued attribute.
Professional
People
Companies
Works For
Degree
Experience
Salary
Address
CName
CID
Date of Joining
Address
Phone
ISA
N
1
DOB
Sex
PName
PID
Solutions to Chapter 15 | Databases
. CareerCup com 236
15.5 Imagine a simple database storing information for students’ grades. Design what this database might look like, and provide a SQL query to return a list of the honor roll students (top 10%), sorted by their grade point average.
pg 80
SOLUTION
In a simplistic database, we’ll have at least these three objects: Students, Courses, and CourseEnrollment. Students will have at least the student name and ID, and will likely have other personal information. Courses will contain the course name and ID, and will likely contain the course description, professor, etc. CourseEnrollment will pair Students and Courses, and will also contain a field for CourseGrade. We will assume that CourseGrade is an integer.
Our SQL query to get the list of honor roll students might look like this:
1 SELECT StudentName, GPA
2 FROM (
3 SELECT top 10 percent Avg(CourseEnrollment.Grade) AS GPA,
4 CourseEnrollment.StudentID
5 FROM CourseEnrollment
6 GROUP BY CourseEnrollment.StudentID
7 ORDER BY Avg(CourseEnrollment.Grade)) Honors
8 INNER JOIN Students ON Honors.StudentID = Students.StudentID
This database could get arbitrarily more complicated if we wanted to add in professor information, billing, etc.
Solutions to Chapter 16 | Low Level
Cracking the Coding Interview | Knowledge Based
237
16.1 Explain the following terms: virtual memory, page fault, thrashing.
pg 82
SOLUTION
Virtual memory is a computer system technique which gives an application program the impression that it has contiguous working memory (an address space), while in fact it may be physically fragmented and may even overflow on to disk storage. Systems that use this technique make programming of large applications easier and use real physical memory (e.g. RAM) more efficiently than those without virtual memory.
http://en.wikipedia.org/wiki/Virtual_memory
Page Fault: A page is a fixed-length block of memory that is used as a unit of transfer between physical memory and external storage like a disk, and a page fault is an interrupt (or exception) to the software raised by the hardware, when a program accesses a page that is mapped in address space, but not loaded in physical memory.
http://en.wikipedia.org/wiki/Page_fault
Thrash is the term used to describe a degenerate situation on a computer where increasing resources are used to do a decreasing amount of work. In this situation the system is said to be thrashing. Usually it refers to two or more processes accessing a shared resource repeatedly such that serious system performance degradation occurs because the system is spending a disproportionate amount of time just accessing the shared resource. Resource access time may generally be considered as wasted, since it does not contribute to the advancement of any process. In modern computers, thrashing may occur in the paging system (if there is not ‘sufficient’ physical memory or the disk access time is overly long), or in the communications system (especially in conflicts over internal bus access), etc.
http://en.wikipedia.org/wiki/Thrash_(computer_science)
Solutions to Chapter 16 | Low Level
238
CareerCup.com
16.2 What is a Branch Target buffer? Explain how it can be used in reducing bubble cycles in cases of branch misprediction.
pg 82
SOLUTION
Branch misprediction occurs when the CPU mispredicts the next instruction to be executed.
The CPU uses pipelining which allows several instructions to be processed simultaneously. But during a conditional jump, the next instruction to be executed depends on the result of the condition. Branch Prediction tries to guess the next instruction. However, if the guess is wrong, we are penalized because the instruction which was executed must be discarded.
Branch Target Buffer (BTB) reduces the penalty by predicting the path of the branch, computing the target of the branch and caching the information used by the branch. There will be no stalls if the branch entry found on BTB and the prediction is correct, otherwise the penalty will be at least two cycles.
Solutions to Chapter 16 | Low Level
239 Cracking the Coding Interview | Knowledge Based
16.3 Describe direct memory access (DMA). Can a user level buffer / pointer be used by kernel or drivers?
pg 82
SOLUTION
Direct Memory is a feature which provides direct access (read/write) to system memory without interaction from the CPU. The “DMA Controller” manages this by requesting the System bus access (DMA request) from CPU. CPU completes its current task and grants access by asserting DMA acknowledgement signal. Once it gets the access, it reads/writes the data and returns back the system bus to the CPU by asserting the bus release signal. This transfer is faster than the usual transfer by CPU. Between this time CPU is involved with processing task which doesn’t require memory access.
By using DMA, drivers can access the memory allocated to the user level buffer / pointer.
Solutions to Chapter 16 | Low Level
. CareerCup com 240
16.4 Write a step by step execution of things that happen after a user presses a key on the keyboard. Use as much detail as possible.
pg 82
SOLUTION
1. The keyboard sends a scan code of the key to the keyboard controller (Scan code for key pressed and key released is different).
2. The keyboard controller interprets the scan code and stores it in a buffer.
3. The keyboard controller sends a hardware interrupt to the processor. This is done by putting signal on “interrupt request line”: IRQ 1.
4. The interrupt controller maps IRQ 1 into INT 9.
5. An interrupt is a signal which tells the processor to stop what it was doing currently and do some special task.
6. The processor invokes the “Interrupt handler”. CPU fetches the address of “Interrupt Service Routine” (ISR) from “Interrupt Vector Table” maintained by the OS (Processor use the IRQ number for this).
7. The ISR reads the scan code from port 60h and decides whether to process it or pass the control to program for taking action.
Solutions to Chapter 16 | Low Level
241 Cracking the Coding Interview | Knowledge Based
16.5 Write a program to find whether a machine is big endian or little endian.
pg 82
SOLUTION
1 #define BIG_ENDIAN 0
2 #define LITTLE_ENDIAN 1
3 int TestByteOrder() {
4 short int word = 0x0001;
5 char *byte = (char *) &word;
6 return (byte[0] ? LITTLE_ENDIAN : BIG_ENDIAN);
7 }
Solutions to Chapter 16 | Low Level
. CareerCup com 242
16.6 Discuss how would you make sure that a process doesn’t access an unauthorized part of the stack.
pg 82
SOLUTION
As with any ambiguously worded interview question, it may help to probe the interviewer to understand what specifically you’re intended to solve. Are you trying to prevent code that has overflowed a buffer from compromising the execution by overwriting stack values? Are you trying to maintain some form of thread-specific isolation between threads? Is the code of interest native code like C++ or running under a virtual machine like Java?
Remember that, in a multi-threaded environment, there can be multiple stacks in a process.
NATIVE CODE
One threat to the stack is malicious program input, which can overflow a buffer and overwrite stack pointers, thus circumventing the intended execution of the program.
If the interviewer is looking for a simple method to reduce the risk of buffer overflows in native code, modern compilers provide this sort of stack protection through a command line option. With Microsoft’s CL, you just pass /GS to the compiler. With GCC, you can use -fstack-protector-all.
For more complex schemes, you could set individual permissions on the range of memory pages representing the stack section you care about. In the Win32 API, you’d use the VirtualProtect API to mark the page PAGE_READONLY or PAGE_NOACCESS. This will cause the code accessing the region to go through an exception on access to the specific section of the stack.
Alternately, you could use the HW Debug Registers (DRs) to set a read or write breakpoint on the specific memory addresses of interest. A separate process could be used to debug the process of interest, catch the HW exception that would be generated if this section of the stack were accessed.
However, it’s very important to note that under normal circumstances, threads and processes are not means of access control. Nothing can prevent native code from writing anywhere within the address space of its process, including to the stack. Specifically, there is nothing to prevent malicious code in the process from calling VirtualProtect and marking the stack sections of interest PAGE_EXECUTE_READWRITE. Equally so, nothing prevents code from zeroing out the HW debug registers, eliminating your breakpoints. In summary, nothing can fully prevent native code from accessing memory addresses, including the stack, within its own process space.
MANAGED CODE
Solutions to Chapter 16 | Low Level
243 Cracking the Coding Interview | Knowledge Based
A final option is to consider requiring this code that should be “sandboxed” to run in a managed language like Java or C# / .NET. By default, the virtual machines running managed code in these languages make it impossible to gain complete access to the stack from within the process.
One can use further security features of the runtimes to prevent the code from spawning additional processes or running “unsafe” code to inspect the stack. With .NET, for example, you can use Code Access Security (CAS) or appdomain permissions to control such execution.
Solutions to Chapter 16 | Low Level
. CareerCup com 244
16.7 What are the best practices to prevent reverse engineering of DLLs?
pg 82
SOLUTION
Best practices include the following:
»»Use obfuscators.
»»Do not store any data (string, etc) in open form. Always compress or encode it.
»»Use a static link so there is no DLL to attack.
»»Strip all symbols.
»»Use a .DEF file and an import library to have anonymous exports known only by their export ids.
»»Keep the DLL in a resource and expose it in the file system (under a suitably obscure name, perhaps even generated at run time) only when running.
»»Hide all real functions behind a factory method that exchanges a secret (better, proof of knowledge of a secret) for a table of function pointers to the real methods.
»»Use anti-debugging techniques borrowed from the malware world to prevent reverse engineering. (Note that this will likely get you false positives from AV tools.)
»»Use protectors.
Solutions to Chapter 16 | Low Level
245 Cracking the Coding Interview | Knowledge Based
16.8 A device boots with an empty FIFO queue. In the first 400 ns period after startup, and in each subsequent 400 ns period, a maximum of 80 words will be written to the queue. Each write takes 4 ns. A worker thread requires 3 ns to read a word, and 2 ns to process it before reading the next word. What is the shortest depth of the FIFO such that no data is lost?
pg 82
SOLUTION
While a perfectly optimal solution is complex, an interviewer is most interested in how you approach the problem.
THEORY
First, note that writes do not have to be evenly distributed within a period. Thus a likely worst case is 80 words are written at the end of the first period, followed by 80 more at the start of the next.
Note that the maximum write rate for a full period is exactly matched by a full period of processing (400 ns / ((3 ns + 2 ns)/process) = 80 processed words/period).
As the 2nd period in our example is fully saturated, adding writes from a 3rd period would not add additional stress, and this example is a true worst case for the conditions.
A SAFE QUEUE DEPTH
For an estimate of maximum queue size, notice that these 160 writes take 640 ns (160 writes * 4 ns / write = 640 ns), during which time only 128 words have been read (640 ns / ((3 ns + 2 ns) / word) = 128 words). However, the first read cannot start until the first write has finished, which fills an extra slot in the queue.
Also, depending on the interactions between read and write timing, a second additional slot may be necessary to ensure a write does not trash the contents of a concurrently occurring read. Thus, a safe estimate is that the queue must be at least 34 words deep (160 - 128 + 1 + 1 = 34) to accommodate the unread words.
FINDING AN OPTIMAL (MINIMAL) QUEUE DEPTH
Depending on the specifics of the problem, it’s possible that the final queue spot could be safely removed. In many cases, the time required to do an edge case analysis to determine safety is not worth the effort. However, if the interviewer is interested, the full analysis follows.
We are interested in the exact queue load during the final (160th) consecutive write to the queue. We can approach this by graphing the queue load from time = 0 ns, observing the pattern, and extending it to time = 716 ns, the time of the final consecutive write.
The graph below shows that the queue load increases as each write begins, and decreases
Solutions to Chapter 16 | Low Level
. CareerCup com 246
3 ns after a read begins. Uninteresting time segments are surrounded by [brackets]. Each character represents 1 ns.
0 - 79 ns
80 - 99 ns
100 - 707 ns
708 - 723 ns
>= 724 ns
Writer
AAAABBBBCCCCDDDDEEEE
XXXXYYYYZZZZ____
Worker
____aaaaabbbbbcccccd
opppppqqqqqrrrrr
Queue Load
11112221222222223222
3333333343333322 *
Y = Writing word 159 @ 712 ns
Z = Writing word 160 @ 716 ns
q = Processing word 127 @ 714 ns
r = Processing word 128
* = Between 708 and 723 ns, the queue load is shown as 30 plus the digit shown at each ns.
Note that the queue load does in fact reach a maximum of 34 at time = 716 ns.
As an interesting note, if the problem had required only 2 ns of the 5 ns processing time to complete a read, the optimal queue depth would decrease to 33.
The below graphs are unnecessary, but show empirically that adding writes from the 3rd period does not change the queue depth required.
< 796 ns
797 - 807 ns
808 - 873 ns
874 - 885 ns
Writer
____AAAABBBB
!!@@@@####$$
Worker
^^^&&&&&****
yyyyyzzzzzaa
Queue Load
877788778887
112111221122 *
A = Writing word 161
& = Processing word 144
# = Writing word 181
z = Processing word 160 @ 779 ns
* = Between 874 and 885 ns, the queue load is shown as 20 plus the digit shown at each ns.
< 1112 ns
1112 - 1123 ns
Writer
YYYYZZZZ____
Worker
^^&&&&&%%%%%
Queue Load
333343333322 *
Z = Writing word 240 @ 1116 ns
& = Processing word 207 @ 1114 ns
* = Between 1112 and 1123 ns, the queue load is shown as 30 plus the digit shown at each ns.
Solutions to Chapter 16 | Low Level
247 Cracking the Coding Interview | Knowledge Based
16.9 Write an aligned malloc & free function that takes number of bytes and aligned byte (which is always power of 2)
EXAMPLE
align_malloc (1000,128) will return a memory address that is a multiple of 128 and that points to memory of size 1000 bytes.
aligned_free() will free memory allocated by align_malloc.
pg 82
SOLUTION
1. We will use malloc routine provided by C to implement the functionality.
Allocate memory of size (bytes required + alignment – 1 + sizeof(void*)) using malloc.
alignment: malloc can give us any address and we need to find a multiple of alignment.
(Therefore, at maximum multiple of alignment, we will be alignment-1 bytes away from any location.)
sizeof(size_t): We are returning a modified memory pointer to user, which is different from the one that would be returned by malloc. We also need to extra space to store the address given by malloc, so that we can free memory in aligned_free by calling free routine provided by C.
2. If it returns NULL, then aligned_malloc will fail and we return NULL.
3. Else, find the aligned memory address which is a multiple of alignment (call this p2).
4. Store the address returned by malloc (e.g., p1 is just size_t bytes ahead of p2), which will be required by aligned_free.
5. Return p2.
1 void* aligned_malloc(size_t required_bytes, size_t alignment) {
2 void* p1; // original block
3 void** p2; // aligned block
4 int offset = alignment - 1 + sizeof(void*);
5 if ((p1 = (void*)malloc(required_bytes + offset)) == NULL) {
6 return NULL;
7 }
8 p2 = (void**)(((size_t)(p1) + offset) & ~(alignment - 1));
9 p2[-1] = p1;
10 return p2;
11 }
12 void aligned_free(void *p) {
13 free(((void**)p)[-1]);
14 }
Solutions to Chapter 16 | Low Level
. CareerCup com 248
16.10 Write a function called my2DAlloc which allocates a two dimensional array. Minimize the number of calls to malloc and make sure that the memory is accessible by the notation arr[i][j].
pg 82
SOLUTION
We will use one call to malloc.
Allocate one block of memory to hold the row vector and the array data. The row vector will reside in rows * sizeof(int*) bytes. The integers in the array will take up another rows * cols * sizeof(int) bytes.
Constructing the array in a single malloc has the added benefit of allowing disposal of the array with a single free call rather than using a special function to free the subsidiary data blocks.
1 #include <malloc.h>
2
3 int** My2DAlloc(int rows, int cols) {
4 int header = rows * sizeof(int*);
5 int data = rows * cols * sizeof(int);
6 int** rowptr = (int**)malloc(header + data);
7 int* buf = (int*)(rowptr + rows);
8 int k;
9 for (k = 0; k < rows; ++k) {
10 rowptr[k] = buf + k*cols;
11 }
12 return rowptr;
13 }
Solutions to Chapter 17 | Networking
Cracking the Coding Interview | Knowledge Based
249
17.1 Explain what happens, step by step, after you type a URL into a browser. Use as much detail as possible.
pg 84
SOLUTION
There’s no right, or even complete, answer for this question. This question allows you to go into arbitrary amounts of detail depending on what you’re comfortable with. Here’s a start though:
1. Browser contacts the DNS server to find the IP address of URL.
2. DNS returns back the IP address of the site.
3. Browser opens TCP connection to the web server at port 80.
4. Browser fetches the html code of the page requested.
5. Browser renders the HTML in the display window.
6. Browser terminates the connection when window is closed.
One of the most interesting steps is Step 1 and 2 - “Domain Name Resolution.” The web addresses we type are nothing but an alias to an IP address in human readable form. Mapping of domain names and their associated Internet Protocol (IP) addresses is managed by the Domain Name System (DNS), which is a distributed but hierarchical entity.
Each domain name server is divided into zones. A single server may only be responsible for knowing the host names and IP addresses for a small subset of a zone, but DNS servers can work together to map all domain names to their IP addresses. That means if one domain name server is unable to find the IP addresses of a requested domain then it requests the information from other domain name servers.
Solutions to Chapter 17 | Networking
250
CareerCup.com
17.2 Explain any common routing protocol in detail. For example: BGP, OSPF, RIP.
pg 84
SOLUTION
Depending on the reader’s level of understanding, knowledge, interest or career aspirations, he or she may wish to explore beyond what is included here. Wikipedia and other websites are great places to look for a deeper understanding. We will provide only a short summary.
BGP: Border Gateway Protocol
BGP is the core routing protocol of the Internet. “When a BGP router first comes up on the Internet, either for the first time or after being turned off, it establishes connections with the other BGP routers with which it directly communicates. The first thing it does is download the entire routing table of each neighboring router. After that it only exchanges much shorter update messages with other routers.
BGP routers send and receive update messages to indicate a change in the preferred path to reach a computer with a given IP address. If the router decides to update its own routing tables because this new path is better, then it will subsequently propagate this information to all of the other neighboring BGP routers to which it is connected, and they will in turn decide whether to update their own tables and propagate the information further.”
Borrowed from http://www.livinginternet.com/i/iw_route_egp_bgp.htm.
RIP: Routing Information Protocol
“RIP provides the standard IGP protocol for local area networks, and provides great network stability, guaranteeing that if one network connection goes down the network can quickly adapt to send packets through another connection. “
“What makes RIP work is a routing database that stores information on the fastest route from computer to computer, an update process that enables each router to tell other routers which route is the fastest from its point of view, and an update algorithm that enables each router to update its database with the fastest route communicated from neighboring routers.”
Borrowing from http://www.livinginternet.com/i/iw_route_igp_rip.htm.
OSPF: Open Shortest Path First
“Open Shortest Path First (OSPF) is a particularly efficient IGP routing protocol that is faster than RIP, but also more complex.”
The main difference between OSPF and RIP is that RIP only keeps track of the closest router for each destination address, while OSPF keeps track of a complete topological database of all connections in the local network. The OSPF algorithm works as described below.
Solutions to Chapter 17 | Networking
251 Cracking the Coding Interview | Knowledge Based
»»Startup. When a router is turned on it sends Hello packets to all of its neighbors, re-ceives their Hello packets in return, and establishes routing connections by synchroniz-ing databases with adjacent routers that agree to synchronize.
»»Update. At regular intervals each router sends an update message called its “link state” describing its routing database to all the other routers, so that all routers have the same description of the topology of the local network.
»»Shortest path tree. Each router then calculates a mathematical data structure called a “shortest path tree” that describes the shortest path to each destination address and therefore indicates the closest router to send to for each communication; in other words -- “open shortest path first”.
See http://www.livinginternet.com/i/iw_route_igp_ospf.htm.
Solutions to Chapter 17 | Networking
. CareerCup com 252
17.3 Compare and contrast the IPv4 and IPv6 protocols.
pg 84
SOLUTION
IPv4 and IPv6 are the internet protocols applied at the network layer. IPv4 is the most widely used protocol right now and IPv6 is the next generation protocol for internet.
»»IPv4 is the fourth version of Internet protocol which uses 32 bit addressing whereas IPv6 is a next generation internet protocol which uses 128 bits addressing.
»»IPv4 allows 4,294,967,296 unique addresses where as IPv6 can hold 340-undecillion (34, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000) unique IP addresses.
»»IPv4 has different class types: A,B,C,D and E. Class A, Class B, and Class C are the three classes of addresses used on IP networks in common practice. Class D addresses are reserved for multicast. Class E addresses are simply reserved, meaning they should not be used on IP networks (used on a limited basis by some research organizations for experimental purposes).
»»IPv6 addresses are broadly classified into three categories:
1. Unicast addresses: A Unicast address acts as an identifier for a single interface. An IPv6 packet sent to a Unicast address is delivered to the interface identified by that address.
2. Multicast addresses: A Multicast address acts as an identifier for a group / set of interfaces that may belong to the different nodes. An IPv6 packet delivered to a multicast address is delivered to the multiple interfaces.
3. Anycast addresses: Anycast addresses act as identifiers for a set of interfaces that may belong to the different nodes. An IPv6 packet destined for an Anycast address is delivered to one of the interfaces identified by the address.
»»IPv4 address notation: 239.255.255.255, 255.255.255.0
»»IPv6 addresses are denoted by eight groups of hexadecimal quartets separated by colons in between them.
»»An example of a valid IPv6 address: 2001:cdba:0000:0000:0000:0000:3257:9652
Because of the increase in the population, there is a need of Ipv6 protocol which can provide solution for:
1. Increased address space
2. More efficient routing
3. Reduced management requirement
Solutions to Chapter 17 | Networking
253 Cracking the Coding Interview | Knowledge Based
4. Improved methods to change ISP
5. Better mobility support
6. Multi-homing
7. Security
8. Scoped address: link-local, site-local and global-address space
Solutions to Chapter 17 | Networking
. CareerCup com 254
17.4 What is a network / subnet mask? Explain how host A sends a message / packet to host B when: (a) both are on same network and (b) both are on different networks. Explain which layer makes the routing decision and how.
pg 84
SOLUTION
A mask is a bit pattern used to identify the network/subnet address. The IP address consists of two components: the network address and the host address.
The IP addresses are categorized into different classes which are used to identify the network address.
Example: Consider IP address 152.210.011.002. This address belongs to Class B, so:
Network Mask: 11111111.11111111.00000000.00000000
Given Address: 10011000.11010101.00001011.00000010
By ANDing Network Mask and IP Address, we get the following network address:
10011000.11010101.00000000.00000000 (152.210.0.0)
Host address: 00001011.00000010
Similarly, a network administrator can divide any network into sub-networks by using subnet mask. To do this, we further divide the host address into two or more subnets.
For example, if the above network is divided into 18 subnets (requiring a minimum of 5 bits to represent 18 subnets), the first 5 bits will be used to identify the subnet address.
Subnet Mask: 11111111.11111111.11111000.00000000 (255.255.248.0)
Given Address: 10011000.11010101.00001011.00000010
So, by ANDing the subnet mask and the given address, we get the following subnet address: 10011000.11010101.00001000.00000000 (152.210.1.0)
How Host A sends a message/packet to Host B:
When both are on same network: the host address bits are used to identify the host within the network.
Both are on different networks: the router uses the network mask to identify the network and route the packet. The host can be identified using the network host address.
The network layer is responsible for making routing decisions. A routing table is used to store the path information and the cost involved with that path, while a routing algorithm uses the routing table to decide the path on which to route the packets.
Routing is broadly classified into Static and Dynamic Routing based on whether the table is fixed or it changes based on the current network condition.
Solutions to Chapter 17 | Networking
255 Cracking the Coding Interview | Knowledge Based
17.5 What are the differences between TCP and UDP? Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s / receiver’s window) and congestion control.
pg 84
SOLUTION
TCP (Transmission Control Protocol): TCP is a connection-oriented protocol. A connection can be made from client to server, and from then on any data can be sent along that connection.
»»Reliable - when you send a message along a TCP socket, you know it will get there unless the connection fails completely. If it gets lost along the way, the server will re-request the lost part. This means complete integrity; data will not get corrupted.
»»Ordered - if you send two messages along a connection, one after the other, you know the first message will get there first. You don’t have to worry about data arriving in the wrong order.
»»Heavyweight - when the low level parts of the TCP “stream” arrive in the wrong order, resend requests have to be sent. All the out of sequence parts must be put back together, which requires a bit of work.
UDP(User Datagram Protocol): UDP is connectionless protocol. With UDP you send messages (packets) across the network in chunks.
»»Unreliable - When you send a message, you don’t know if it’ll get there; it could get lost on the way.
»»Not ordered - If you send two messages out, you don’t know what order they’ll arrive in.
»»Lightweight - No ordering of messages, no tracking connections, etc. It’s just fire and forget! This means it’s a lot quicker, and the network card / OS have to do very little work to translate the data back from the packets.
Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s/receiver’s window).
For each TCP packet, the receiver of a packet must acknowledge that the packet is received. If there is no acknowledgement, the packet is sent again. These guarantee that every single packet is delivered. ACK is a packet used in TCP to acknowledge receipt of a packet. A TCP window is the amount of outstanding (unacknowledged by the recipient) data a sender can send on a particular connection before it gets an acknowledgment back from the receiver that it has gotten some of it.
For example, if a pair of hosts are talking over a TCP connection that has a TCP window with a size of 64 KB, the sender can only send 64 KB of data and then it must wait for an acknowledgment from the receiver that some or all of the data has been received. If the receiver
Solutions to Chapter 17 | Networking
. CareerCup com 256
acknowledges that all the data has been received, then the sender is free to send another 64 KB. If the sender gets back an acknowledgment from the receiver that it received the first 32 KB (which could happen if the second 32 KB was still in transit or it could happen if the second 32 KB got lost), then the sender can only send another additional 32 KB since it can’t have more than 64 KB of unacknowledged data outstanding (the second 32 KB of data plus the third).
Congestion Control
The TCP uses a network congestion avoidance algorithm that includes various aspects of an additive-increase-multiplicative-decrease scheme, with other schemes such as slow-start in order to achieve congestion avoidance.
There are different algorithms to solve the problem; Tahoe and Reno are the most well known. To avoid congestion collapse, TCP uses a multi-faceted congestion control strategy. For each connection, TCP maintains a congestion window, limiting the total number of unacknowledged packets that may be in transit end-to-end. This is somewhat analogous to TCP’s sliding window used for flow control. TCP uses a mechanism called slow start to increase the congestion window after a connection is initialized and after a timeout. It starts with a window of two times the maximum segment size (MSS). Although the initial rate is low, the rate of increase is very rapid: for every packet acknowledged, the congestion window increases by 1 MSS so that for every round trip time (RTT), the congestion window has doubled. When the congestion window exceeds a threshold ssthresh the algorithm enters a new state, called congestion avoidance. In some implementations (i.e., Linux), the initial ssthresh is large, and so the first slow start usually ends after a loss. However, ssthresh is updated at the end of each slow start, and will often affect subsequent slow starts triggered by timeouts.
Solutions to Chapter 18 | Threads and Locks
Cracking the Coding Interview | Knowledge Based
257
18.1 What’s the difference between a thread and a process?
pg 86
SOLUTION
Processes and threads are related to each other but are fundamentally different.
A process can be thought of as an instance of a program in execution. Each process is an independent entity to which system resources (CPU time, memory, etc.) are allocated and each process is executed in a separate address space. One process cannot access the variables and data structures of another process. If you wish to access another process’ resources, inter-process communications have to be used such as pipes, files, sockets etc.
A thread uses the same stack space of a process. A process can have multiple threads. A key difference between processes and threads is that multiple threads share parts of their state. Typically, one allows multiple threads to read and write the same memory (no processes can directly access the memory of another process). However, each thread still has its own registers and its own stack, but other threads can read and write the stack memory.
A thread is a particular execution path of a process; when one thread modifies a process resource, the change is immediately visible to sibling threads.
Solutions to Chapter 18 | Threads and Locks
258
CareerCup.com
18.2 How can you measure the time spent in a context switch?
pg 86
SOLUTION
This is a tricky question, but let’s start with a possible solution.
A context switch is the time spent switching between two processes (e.g., bringing a waiting process into execution and sending an executing process into waiting/terminated state). This happens in multitasking. The operating system must bring the state information of waiting processes into memory and save the state information of the running process.
In order to solve this problem, we would like to record timestamps of the last and first instruction of the swapping processes. The context switching time would be the difference in the timestamps between the two processes.
Let’s take an easy example: Assume there are only two processes, P1 and P2.
P1 is executing and P2 is waiting for execution. At some point, the OS must swap P1 and P2—let’s assume it happens at the Nth instruction of P1. So, the context switch time for this would be Time_Stamp(P2_1) – Time_Stamp(P2_N)
Easy enough. The tricky part is this: how do we know when this swapping occurs? Swapping is governed by the scheduling algorithm of the OS. We can not, of course, record the timestamp of every instruction in the process.
Another issue: there are many kernel level threads which are also doing context switches, and the user does not have any control over them.
Overall, we can say that this is mostly an approximate calculation which depends on the underlying OS. One approximation could be to record the end instruction timestamp of a process and start timestamp of a process and waiting time in queue.
If the total timeof execution of all the processes was T, then the context switch time = T – (SUM for all processes (waiting time + execution time)).
Solutions to Chapter 18 | Threads and Locks
259 Cracking the Coding Interview | Knowledge Based
18.3 Implement a singleton design pattern as a template such that, for any given class Foo, you can call Singleton::instance() and get a pointer to an instance of a singleton of type Foo. Assume the existence of a class Lock which has acquire() and release() methods. How could you make your implementation thread safe and exception safe?
pg 86
SOLUTION
1 using namespace std;
2 /* Place holder for thread synchronization lock */
3 class Lock {
4 public:
5 Lock() { /* placeholder code to create the lock */ }
6 ~Lock() { /* placeholder code to deallocate the lock */ }
7 void AcquireLock() { /* placeholder to acquire the lock */ }
8 void ReleaseLock() { /* placeholder to release the lock */ }
9 };
10
11 /* Singleton class with a method that creates a new instance of the
12 * class of the type of the passed in template if it does not
13 * already exist. */
14 template <class T> class Singleton {
15 private:
16 static Lock lock;
17 static T* object;
18 protected:
19 Singleton() { };
20 public:
21 static T * instance();
22 };
23 Lock Singleton::lock;
24
25 T * Singleton::Instance() {
26 /* if object is not initialized, acquire lock */
27 if (object == 0) {
28 lock.AcquireLock();
29 /* If two threads simultaneously check and pass the first “if”
30 * condition, then only the one who acquired the lock first
31 * should create the instance */
32 if (object == 0) {
33 object = new T;
34 }
35 lock.ReleaseLock();
36 }
37 return object;
38 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 260
39
40 int main() {
41 /* foo is any class defined for which we want singleton access */
42 Foo* singleton_foo = Singleton<Foo>::Instance();
43 return 0;
44 }
The general method to make a program thread safe is to lock shared resources whenever write permission is given. This way, if one thread is modifying the resource, other threads can not modify it.
Solutions to Chapter 18 | Threads and Locks
261 Cracking the Coding Interview | Knowledge Based
18.4 Design a class which provides a lock only if there are no possible deadlocks.
pg 86
SOLUTION
For our solution, we implement a wait / die deadlock prevention scheme.
1 class MyThread extends Thread {
2 long time;
3 ArrayList<Resource> res = new ArrayList<Resource>();
4 public ArrayList<Resource> getRes() { return res; }
5
6 public void run() {
7 /* Run infinitely */
8 time = System.currentTimeMillis();
9 int count = 0;
10 while (true) {
11 if (count < 4) {
12 if (Question.canAcquireResource(this,
13 Question.r[count])) {
14 res.add(Question.r[count]);
15 count++;
16 System.out.println(“Resource: [“ +
17 Question.r[count - 1].getId() + “] acquired by
18 thread: [“ + this.getName() + “]”);
19 try {
20 sleep(1000);
21 } catch (InterruptedException e) {
22 e.printStackTrace();
23 }
24 }
25 }
26 else {
27 this.stop();
28 }
29 }
30 }
31
32 public long getTime() { return time; }
33 public void setRes(ArrayList<Resource> res) { this.res = res; }
34 MyThread(String name) {
35 super(name);
36 }
37 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 262
18.5 Suppose we have the following code:
class Foo {
public:
A
(.....); /* If A is called, a new thread will be created and
* the corresponding function will be executed. */
B(.....); /* same as above */
C(.....); /* same as above */
}
Foo f;
f.A(.....);
f.B(.....);
f.C(.....);
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
iii) Suppose we have the following code to use class Foo. We do not know how the threads will be scheduled in the OS.
Foo f;
f.A(.....); f.B(.....); f.C(.....);
f.A(.....); f.B(.....); f.C(.....);
Can you design a mechanism to make sure that all the methods will be executed in sequence?
pg 86
SOLUTION
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
1 Semaphore s_a(0);
2 Semaphore s_b(0);
3 A {
4 /***/
5 s_a.release(1);
6 }
7 B {
8 s_a.acquire(1);
9 /****/
10 s_b.release(1);
11 }
12 C {
13 s_b.acquire(1);
14 /******/
15 }
ii) Can you design a mechanism to make sure that all the methods will be executed in sequence?
1 Semaphore s_a(0);
Solutions to Chapter 18 | Threads and Locks
263 Cracking the Coding Interview | Knowledge Based
2 Semaphore s_b(0);
3 Semaphore s_c(1);
4 A {
5 s_c.acquire(1);
6 /***/
7 s_a.release(1);
8 }
9 B {
10 s_a.acquire(1);
11 /****/
12 s_b.release(1);
13 }
14 C {
15 s_b.acquire(1);
16 /******/
17 s_c.release(1);
18 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 264
18.6 You are given a class with synchronized method A, and a normal method C. If you have two threads in one instance of a program, can they call A at the same time? Can they call A and C at the same time?
pg 86
SOLUTION
Java provides two ways to achieve synchronization: synchronized method and synchronized statement.
Synchronized method: Methods of a class which need to be synchronized are declared with “synchronized” keyword. If one thread is executing a synchronized method, all other threads which want to execute any of the synchronized methods on the same objects get blocked.
Syntax: method1 and method2 need to be synchronized
1 public class SynchronizedMethod {
2 // Variables declaration
3 public synchronized returntype Method1() {
4 // Statements
5 }
6 public synchronized returntype method2() {
7 // Statements
8 }
9 // Other methods
10 }
Synchronized statement: It provides the synchronization for a group of statements rather than a method as a whole. It needs to provide the object on which these synchronized statements will be applied, unlike in a synchronized method.
Syntax: synchronized statements on “this” object
1 synchronized(this) {
2 /* statement 1
3 * ...
4 * statement N */
5 }
i) If you have two threads in one instance of a program, can they call A at the same time?
Not possible; read the above paragraph.
ii) Can they call A and C at the same time?
Yes. Only methods of the same object which are declared with the keyword synchronized can’t be interleaved.
Solutions to Chapter 19 | Moderate
Cracking the Coding Interview | Additional Review Problems
265
19.1 Write a function to swap a number in place without temporary variables.
pg 89
SOLUTION
This is a classic interview problem. If you haven’t heard this problem before, you can approach it by taking the difference between a and b:
1 public static void swap(int a, int b) {
2 a = b - a; // 9 - 5 = 4
3 b = b - a; // 9 - 4 = 5
4 a = a + b; // 4 + 5 = 9
5
6 System.out.println(“a: “ + a);
7 System.out.println(“b: “ + b);
8 }
You can then optimize it as follows:
1 public static void swap_opt(int a, int b) {
2 a = a^b;
3 b = a^b;
4 a = a^b;
5
6 System.out.println(“a: “ + a);
7 System.out.println(“b: “ + b);
8 }
Solutions to Chapter 19 | Moderate
266
CareerCup.com
19.2 Design an algorithm to figure out if someone has won in a game of tic-tac-toe.
pg 89
SOLUTION
The first thing to ask your interviewer is whether the hasWon function will be called just once, or multiple times. If it will be called multiple times, you can get a very fast algorithm by amortizing the cost (especially if you can design your own data storage system for the tic-tac-toe board).
Approach #1: If hasWon is called many times
There are only 3^9, or about twenty thousand tic-tac-toe boards. We can thus represent our tic-tac-toe board as an int, with each digit representing a piece (0 means Empty, 1 means Red, 2 means Blue). We set up a hashtable or array in advance with all possible boards as keys, and the values are 0, 1, and 2. Our function then is simply this:
int hasWon(int board) {
return winnerHashtable[board];
}
Easy!
Approach #2: If hasWon is only called once
1 enum Piece { Empty, Red, Blue };
2 enum Check { Row, Column, Diagonal, ReverseDiagonal }
3
4 Piece getIthColor(Piece[][] board, int index, int var, Check check) {
5 if (check == Check.Row) return board[index][var];
6 else if (check == Check.Column) return board[var][index];
7 else if (check == Check.Diagonal) return board[var][var];
8 else if (check == Check.ReverseDiagonal)
9 return board[board.length - 1 - var][var];
10 return Piece.Empty;
11 }
12
13 Piece getWinner(Piece[][] board, int fixed_index, Check check) {
14 Piece color = getIthColor(board, fixed_index, 0, check);
15 if (color == Piece.Empty) return Piece.Empty;
16 for (int var = 1; var < board.length; var++) {
17 if (color != getIthColor(board, fixed_index, var, check)) {
18 return Piece.Empty;
19 }
20 }
21 return color;
22 }
23
Solutions to Chapter 19 | Moderate
267 Cracking the Coding Interview | Additional Review Problems
24 Piece hasWon(Piece[][] board) {
25 int N = board.length;
26 Piece winner = Piece.Empty;
27
28 // Check rows and columns
29 for (int i = 0; i < N; i++) {
30 winner = getWinner(board, i, Check.Row);
31 if (winner != Piece.Empty) {
32 return winner;
33 }
34
35 winner = getWinner(board, i, Check.Column);
36 if (winner != Piece.Empty) {
37 return winner;
38 }
39 }
40
41 winner = getWinner(board, -1, Check.Diagonal);
42 if (winner != Piece.Empty) {
43 return winner;
44 }
45
46 // Check diagonal
47 winner = getWinner(board, -1, Check.ReverseDiagonal);
48 if (winner != Piece.Empty) {
49 return winner;
50 }
51
52 return Piece.Empty;
53 }
SUGGESTIONS AND OBSERVATIONS:
»»Note that the runtime could be reduced to O(N) with the addition of row and column count arrays (and two sums for the diagonals)
»»A common follow up (or tweak) to this question is to write this code for an NxN board.
Solutions to Chapter 19 | Moderate
. CareerCup com 268
19.3 Write an algorithm which computes the number of trailing zeros in n factorial.
pg 89
SOLUTION
Trailing zeros are contributed by pairs of 5 and 2, because 5*2 = 10. To count the number of pairs, we just have to count the number of multiples of 5. Note that while 5 contributes to one multiple of 10, 25 contributes two (because 25 = 5*5).
1 public static int numZeros(int num) {
2 int count = 0;
3 if (num < 0) {
4 System.out.println(“Factorial is not defined for < 0”);
5 return 0;
6 }
7 for (int i = 5; num / i > 0; i *= 5) {
8 count += num / i;
9 }
10 return count;
11 }
Let’s walk through an example to see how this works: Suppose num = 26. In the first loop, we count how many multiples of five there are by doing 26 / 5 = 5 (these multiples are 5, 10, 15, 20, and 25). In the next loop, we count how many multiples of 25 there are: 26 / 25 = 1 (this multiple is 25). Thus, we see that we get one zero from 5, 10, 15 and 20, and two zeros from 25 (note how it was counted twice in the loops). Therefore, 26! has six zeros.
OBSERVATIONS AND SUGGESTIONS:
»»This is a bit of a brain teaser, but it can be approached logically (as shown above). By thinking through what exactly will contribute a zero, and what doesn’t matter, you can come up with a solution. Again, be very clear in your rules up front so that you can implement this correctly.
Solutions to Chapter 19 | Moderate
269 Cracking the Coding Interview | Additional Review Problems
19.4 Write a method which finds the maximum of two numbers. You should not use if-else or any other comparison operator.
EXAMPLE
Input: 5, 10
Output: 10
pg 89
SOLUTION
Let’s try to solve this by “re-wording” the problem. We will re-word the problem until we get something that has removed all if statements.
Rewording 1: If a > b, return a; else, return b.
Rewording 2: If (a - b) is negative, return b; else, return a.
Rewording 3: If (a - b) is negative, let k = 1; else, let k = 0. Return a - k * (a - b).
Rewording 4: Let c = a - b. Let k = the most significant bit of c. Return a - k * c.
We have now reworded the problem into something that fits the requirements. The code for this is below.
1 int getMax(int a, int b) {
2 int c = a - b;
3 int k = (c >> 31) & 0x1;
4 int max = a - k * c;
5 return max;
6 }
Solutions to Chapter 19 | Moderate
. CareerCup com 270
19.5 The Game of Master Mind is played as follows:
The computer has four slots containing balls that are red (R), yellow (Y), green (G) or blue (B). For example, the computer might have RGGB (e.g., Slot #1 is red, Slots #2 and #3 are green, Slot #4 is blue).
You, the user, are trying to guess the solution. You might, for example, guess YRGB.
When you guess the correct color for the correct slot, you get a “hit”. If you guess a color that exists but is in the wrong slot, you get a “pseudo-hit”. For example, the guess YRGB has 2 hits and one pseudo hit.
For each guess, you are told the number of hits and pseudo-hits.
Write a method that, given a guess and a solution, returns the number of hits and pseudo hits.
pg 89
SOLUTION
This problem is straight-forward. We simply check the number of hits and pseudo-hits. We will store the number of each in a class. To do a quick lookup to see it an element is a pseudo-hit, we will use a bit mask.
1 public static class Result {
2 public int hits;
3 public int pseudoHits;
4 };
5
6 public static Result estimate(String guess, String solution) {
7 Result res = new Result();
8 int solution_mask = 0;
9 for (int i = 0; i < 4; ++i) {
10 solution_mask |= 1 << (1 + solution.charAt(i) - ‘A’);
11 }
12 for (int i = 0; i < 4; ++i) {
13 if (guess.charAt(i) == solution.charAt(i)) {
14 ++res.hits;
15 } else if ((solution_mask &
16 (1 << (1 + guess.charAt(i) - ‘A’))) >= 1) {
17 ++res.pseudoHits;
18 }
19 }
20 return res;
21 }
Solutions to Chapter 19 | Moderate
271 Cracking the Coding Interview | Additional Review Problems
19.6 Given an integer between 0 and 999,999, print an English phrase that describes the integer (eg, “One Thousand, Two Hundred and Thirty Four”).
pg 89
SOLUTION
This is not an especially challenging problem, but it is a long and tedious one. Your interviewer is unlikely to ask to see every detail, but he / she will be interested in how you approach the problem.
1 public static String numtostring(int num) {
2 StringBuilder sb = new StringBuilder();
3
4 // Count number of digits in num.
5 int len = 1;
6 while (Math.pow((double)10, (double)len ) < num) {
7 len++;
8 }
9
10 String[] wordarr1 = {“”,”One ”, “Two ”, “Three ”, “Four ”,
11 “Five ”, “Six ”, “Seven ”, “Eight ”,”Nine ”};
12 String[] wordarr11 = {“”, “Eleven ”, “Twelve ”, “Thirteen ”,
13 “Fourteen ”, “Fifteen ”, “Sixteen ”,
14 “Seventeen ”, “Eighteen ”, “Nineteen ”};
15 String[] wordarr10 = {“”,”Ten ”, “Twenty ”, “Thirty ”, “Forty ”,
16 “Fifty ”, “Sixty ”, “Seventy ”, “Eighty ”,
17 “Ninety “};
18 String[] wordarr100 = {“”, “Hundred ”, “Thousand ”};
19 int tmp;
20 if (num == 0) {
21 sb.append(“Zero”);
22 } else {
23 if (len > 3 && len % 2 == 0) {
24 len++;
25 }
26 do {
27 // Number greater than 999
28 if (len > 3) {
29 tmp = (num / (int)Math.pow((double)10,(double)len-2));
30 // If tmp is 2 digit number and not a multiple of 10
31 if (tmp / 10 == 1 && tmp%10 != 0) {
32 sb.append(wordarr11[tmp % 10]) ;
33 } else {
34 sb.append(wordarr10[tmp / 10]);
35 sb.append(wordarr1[tmp % 10]);
36 }
Solutions to Chapter 19 | Moderate
. CareerCup com 272
37 if (tmp > 0) {
38 sb.append(wordarr100[len / 2]);
39 }
40 num = num % (int)(Math.pow((double)10,(double)len-2));
41 len = len-2;
42 } else { // Number is less than 1000
43 tmp = num / 100;
44 if (tmp != 0) {
45 sb.append(wordarr1[tmp]);
46 sb.append(wordarr100[len / 2]);
47 }
48 tmp = num % 100 ;
49 if(tmp / 10 == 1 && tmp % 10 != 0) {
50 sb.append(wordarr11[tmp % 10]) ;
51 } else {
52 sb.append(wordarr10[tmp / 10]);
53 sb.append(wordarr1[tmp % 10]);
54 }
55 len = 0;
56 }
57 } while(len > 0);
58 }
59 return sb.toString();
60 }
Solutions to Chapter 19 | Moderate
273 Cracking the Coding Interview | Additional Review Problems
19.7 You are given an array of integers (both positive and negative). Find the continuous sequence with the largest sum. Return the sum.
EXAMPLE
Input: {2, -8, 3, -2, 4, -10}
Output: 5 (i.e., {3, -2, 4} )
pg 89
SOLUTION
A simple linear algorithm will work by keeping track of the current subsequence sum. If that sum ever drops below zero, that subsequence will not contribute to the subsequent maximal subsequence since it would reduce it by adding the negative sum.
1 public static int getMaxSum(int[] a) {
2 int maxsum = 0;
3 int sum = 0;
4 for (int i = 0; i < a.length; i++) {
5 sum += a[i];
6 if (maxsum < sum) {
7 maxsum = sum;
8 } else if (sum < 0) {
9 sum = 0;
10 }
11 }
12 return maxsum;
13 }
NOTE: If the array is all negative numbers, what is the correct behavior? Consider this simple array {-3, -10, -5}. You could make a good argument that the maximum sum is either: (A) -3 (if you assume the subsequence can’t be empty) (B) 0 (the subsequence has length 0) or (C) MINIMUM_INT (essentially the error case). We went with option B (max sum = 0), but there’s no “correct” answer. This is a great thing to discuss with your interviewer to show how careful you are.
Solutions to Chapter 19 | Moderate
. CareerCup com 274
19.8 Design a method to find the frequency of occurrences of any given word in a book.
pg 89
SOLUTION
The first question – which you should ask your interviewer – is if you’re just asking for a single word (“single query”) or if you might, eventually, use the same method for many different words (“repetitive queries”)? That is, are you simply asking for the frequency of “dog”, or might you ask for “dog,” and then “cat,” “mouse,” etc?
Solution: Single Query
In this case, we simply go through the book, word by word, and count the number of times that a word appears. This will take O(n) time. We know we can’t do better than that, as we must look at every word in the book.
Solution: Repetitive Queries
In this case, we create a hash table which maps from a word to a frequency. Our code is then like this:
1 Hashtable<String, Integer> setupDictionary(String[] book) {
2 Hashtable<String, Integer> table =
3 new Hashtable<String, Integer>();
4 for (String word : book) {
5 word = word.toLowerCase();
6 if (word.trim() != “”) {
7 if (!table.containsKey(word)) table.put(word, 0);
8 table.put(word, table.get(word) + 1);
9 }
10 }
11 return table;
12 }
13
14 int getFrequency(Hashtable<String, Integer> table, String word) {
15 if (table == null || word == null) return -1;
16 word = word.toLowerCase();
17 if (table.containsKey(word)) {
18 return table.get(word);
19 }
20 return 0;
21 }
Note: a problem like this is relatively easy. Thus, the interviewer is going to be looking heavily at how careful you are. Did you check for error conditions?
Solutions to Chapter 19 | Moderate
275 Cracking the Coding Interview | Additional Review Problems
19.9 Since XML is very verbose, you are given a way of encoding it where each tag gets mapped to a pre-defined integer value. The language/grammar is as follows:
Element --> Element Attr* END Element END [aka, encode the element
t
ag, then its attributes, then tack on an END character, then
e
ncode its children, then another end tag]
Attr --> Tag Value [assume all values are strings]
END --> 01
Tag --> some predefined mapping to int
Value --> string value END
Write code to print the encoded version of an xml element (passed in as string).
FOLLOW UP
Is there anything else you could do to (in many cases) compress this even further?
pg 90
SOLUTION
Part 1: Solution
This solution tokenizes the input and then encodes the items, element by element.
NOTE: See code attachment for full, executable code. We have included an abbreviated section here.
1 private Map<String, Byte> tagMap;
2 private static final Byte[] END = { 0, 1 };
3 private List<String> tokens;
4 private int currentTokenIndex;
5
6 byte[] encode(char[] input) throws IOException {
7 tokenize(input);
8 currentTokenIndex = 0;
9 ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
10 encodeTokens(outputStream);
11 return outputStream.toByteArray();
12 }
13
14 void encodeTokens(ByteArrayOutputStream output) {
15 nextToken(“<”);
16
17 // read tag name
18 String tagName = nextToken();
19 output.write(getTagCode(tagName));
20
21 // read attributes
Solutions to Chapter 19 | Moderate
. CareerCup com 276
22 while (!hasNextToken(“>”) && !hasNextTokens(“/”, “>”)) {
23 // read next attribute
24 String key = nextToken();
25 nextToken(“=”);
26 String value = nextToken();
27 output.write(getTagCode(key));
28 for (char c : value.toCharArray()) {
29 output.write(c);
30 }
31 output.write(END[0]);
32 output.write(END[1]);
33 }
34 // end of attributes
35 output.write(END[0]);
36 output.write(END[1]);
37 // finish this element
38 if (hasNextTokens(“/”, “>”)) {
39 nextToken(“/”);
40 nextToken(“>”);
41 } else {
42 nextToken(“>”);
43 // while not the end tag
44 while (!hasNextTokens(“<”, “/”)) {
45 encodeTokens(output); // encode child
46 }
47 // ending tag
48 nextToken(“<”);
49 nextToken(“/”);
50 nextToken(tagName);
51 nextToken(“>”);
52 }
53 output.write(END[0]);
54 output.write(END[1]);
55 }
Part 2: Is there anything you can do to compress this further?
You can treat the file as a general stream of characters and use any number of compression techniques: Shannon–Fano coding, Huffman coding or Arithmetic coding.
Solutions to Chapter 19 | Moderate
277 Cracking the Coding Interview | Additional Review Problems
19.10 Write a method to generate a random number between 1 and 7, given a method that generates a random number between 1 and 5 (i.e., implement rand7() using rand5()). .
pg 90
SOLUTION
First, observe that we cannot do this in a guaranteed finite amount of time. Why? Let’s see by a parallel example: How would you use rand2() to create rand3()?
Observe that each call of rand2() and the corresponding decision you make can be represented by a decision tree. On each node, you have two branches. You take the left one when rand2() equals 0 (which happens with 1/2 probability). You take the right one when rand2() equals 1 (which happens with 1/2 probability). You continue branching left and right as you continue to call 1/2. When you reach a leaf, you return a result of 1, 2 or 3 (your rand3() results).
»»What’s the probability of taking each branch? 1/2.
»»What’s the probability to reach a particular leaf node? 1/2^j (for some j).
»»What the probability of returning 3 (for example)? We could compute this by summing up the probabilities of reaching each leaf node with value 3. Each of these paths has probability 1/2^j, so we know that the total probability of returning 3 must be a series of terms of reciprocal powers of 2 (e.g., 1/2^x + 1/2^y + 1/2^z + …).
We also know, however, that the probability of returning 3 must be 1/3 (because rand3() should be perfectly random). Can you find a series of reciprocal powers of 2 that sum to 1/3? No, because 3 and 2 are relatively prime.
We can similarly conclude that to solve this problem, we will need to accept a small (infinitesimally small) chance that this process will repeat forever. That’s ok.
So, how do we solve this?
In order to generate a random number between 1 and 7, we just need to uniformly generate a larger range than we are looking for and then repeatedly sample until we get a number that is good for us. We will generate a base 5 number with two places with two calls to the RNG.
public static int rand7() {
while (true) {
i
nt num = 5 * (rand5() - 1) + (rand5() - 1);
i
f (num < 21) return (num % 7 + 1);
}
}
Solutions to Chapter 19 | Moderate
. CareerCup com 278
19.11 Design an algorithm to find all pairs of integers within an array which sum to a specified value.
pg 90
SOLUTION
One easy and (time) efficient solution involves a hash map from integers to integers. This algorithm works by iterating through the array. On each element x, look up sum - x in the hash table and, if it exists, print (x, sum - x). Add x to the hash table, and go to the next element.
Alternate Solution
Definition of Complement: If we’re trying to find a pair of numbers that sums to z, the complement of x will be z - x (that is, the number that can be added to x to make z). For example, if we’re trying to find a pair of numbers that sum to 12, the complement of –5 would be 17.
The Algorithm: Imagine we have the following sorted array: {-2 -1 0 3 5 6 7 9 13 14 }. Let first point to the head of the array and last point to the end of the array. To find the complement of first, we just move last backwards until we find it. If first + last < sum, then there is no complement for first. We can therefore move first forward. We stop when first is greater than last.
Why must this find all complements for first? Because the array is sorted and we’re trying progressively smaller numbers. When the sum of first and last is less than the sum, we know that trying even smaller numbers (as last) won’t help us find a complement.
Why must this find all complements for last? Because all pairs must be made up of a first and a last. We’ve found all complements for first, therefore we’ve found all complements of last.
1 public static void printPairSums(int[] array, int sum) {
2 Arrays.sort(array);
3 int first = 0;
4 int last = array.length - 1;
5 while (first < last) {
6 int s = array[first] + array[last];
7 if (s == sum) {
8 System.out.println(array[first] + “ “ + array[last]);
9 ++first;
10 --last;
11 } else {
12 if (s < sum) ++first;
13 else --last;
14 }
15 }
16 }
Solutions to Chapter 20 | Hard
Cracking the Coding Interview | Additional Review Problems
279
20.1 Write a function that adds two numbers. You should not use + or any arithmetic operators.
pg 91
SOLUTION
To investigate this problem, let’s start off by gaining a deeper understanding of how we add numbers. We’ll work in Base 10 so that it’s easier to see. To add 759 + 674, I would usually add digit[0] from each number, carry the one, add digit[1] from each number, carry the one, etc. You could take the same approach in binary: add each digit, and carry the one as necessary.
Can we make this a little easier? Yes! Imagine I decided to split apart the “addition” and “carry” steps. That is, I do the following:
1. Add 759 + 674, but “forget” to carry. I then get 323.
2. Add 759 + 674 but only do the carrying, rather than the addition of each digit. I then get 1110.
3. Add the result of the first two operations (recursively, using the same process described in step 1 and 2): 1110 + 323 = 1433.
Now, how would we do this in binary?
1. If I add two binary numbers together but forget to carry, bit[i] will be 0 if bit[i] in a and b are both 0 or both 1. This is an XOR.
2. If I add two numbers together but only carry, I will have a 1 in bit[i] if bit[i-1] in a and b are both 1’s. This is an AND, shifted.
3. Now, recurse until there’s nothing to carry.
1 int add_no_arithm(int a, int b) {
2 if (b == 0) return a;
3 int sum = a ^ b; // add without carrying
4 int carry = (a & b) << 1; // carry, but don’t add
5 return add_no_arithm(sum, carry); // recurse
6 }
OBSERVATIONS AND SUGGESTIONS:
The Approach: There are a couple of suggestions for figuring out this problem:
1. Our first instinct in problems like these should be that we’re going to have to work with bits. Why? Because when you take away the + sign, what other choice do we have? Plus, that’s how computers do it.
Solutions to Chapter 20 | Hard
280
CareerCup.com
2. Our next thought in problems like these should be to really, really understand how you add. Walk through an addition problem to see if you can understand something new—some pattern—and then see if you can replicate that with code.
Your interviewer is looking for two things in this problem:
1. Can you break down a problem and solve it?
2. Do you understand how to work with bits?
Solutions to Chapter 20 | Hard
281 Cracking the Coding Interview | Additional Review Problems
20.2 Write a method to shuffle a deck of cards. It must be a perfect shuffle - in other words, each 52! permutations of the deck has to be equally likely. Assume that you are given a random number generator which is perfect.
pg 91
SOLUTION
This is a very well known interview question, and a well known algorithm. If you aren’t one of the lucky few to have already know this algorithm, read on.
Let’s start with a brute force approach: we could randomly selecting items and put them into a new array. We must make sure that we don’t pick the same item twice though by somehow marking the node as dead.
Array: [1] [2] [3] [4] [5]
Randomly select 4: [4] [?] [?] [?] [?]
Mark element as dead: [1] [2] [3] [X] [5]
The tricky part is, how do we mark [4] as dead such that we prevent that element from being picked again? One way to do it is to swap the now-dead [4] with the first element in the array:
Array: [1] [2] [3] [4] [5]
Randomly select 4: [4] [?] [?] [?] [?]
Swap dead element: [X] [2] [3] [1] [5]
Array: [X] [2] [3] [1] [5]
Randomly select 3: [4] [3] [?] [?] [?]
Swap dead element: [X] [X] [2] [1] [5]
By doing it this way, it’s much easier for the algorithm to “know” that the first k elements are dead than that the third, fourth, nineth, etc elements are dead. We can also optimize this by merging the shuffled array and the original array.
Randomly select 4: [4] [2] [3] [1] [5]
Randomly select 3: [4] [3] [2] [1] [5]
This is an easy algorithm to implement iteratively:
1 public static void shuffleArray(int[] cards) {
2 int temp, index;
3 for (int i = 0; i < cards.length; i++){
4 index = (int) (Math.random() * (cards.length - i)) + i;
5 temp = cards[i];
6 cards[i] = cards[index];
7 cards[index] = temp;
8 }
9 }
Solutions to Chapter 20 | Hard
. CareerCup com 282
20.3 Write a method to randomly generate a set of m integers from an array of size n. Each element must have equal probability of being chosen.
pg 91
SOLUTION
Our first instinct on this problem might be to randomly pick elements from the array and put them into our new subset array. But then, what if we pick the same element twice? Ideally, we’d want to somehow “shrink” the array to no longer contain that element. Shrinking is expensive though because of all the shifting required.
Instead of shrinking / shifting, we can swap the element with an element at the beginning of the array and then “remember” that the array now only includes elements j and greater. That is, when we pick subset[0] to be array[k], we replace array[k] with the first element in the array. When we pick subset[1], we consider array[0] to be “dead” and we pick a random element y between 1 and array.size(). We then set subset[1] equal to array[y], and set array[y] equal to array[1]. Elements 0 and 1 are now “dead.” Subset[2] is now chosen from array[2] through array[array.size()], and so on.
1 /* Random number between lower and higher, inclusive */
2 public static int rand(int lower, int higher) {
3 return lower + (int)(Math.random() * (higher - lower + 1));
4 }
5
6 /* pick M elements from original array. Clone original array so that
7 * we don’t destroy the input. */
8 public static int[] pickMRandomly(int[] original, int m) {
9 int[] subset = new int[m];
10 int[] array = original.clone();
11 for (int j = 0; j < m; j++) {
12 int index = rand(j, array.length - 1);
13 subset[j] = array[index];
14 array[index] = array[j]; // array[j] is now “dead”
15 }
16 return subset;
17 }
Solutions to Chapter 20 | Hard
283 Cracking the Coding Interview | Additional Review Problems
20.4 Write a method to count the number of 2s between 0 and n.
pg 91
SOLUTION
Picture a sequence of numbers:
0 1 2 3 4 5 6 7 8 9
10 11 12 13 14 15 16 17 18 19
20 21 22 23 24 25 26 27 28 29
...
110 111 112 113 114 115 116 117 118 119
The last digit will be repeated every 10 numbers, the last two digits will be repeated every 10^2 numbers, the last 3 digits will be repeated every 10^3 numbers, etc.
So, if there are X 2s between 0 and 99, then we know there are 2x twos between 0 and 199. Between 0 and 299, we have 3x twos from the last two digits, and another 100 2s from the first digit.
In other words, we can look at a number like this:
f(513) = 5 * f(99) + f(13) + 100
To break this down individually:
»»The sequence of the last two digits are repeated 5 times, so add 5 * f(99)
»»We need to account for the last two digits in 500 -> 513, so add f(13)
»»We need to account for the first digit being two between 200 -> 299, so add 100
Of course, if n is, say, 279, we’ll need to account for this slightly differently:
f(279) = 2 * f(99) + f(79) + 79 + 1
To break this down individually:
»»The sequence of the last two digits are repeated 2 times, so add 2 * f(99)
»»We need to account for the last two digits in 200 -> 279, so add f(79)
»»We need to account for the first digit being two between 200 -> 279, so add 79 + 1
Recu rsive Code:
1 public static int count2sR(int n) {
2 // Base case
3 if (n == 0) return 0;
4
5 // 513 into 5 * 100 + 13. [Power = 100; First = 5; Remainder = 13]
6 int power = 1;
7 while (10 * power < n) power *= 10;
8 int first = n / power;
9 int remainder = n % power;
Solutions to Chapter 20 | Hard
. CareerCup com 284
10
11 // Counts 2s from first digit
12 int nTwosFirst = 0;
13 if (first > 2) nTwosFirst += power;
14 else if (first == 2) nTwosFirst += remainder + 1;
15
16 // Count 2s from all other digits
17 int nTwosOther = first * count2sR(power - 1) + count2sR(remainder);
18
19 return nTwosFirst + nTwosOther;
20 }
We can also implement this algorithm iteratively:
1 public static int count2sI(int num) {
2 int countof2s = 0, digit = 0;
3 int j = num, seendigits=0, position=0, pow10_pos = 1;
4 /* maintaining this value instead of calling pow() is an 6x perf
5 * gain (48s -> 8s) pow10_posMinus1. maintaining this value
6 * instead of calling Numof2s is an 2x perf gain (8s -> 4s).
7 * overall > 10x speedup */
8 while (j > 0) {
9 digit = j % 10;
10 int pow10_posMinus1 = pow10_pos / 10;
11 countof2s += digit * position * pow10_posMinus1;
12 /* we do this if digit <, >, or = 2
13 * Digit < 2 implies there are no 2s contributed by this
14 * digit.
15 * Digit == 2 implies there are 2 * numof2s contributed by
16 * the previous position + num of 2s contributed by the
17 * presence of this 2 */
18 if (digit == 2) {
19 countof2s += seendigits + 1;
20 }
21 /* Digit > 2 implies there are digit * num of 2s by the prev.
22 * position + 10^position */
23 else if(digit > 2) {
24 countof2s += pow10_pos;
25 }
26 seendigits = seendigits + pow10_pos * digit;
27 pow10_pos *= 10;
28 position++;
29 j = j / 10;
30 }
31 return(countof2s);
32 }
Solutions to Chapter 20 | Hard
285 Cracking the Coding Interview | Additional Review Problems
20.5 You have a large text file containing words. Given any two words, find the shortest distance (in terms of number of words) between them in the file. Can you make the searching operation in O(1) time? What about the space complexity for your solution?
pg 91
SOLUTION
We will assume for this question that the word order does not matter. This is a question you should ask your interviewer. If the word order does matter, we can make the small modification shown in the code below.
To solve this problem, simply traverse the file and for every occurrence of word1 and word2, compare difference of positions and update the current minimum.
1 int shortest(String[] words, String word1, String word2) {
2 int pos = 0;
3 int min = Integer.MAX_VALUE / 2;
4 int word1_pos = -min;
5 int word2_pos = -min;
6 for (int i = 0; i < words.length; i++) {
7 String current_word = words[i];
8 if (current_word.equals(word1)) {
9 word1_pos = pos;
10 // Comment following 3 lines if word order matters
11 int distance = word1_pos - word2_pos;
12 if (min > distance)
13 min = distance;
14 } else if (current_word.equals(word2)) {
15 word2_pos = pos;
16 int distance = word2_pos - word1_pos;
17 if (min > distance) min = distance;
18 }
19 ++pos;
20 }
21 return min;
22 }
To solve this problem in less time (but more space), we can create a hash table with each word and the locations where it occurs. We then just need to find the minimum (arithmetic) difference in the locations (e.g., abs(word0.loc[1] - word1.loc[5])).
To find the minimum arithmetic difference, we take each location for word1 (e.g.: 0, 3} and do a modified binary search for it in word2’s location list, returning the closest number. Our search for 3, for example, in {2, 7, 9} would return 1. The minimum of all these binary searches is the shortest distance.
Solutions to Chapter 20 | Hard
. CareerCup com 286
20.6 Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. Assume that the computer memory can hold all one billion numbers.
pg 91
SOLUTION
Approach 1: Sorting
Sort the elements and then take the first million numbers from that. Complexity is O(n log n).
Approach 2: Max Heap
1. Create a Min Heap with the first million numbers.
2. For each remaining number, insert it in the Min Heap and then delete the minimum value from the heap.
3. The heap now contains the largest million numbers.
4. This algorithm is O(n log m), where m is the number of values we are looking for.
Approach 3: Selection Rank Algorithm (if you can modify the original array)
Selection Rank is a well known algorithm in computer science to find the ith smallest (or largest) element in an array in expected linear time. The basic algorithm for finding the ith smallest elements goes like this:
»»Pick a random element in the array and use it as a ‘pivot’. Move all elements smaller than that element to one side of the array, and all elements larger to the other side.
»»If there are exactly i elements on the right, then you just find the smallest element on that side.
»»Otherwise, if the right side is bigger than i, repeat the algorithm on the right. If the right side is smaller than i, repeat the algorithm on the left for i – right.size().
Given this algorithm, you can either:
»»Tweak it to use the existing partitions to find the largest i elements (where i = one million).
»»Or, once you find the ith largest element, run through the array again to return all elements greater than or equal to it.
This algorithm has expected O(n) time.
Solutions to Chapter 20 | Hard
287 Cracking the Coding Interview | Additional Review Problems
20.7 Write a program to find the longest word made of other words.
pg 91
SOLUTION
The solution below does the following:
1. Sort the array by size, putting the longest word at the front
2. For each word, split it in all possible ways. That is, for “test”, split it into {“t”, “est”}, {“te”, “st”} and {“tes”, “t”}.
3. Then, for each pairing, check if the first half and the second both exist elsewhere in the array.
4. “Short circuit” by returning the first string we find that fits condition #3.
What is the time complexity of this?
»»Time to sort array: O(n log n)
»»Time to check if first / second half of word exists: O(d) per word, where d is the average length of a word.
»»Total complexity: O(n log n + n * d). Note that d is fixed (probably around 5—10 characters). Thus, we can guess that for short arrays, the time is estimated by O(n * d) , which also equals O(number of characters in the array). For longer arrays, the time will be better estimated by O(n log n).
»»Space complexity: O(n).
Optimizations: If we didn’t want to use additional space, we could cut out the hash table. This would mean:
»»Sorting the array in alphabetical order
»»Rather than looking up the word in a hash table, we would use binary search in the array
»»We would no longer be able to short circuit.
1 class LengthComparator implements Comparator<String> {
2 @Override
3 public int compare(String o1, String o2) {
4 if (o1.length() < o2.length()) return 1;
5 if (o1.length() > o2.length()) return -1;
6 return 0;
7 }
8 }
Solutions to Chapter 20 | Hard
. CareerCup com 288
20.8 Given a string s and an array of smaller strings T, design a method to search s for each small string in T.
pg 91
SOLUTION
First, create a suffix tree for s. For example, if your word were bibs, you would create the following tree:
Then, all you need to do is search for each string in T in the suffix tree. Note that if “B” were a word, you would come up with two locations.
1 public class SuffixTree {
2 SuffixTreeNode root = new SuffixTreeNode();
3 public SuffixTree(String s) {
4 for (int i = 0; i < s.length(); i++) {
5 String suffix = s.substring(i);
6 root.insertString(suffix, i);
7 }
8 }
9
10 public ArrayList<Integer> getIndexes(String s) {
11 return root.getIndexes(s);
12 }
13 }
14
15 public class SuffixTreeNode {
16 HashMap<Character, SuffixTreeNode> children = new
17 HashMap<Character, SuffixTreeNode>();
18 char value;
19 ArrayList<Integer> indexes = new ArrayList<Integer>();
S
B
I
B
S
S
B
I
S
Solutions to Chapter 20 | Hard
289 Cracking the Coding Interview | Additional Review Problems
20 public SuffixTreeNode() { }
21
22 public void insertString(String s, int index) {
23 indexes.add(index);
24 if (s != null && s.length() > 0) {
25 value = s.charAt(0);
26 SuffixTreeNode child = null;
27 if (children.containsKey(value)) {
28 child = children.get(value);
29 } else {
30 child = new SuffixTreeNode();
31 children.put(value, child);
32 }
33 String remainder = s.substring(1);
34 child.insertString(remainder, index);
35 }
36 }
37
38 public ArrayList<Integer> getIndexes(String s) {
39 if (s == null || s.length() == 0) {
40 return indexes;
41 } else {
42 char first = s.charAt(0);
43 if (children.containsKey(first)) {
44 String remainder = s.substring(1);
45 return children.get(first).getIndexes(remainder);
46 }
47 }
48 return null;
49 }
50 }
51
52 public class Question {
53 public static void main(String[] args) {
54 String testString = “mississippi”;
55 String[] stringList = {“is”, “sip”, “hi”, “sis”};
56 SuffixTree tree = new SuffixTree(testString);
57 for (String s : stringList) {
58 ArrayList<Integer> list = tree.getIndexes(s);
59 if (list != null) {
60 System.out.println(s + “: “ + list.toString());
61 }
62 }
63 }
64 }
Solutions to Chapter 20 | Hard
. CareerCup com 290
20.9 Numbers are randomly generated and passed to a method. Write a program to find and maintain the median value as new values are generated.
pg 91
SOLUTIONS
One solution is to use two priority heaps: a max heap for the values below the median, and a min heap for the values above the median. The median will be largest value of the max heap. When a new value arrives it is placed in the below heap if the value is less than or equal to the median, otherwise it is placed into the above heap. The heap sizes can be equal or the below heap has one extra. This constraint can easily be restored by shifting an element from one heap to the other. The median is available in constant time, so updates are O(lg n).
1 private Comparator<Integer> maxHeapComparator, minHeapComparator;
2 private PriorityQueue<Integer> maxHeap, minHeap;
3 public void addNewNumber(int randomNumber) {
4 if (maxHeap.size() == minHeap.size()) {
5 if ((minHeap.peek() != null) &&
6 randomNumber > minHeap.peek()) {
7 maxHeap.offer(minHeap.poll());
8 minHeap.offer(randomNumber);
9 } else {
10 maxHeap.offer(randomNumber);
11 }
12 }
13 else {
14 if(randomNumber < maxHeap.peek()){
15 minHeap.offer(maxHeap.poll());
16 maxHeap.offer(randomNumber);
17 }
18 else {
19 minHeap.offer(randomNumber);
20 }
21 }
22 }
23 public static double getMedian() {
24 if (maxHeap.isEmpty()) return minHeap.peek();
25 else if (minHeap.isEmpty()) return maxHeap.peek();
26 if (maxHeap.size() == minHeap.size()) {
27 return (minHeap.peek() + maxHeap.peek()) / 2;
28 } else if (maxHeap.size() > minHeap.size()) {
29 return maxHeap.peek();
30 } else {
31 return minHeap.peek();
32 }
33 }
Solutions to Chapter 20 | Hard
291 Cracking the Coding Interview | Additional Review Problems
20.10 Given two words of equal length that are in a dictionary, write a method to transform one word into another word by changing only one letter at a time. The new word you get in each step must be in the dictionary.
EXAMPLE:
Input: DAMP, LIKE
Output: DAMP -> LAMP -> LIMP -> LIME -> LIKE
pg 91
SOLUTION
Though this problem seems tough, it’s actually a straightforward modification of breadth-first-search. Each word in our “graph” branches to all words in the dictionary that are one edit away. The interesting part is how to implement this—should we build a graph as we go? We could, but there’s an easier way. We can instead use a “backtrack map.” In this backtrack map, if B[v] = w, then you know that you edited v to get w. When we reach our end word, we can use this backtrack map repeatedly to reverse our path. See the code below:
1 LinkedList<String> transform(String startWord, String stopWord,
2 Set<String> dictionary) {
3 startWord = startWord.toUpperCase();
4 stopWord = stopWord.toUpperCase();
5 Queue<String> actionQueue = new LinkedList<String>();
6 Set<String> visitedSet = new HashSet<String>();
7 Map<String, String> backtrackMap = new TreeMap<String, String>();
8
9 actionQueue.add(startWord);
10 visitedSet.add(startWord);
11
12 while (!actionQueue.isEmpty()) {
13 String w = actionQueue.poll();
14 // For each possible word v from w with one edit operation
15 for (String v : getOneEditWords(w)) {
16 if (v.equals(stopWord)) {
17 // Found our word! Now, back track.
18 LinkedList<String> list = new LinkedList<String>();
19 // Append v to list
20 list.add(v);
21 while (w != null) {
22 list.add(0, w);
23 w = backtrackMap.get(w);
24 }
25 return list;
26 }
27 // If v is a dictionary word
Solutions to Chapter 20 | Hard
. CareerCup com 292
28 if (dictionary.contains(v)) {
29 if (!visitedSet.contains(v)) {
30 actionQueue.add(v);
31 visitedSet.add(v); // mark visited
32 backtrackMap.put(v, w);
33 }
34 }
35 }
36 }
37 return null;
38 }
39
40 Set<String> getOneEditWords(String word) {
41 Set<String> words = new TreeSet<String>();
42 for (int i = 0; i < word.length(); i++) {
43 char[] wordArray = word.toCharArray();
44 // change that letter to something else
45 for (char c = ‘A’; c <= ‘Z’; c++) {
46 if (c != word.charAt(i)) {
47 wordArray[i] = c;
48 words.add(new String(wordArray));
49 }
50 }
51 }
52 return words;
53 }
Let n be the length of the start word and m be the number of like sized words in the dictionary. The runtime of this algorithm is O(n*m) since the while loop will dequeue at most m unique words. The for loop is O(n) as it walks down the string applying a fixed number of replacements for each character.
Solutions to Chapter 20 | Hard
293 Cracking the Coding Interview | Additional Review Problems
20.11 Imagine you have a square matrix, where each cell is filled with either black or white. Design an algorithm to find the maximum subsquare such that all four borders are filled with black pixels.
pg 92
SOLUTION
Assumption: Square is of size NxN.
This algorithm does the following:
1. Iterate through every (full) column from left to right.
2. At each (full) column (call this currentColumn), look at the subcolumns (from biggest to smallest).
3. At each subcolumn, see if you can form a square with the subcolumn as the left side. If so, update currentMaxSize and go to the next (full) column.
4. If N - currentColumn <= currentMaxSize, then break completely. We’ve found the largest square possible. Why? At each column, we’re trying to create a square with that column as the left side. The largest such square we could possibly create is N - currentColumn. Thus, if N-currentColumn <= currentMaxSize, then we have no need to proceed.
Time complexity: O(N^2).
1 public static Subsquare findSquare(int[][] matrix){
2 assert(matrix.length > 0);
3 for (int row = 0; row < matrix.length; row++){
4 assert(matrix[row].length == matrix.length);
5 }
6
7 int N = matrix.length;
8
9 int currentMaxSize = 0;
10 Subsquare sq = null;
11 int col = 0;
12
13 // Iterate through each column from left to right
14 while (N - col > currentMaxSize) { // See step 4 above
15 for (int row = 0; row < matrix.length; row++){
16 // starting from the biggest
17 int size = N - Math.max(row, col);
18 while (size > currentMaxSize){
19 if (isSquare(matrix, row, col, size)){
20 currentMaxSize = size;
21 sq = new Subsquare(row, col, size);
22 break; // go to next (full) column
Solutions to Chapter 20 | Hard
. CareerCup com 294
23 }
24 size--;
25 }
26 }
27 col++;
28 }
29 return sq;
30 }
31
32 private static boolean isSquare(int[][] matrix, int row, int col,
33 int size) {
34 // Check top and bottom border.
35 for (int j = 0; j < size; j++){
36 if (matrix[row][col+j] == 1) {
37 return false;
38 }
39 if (matrix[row+size-1][col+j] == 1){
40 return false;
41 }
42 }
43
44 // Check left and right border.
45 for (int i = 1; i < size - 1; i++){
46 if (matrix[row+i][col] == 1){
47 return false;
48 }
49 if (matrix[row+i][col+size-1] == 1){
50 return false;
51 }
52 }
53 return true;
54 }
55
56 public class Subsquare {
57 public int row, column, size;
58 public Subsquare(int r, int c, int sz) {
59 row = r;
60 column = c;
61 size = sz;
62 }
63 }
Solutions to Chapter 20 | Hard
295 Cracking the Coding Interview | Additional Review Problems
20.12 Given an NxN matrix of positive and negative integers, write code to find the sub-matrix with the largest possible sum.
pg 92
SOLUTION
Brute Force: Complexity O(N^6)
Like many “maximizing” problems, this problem has a straight forward brute force solution. The brute force solution simply iterates through all possible sub-matrixes, computes the sum, and finds the biggest.
To iterate through all possible sub-matrixes (with no duplicates), we simply need to iterate through all order pairings of rows, and then all ordered pairings of columns.
This solution is O(N^6), since we iterate through O(N^4) sub-matrixes, and it takes O(N^2) time to compute the area of each.
Optimized Solution: O(N^4)
Notice that the earlier solution is made slower by a factor of O(N^2) simply because computing the sum of a matrix is so slow. Can we reduce the time to compute the area? Yes! In fact, we can reduce the time of computeSum to O(1).
Consider the following:
If we had the sum of the smaller rectangle (the one including A, B, C, D), and we could compute the sum of D as follows: area(D) = area(A through D) - area(A) - area(B) - area(C).
What if, instead, we had the following:
with the following values (notice that each Val_* starts at the origin):
x1
x2
y1
y2
A
B
C
D
A
B
C
D
Solutions to Chapter 20 | Hard
. CareerCup com 296
Val_D = area(point(0, 0) -> point(x2, y2))
Val_C = area(point(0, 0) -> point(x2, y1))
Val_B = area(point(0, 0) -> point(x1, y2))
Val_A = area(point(0, 0) -> point(x1, y1))
With these values, we know the following:
area(D) = Val_D - area(A union C) - area(A union B) + area(A).
Or, written another way:
area(D) = Val_D - Val_B - Val_C + Val_A
Can we efficiently compute these Val_* values for all points in the matrix? Yes, by using similar logic:
Val_(x, y) = Val(x - 1, y) + Val(y - 1, x) - Val(x - 1, y - 1)
We can precompute all such values, and then efficiently find the maximum submatrix. See the following code for this implementation
1 public static int getMaxMatrix(int[][] original) {
2 int maxArea = Integer.MIN_VALUE; // Important! Max could be < 0
3 int rowCount = original.length;
4 int columnCount = original[0].length;
5 int[][] matrix = precomputeMatrix(original);
6 for (int row1 = 0; row1 < rowCount; row1++) {
7 for (int row2 = row1; row2 < rowCount; row2++) {
8 for (int col1 = 0; col1 < columnCount; col1++) {
9 for (int col2 = col1; col2 < columnCount; col2++) {
10 maxArea = Math.max(maxArea, computeSum(matrix,
11 row1, row2, col1, col2));
12 }
13 }
14 }
15 }
16 return maxArea;
17 }
18
19 private static int[][] precomputeMatrix(int[][] matrix) {
20 int[][] sumMatrix = new int[matrix.length][matrix[0].length];
21 for (int i = 0; i < matrix.length; i++) {
22 for (int j = 0; j < matrix.length; j++) {
23 if (i == 0 && j == 0) { // first cell
24 sumMatrix[i][j] = matrix[i][j];
25 } else if (j == 0) { // cell in first column
26 sumMatrix[i][j] = sumMatrix[i - 1][j] + matrix[i][j];
27 } else if (i == 0) { // cell in first row
28 sumMatrix[i][j] = sumMatrix[i][j - 1] + matrix[i][j];
29 } else {
30 sumMatrix[i][j] = sumMatrix[i - 1][j] +
31 sumMatrix[i][j - 1] - sumMatrix[i - 1][j - 1] +
Solutions to Chapter 20 | Hard
297 Cracking the Coding Interview | Additional Review Problems
32 matrix[i][j];
33 }
34 }
35 }
36 return sumMatrix;
37 }
38
39 private static int computeSum(int[][] sumMatrix, int i1, int i2,
40 int j1, int j2) {
41 if (i1 == 0 && j1 == 0) { // starts at row 0, column 0
42 return sumMatrix[i2][j2];
43 } else if (i1 == 0) { // start at row 0
44 return sumMatrix[i2][j2] - sumMatrix[i2][j1 - 1];
45 } else if (j1 == 0) { // start at column 0
46 return sumMatrix[i2][j2] - sumMatrix[i1 - 1][j2];
47 } else {
48 return sumMatrix[i2][j2] - sumMatrix[i2][j1 - 1]
49 - sumMatrix[i1 - 1][j2] + sumMatrix[i1 - 1][j1 - 1];
50 }
51 }
Solutions to Chapter 20 | Hard
. CareerCup com 298
20.13 Given a dictionary of millions of words, give an algorithm to find the largest possible rectangle of letters such that every row forms a word (reading left to right) and every column forms a word (reading top to bottom).
pg 92
SOLUTION
Many problems involving a dictionary can be solved by doing some preprocessing. Where can we do preprocessing?
Well, if we’re going to create a rectangle of words, we know that each row must be the same length and each column must have the same length. So, let’s group the words of the dictionary based on their sizes. Let’s call this grouping D, where D[i] provides a list of words of length i.
Next, observe that we’re looking for the largest rectangle. What is the absolute largest rectangle that could be formed? It’s (length of largest word) * (length of largest word).
1 int max_rectangle = longest_word * longest_word;
2 for z = max_rectangle to 1 {
3 for each pair of numbers (i, j) where i*j = z {
4 /* attempt to make rectangle. return if successful. */
5 }
6 }
By iterating in this order, we ensure that the first rectangle we find will be the largest.
Now, for the hard part: make_rectangle. Our approach is to rearrange words in list1 into rows and check if the columns are valid words in list2. However, instead of creating, say, a particular 10x20 rectangle, we check if the columns created after inserting the first two words are even valid pre-fixes. A trie becomes handy here.
1 WordGroup[] groupList = WordGroup.createWordGroups(list);
2 private int maxWordLength = groupList.length;
3 private Trie trieList[] = new Trie[maxWordLength];
4
5 public Rectangle maxRectangle() {
6 int maxSize = maxWordLength * maxWordLength;
7 for (int z = maxSize; z > 0; z--) {
8 for (int i = 1; i <= maxWordLength; i ++ ) {
9 if (z % i == 0) {
10 int j = z / i;
11 if (j <= maxWordLength) {
12 Rectangle rectangle = makeRectangle(i,j);
13 if (rectangle != null) {
14 return rectangle;
15 }
16 }
Solutions to Chapter 20 | Hard
299 Cracking the Coding Interview | Additional Review Problems
17 }
18 }
19 }
20 return null;
21 }
22
23 private Rectangle makeRectangle(int length, int height) {
24 if (groupList[length - 1] == null ||
25 groupList[height - 1] == null) {
26 return null;
27 }
28 if (trieList[height - 1] == null) {
29 LinkedList<String> words = groupList[height - 1].getWords();
30 trieList[height - 1] = new Trie(words);
31 }
32 return makePartialRectangle(length, height,
33 new Rectangle(length));
34 }
35
36 private Rectangle makePartialRectangle(int l, int h,
37 Rectangle rectangle) {
38 if (rectangle.height == h) { // Check if complete rectangle
39 if (rectangle.isComplete(l, h, groupList[h - 1])) {
40 return rectangle;
41 } else {
42 return null;
43 }
44 }
45
46 // Compare columns to trie to see if potentially valid rect */
47 if (!rectangle.isPartialOK(l, trieList[h - 1])) return null;
48
49 for (int i = 0; i < groupList[l-1].length(); i++) {
50 Rectangle org_plus =
51 rectangle.append(groupList[l-1].getWord(i));
52 Rectangle rect = makePartialRectangle(l, h, org_plus);
53 if (rect != null) {
54 return rect;
55 }
56 }
57 return null;
58 }
NOTE: See code attachment for full code.

Cracking the Coding Interview
301
Index
A
arithmetic 108, 131, 143, 190, 265, 269, 271, 273, 278, 279, 283, 295
arraylists 126, 142, 152, 170, 171, 173, 185, 200, 261, 288
arrays 100, 102, 111, 179, 273, 278, 281, 282, 286
B
big-O 95, 97, 102, 113, 121, 130, 131, 141, 142, 172, 173, 181, 182, 193, 207, 216, 267, 274, 286, 287, 290, 293, 295
bit manipulation 95, 133, 134, 135, 138, 140, 141, 172, 202, 254, 265, 269, 270, 279
bit vectors 95, 142, 202, 205
breadth first search 124, 126, 199, 206, 291
C
C++ 166, 167, 215, 216, 217, 218, 219, 220, 221, 223, 224, 228, 241, 242, 247, 248, 259
combinatorics 170, 171, 266
D
databases 197, 208, 231, 232, 234
G
graphs 124, 199, 206, 291
H
hash tables 95, 98, 99, 105, 193, 200, 216, 230, 266, 270, 274, 275, 285, 287, 288, 291
heaps 286, 290
J
Java 225, 226, 227, 228, 229, 230, 264
L
lines 189, 192, 193
linked lists 105, 106, 107, 108, 109, 124, 126, 152, 196, 223, 291, 299
302
CareerCup.com
Index
M
matrixes 101, 102, 163, 169, 184, 293, 295
maximize and minimize 125, 148, 273, 293, 295, 298
O
object oriented design 113, 115, 118, 120, 124, 151, 152, 154, 156, 157, 159, 161, 163, 166, 167, 175, 189, 192, 199, 200, 217, 218, 221, 225, 259, 261, 266, 270, 288, 298
P
probability and randomness 187, 188, 277, 281, 282
Q
queues 113, 120, 152, 155, 195, 196, 291
R
recursion 106, 108, 113, 116, 118, 123, 125, 128, 130, 131, 141, 142, 146, 169, 170, 173, 174, 175, 176, 177, 223, 275, 279, 283, 295, 298
S
searching 148, 181, 183, 184, 285
sortings 99, 121, 159, 179, 180, 181, 182, 185, 278, 286, 287
stacks 111, 113, 115, 118, 120, 121, 124
strings 95, 96, 97, 99, 100, 103, 134, 173, 174, 180, 183, 271, 275, 287, 288
T
testing 97, 98, 209, 210, 211, 214
threading 219, 226, 242, 257, 258, 259, 262, 264
trees 123, 125, 126, 127, 128, 130, 131, 166, 208, 216, 286, 288, 290, 298
303 Cracking the Coding Interview
Mock Interviews
Mock Interviews
Studying helps, but nothing can prepare you like the real thing. Each CareerCup interviewer has given over a hundred interviews at Google, Microsoft, or Amazon. To nail your interview, sit down with a trained interviewer and get their experienced feedback.
See www.careercup.com/interview for more details.
One Hour Interview with Real Interviewers
Our interviewers will give you a real interview, just like you'd get at Google, Microsoft or Amazon. We'll test you on the same types of questions that they do. We'll grade you the same way they do. How can we do this? We’ve done over 100 interviews each for these companies. We’ve screened resumes. We’ve been part of their hiring committees. We know what they want.
We'll Also Give You...
»»An .mp3 recording of your interview.
»»Feedback on where you shined and where you struggled.
»»Specific suggestions on how to improve.
»»Instructions on how to approach tough problems
»»Lessons on what interviewers look for in your code.
Schedule Your Interview Today!
See www.careercup.com/interview for pricing and details. Check out our special student rates!
. CareerCup com 304
About the Author
Gayle Laakmann’s interviewing expertise comes from vast experience on both sides of the desk. She has completed Software Engineering interviews with - and received offers from - Microsoft, Google, Amazon, Apple, IBM, Goldman Sachs, Capital IQ, and a number of other firms.
Of these top companies, she has worked for Microsoft, Apple and Google, where she gained deep insight into each company’s hiring practices.
Most recently, Gayle spent three years at Google as a Software Engineer and was one of the company’s lead interviewers. She interviewed over 120 candidates in the U.S. and abroad, and led much of the recruiting for her alma mater, the University of Pennsylvania.
Additionally, she served on Google’s Hiring Committee, where she reviewed each candidate’s feedback and made hire / no-hire decisions. She assessed over 700 candidates in that role, and evaluated hundreds more resumes.
In 2005, Gayle founded CareerCup.com to bring her wealth of experience to candidates around the world. Launched first as a free forum for interview questions, CareerCup now offers a book, a video and mock interviews.
Gayle holds a bachelor’s and master’s degree in Computer Science from the University of Pennsylvania.








