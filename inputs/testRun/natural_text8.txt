billionaire prince,
pregnant mistress
Sandra Marton
TORONTO • NEW YORK • LONDON
AMSTERDAM • PARIS • SYDNEY • HAMBURG
STOCKHOLM • ATHENS • TOKYO • MILAN • MADRID
PRAGUE • WARSAW • BUDAPEST • AUCKLAND
billionaire prince,
pregnant mistress
Sandra Marton
CHAPTER ONE
PRINCEALEXANDROS KAREDES, second in line to the throne of the
Kingdom of Aristo, did not like to be kept waiting.
Indeed, he never was.
Whowould be so foolish as to let a man like him cool his heels?
His own father, Alexandros thought with a sigh of resignation
as he strode past the marble fireplace outside the throne room for
what had to be the tenth time in as many minutes. The hands on
the French ormolu clock that graced the mantel stood upright at
six.Alexandros had been told the kingwould see him at five-thirty
butAegeuswas not known for promptness, even with his children.
“An unfortunate habit,” Queen Tia called it, but Alex was not
as kind. He knew his father well; he was certain Aegeus’s chronic
lateness was yet another subtle way of reminding everyone,
family included, that, though he was getting on in years, he was
still king.
It was undoubtedly the same reason he’d asked Alex to meet
him here, in such formal surroundings, rather than in the privacy
of the royal apartments.
That was just the way it was. There was no point in questioning
it. Aegeus was a more than competent ruler. He led the people
of Aristo well but he had always been distant in his dealings with
his wife, sons and daughters.
Alex had no objection. At six or seven, a display of affection,
a lessening of formalities might have meant something, but he
8 BILLIONAIRE PRINCE, PREGNANT MISTRESS
was thirty-one now, he had created his own eminently successful
life by bringing ever-increasing international recognition and
resources to the kingdom.
He had no need for signs of affection from his father. Affection
was for puppies and kittens, not grown men.
Alexandros glanced at the clock again.
Even though he understood the reason for it, being kept
waiting was irritating. And inconvenient. The meeting with his
father would not take long. He knew that from past experience.
He’d just returned from a business trip to the Far East. Aegeus
would simplywant to knowif things had gone well, if newbanks
and corporations would be joining the impressive list of those
already on Aristo, but he would not wish to hear the details.
Results were all that mattered, was Aegeus’s motto. How one
got to those results was immaterial.
That was okay with Alex. He didn’t need pats on the back any
more than he needed signs of affection. Itwas only that if the king
kept him waiting much longer, he’d be late getting into town.
Not that it mattered.
His new Ferrari would easily conquer the narrow roads that
wound along the cliffs looming above the Mediterranean. And
even if he arrived at The Grand Hotel in Ellos past the time he’d
told his date he’d pick her up, she would not complain.
A little smile lifted the corners of his lips.
Why be unduly modest? He did well with all the things he
most enjoyed. Beautiful women, fast cars, baccarat, the vast
business empire he’d created here and in New York.
His smile faded.
Actually, he had not done as well with women lately.
Not that they weren’t his for the taking. The woman waiting
for him tonight was what the world called a supermodel. Simone
had been doing a Vogue cover shoot outside the casino just as
Alex had arrived to discuss the casino’s expansion with its
manager, but that had not kept him from pausing to admire the
leggy blonde posing on the wide marble steps, dressed in a silk
gown that clung like a second skin.
SANDRA MARTON 9
Their eyes had met. Alex had grinned and without hesitation
she’d come down the steps, hips swaying thanks to heels so high
they seemed to be made for sin, oblivious to the frenzied ‘Hey!’
of the photographer.
“Hello,” she’d cooed when she had reached him, smiling the
smile that was worth ten thousand dollars an hour to an advertiser.
“I’m free this evening, Your Highness, and I certainly hope
that you are, too.”
He’d said he was leaving for Tokyo but he’d be back in three
days. “Call me,” she’d purred, and he had, first thing this
morning. What man wouldn’t? She was stunning. Sexy as hell.
He knew she’d be in his bed at the apartment he kept in town
before the night ended…
So what?
A crazy thought. But there it was. A gorgeous woman, another
hot liaison and all he could think was, So what? He’d have the
model and, come morning, she’d be looking for a way to turn a
night into an affair.
He’d be looking for a polite way to make it clear he wasn’t
interested.
Lately, ending an affair before it really had time to start had
become a pattern. He liked sex. Liked women. Their feel, their
scent, their company. It was just that he couldn’t seem to concentrate
on any one woman lately. For weeks now, he’d drifted
from one to another.
He knewdamned well there were men who’d find that exciting.
He didn’t.
Not that he believed in long-term affairs. A month. Two.
Three, that was about it and then he’d do the right thing, send an
incredibly expensive gift and move on.
Alex frowned.
The past couple of months, the only part of that familiar plan
he seemed to get right was the part about moving on.
His brothers had noticed. They’d taken to teasing him about
what they called his wanderlust. With the emphasis on ‘lust,’
Sebastian said, while Andreas grinned. Even his sisters got in on
10 BILLIONAIRE PRINCE, PREGNANT MISTRESS
the act, Lissa long-distance from Paris, Kitty sighing dramatically
and saying, Poor Alex. He just can’t find a woman to love.
Well, no. He wasn’t about to explain the difference between
love and lust to either of them but, of course, love had nothing
to do with it. Why would it? Love was one of those things people
talked about that didn’t really exist.
Myths. Myths as creative as any of the tall tales his long-ago
Greek and Roman ancestors had believed.
What people called ‘love’ was hormonal nonsense—though
he couldn’t call what had drawn his parents together hormonal.
They had come together because it was necessary. Carrying on
a name, a bloodline that had existed for centuries was in the
destiny of royals.
It would surely be the same for Sebastian, heir to the throne,
when the time came. Sebastianwould get to choose his ownwife—
thiswas the twenty-first century, after all—but hewould make that
choice from a carefully vetted list of acceptable young women.
Alex, second in line, would be under somewhat less pressure
but he knew the responsibility of marriage to an appropriate
bride, then children to bear his name, was in his future. It was
all part of his duty to the house of Karedes.
He would demand only that his future wife be attractive.
Beyond that, he had no expectations. Companionship, passion—
those things he would find in a mistress. He would be discreet;
he would never deliberately do anything to insult the woman he
married but a royal wife would understand that her role was to
bear him children.
Neither of them would be foolish enough to look for love.
Discretion in their extra-marital affairs would be enough.
Alex stopped pacing, jammed his hands into his trouser
pockets and stared at the coat of arms on the wall over the
enormous fireplace.
There had been a woman once, years ago. A girl, really. He’d
thought—never mind what he’d thought. What mattered was
what she had thought, that she could use her kisses, her touch,
her soft whispers to bewitch him. He’d been a boy then, led
SANDRA MARTON 11
around by a part of his anatomy that had nothing to do with his
brain, but he’d learned the truth about her in time and had been
wiser for it.
Since then, he had not let a woman lure him into complacency.
Into forgetting that a man always had to look beyond a beautiful
face to see a woman’s true agenda…
Until that night two months ago.
A night when a stranger had stepped into his arms, her face
radiant with seeming innocence. She’d lifted her mouth to his,
parted her lips to the whisper of his breath, the thrust of his
tongue and the world had blurred—until the next morning, when
he’d learned it had all been a lie.
“Prince Alexandros.”
Not just a lie. Alex’s jaw tightened. A scam. A fraud. A
swindle of the first magnitude, and he had fallen for it.
“Sir? The king and queen ask that you join them.”
But she hadn’t gotten away with it. Instead, he’d pretended
he knew nothing of her deception. She had played a part; in the
harsh light of day, it had been his turn to play one, too.
He’d taken her back to bed. Had sex with her again. And that
time, when it was over and she lay sated beneath him, he’d
watched her eyes fill with shock as he told her he knew who she
was, what she was, and promised her that all that would come
of her despicable game was defeat.
Then he’d sent her packing.
The incident had meant so little to him that he could not even
remember her name. Despite her wiles, he’d been the victor.
He’d had hours of sex that had seemed incredible, though he
knew now it had only been, well, sex. And the moment of sweet
revenge that followed had made everything right.
“Your Highness? Their majesties will see you now.”
Or had it?
It wasn’t just women he’d had a lot of these past weeks; it was
everything. He’d put endless miles on the royal private jets with
business trips from his offices in New York and Aristo to
Bermuda. To the Bahamas. To the Virgin Islands, to Florida, to
12 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Mexico and, most recently, Japan. Successful trips, all of them,
but he’d set one hell of a pace. Meetings by day; by night, the
baccarat tables, high-stakes poker…
And sex.
Was it possible he’d spent the last weeks going from country
to country, bed to bed, trying to wipe away the ugly memories
of a night when he’d come as close as a man could to letting a
woman use him?
“Sir. The king and queen are waiting for you.”
Alex blinked. Galen, his father’s major-domo, stood at stiff
attention before him. From the expression on his face, he’d been
there a while.
“Thank you, Galen. Efcharisto.”
“Are you well, sir?”
“Yes, yes, I’m fine. A little distracted.” Alex forced a grin.
“There’s a lady waiting for me in town.You know how that is.”
Galen permitted himself a small smile. “I am sure the lady is
happy to wait, sir,” he said, and stepped aside with a deep bow
as Alex walked past him into the throne room.
His parents were not alone.
A handful of aides hovered around his father, who was seated
at an antique desk liberally strewn with sheets of paper. His
mother stood on the throne platform, encircled by several of her
ladies-in-waiting who held lengths of silk brocade against her
while a seamstress sat on the floor, pinning and tucking and
doing whatever in heaven’s name women did with all those yards
and yards of fabric.
Alex’s lips twitched.
Despite its elegance, the frescoes, the ceiling painted by a sixteenth-
century master and a wall hung with exquisite Byzantine
icons, right now the room looked more like someone’s slightly
messy sitting room than a place in which the kingdom’s most
formal ceremonies were held.
His father looked up. “There you are,” he said in a tone that
suggested it was he who’d been kept waiting. “Well, what do
you think?”
SANDRA MARTON 13
Alex raised his eyebrows. “About what?”
“About these plans, of course.” Aegeus slapped a hand on the
papers spread over his desk. “Do wewant a theme, or do we not?”
No, Alex thought, this was not someone’s sitting room, this
was more like the Mad Hatter’s tea party.
“A theme for what?” he said carefully.
Aegeus shot to his feet, scattering the aides crowded around him.
“For your mother’s sixtieth birthday celebration, of course!
If you hadn’t spent the last month doing God knows what, you’d
know what was going on here!”
“Now, Aegeus.” Husband and son looked at the queen, who
smiled at them both. “You know Alexandros has been busy convincing
foreigners that our kingdom is the perfect place for them
to invest in the future. And I’m sure we can assume he’s been
successful. Haven’t you, Alex?”
Alex smiled and went to his mother. She bent toward him and
he took her hand and brought it to his lips.
“Mother. I’ve missed you.”
“How was your trip?”
“It was fine.” Alex smiled. “We snared a lot of foreigners who
look forward to a happy future.”
His mother laughed. “You see, Aegeus? It’s just as I said.” Tia
waved the women away and came gracefully down the steps. “It’s
good to have you home again, Alexandros.”
“It’s good to be here.” Alex nodded at the women gathering
up the fabrics. “What’s all this?”
“I just told you what it is,” Aegeus said with impatience.
“Preparation for your mother’s birthday celebration. I thought we
should make the final selections of décor, color and fabric here
in the throne room, where the most formal part of the ceremony
will take place. Isn’t that right, gentlemen?”
The aides nodded.
“We want to be certain everything comes together properly.”
Aegeus looked at his aides, who nodded again. Alex thought
of turkeys pecking for grain at the feet of the farmer who owned
them and repressed a smile.
14 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“So, what do you think, Alexandros? What theme shall we
use? Our history as part of the ancient world? A link to the days
of the Crusades? The time of the Ottoman Empire? All those
things, as you well know, are in our bloodline.”
Who gave a damn? What mattered was the celebration of his
mother’s sixtieth birthday, not his father’s lineage.
“Any of those would be fine,” Alex said smoothly, with a
quick glance at his mother. “Something big and splashy. After
all, we don’t want it said that only the Calistans can do parties
that are showy.”
He saw his mother bite back a smile.Any mention of Calista,
which had once been part of the Adamas empire along with
Aristo, was enough to make his father’s hackles rise.
“Showy,” Aegeus said, frowning.
“Exactly.” Alex shook his head. “I’ve never understood why
there was so much coverage of the Queen of England’s birthday
celebration a couple of years ago when it was all so low-key.
Have you, Mother?”
“No,” Tia said with perfect innocence, “I’ve never understood
it, either. All those reporters and television people, theworldwide
interest in Elizabeth and the British royals…and all of it done,
as you say, Alexandros, with such quiet elegance.”
The king snorted. “What is there to understand? One either
knows the virtue of simplicity or one doesn’t.” He looked down
at the papers on his desk, studied them for a long moment, then
swept them to the floor with his hand. “I have just chosen a theme
for your birthday celebration, Tia. The coming of spring. I can
envision it now. Masses of early spring flowers. The Venetian
dinnerware in shades of palest green and yellow. And you, the
queen, dressed in a gown the same pale pink as the diamond in
the Aristan crown.”
Thank you, Tia mouthed to her son. Alex flashed her a grin.
“That sounds very nice,” she said demurely.
“Nice? It will be magnificent, especially with you resplendent
in the new necklace I’ll commission as your birthday gift.
Although we could add a brooch…”
SANDRA MARTON 15
“No brooch,” the queen said. “It would be inappropriate,
Aegeus, to wear both a brooch and a necklace.”
The king waved his hand. “Whatever. Take it up with the
craftsman.”
“The jewelry designer,” Tia commented. “That’s what she is.”
She? Alex frowned and thought back to the weekend the final
half-dozen jewelry designers, selected from all around the globe,
had been invited to Aristo to meet with his parents. Had there been
another woman in the group? He only recalled one.
But then, he thought grimly, that had been the plan, hadn’t it?
That the prince who might influence the choice of designer
should have been so bewitched he would notice only one?
Besides, what did his father mean by talking about the
necklace he would commission? The commission had been made
weeks before.
“—don’t you agree, Alexandros?”
Alex looked at his father. “Sorry, Father. I missed that.”
“I said, it doesn’t matter what this woman is called. Designer,
artisan, craftsman—craftswoman,” the king amended, with a nod
to his wife. “She simply must understand the importance of this
commission… And why are all the rest of you still hanging
about?” Aegeus clapped his hands and the seamstress, aides and
ladies-in-waiting scurried from the room. “She must understand
that, Tia. That is a given.”
The queen nodded. “I am certain that she will.”
“I hope you’re right. She seemed very young to me.”
Things were becoming more confusing. His parents were
definitely referring to a woman designer. A young designer…
Alex stiffened.
No. They couldn’t be talking about her. About Maria Santos
and, yes, he damned well did remember her name. How could
he not? A man who was marked to be the gullible victim of a
scam didn’t forget the person who’d been the scammer.
“She couldn’t have seemed anything to you, Aegeus,” Tia
said, with a little smile. “Remember? We never had the pleasure
of meeting her. She sent us a note and explained she’d been
16 BILLIONAIRE PRINCE, PREGNANT MISTRESS
taken ill that morning. But, of course, we already had seen Miss
Santos’s sketches, and—”
A fist seemed to clench Alex’s gut. He took a deep breath and
forced himself to speak calmly. “Maria Santos? But you said the
commission went to a French company.”
“It did, but they just notified us that the owner passed away
and left the firm tied up in all kinds of unpleasant litigation.” Tia
took Alex’s arm. “I know. It’s all very last minute, and Miss
Santos doesn’t even know that we’re going to ask her to implement
her design.”
“That’s why your trip to New York has such urgency,
Alexandros.”
Alex stared at his father. “What trip to New York?”
“You will see the Santoswoman and inform her of our decision.”
“What your father means,” Queen Tia said, “is that you’ll
explain what’s happened and ask Miss Santos if she will be
generous enough to take on the job at such short notice.”
Another snort from the king. “She’ll leap at the chance.”
“But she might not,” the queen said softly. “This is very last
minute. And true artists have tender egos. Miss Santos may not
like thinking of herself as second choice.”
Alex wanted to laugh. A tender ego? He’d bet Maria Santos
had an ego that could dent cast iron.
“You’re the diplomat in the family,” the king said briskly.
“All that talking and contracting with the businesses you’ve lured
to our island over the years…”
It was as close to a compliment as his father had ever offered
but it wasn’t enough to make Alex go to Maria Santos and present
her with the chance of a lifetime.
“Iwould be happy to help,”Alex said briskly, “but I have pressing
commitments here on the island. Surely someone else can—”
“Someone else cannot,” Aegeus retorted. “You have offices
and an apartment in New York. You know the city. You know its
tempo, its attitude. You’ll be better able to work with the Santos
woman and ensure the necklace is ready in time.”
So much for compliments. This was a royal command. That
SANDRA MARTON 17
the woman who’d wanted this job badly enough to damned near
sell herself to secure it would now get it by default, that he would
be the man who’d have to offer it to her, was almost too ironic
to believe.
“There were other designs submitted,” he said. “Surely one
of them would do?”
His mother’s small hand tightened on his arm. “I preferred
Miss Santos’s work from the beginning, Alex. I deferred to your
father when he selected the French firm, of course, but now…”
Alex looked at the queen as her words trailed away. He knew
it would take little for his father to tell her he had decided on a
different designer. Tia was as restrained as Aegeus was quicktempered,
as gentle as the king was stern. He’d always had the
feeling his mother’s life was not quite the life she had hoped for.
Growing up, he’d spent little time at her side. Boarding school,
tutors, the expected rigor of life as a king’s son had seen to that,
but he loved her deeply none the less. And if a birthday gift
designed by Maria Santos was what she wanted…
“Alexandros?” Tia said softly. “Do you think I’m making
a mistake?”
Alex put his arm around his mother’s shoulders and hugged her.
“What I think is that you should have precisely what you
want on your birthday.”
His mother beamed. “Thank you.”
“Thank me, you mean,” the king said briskly, and gave his
wife what passed for a loving smile. “I’m the one commissioning
your gift.”
The queen laughed. She rose on her toes and kissed her son’s
cheek, then reached for her husband’s hand.
“Thank you both,” she said. “How’s that?”
“It’s fine,” Alex replied.
And that was what he kept telling himself, that it would be
fine, during the seemingly endless flight all the way from Aristo
to New York.
CHAPTER TWO
EVERYTHING was going to be fine.
Absolutely fine, Maria told herself wearily as the Lexington
Avenue local rumbled to a stop at the Spring Street subway station.
Never mind that the man next to her smelled like a skillet of
sautéing garlic. Forget that her feet were shrieking after a day
strapped into gorgeous-but-impossible Manolo stilettos. Pretend
the rain that had become sleet hadn’t turned her sleek, three
hundred dollar Chez Panache blow-out right back into her usual
tumble of coffee-colored wild curls, or that she was obviously
coming down with the flu or something suspiciously like it.
Oh, yes, everything was going to be fine.
And if it wasn’t…if it wasn’t…
The train gave a lurch as it left the station. Garlic Man fell
into her, Maria stumbled sideways and felt one of her sky-high
heels give way.
A word sprang to her lips. It was a word ladies didn’t use, even
if they knew how to say it in Spanish as well as English. Not that
Maria felt much like a lady right now. Still, she bit back the word,
instead visualized it in big neon letters and decided that trying
to figure a way to find the lost heel on the floor of the packed
subway car was something only a madwoman would attempt.
Goodbye, Manolo Blahniks. Goodbye, Chez Panache.
Goodbye, Jewels by Maria.
No. Absolutely, no. She was not going to think like that. What
was it she’d learned in that stress reduction class? Okay, she
hadn’t taken the class, not exactly; there was no time for anything
like taking classes in her life but she’d read the course description
in The New School catalog…
Live in the now.
That was it. Reduce stress by learning to live in the now. At
the moment, that meant—damn!—that meant the train was
pulling into Canal Street.
“Excuse me. Sorry. Coming through!”
She pushed her way through the rush-hour crowd, reached the
doors just as they began to shut and hurled herself onto the
platform. The doors closed; the train started. People surged
toward the stairs, carrying a hobbling Maria in their midst.
Climbing the steps to the street with one shoe now four inches
shorter than the other was an interesting experience. Why did
they make shoes with heels like these? Better still, why had she
bought them? Because men thought they looked good? Well, they
did, but that wasn’t the reason. There was no man in her life; she
couldn’t imagine there would be, not for a long time after that
incident two months ago on Aristo.
The prince. The prince of darkness, was how she’d taken to
thinking of him, and she felt the anger rise inside her again.
Damn it, why was she remembering him, anyway? Why waste
time on him or that night? It had all been a nightmare. She hated
herself for it, would probably always hate herself for it, thought
not half as much as she hated him and…
And, there was no point in this.
Aristo, the commission she’dwanted so much and lost because
of him, were behind her. She had to concentrate on the present.
On how to convince shops like L’Orangerie to buy her designs.
That, she thought grimly, that was why she’d worn these
shoes.Why she’d spent as much on a stupid blow-out as she could
have spent to buy gold wire for the new earrings she’d been
sketching.Why she’d all but begged for today’s meeting with the
buyer from L’Orangerie. And where had it gotten her?
Nowhere, Maria thought as she reached the sidewalk. Nowhere
SANDRA MARTON 19
20 BILLIONAIRE PRINCE, PREGNANT MISTRESS
except out here, limping home like a derelict in sleet that was
rapidly turning to snow.
The weather, coupled with the fact that it was Friday, had sent
people fleeing their offices earlier than usual. Still, the street was
crowded. This was Manhattan, after all. The good news was that
because this was Manhattan, nobody so much as looked at her.
Still, she felt ridiculous, hobbling like this.
Yes, Maria, but the better news is that your heel could have
come off when you were on Fifth Avenue, heading for that
meeting with the man from L’Orangerie.
What an impression she’d have made then.
Not that it would have mattered.
L’Orangerie’s head buyer had been polite enough to keep the
lunch appointment and honest enough to begin it by telling her
he wasn’t going to buy her designs.
“I like them, Ms. Santos,” he’d said, “I like them very much—
but your name will mean nothing to our clients. Perhaps after
you’ve had a bit more exposure…?”
More exposure? Maria gave an inelegant snort as she turned
the corner. How much more exposure did she need? After
winning the Caligari prize, she’d sold to Tiffany’s. To Harry
Winston. To Barney’s.
She’d said all that to her luncheon companion. And he had
said yes, he knew she had, but her status in those places was insignificant
compared to designers like Paloma Picasso and Elsa
Peretti, n’est-ce pas?
Not, she’d wanted to say. Not n’est-ce pas.
Maybe she didn’t have a lot of pieces in the display cases.
Maybe the stores didn’t buy whole page ads for her in The New
York Times and the high fashion magazines. Okay, maybe they
didn’t advertise her name at all.
But she’d sold to the big players. That mattered. And the
pieces she’d designed were certainly more significant than that
phony French accent laid over the unmistakable underpinnings
of his Brooklyn upbringing.
She almost told him so.
SANDRA MARTON 21
Fortunately, sanity had made her put a forkful of salad instead
of her foot in her mouth.
She couldn’t afford to insult a jewelry buyer of such influence.
The world to which she wanted entry was small.
Gossipy. Insulting one of its door-keepers came under the
heading of ShootingYourself in the Head Just to See if the Gun
Would Fire.
Besides, he was right.
She’d been incredibly lucky to sell a few pieces to those
stores. Who knew if she’d ever sell them others? Who knew how
she’d sell them others? Not landing the Aristan commission had
been an enormous setback.
When you could add a discreet line to your business card that
said ‘By commission to Their Majesties, King Aegeus and Queen
Tia of Aristo,’ you had the world by the tail.
She’d lost the chance to have that happen.
Correction. A man had taken that chance from her. A man who
had seduced her and then tossed her out of his bed as if she’d
been a twenty-dollar whore.
“Stop that,” she muttered to herself. Why think of him now?
Why waste time looking back? There was no point.
Maria made a left on Broome Street, hobbled to the next
corner, turned down that street and, finally, there it was. Her
building. Well, not hers. The building in which she lived. And
worked. That was the great thing about renting a loft. There was
plenty of space within its high walls, room for sleeping and
eating, but mostly room for working.
If she could keep working.
The fact of the matter was, she was in debt up to her ears.
The loft cost thousands a month to rent. The gold and silver,
the precious and semi-precious stones with which she worked,
cost thousands, too. She had only one employee, Joaquin, but she
had to meet his salary every week. And designing something that
would be a fit gift for the Queen of Aristo’s sixtieth birthday had
taken hours and hours of time.
So she’d borrowed the small fortune she’d needed to pay her
22 BILLIONAIRE PRINCE, PREGNANT MISTRESS
rent, her bills, to set aside other projects and devote endless
hours to a design for the competition.
Useless, all of it. Useless.
She had been one of the three finalists. They’d all been invited
to Aristo, where the winner would be announced at a ceremony.
And she’d lost any possibility of being that winner in one night.
One foolish night.
A handful of hours had ruined her hopes and dreams, had left
her humiliated beyond measure and the truth was, it was her fault,
all of it. Not the fault of the man who’d seduced her.
Alexandros, the Prince of Aristo, had only proved what she
already knew. The hell with soft lights and sweet talk. All a man
wanted from a woman was sex. That she, of all women, should
have forgotten that cold truth and given in to a moment’s
weakness, was unforgivable.
Once you’d warmed a man’s bed, he had no further use for
you. If something unexpected happened, like, in this case, it
turning out that he was an Aristan prince and you were a finalist
in the competition to design his mother’s birthday gift, he’d lay
the blame for the seduction on you, even when he was the one
who’d done the seducing.
Her father had put the blame on her mother.
The mighty prince had put the blame on her.
“Damn this useless shoe,” Maria said furiously. To hell with
the snow and the icy pavement. She bent down, ripped off both
the broken shoe and its mate, and strode the last few wet yards
to her front door.
It swung open just as she reached it. Joaquin stepped onto the
street, smiled when he saw her but his smile changed to bewilderment
as his startled gaze dropped to her nylon-clad feet.
“Maria? ¿Cuál es la materia? ¿Por qué está usted descalzo
en este tiempo?”
Maria forced a smile. “Nothing’s wrong. I broke my heel,
that’s all.” She stepped past him into the vestibule. “I thought
you’d be gone by now.”
The door swung shut behind her. She started up the stairs to
SANDRA MARTON 23
the loft, Joaquin at her heels. There was a freight elevator, but,
as usual, it wasn’t working.
“I am still here, as you can see. I waited in hopes you would
return to tell me good news.”
Maria nodded but said nothing. When they reached the third
floor, she stabbed her key into the lock, walked briskly across
the age-dulled hardwood floor, dropped her shoes and bag on a
table near one of the loft’s big windows and turned toward her
old friend and co-worker.
“That was good of you.”
Joaquin’s warm brown eyes searched her face. “It did not go
well?”
Maria sighed as she slipped her coat from her shoulders. She
could lie or at least make the meeting with the buyer sound more
hopeful, but there was no point. Joaquin knew her too well. He’d
been working for her for five years. More than that, they’d grown
up in neighboring apartments in a crumbling building in the
Bronx, which was not a place most people thought of when they
spoke of New York.
Joaquin and his family had come from Puerto Rico to the
mainland when he was five and she was six. He was the brother
she’d never had.
So, no. Trying to fool him was useless.
“Maria?” he said softly, and she sighed.
“We didn’t get the contract.”
His expression softened. “Ah. I am so sorry. What happened?
I thought this Frenchman had good taste.”
“He’s not even a Frenchman,” Maria said with a little laugh.
“As for taste, well, he says he likes my work. But—”
“But?”
“But, I should get in touch with him when Jewels by Maria
is better known.”
“When it is,” Joaquin said stoutly, “you won’t need him.”
Maria grinned. “It’s just a good thing you’re married or I’d
nab you for myself.”
Joaquin grinned, too. It was an old joke and they both knew
24 BILLIONAIRE PRINCE, PREGNANT MISTRESS
it had no meaning. So did Joaquin’s wife, who was Maria’s
best friend.
“I’ll be sure and tell Sela you said that.”
“Tell her, too, that I’m looking forward to dinner on Sunday.”
“I will.” Joaquin tucked his hands in his overcoat pockets. “I
left the new wax castings on the workbench.”
“Thank you.”
“FedEx delivered the opals you ordered. I put them in the safe.”
“Excellent.”
Joaquin hesitated. “There is also a letter—a registered letter—
from the bank.”
“Of course there is,” Maria said sharply. She sighed and put
her hand lightly on Joaquin’s arm in apology. “Sorry.” She
smiled. “No need to kill the messenger, right?”
“You might change your mind when I tell you that your
mother phoned.”
Joaquin said it lightly but they both knew a call from Luz
Santos was rarely pleasant. Maria’s mother’s life had not gone
well; she held her daughter responsible. Having Maria had
changed her life. It had ended her dreams. Her plans. Not that she
had regrets. Oh, no. No regrets. She had sacrificed everything for
Maria but that was what mothers were supposed to do.
If only Maria would make the sacrifice worthwhile. If only
she would stop playing with trinkets and get a real job…
“My mother,” Maria said, and sighed again. “Did she say
what she wanted?”
“Her back is acting up. She has indigestion. Her doctor is of
no use to her.” Joaquin cleared his throat. “Mrs. Ferrara’s
daughter was just promoted.”
Maria nodded. “Of course.”
“So was your cousin Angela.”
“Again,” Maria said, deadpan.
“Again,” Joaquin agreed.
Suddenly, it seemed too much. The day. The disappointment.
The overdue bank loan. The flu symptoms she couldn’t shake,
and now a call from Mama… A little moan escaped her lips.
SANDRA MARTON 25
Joaquin put his arms around her and she gave in and leaned her
head against his shoulder.
“Maria, I have a fine idea. Come with me. You know Sela will
be thrilled to see you. She is making Chile Colorado for supper.
When was the last time you had something so delicious, hmm?”
She smiled, stood straight and knotted the woolen scarf at his
neck.
“Joaquin,” she said gently, “go home.”
“If there was a way Sela and I could help you—”
“I know.”
“If only you had gotten that commission. I still cannot understand
the reason you didn’t win.”
She understood it, but she’d sooner have died than divulge it.
“You’ll see, Joaquin. Everything will work out.”
“De su boca al oído del Dios.”
From her mouth to God’s ear. It made her smile again. She
clasped his face in her hands and kissed him lightly on the mouth.
“Go home, mi amigo.”
“Sela will be angry I left you alone at a time like this.”
“Tell Sela I love her but I am your boss,” Maria said with mock
severity, “and I sent you home.”
Joaquin grinned. “Yes, boss,” he said, and pressed a kiss to
her forehead.
She watched as he made his way to the door. It swung shut
after him and she wrapped her arms around herself and shuddered.
It was very cold in the loft. The high ceiling seemed to
steal the landlord’s miserly allotment of heat from the radiators
and the windows, though wonderfully big, were as old as the
building. On a day like this, the wind was relentless and sent
chilly air straight into the cavernous room.
A draft was blowing right on her. And a film of frost was just
beginning to form on the glass. Maria rubbed at it with her fist…
What was that car doing here?
It was parked just across the street. A big car, long and black
and elegant. She knew little about automobiles but in this stillungentrified
stretch of Lower Manhattan a Rolls or a Mercedes
26 BILLIONAIRE PRINCE, PREGNANT MISTRESS
or a Bentley, whatever the vehicle was, stood out like the proverbial
sore thumb.
Her lips turned down.
It was probably a realtor, trying to get a feel for things. They’d
been showing up as regularly as rats in the alley, a sure sign that
the area was about to become too expensive for people like her.
One realtor had even turned up at her door a couple of weeks ago,
oozing charm. She’d only managed to get rid of him by assuring
him she didn’t own her loft—though she hadn’t been able to keep
from telling him that if she did, there wasn’t a way in the world
she’d sell it to him.
In a gesture of defiance and frustration, she glared at the car
and stuck out her tongue. Then she drew back into the darkness,
laughing nervously at herself. What a crazy thing to do but on a
day that had gone as badly as this, it was better than nothing.
Alex, sitting in the back of the Bentley limo, blinked in surprise.
Had the Santos woman just stuck her tongue out at him?
No. Why would she do that? She couldn’t even see him. It was
dark. The windows of the car were tinted. She had no way of
knowing if there was someone in the car or not.
A distortion, then, caused by the cold and the heavily
falling snow.
Not that it had been falling heavily enough to have kept him
from seeing that cozy lovers’ greeting between her and the man
who’d just left. And not that he gave a damn. Five minutes to
explain why he was here, that the commission was hers, and that
would be the end of it.
This was for his mother. He could ignore his anger. His
disgust. He could do this.
He just wished he hadn’t had to view such a charming little
scene. It was enough to make his belly knot. A snowy evening.
A lover, so eager for his woman that he met her downstairs.
Greeted her with tenderness. Went back upstairs with her. Talked
to her. Kissed her…
And walked away.
SANDRA MARTON 27
Alex frowned.
What sort of lover was this man? Why had he chosen the cold
night instead of a woman’s heat? As for tenderness… Did he not
know that tenderness was not what Maria Santos wanted? She
was hot. Wild. Eager in bed.
Even now, he could remember how she had been that night.
Her scent. Lilies of the valley, he had thought, as delicate and
fragrant as those that grew wild in the hills near his home on the
cliffs. Her skin, warm and soft under his questing hands. Her hair,
brushing like silk against his throat.
Her nipples, sweet on his tongue.
Her mouth hot, so hot against his.
Her little cries. Her moans. That one incredible moment as
he’d entered her when he’d thought—when he’d imagined—
that she had never before known a man’s possession.
And, damn it, what in hell was he doing? His body had grown
hard, just remembering. Alex let down the window and drew a
long breath of cold, snow-laden air into his lungs.
The thing to remember was not how she had been in his bed
but the reason she had been there. It had not been an accident;
that she’d stood in seeming uncertainty just in front of the
building in which he had his offices in Ellos, guidebook in hand,
had been, he knew, deliberate.
He had not suspected it then.
But he’d noticed her right away. What man wouldn’t?
Slender, very pretty, her dark mane of hair pulled away from
her face by a simple gold clasp and left to tumble down her
back, her figure limned by the fading light of the day, she’d been
a delightful sight.
He’d paused as he came out the door. She had a pair of small
reading glasses perched on the end of her nose; somehow, that
had added to her charm.
American, he’d thought, a tourist. And, without question, lost.
He’d been in no particular hurry to go anywhere. Okay, why
not? he’d said to himself, and smiled as he’d approached her.
“Excuse me,” he’d said pleasantly, “but do you need some help?”
28 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She’d looked up from the slim guidebook, her eyes a little blurry
because of the glasses. Her hesitation had been artful, just enough
to make her seem not just cautious but almost old-fashioned.
“Well—well—thank you. Yes, actually, I do. If you could tell
me… I’m looking for the Argus. It’s a restaurant. Well, a café.
The guidebook says it’s supposed to be right here. The hotel desk
clerk said so, too. But—”
“But it isn’t,” Alex had said, smiling again. “And, I’m afraid,
it hasn’t been, not for at least a year.”
Her face had fallen. Disappointment had only made her lovelier.
“Oh. Oh, I see. Well—thank you again.”
“You’re most welcome.”
She’d taken off her glasses and gone on looking up at him,
her eyes—hazel, he’d noted, neither brown nor green nor gold
but a veritable swirl of colors—as wide and innocent as a fawn’s.
Innocent as a fox approaching a hen house, he thought now,
his mouth thinning to a tight line.
Maria Santos had known exactly what she was doing, right up
to howshe’d reacted when he’d suggested another restaurant nearby.
“Is it…?” She’d hesitated. “I mean, is this other restaurant—?”
“As good as the Argus?” Truth was, he had no idea. He’d never
been to the Argus. From what little he recalled, it had been a tiny
café, just a place to get a quick bite.
“As inexpensive.” Color had swept into her cheeks. “The
guidebook says—”
“You don’t have to worry about that,” he’d said, because
she wouldn’t.
The restaurant he’d recommended was incredibly expensive—
but he would take her to it. He would dine with her and
pay the bill. Just to talk, he’d told himself. Just to be a good ambassador
for his country, even though—to his surprise—this
beautiful stranger did not seem to recognize his face when the
simple truth, much to his chagrin, was that spotting him was as
much a tourist attraction as the beaches, the yachts and the casino.
The hell she hadn’t recognized him.
She’d known who he was. She’d set the entire thing up.
SANDRA MARTON 29
But he had not known it, then.
She’d protested prettily that she couldn’t possibly let him pay
for her meal but she’d let him think he’d overcome her protests.
And, after dinner, when they’d walked along the sea wall, when
he’d kissed her while they stood surrounded by the tall pines that
grew on a little promontory and their kisses had gone from soft
and exploratory to hot and deep, when his hands had gone under
her silk skirt and she’d moaned into his mouth, when he’d put his
arm tightly around herwaist, still kissing her, and led her through
the now-quiet streets to his flat, to his bed, when she’d clung to
him and whispered she’d never done anything like this before…
When she’d come apart in his arms, her cries so sweet, so
wild, so real…
Alex cursed.
“Sir?” his driver said, but Alex ignored him, swung open the
door of the Bentley himself and stepped into the night.
Lies, all of it, lies that had come undone in the early morning
when he’d reached for her again and found her side of the bed
empty. He’d assumed she was in the bathroom.
She wasn’t.
He’d heard her voice, soft as the breeze from the sea. Was she
on the phone? Without knowing why he did it, he’d carefully
lifted the one on his night table and brought it to his ear.
Yes, he’d heard her say with a breathy little laugh, yes,
Joaquin, I think I really do have a good chance of being named
the winner. I know the competition is tough but I have every
reason to believe my chances are really excellent.
She’d looked up from the telephone when he walked into the
kitchen. Her face had gone crimson.
“You’re awake,” she’d begun to say, with an awkward smile.
He’d taken the phone from her hand. Pressed the ‘end’button.
Carried her back to bed without saying a word, taken her in
passion born of anger.
Then he’d told her to get her clothes on. To get the hell out.
And not to bother showing up at the palace, later.
“Your chances of being named to design my mother’s
30 BILLIONAIRE PRINCE, PREGNANT MISTRESS
birthday gift,” he’d said in clipped tones, “are less than those
of a snowball in hell.”
Alex strode across the street.
It had taken two months but that prediction was no longer just
a metaphor. Here was the snow. And, in just a couple of minutes,
Maria Santos would get a first-hand introduction to hell.
And he would get the satisfaction of putting her, and that
night, out of his head.
Forever.
CHAPTER THREE
MARIA sighed, peeled off her dressed-for-success suit jacket,
tossed it over the back of a chair and automatically reached for
the phone to return her mother’s call.
Her hand stilled.
What was she doing? A ten-minute litany of aches and pains,
followed by a lecture about how she needed to get a real job, were
the last things she wanted right now.
Get out of her clothes. Run a hot bath. Eat something. Then
she’d make the call.
Maria looked at her shoes, made a face and heaved them into
the big trash can beside her work table. Gorgeous but impractical.
She should have known better than to have bought them.
Gorgeous but impractical was not for her. It never had been.
And she hadn’t bought the shoes for today, she’d bought them
for the weekend she’d gone to Aristo. She’d wanted to look sophisticated,
but the shoes hadn’t done her much good then, either.
Even if she’d looked sophisticated, she’d behaved like a—likea—
No. She wasn’t going there. Not tonight. Rejected by a phony
Frenchman today, rejected by an arrogantAristan two months ago.
That was more than enough.
She stepped out of her skirt and padded, barefoot, to the end
of the loft that served as a sleeping area. She tossed the skirt on
the futon, peeled off her bra and pantyhose, yanked the clasp
from her hair, bent forward and ran her hands briskly through
32 BILLIONAIRE PRINCE, PREGNANT MISTRESS
the now-wildly curling strands. Then she tossed her head back,
grabbed a pair of old, scruffy sweats, and put them on.
Time for supper, though the thought of eating made her feel
vaguely queasy.
Nothing new in that. On top of everything else, she’d felt
vaguely ill for the past week or so. No big surprise, considering
that half the city was down with the flu. She probably had it, too,
but she couldn’t afford to give in to it right now, not with half a
dozen pieces to complete by the end of the month.
Her buyers expected her to be prompt. And she needed the
money they’d owe her on delivery.
So, no, she wouldn’t even admit to the possibility that she
might be sick. Absolutely not. She was under stress, she was
working hard. The fatigue, the heaviness in her limbs, the faint
sense of nausea that came and went…
Stress, was what it was.
Something to eat, something bland, would make her feel
better. Nerves had made her bypass breakfast; lunch had been a
joke. Definitely, she had to put something in her stomach.
Soup? Scrambled eggs? Grilled cheese? Better still, she could
order in from Lo Ming’s, down on the corner. Egg drop soup.
Steamed dumplings. Forget the calories. Forget the cost. An
order of Chinese comfort food, then she’d turn on the TV, curl
up on the sofa, get lost in something mindless while—
The doorbell rang.
Now what? It was late. Who would come here at this hour?
Of course. Joaquin. He knew what a setback today had been.
He’d probably gone half a block, phoned Sela on his cell phone
and she’d ordered him to go back and insist Maria come for supper.
The bell rang again. Maria pinned a smile to her lips, went to
the heavy door, undid the lock and pulled it open.
“Joaquin,” she said, “honestly, you have to learn to take ‘no’
for an ans…”
Alexandros Karedes, snow dusting the shoulders of his leather
jacket and glittering like jewels in his dark hair, stood at the door.
Maria felt the blood drain from her head.
SANDRA MARTON 33
“Good evening, Ms. Santos.”
His voice was as she remembered it. Deep. Husky. Perfect
English, but with the faintest hint of a Greek accent. And cold, as
cold as it had been that awful morning shewould never forget, when
he’d accused her of horrible things, called her terrible names…
“Aren’t you going to ask me in?”
She fought for composure. Last time they’d faced each other,
they’d been on his turf. Now they were on hers. She was in
command here, and that meant everything.
“There’s a sign on the door downstairs,” she said, her tone
every bit as frigid as his. “It says, ‘No soliciting or vagrants.’”
His lips drew back in a wolfish grin. “Very amusing.”
“What do you want, Prince Alexandros?”
Atight smile eased across his mouth and it killed her that even
now, knowing he was a vicious, arrogant man, she couldn’t help
but notice what a handsome mouth it was. Chiseled. Generous.
Beautiful, like the rest of him, which made him living proof that
beauty could, indeed, be only skin deep.
“Such formality, Maria. You were hardly so proper the last
time we were together.”
She knew his choice of words was deliberate. She felt her face
heat; she couldn’t help that but she damned well didn’t have to
let him lure her into a verbal sparring match.
“I’ll ask you once more,Your Highness. What do you want?”
“Ask me in and I’ll tell you.”
“I have no intention of asking you in. Tell me why you’re here
or don’t. It’s your choice, just as it will be my choice to shut the
door in your face.”
He laughed. It infuriated her but she could hardly blame him.
He was tall—six two, six three—and though he stood with one
shoulder leaning against the door frame, hands tucked casually
into the pockets of the jacket, his pose was deceptive. He was
strong, with the leanly muscled body of a well-trained athlete.
She remembered his body with painful clarity. The feel of him
under her hands. The power of him moving over her. The taste
of him on her tongue.
34 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Suddenly, he straightened, his laughter gone. “I have not come
this distance to stand in your doorway,” he said coldly, “and I am
not going to leave until I am ready to do so. I suggest you stand
aside and stop behaving like a petulant child.”
A petulant child? Was that what he thought? This man who
had spent hours making love to her and had then accused her of—
of trading her body for profit?
Except, it had not been love, it had been sex. And the sooner
she got rid of him, the better.
She let go of the door knob and stepped aside. “You have
five minutes.”
He strolled past her, bringing cold air and the scent of the night
with him. She swung toward him, arms folded. He reached past
her, pushed the door closed, then folded his arms, too. She
wanted to open the door again but she’d be damned if she was
going to get into a who’s-in-charge-here argument with him.
She was in charge, and he would surely see a tussle over the
ground rules as a sign of weakness.
Instead, she looked past him at the big clock above her
work table.
“Ten seconds gone,” she said briskly. “You’re wasting time,
Your Highness.”
“What I have to say will take longer than five minutes.”
“Then you’ll just have to learn to economize. More than five
minutes, I’ll call the police.”
Instantly, his hand was wrapped around her wrist. He tugged
her toward him, his dark chocolate eyes almost black with anger.
“You do that.And I’ll tell every tabloid shark I can contact about
howMaria Santos tried to buy a five-hundred-thousand-dollar commission
by seducing a prince.” He smiled thinly. “They’ll lap it up.”
She blanched, but she kept her chin up and her eyes on his.
“Don’t try to scare me with lies! You can’t afford that kind
of gossip.”
“I’ve learned to endure that kind of gossip, Ms. Santos. It’s
part of my life. Besides, I’m the righteous prince who discovered
what you wanted and tossed you out on your backside.” Another
SANDRA MARTON 35
of those cold smiles twisted his lips. “They’ll eat you alive. How
do you think that will go over with the handful of reputable
clients you’ve somehow managed to snare?”
Maria yanked her hand free. “!Usted es un cochon!” she
hissed. “!Un cochon malnacido!”
“I think not. If I truly were an ill-bred pig, I would have told
you exactly what I thought of you eight weeks ago instead of just
throwing you out of my apartment.”
Color rushed to her cheeks. She hadn’t figured he understood
Spanish but, then, she’d been wrong in every judgment she’d
made about this man from the start.
“You did tell me,” she snapped, “and now it’s my pleasure to
return the favor.You’re down to four minutes before I call the cops.
Dealing with the media will beworth it, if I can just get rid of you.”
“What’s the problem, Maria? Expecting your lover to return?”
“What?”
“Your lover. What did you call him that morning? Joaquin?”
Joaquin. The idea was so ludicrous she almost laughed, but
laughter would take more energy than she could spare. Besides,
she didn’t have to explain anything.
“Joaquin is none of your business.”
“You’re right, of course.” Alex strolled across the room to the
front windowand peered out at his limo,waiting at the curb across
the street. “But I had a front-row seat for your little welcome
home this evening.You can’t blame me for being curious.”
Maria rushed to the window. A front-row seat? Impossible.
The Prince of Arrogance would surely not have stood in the cold
and the snow, watching her window…
The big car. It was his. Furious, she swung toward her unwelcome
guest.
“You were sitting out there, spying on me?”
“You might want to consider curtains,” he said with lazy
self-assurance.
“You—you…” She pointed a finger at the door. “Get out
of my home!”
Alex didn’t move. Instead, he tucked his hands in the slash
36 BILLIONAIRE PRINCE, PREGNANT MISTRESS
pockets of his jacket and gave her a long look, starting at her feet
and working slowly up to her face. She certainly wasn’t dressed
like a woman waiting for her lover to come back. Not in a pair
of baggy sweats that had seen better days. There was a hole in
one knee, what looked like a burn in the shirt just below her collarbone.
Her feet were bare, her hair a wild mass of curls.
His belly knotted.
Her hair had been like that the last time he’d seen her, a tumble
of long, glorious curls falling around her heart-shaped face. She’d
been wearing his robe; she’d been lost in it and somehowthat had
made her look even sexier, maybe because he’d known, intimately,
what was beneath that robe. The delicate, golden-hued
skin. The small, uptilted breasts. The slim curve of her waist, the
surprisingly feminine richness of her hips.
Her face had been sexy, too. Glowing eyes. Dewy skin. No
make-up, not even lipstick, though her mouth had been rosy and
softly swollen from his kisses.
She had looked—what did the French call it? Déshabillé. As
if she had just come from bed.
Which she had. His bed. His bed and his possession, and that
memory was enough to do more than make his belly knot. It sent
a bolt of pure lust straight to his loins.
He still wanted her.
It had taken the sight of her in a scruffy sweatshirt and
baggy sweatpants before he’d permitted himself to admit it.
What man wanted to acknowledge he still desired a woman
who’d tried to use him?
One who was a fool, he told himself. And then he thought,
no. Hell, no. That wasn’t it at all. Maria Santos owed him and
that was her fault, not his. She had lured him into bed. Seduced
him, though he’d thought he was the one doing the seducing.
She’d plotted everything, from that supposedly accidental
meeting on the street to the moment he’d first kissed her. The
only thing surprising about that night was that she’d been able
to keep from smirking triumphantly when he’d asked her to
come home with him.
SANDRA MARTON 37
She’d made a fool of him, and she still owed him for that.
Owed him big time, as the Americans said. And until that
debt was paid, the memory of his humiliation would continue
to haunt him.
He had no doubt what it would take to expunge that memory.
Her, in his bed again. Moving beneath him. Coming on a
long, explosive cry as he watched her with clinical detachment.
There’d be no phony little cries. No subterfuge. He would make
her want him, make her react to him.
And then he’d send her packing for the second, and last, time.
“Your five minutes are up, Prince Alexandros.”
Alex looked at her. Her expression, her body language, were
defiant. She thought she was in charge.
That made him smile.
“You find this amusing?”
“Indeed.”
Her eyes narrowed. “I’m going to count to ten. It’s your last
chance. If you’re not out the door by then—”
“Safir et Fils is on the verge of collapse.”
She blinked. “Who?”
“Safir et Fils,” he repeated impatiently. “The French firm that
was awarded the commission.” She was staring at him blankly.
“Come on, Ms. Santos,” he said silkily. “Don’t try and tell me
the name of the company that won a commission you were
willing to prostitute yourself to get has slipped your—”
Her hand flew through the air but he was quicker than she was.
He caught her wrist, dragged her forward and hauled her to her toes.
“Do not,” he said with quiet menace, “ever raise your hand
to me again!”
“Let go of me!”
“Did you hear what I said?—”
“What a bastard you are!”
Her voice shook; tears glittered in her eyes and she was
breathing hard. So what? He was unimpressed.
“Playing the righteous innocent will get you nowhere, agapi
mou. You made a fool of me once but I promise you, it will never
38 BILLIONAIRE PRINCE, PREGNANT MISTRESS
happen again. And do not call me names. I am a prince. I urge
you to remember that.”
He almost winced. He sounded like an ass but how could he
think while hot rage pumped through his blood? She was an excellent
actress; he knew that. And this was another stellar performance.
The damp eyes. The trembling voice. The patches of
crimson on her face.
Her face. Beautiful, even now.
“Did you think you could get away with what you did, Maria?
Letting me think you’d been carried away by passion when what
carried you away was the greedy hope that sleeping with me
would give you an advantage in the design competition?”
He paused. Maria stared at him.
Was he waiting for her to answer? What was the point? If she
said he was wrong, he wouldn’t believe her. He hadn’t, that
awful morning.
“Liar,” he’d said, in a voice cold as death, and then he’d hurled
words at her in Greek that she hadn’t understood, though their
meaning had been painfully clear.
Trying to make him listen now would not only be pointless,
it would be demeaning.
The truth was, she hadn’t even known who he was that night.
Aprince? The son of Queen Tia and KingAegeus?As far as she’d
known, he was just a man.A gorgeous, incredibly sexy, fascinating
stranger whose smile, whose touch had made her breathless.
When he’d kissed her and the kisses hadn’t been enough,
when he’d touched her and those touches weren’t enough, she’d
forgotten everything—that they were in a public place, that she
was a moral woman, that she had never been with a man before.
And when he’d whispered, Come with me, she had gone with
him. How could she have done anything else?
Her world had been reduced to him. To his mouth. His hands.
His hard, flagrantly aroused masculinity. She still couldn’t
believe she’d let such a thing happen. You didn’t sleep with a
stranger. She didn’t, anyway.
“What’s the matter, sweetheart? Is that busy little brain of
SANDRA MARTON 39
yours trying to come up with an answer that will satisfy me?”
His voice roughened. “Don’t waste your time. There’s only one
thing that will satisfy me, and you know what that is.”
What he meant was in his eyes.
She saw it and stumbled back. He could see the beat of her
pulse in the hollow of her throat. Good, he thought coldly. This
time, at least, he had the advantage. Command had slipped from
her hands to his and she hadn’t even heard the worst of what he’d
come to tell her.
“Get out.”
She spoke in a papery whisper that he ignored. Instead, he
turned his back and walked to her work table. Sketches were
tacked to an enormous corkboard on the wall above it.
Something that looked as if it had been molded from wax stood
on a shelf.
“Didn’t you hear me? I said—”
“Didn’t you hear me?” He swung toward her, arms folded, feet
crossed at the ankles. “Safir et Fils are going under.”
“Do you expect me to weep for them?”
“They will not be able tomake the gift formymother’s birthday.”
Her smile was pure saccharine. “Stop at Wal-Mart before
you fly home.”
“I know you find this amusing, Maria, but it’s deadly serious.
March the seventh will be an important day. My father has
declared it a national holiday.”
Again, that glittery smile. She had her composure back—but
not for long.
“There will be a ball attended by dignitaries from around
the world.”
“Yes, well if you can’t find anything you like atWal-Mart—”
“My parents have chosen you to execute the commission.”
Her jaw dropped. She was speechless. Twice in one evening.
He had the feeling it was some kind of record.
“Me?”
“You.” His mouth twisted. “You see, despite what I told you
that night, I never mentioned your little game to either the king
40 BILLIONAIRE PRINCE, PREGNANT MISTRESS
or the queen. I didn’t have to. My father had chosen the French
jewelers. He preferred their submission.”
Maria swallowed hard. She wanted to shriek with delight but
she’d be damned if she gave him that.
“How—how nice. To be second-best.”
“Please. Sarcasm doesn’t become you.” Why mention that the
queen had preferred her design all along? “We both know that
this is the chance of a lifetime for a woman like you.”
Her cheeks flushed again. “What, exactly, is that supposed to
mean?”
“Why, only that your name, your career will be made when
word gets out, Maria. What else could it possibly mean?”
She was sure that hadn’t been his meaning but why argue
about it? The fact was, he had it right. Orders would double.
They’d triple! Tiffany would give her a window display; so would
Barney’s. Vogue, Vanity Fair, Allure, Elle, Marie Claire…every
fashion magazine in the world would camp on her doorstep and
the noxious pseudo-Frenchman would be on his knees, begging
her to design for L’Orangerie.
If only the court hadn’t sent the prince to give her the news.
“They sent me,” Alex said, as if he’d read her mind, “because
they wanted to be sure you understood the full importance of this
commission.”
“You mean,” Maria countered sweetly, “because the king
thought your illustrious royal presence would impress me.” He
grinned. Her gaze on him narrowed. “Too bad your father doesn’t
know you as well as I do.”
All at once, Alex was weary of the game. Why in hell had he
ever thought he needed to settle scores? He was not a man who
enjoyed revenge; God knew there was plenty of opportunity for
it in business but he had always seen vengeance-seeking as a low
sport.And payback against awoman, even one who really needed
to be taught a lesson, suddenly held no appeal.
“What’s your answer?” he said brusquely. He pushed back his
sleeve, shot an impatient glance at his watch. “My pilot is
standing by. Weather permitting, I want to fly home tonight.”
SANDRA MARTON 41
Maria chewed on her lip. God, the man was arrogant. If
only she could tell him what he could do with his offer, but
he was right. This would jump-start her career. Nothing she
could ever do would match its importance. She had to say
‘yes’, but surely there was a way to do it so she could regain
her authority.
“Very well,” she said. “I’ll accept the commission.”
He nodded and reached into the inside breast pocket of his
leather jacket.
“Good. I have some papers here…”
“There are certain conditions to be met,” she said as she took
the documents from him.
His dark eyebrows rose. “There are, indeed. Dates of
approval. A date of completion. An agreement as to what you
may and may not discuss with the media—”
“One,” Maria said, “I work alone. If I need an assistant, that
person will be of my choosing.”
“I don’t think you understand. This agreement concerns the
demands of the—”
“Two, I’ll need some new equipment.” She smiled thinly.
“Aristo’s cost. Not mine.”
Alex’s mouth flattened. “You’re fortunate to be getting this
commission, Ms. Santos. Perhaps you’ve forgotten that.”
“Three. I do not work well with anyone watching over my
shoulder. In otherwords, I’ll be happy to showmywork, as it progresses,
to the king and queen at their request—but no one else.”
The muscle in Alex’s jaw jumped to attention. “Is that last
directed at me?”
“Four,” Maria said, raising her hand and ticking the point off
on her finger, but he had stopped listening.
Who did she think she was, this snippet of a female? He was
not of the old school; nobody had to bow to him or bend a knee
in a deep curtsy, well, except, of course, on formal occasions of
the court, but he was entitled to the respect he had been born to
as a prince, the respect he had won as a man—
“If all those conditions are agreeable, I’ll sign your document.”
42 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Alex didn’t answer. He stood watching her from dark, unreadable
eyes and felt the tension inside him growing.
He had left Aristo knowing he had to deal with Maria Santos
and keep his composure. Nothing more.
Then another thought had come to him. He would bed her
again. Right here. Tonight. It was he who would do the seducing
this time, if not with his body then with the commission she’d so
willingly sold her soul to get. He’d strip her naked, touch her
everywhere, kneel between her thighs and take her again and
again and again, until she was out of his system.
A moment ago, he’d come full circle. Told himself that plan
was crazy. It was not him. Taking a woman out of revenge was
beneath him. It was, he’d told himself, enough that she’d know
she was getting the commission only because the true winner of
the competition was out of the picture.
There’d been that instant of pleasure.
Then she’d taken that instant and crushed it.
Who did she think she was, to make demands of him? Of the
royal court? Did she think she had the right to treat him as if he
were an errand boy?
“Are you listening to me, Your Highness?”
Alex looked at her. Her eyes glittered with contempt; her very
posture confirmed it. Oh yes. She saw him as an errand boy. Not
her mark this time. The court’s errand boy.
“I take it you heard my last stipulation,” she said. “I will not
deal with you after tonight. Is that clear?”
He could feel his body humming with anger. He wanted to
haul her into his arms and shake her. Humiliate her. Conquer her.
Strip her of that ridiculous pair of sweats, bare her to his eyes,
his hands, his mouth…
He took a step forward. Something of what he felt must have
shown in his face because she paled and took a step back. That’s
right, he thought coldly. Be afraid of me, Maria. Be afraid of
what I’m going to do…
The phone rang. She grabbed it as if it were a lifeline.
“Hello?” She listened, then cleared her throat. “Yes, sí, I
SANDRA MARTON 43
know. Yes, I know that, too. I’m sorry you had to wait for my
call.” Her eyes swept to Alex; she turned her back as if that
would give her the privacy she needed. “Could we discuss this
another time?” she said in a low voice.
Alex had moved with her; his eyes, fixed on her, still held that
dangerous glitter. Didn’t he understand she needed privacy? Who
did he think she was talking to? Joaquin, probably. That almost
made her laugh. The voice whining in her ear was her mother’s.
And hearing from Luz was the last thing she needed right now.
She turned again, desperately wishing this were a cordless
phone so she could walk further away. Her mother was telling
her about her cousin Angela—snide, holier-than-thou Angela—
and her latest promotion at the insurance company. Maria had
only to ask, Luz was saying, as she did every few weeks, and
Angela would get her a job interview.
“Let me tell you my wonderful news,” she said quickly,
breaking into her mother’s endless praise for Angela. “That commission?
The one to design the birthday jewels for Queen Tia of
Aristo? Well, I’ve landed it.”
She waited, although she really didn’t know what she was
waiting for. She knew better than to think her mother would
shriek with joy and say, I’m so proud of you, mia bella, or even,
That’s wonderful news. But she didn’t really expect Luz to say,
“You?” as if such a thing were impossible.
“You lost the competition.You were not good enough to win it.”
Maria winced. “Yes. Well—well, things changed. There was
a problem with the winner and so—”
“Ah.” Her mother’s sigh spoke volumes. “Well, no matter
how you came by it, it is an opportunity. Be sure you do nothing
to ruin it.”
Maria felt like weeping, which was ridiculous. Why should
tonight be different than the past twenty-eight years?
What made it evenworsewas that Alex had not taken his eyes
from her. It was infuriating. His Royal Highness was a Royal
Boor when it came to manners. Didn’t he know enough to walk
away when someone was having a private conversation?
44 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“One of the things your cousin Angela has always done is to
make the most of her chances.”
“Yes. I know.” Maria cleared her throat. “It’s late. I’ll talk to
you tomorrow.”
Another deep sigh came over the phone. “God willing I will
be here tomorrow. And please, Maria, do not waste time telling
me the doctors saymyhealth is excellent.What do doctors know?”
There was no point in answering. That road, well-traveled
over the decades, led nowhere.
“Good night,” Maria said, “I love—”
Too late. Luz had already disconnected. Maria put down the
phone and swallowed hard. The Prince of Arrogance hadn’t
moved; he was still standing right next to her. She drew a long
breath, let it out as slowly as she could, then turned to face him.
“Wasn’t he interested in your charming declaration of affection?”
“Excuse me?”
“Your lover. Joaquin. I had the impression he ended the call
rather abruptly. Didn’t your news please him?”
“That wasn’t—” She bit her lip. Would having a lover,
however imaginary, offer her some protection? She needed protection;
every instinct told her that. “That wasn’t polite,” she said.
“Listening to my conversation.”
He smiled thinly. “And you, Ms. Santos, are the expert on etiquette,
are you not?” The smile vanished; he shoved a gold pen
at her. “Sign the contract.”
Why did that sound so ominous? “I insist you meet my conditions
before I—”
Suddenly, his hands were on her, cupping her shoulders,
lifting her to her toes.
“You’re lucky to be getting this commission,” he said coldly.
“We both know that. You’re desperate for money—please, don’t
waste my time denying it. And you need the prestige that comes
with creating a necklace for a queen.” His tone hardened. “Sign
the contract, Maria.”
SANDRA MARTON 45
Her lip trembled. She looked away from him and, for a heartbeat,
he hated himself.
Was he really reduced to this? Bullying a woman on the skids?
A woman whose lover had obviously not said a word to congratulate
her on winning this commission?
And why should he give a damn? Maria Santos was nothing
to him.
“Sign the papers,” he growled.
She picked up the pen, smoothed out the documents, laid them
on the table and scribbled her name where he indicated. He felt a
surge of heat sweep through him. But he said nothing, simply took
the papers, folded them and tucked them back in his pocket.
“As for conditions…there are others besides the ones I mentioned.
There are my conditions,” he said in a softly ominous
tone. “And you will meet them.”
His gaze dropped to her lips. She felt her pulse begin to race.
Whatever he was about to say was going to turn her world upside
down; she could sense it.
“One,” he said, still in that soft voice, “you shall have the
studio of your dreams—but on Aristo, not here.”
“Are you insane? I have no intention of—”
“I assume your passport is in order.”
“Of course, but—”
“You will leave with me, tonight.”
“You cannot do this to—”
He bent to her and kissed her. Kissed her as if she belonged
to him, his tongue in her mouth, his hands cupping her bottom,
lifting her to him, into him, into the heavy thrust of his erection.
“And,” he said thickly, when he finally raised his head, “you
will warm my bed until you finish the job.”
“No!” She shook her head as if to emphasize her refusal.
“No,” she said again, her voice high and wild, “I’ll never—”
“You will, or I’ll do what I should have done when you left
my bed the first time. I’ll tell the queen about our little adventure.
I’ll tell her you’re not worthy of designing her gift or of
working in proximity to her. And then you can stay in this loft
46 BILLIONAIRE PRINCE, PREGNANT MISTRESS
and forever live with the knowledge that you failed at the one
thing that could have changed your life.”
Maria wanted to weep but she knew damned well her tears
would have made him not just the victor but the conqueror.
Instead, she forced herself to meet his gaze without flinching.
“Is this how you get your women, Your Highness? Through
blackmail?”
His eyes flashed a warning. She tried to pull away but his
mouth swooped down to hers; his hands swept into her hair,
holding her captive to his merciless kiss until, at last, a sweet
moan whispered from her throat.
Despite her fury and her hatred, it was happening again.
The hot spiral of desire she’d felt that night all those weeks ago.
The sudden swift race of her heart. All those amazing, incredible
feelings she’d never known before were sweeping her away.
She was dizzy in his arms, dizzy from the taste of him, the
scent, the feel of his hand, now in her hair, his fingers cupping
her scalp. He gathered her even closer. The hard press of his
arousal dug into her belly.
Oh God. She wanted him, wanted him, wanted him…
Mariawound her arms aroundAlex’s neck and kissed him back.
CHAPTER FOUR
SHE was on fire.
It had been like this that night on Aristo.
Alex had kissed her, and it had been like touching a match to
tinder.
Until then, she’d always thought descriptions like that were
clichés, the stuff of novels and movies, but Alex had taken her
in his arms and taught her that a man’s touch could change everything
you knew, everything you believed, forever.
One kiss. Onewarm mingling of breath. One caress of lips and
tongues and you were transformed, became someone else.
Someone you didn’t know, didn’t understand.
Didn’t respect.
Her eyes flew open. She slammed her hands against Alex’s
broad chest. He made a sound that was almost a growl and
gathered her closer. Her struggles increased.
“Alex! Damn you, let me go.”
For a moment, herwords didn’t penetrate.Hewas lost in the taste
of Maria, the feel of her soft body against his. But her hands became
small fists, hammering at his shoulders. The message was clear.A
moment’s tease, just enough to drive him half out of his mind…
It wasn’t going to work.
He opened his eyes and let go of her.
“Pack your things.”
His voice was low and rough, his muscles taut with anger and
48 BILLIONAIRE PRINCE, PREGNANT MISTRESS
frustration. She had a way of getting to him and he didn’t like it.
It was not something he was accustomed to.
“That’s it?” Her voice shook with indignation. “You walk in
here, announce that I’m going to—to be your sex slave—”
“My mistress,” he said, mentally cursing himself. How had
she reduced him to this?
“You think that makes it better? You cannot walk in here, manhandle
me and expect—expect—”
“Is that what you call it when you turn soft and hot in a man’s
arms and all but beg him to take you?”
Her face colored. “Get out!”
“Try singing a different tune, agapimeni. The one about being
a shocked virgin is getting tiresome.”
“Is there something about ‘get out’ you don’t understand?”
“And what of the contract you just signed, Maria? Shall I take
that to court and have a judge deal with it?”
“Don’t threaten me!”
“It’s not a threat, it’s a warning. You’ve committed to creating
the queen’s birthday gift, to be completed by twenty-eight
February and subject to my approval.”
“Your approval?”
“Indeed,” he said coldly. “Perhaps you should have read the
contract more thoroughly.”
Maria wanted to laugh. Or cry. Either seemed appropriate.A
minute ago, Alex had been kissing her passionately. Now, he was
talking to her as if he were a prosecutor and she a balky witness.
Did he think he could use sex to control her? Or maybe he
thought he could bully her. Big mistake! She’d grown up on the
streets of the Bronx. What was royal arrogance compared to
Bronx attitude?
“Contracts,” she said, just as coldly, “are made to be broken.”
Alex raised an eyebrow. “Did you lift that line from some
trendy legal show?”
She flushed. Close enough. She’d taken it from an article
about how a hotshot movie star had gotten away with walking
out on a film.
SANDRA MARTON 49
“And you’re right,” he said, taking the contract from his
pocket, flipping to a page and holding it out. “Some are. This one
is not. Take a look at paragraph three.”
Why did doing as he’d suggested smack of defeat? Was it his
smug tone, or was it the instinctive knowledge that what she’d
find in that paragraph would not be good? She snatched the
contract from his hand, read the pertinent sentences…and felt a
shock of disbelief flash through every nerve in her body.
Failure of the party of the first part to complete the agreedupon
commission and/or to fulfill the additional duties required
of her in their entirety…
Her head snapped up. “What?”
“Ah,” he said, his voice a low purr, “I can see that you really
didn’t read this before you signed it.A bad decision, I’m afraid.”
“That’s insane! You cannot contract for—for a mistress…”
“Keep reading,” he said softly.
Did she have a choice? Her gaze dropped to the contract.
Such failure shall result in forfeiture of all goods and services
already provided and repayment for same.
“What goods and services?” She looked up and flashed a triumphant
smile. “You haven’t provided any.”
“Have you forgotten you’re flying to Aristo with me? Did you
think I wouldn’t provide you with a workshop and tools?” He
jerked his chin toward the contract. “There’s more.”
Finally, in the event of forfeiture, an additional penalty to be
paid by Maria Santos in the amount of…
The typed-in number had so many zeroes it made her laugh.
Alex’s eyes narrowed.
“I assure you, this is not meant for your amusement.”
No. Of course not, but what else could she do when the
penalty for walking away was easily ten times the value of everything
she owned?
“You must know I can’t afford anything even close to that!”
He shrugged. “I know only what is in the agreement you
just signed.”
He sounded as removed as if they were discussing when the
50 BILLIONAIRE PRINCE, PREGNANT MISTRESS
snow might stop. It not only killed her hysterical laughter, it
killed any hope she’d had that this was a joke.
“But—but I’d lose everything. This loft. My clients. The
people I deal with would suffer, the ones who subcontract to me.
And Joaquin, who’s been with me from the start—”
“Your lover’s welfare is not my concern.”
“Joaquin is not my lover.” Maria flung the contract at his feet.
“He works for me.”
He bent and picked it up, smoothing the pages, his expression
blank.
“It doesn’t matter one way or the other. My only concern is
this contract. Are you going to abide by it or not?
She stared at him, hating him, hating herself even more. How
could she have slept with him that night? Better still, how could
she have returned his kisses just now? Was she truly, pathetically
her mother’s daughter?
She wanted to curse him. To pummel those broad shoulders
with her fists, but what would that change? Nothing, she thought
bitterly, nothing at all.
“This is usury!”
He grinned. Such a ruggedly beautiful face, she thought
wildly, made even sexier by that quick devil’s smile.
“An impressive legal term,” he said. “But incorrect. The
penalty to which you’ve agreed has nothing to do with a loan.”
“Damn it,” she exploded, “do not play word games with me!
I know what usury means. And I know what this contract is.
Unconscionable. Immoral. Cruel and mean-spirited and—”
“And enforceable.”
“You cannot coerce a woman into—what was your phrase?
Into warming your bed!”
Suddenly, he was standing much too close. She stumbled back
but his big hands were already framing her face and lifting it to him.
“There’s not a word that even hints of coercion in that
contract,” he said softly. “You signed it of your own free will.”
“How can you do this?” she said shakily. “Don’t you have
any scruples?”
SANDRA MARTON 51
He laughed softly. “An interesting question, coming from
you.” His smile faded; his gaze dropped to her lips. “One month,
agapi mou. That’s all it will be. One month of being in my bed.
Of spending the nights with me deep inside you.” His lips
twitched, as if he’d made a joke, but his eyes were so dark they
seemed bottomless. “I can endure it, if you can.”
His words made her blush. How could he joke about the
devil’s bargain he was forcing on her?
“I hate you,” Maria snapped.
Alex grinned. “Hate me all you like, sweetheart. It’s not your
heart I’m after.”
No, she thought, no, it wasn’t. And that was fine because her
heart would never be part of this arrangement.
“Understand something, Your Highness,” she said, searching
for and finding a way to salvage one tiny bit of pride. “Being in
your bed is one thing. Participating in what happens there is not
something you can ever expect.”
His teeth flashed in a quick smile. “A challenge?”
“A statement of fact.”
“A challenge,” he said flatly. “One I am happy to accept.”
He bent his head, brushed his lips over hers. His mouth moved
against hers again and again in the lightest of kisses. She wanted
to lean into him. Wanted to close her eyes, part her lips, clasp his
head and bring it down closer to hers…
I feel nothing, she told herself.
And wished to God it were true.
What in the name of Chronos was she doing? Was she packing
everything she owned? Jeans. T-shirts. Sweaters. Sneakers and
sandals and, hell, another pair of jeans.
Alex looked at his watch, scowled and shook his wrist. Was
the damned thing working? Impossible that only five minutes had
passed since she’d first turned on her heel, marched away from
him and dragged a suitcase from a corner of the loft.
The loft. Her loft. His lip all but curled. He’d been in
Manhattan lofts before. Soaring ceilings. Enormous windows.
52 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Brick walls and polished wood floors. Furniture from
Scandinavia that made the most of all that open space.
Maria’s loft lacked only whatever machines had once been installed
here. Raw space, NewYork realtors called it, and made it
sound as if that was a good thing—which, he supposed, it was if
you intended to transform it into something habitable.
This was not habitable.
The floor was wood but the finish had long since worn away.
The walls were brick. Not warm brick, just brick. Old, dark, depressing.
The ceiling soared, all right. It soared straight up to an
intimidating tangle of pipes and electrical lines.
As for furniture…there were a couple of work tables. Some
cabinets and benches. Boxes. More boxes. And, in this end of the
room, farthest from the entry door, a screen that he assumed concealed
the bathroom, or what passed for a bathroom, and in front
of that, a bed.
Maria’s bed.
Neatly made. Simple. Almost convent-like in appearance…
A double bed.
Alex’s jaw tightened.
His own bed—his beds, considering the number of homes he
owned—his beds were always king-sized. A bachelor’s necessity,
his brothers called them. Plenty of room for a man and a
woman and hours of hot sex.
But a double bed might have advantages.
There’d be little space in which to sprawl while the lovers in
Maria’s bed took some needed rest. They would have to sleep
on their sides, spoon fashion, she with her backside tucked into
his groin, her spill of wild, sexy curls tucked beneath his chin.
He would wake during the night, feel the heat of her against him
and his sex would engorge, fill with heat, throb as he shifted his
weight, as she backed up to him, as she awoke and drowsily whispered
his name while he sought her moist entrance, while he
pistoned within her until she cried out…
Skata!
He was watching Maria pack and turning himself on.
SANDRA MARTON 53
How could she have that much power over him? He didn’t like
it, not one bit. Men were the ones who held power and if that
marked him as old-fashioned, so be it.
He had surely made the right move. Taking her to his bed as
often as he wishedwould purge her from his system.And no matter
what she said, she would not be unwilling for long. She could talk
about not wanting him all she liked but when he touched her, all
that staunch denial fled.To hell with the fact that she despised him.
He felt the same about her. What he’d told her was true enough.
Sex had nothing to do with emotion.
As for her threat not to react in his bed… A lie. A magnificent
lie. He knew a thousand ways to make her react. His mouth
at her breasts. Between her thighs. On her clitoris…
“Damn it,” he growled, and strode toward the bed on which
her suitcase stood open. “That’s enough!”
She swung toward him. “What?”
“Perhaps you have forgotten what my country is like,” he said
through his teeth. “It is not the wilderness.We have shops.”
The understatement of the year, Maria thought. Ellos had all
the shops that made Fifth Avenue paradise and dozens more.
Unfortunately, it had the prices to go with them. She wouldn’t
have the money to step through those doors until she completed
this commission. One new outfit, she’d been in debt for life.
Not that that was a possibility. The outfit she’d worn today
had pretty much melted her credit card.
“Excuse me,” she said with enough sugar in the words to
cause diabetic coma, “but I’m not done.”
“You are done,” he said grimly. “You’ve packed enough for
ten women.”
What she’d done was pack enough for one woman who had
no idea what the weather was like halfway around the world this
time of year.Yes, she could ask him, but that would be a show of
weakness. Stupid, perhaps, but that was the way she felt.
So she’d taken jeans. T-shirts. Sandals. Hiking boots.
Sweaters. She’d considered something dressy, but what for? She
would not be going out in the evenings.
54 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She would be going to the prince’s bed.
She stared at him as he closed the suitcase. She hated him as
a woman; as an artist, she couldn’t help but admire him.Well, no.
Not him. NotAlexandros Karedes.What she admiredwas his long,
leanly muscled body. His wide shoulders and broad chest. Narrow
hips and long legs. The black-as-midnight hair, the dark eyes, the
face that Praxiteles might have chiseled from the finest marble.
He was even more beautiful nude.
She remembered that. The corded muscles in his arms. The
ridged abs. The powerful thrust of his penis rising from a cluster
of dark curls…
Maria swung away and went to the workshop end of the loft.
Forget that. Block it from her mind. Besides, despite all that
about the contract, he couldn’t mean to enforce such a demand.
The more she thought about it, the more assured she grew that
the sleep-with-me nonsense was just a particularly nasty way of
reminding her that she had no standing in his world.
Fine, she thought, plucking a big leather tote from a shelf and
sweeping a handful of tools into it, absolutely fine. Let him play
his stupid games. One month, that was all, a month of his
bullying tactics and then—
Unless she was wrong.
What if hewas serious? What if he really expected her to sleep
with him? Well, not ‘sleep’. She remembered that one night in
his bed. They hadn’t slept at all. He’d taken her over and over,
driven her out of her mind each time, made her do things…
No. Her breath caught.
He hadn’t ‘made’ her do anything. She’d wanted to do them,
things she’d heard of and read about but never, ever imagined
she’d want to do.
And would never do again.
Blindly, she grabbed another handful of tools and dumped
them in the tote.
What she’d told him was true. If he insisted on holding her to
their devil’s bargain, she would not participate. She would lie in
his bed but shewould not move. She’d let his hands seek out every
SANDRA MARTON 55
shadowed valley. Let him put his mouth on hers. On her breasts.
Between her thighs. She’d let him do everything he wanted but
she would not react, she would not, would not…
She gasped as Alex grabbed the tote from her, snapped the
lock, then hoisted it and her suitcase from the floor.
“We’re leaving.”
“I need the rest of those tools—or maybe you thought I work
gold and precious stones with tweezers and a crowbar?”
“Did you not hear me when I said you will have the studio of
your dreams?”
“I heard you. I still want my own things. It’s how people are,
when they’ve worked at the same job for a while. They want the
stuff they’re familiar with, whether it’s a pen or a chisel. I know
that’s difficult for you to get your head around, considering that
you’ve never had to do a day’s work in your life, so you’ll just
have to take my word for it.”
Alex narrowed his eyes. Was that really how she saw him? As
a royal dilettante? He thought back to his father’s initial reaction
when he’d first approached him about bringing new economic
life to Aristo.
“What could you possibly bring to Aristo that I have not?”
Aegeus said, with his usual imperialistic charm.
A casino, for one.A new commercial port that specialized in
handling enormous cargo ships. A colony of upscale second or
third or fourth or even fifth homes for multi-billionaires looking
for seclusion on the island’s northeast coast overlooking the Bay
of Apollonia. He had even managed to divert some of the superrich
from building in the new resort town of Jaladhar on the
island of Calista, which, together with Aristo, had made up the
Kingdom of Adamas until they’d been declared separate nations
by his grandfather, King Christos, more than three decades ago.
So, no. Oh, no. He had never worked a day in his life. He travelled
between his offices in New York and Ellos, he flew to all
the major cities of the world, met and negotiated with hardheaded
businessmen and heads of state and it was all nothing but
a wealthy man’s hobby. Or so this woman thought.
56 BILLIONAIRE PRINCE, PREGNANT MISTRESS
He glared at Maria. At the smug little smile on her lips. Part
of him wanted to grab her and shake her.
Part wanted to pull her into his arms and kiss her until she
begged for him to do more.
Thank God he wasn’t fool enough to do either. Instead, he
jerked his chin in her direction.
“Coat,” he said briskly. “And shoes. Make it quick or I’ll sling
you over my shoulder and carry you downstairs just as you are.”
He would do it, too.
Maria knew that.
So she pulled on heavy socks, a pair of bulky boots she’d
bought the winter she’d almost—almost—decided to try skiing,
stuffed her arms through the sleeves of a warm but ugly vintage
parka she’d found at the Hell’s Kitchen flea market, secured her
wild mop of hair with a scrunchy and marched to the door.
Let His Mightiness see what kind of bed-warmer he’d bought
himself, she thought grimly.
Useless. He didn’t even blink. Instead, he motioned her
toward the steps and followed her out of the building. The snow
was still coming down but the flakes were big and slow, the kind
that normally turned the city into a wonderland.
She could see nothing wonderful about it tonight.
As they stepped off the curb a uniformed driver sprang from
behind the wheel of the big limo, touched a finger to his cap and
clicked his heels.
Maria snorted.
Alex ignored her.
“Hans,” he said.
Hans clicked his heels again. Alex thought about telling him
to stop doing that but he’d already told him the same thing at least
a dozen times. Apparently, Hans was one of those people who
dreamed of the grandeur that was royalty.
Maria, clearly, was not.
Hans reached for the bags. “I’ll put them in the trunk,” Alex
said sharply. “You see to Ms. Santos.”
Another click. Maria rolled her eyes. Hans swept open the rear
SANDRA MARTON 57
passenger door, gave her a little bow as she stepped inside the
car. The door shut with the sort of solid ‘thunk’ she figured you
expected when a car cost as much as a house. A swirl of warm
air, perfumed with the scent of expensive leather, swallowed her
up as she fell back into the soft seat.
The only thing that spoiled it was Alex, who opened the other
rear door and got in beside her.
“The airport,” he said.
The car moved gracefully from the curb. Maria’s gut moved,
too, but not gracefully. What in the world was she doing? She
had to phone Joaquin to say she was leaving, and she certainly
had to say goodbye to her mother.
“Wait!”
The car stopped. Alex turned toward her. “Whatever you
forgot,” he said coldly, “can stay right where it is.”
“No. I mean, it can’t. I mean…” She took a deep breath. “I
can’t go with you.”
Alex folded his arms. “We’ve been through all this.”
“I can’t just leave. I mean…I have to let people know. I have
to say goodbye.”
“People,” he said coldly. “You mean, your ‘friend’, Joaquin.”
She thought of correcting him, but what for? He could believe
what he liked.
“And will you tell him the intimate details of our arrangement,
glyka mou?” he said with a sly smile.
Her head came up. “I will never tell anyone about that.”
He stared at her for a long minute. For some insane reason,
he wanted to take her in his arms and tell her he would not hurt
her, that he would do all he could to bring her pleasure…
To hell with that.
“What’s his address?”
“Why?”
“Hans is an excellent driver,” Alex said with a tight smile,
“but he has one flaw. He can’t find a place unless I give him
its address.”
“Oh,” she said quickly, “no, that isn’t necessary. Just—Driver?
58 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Driver, there’s a subway stop two blocks up. If you’d drop me
off there—and then I can, ah, I can meet you somewhere later…”
“The address,” Alex said quietly, but in a tone so filled with
authority that Maria knew she’d lost.
She sank back in her seat.
“One seven four oh Grandview Avenue,” she said in a small
voice. “That’s in the Bronx.”
“The Bronx?” the driver said.
“The Bronx,” Alex repeated firmly, and the big car started up
again.
Alex watched Maria’s face as the limo made its way along the
snow-laden streets.
She sat huddled in the corner, as far from him as she could
get, staring straight ahead, her face pale in the glaring headlights
of the few cars coming toward them. The snow had all but
emptied the city streets.
She was trembling.
He frowned. Was she cold? Impossible. The sole virtue of that
ugly jacket had to be its warmth. Besides, the car’s interior was
warm.
Shewas nervous, then. Or anxious.About agreeing to go with
him? Not that she’d actually agreed. He’d forced her into it.
Never mind.
Was she nervous about telling her lover she was going away
with another man? Alex’s jaw tightened.A week from now, hell,
a couple of hours from now, her loverwould be history. Once they
boarded his private plane, he’d take her to the big bedroom in the
rear of the cabin, strip her out of that foolish outfit and touch her
in ways that would make her forget any man but him.
That was how it had been that night.
Maria, blind with passion. Her skin, silken to the touch. Her
mouth drinking from his, her fingers cool against his body, her
hands trembling when he clasped them, brought them to his
chest, his belly, his erection.
Touch me this way, he’d murmured. Yes. Like that. Like that.
SANDRA MARTON 59
She’s never done this before, he’d thought in amazement.
And then he’d simply stopped thinking, lost in the heat that
consumed them.
What a lie!
She’d done everything before. He’d known it as soon as he
heard her on the telephone that morning. Until then, she’d had
him fooled. And that wasn’t easy. He’d been with a lot of women.
Too many, he sometimes thought; their faces and names and
bodies had become blurred over the years.
Not hers.
Maria’s name, her heart-shaped face and its delicate features,
her body that was softly curved and not a fashionable arrangement
of hard bones and flesh, even her voice…
He had forgotten nothing. She came to him in his dreams,
telling him she wanted him.
Turning yourself on again, you idiot? he thought angrily as
he shifted in the deep leather seat.
Well, there’d be no more of that.
He knew what this was all about, if he was honest. Ego?
Maybe a little. Anger? Okay, that, too. Payback? Absolutely.
But the real reason he wanted her was much more basic.
The hair of the dog that bit you. Driving out demons.
Whatever you wanted to call it. Have enough sex with Maria
Santos and he’d wipe her name, her face, everything about her
from his mind.
A month from now, he’d be happy to see the last of her.
Whether she was clever in bed or not, he’d never come across a
woman who could hold his interest for much longer than that.
This one would be no exception, not even if she went from waif
to temptress, fire to ice…
“It’s the building right over there.”
Her voice was low. Alex blinked and realized the car had
slowed to a crawl. He looked out the window and saw a nondescript
street, cars packed tightly along the curb, and a looming
wall of apartment buildings.
“This one, miss?” Hans asked.
60 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Sí.Yes.”
It was the first time she’d lapsed into Spanish since the phone
call—and since she’d cursed him. She sounded breathless.
Stressed. His jaw tightened. Was she nervous about visiting her
lover and telling him her plans?
If he’d been her lover, she’d have had the right to be terrified.
He could not imagine agreeing to her going off with another man
for a month. Not for a day. Not if she belonged to him.
The limo eased into the space beside a fire hydrant. The driver
turned off the engine and reached for the door handle.
“Thank you,” Maria said quickly, “but that isn’t necessary. I
can open the door my—”
“Stay in the car, Hans.” Alex’s voice was cold. “I’ll take care
of Ms. Santos.”
A blast of frigid air swept in as he opened the door. Maria’s
heart skipped a beat. Did the Prince of Arrogance think he was
going inside with her? Not in a million years.
“Thank you,” she said, forcing a polite smile, “but I can manage.”
“Don’t be silly, glyka mou. It’s late, the street is nearly
deserted. What kind of gentleman would permit a woman to be
alone under such conditions?”
His tone had gone from harsh to silken. A spider’s web was
silken, too. She didn’t want him with her, not only because then
he would know she hadn’t come to see Joaquin but because he
would know too much.
“Maria. I’m waiting.”
Hewas leaning into the car, his patrician face rigid.Anger swept
through her. Did he think he could take over every aspect of her life?
“Keepwaiting, then. I don’t require your assistance.And let me
assure you,Your Highness, if you think you are a gentleman—”
She gasped as he caught her shoulders and pulled her from
the car.
“You will not talk to me that way,” he growled. “I don’t give
a damn what you do or do not require. What matters is what I
require. For the next month, you’ll do things my way or not at
all. Is that clear?”
SANDRA MARTON 61
“Yessir,” she said, and touched her stiff fingers to her
forehead. “Of course, sir,” she added, and clicked her heels. Then
she jerked her chin up, stepped around him and marched over
the snowy sidewalk to the building’s entry.
Alex could feel his face burning.
He shot a furious glare at Hans, sitting straight as a ramrod
behind the wheel. He gave no sign that he’d seen or heard what
had just happened.
Alex took a deep breath. Then he trudged after Maria through
the snow. Her feet, in those hideous boots, moved up and down
without interference but he was wearing leather mocs—
handmade leather mocs, he thought grimly, and they were
already cold and sodden.
Great. He was about to come face to face with the man who’d
been her lover and his damned shoes would probably fall off his
feet when he…
Panagia mou!
What kind of place was this for a love nest? The entrance door
had a broken lock. The lobby smelled of mice and mildew. What
remained of a mural clung pathetically to a graffiti-scarred wall.
Therewas an elevator but Maria ignored it and headed for the stairs.
“Four flights,” she said briskly, without looking back at him.
“Are you up to that, Your Highness?”
He didn’t bother replying, he simply climbed the steps
behind her. One flight. Two. Three. At last, they reached the
fifth-floor landing.
“This is where he lives?”
Alex sounded incredulous. She hated him for that, and for
forcing himself into this part of her life.
“Answer me!” He clasped her wrist and spun her toward him.
“Your lover expects you to come to him in a dump like this?”
The door to the apartment directly ahead swung open. Alex
looked up, angry at himself, at Maria, at the unwanted intrusion.
“What the hell do you want?” he snarled at the shadowy
figure in the doorway.
The figure stepped forward into the dim light of the stairwell
62 BILLIONAIRE PRINCE, PREGNANT MISTRESS
landing. It was a woman. Small. Dark-haired. Wrapped in a
wool bathrobe.
“Maria?”
Maria took a deep breath. “Sí, Mama. It’s me.”
CHAPTER FIVE
IT’S ME, Mama, Maria said.
And then no one said anything.
For an eternity? For a few seconds? Alex couldn’t be sure.
The only certainty was that he’d made one hell of a mistaken
assumption.
And he’d mortified Maria. The proof was in the rigidity of her
posture, the angle of her head. This place, this depressing setting,
this woman making absolutely no move toward her daughter,
were not things she’d wanted him to see.
So what? he asked himself coldly. Wasn’t it his intention to
humiliate Maria Santos? This was just one more way to do it.
But even as he thought that he found himself moving closer
to her, putting his hand lightly on her shoulder in a gesture of
unspoken support.
The woman in the doorway spoke first. Her words were not
those of a loving mother, delighted to see her child. They were,
instead, accusatory.
“Do you have any idea how late it is, Maria? I was on my
way to bed.”
He saw the color rise in Maria’s face. His hand tightened on
her shoulder.
“I’m sorry, Mama. I should have phoned first—”
“And who is this with you? Why have you brought a man
to my home?”
64 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Forgive me, Mrs. Santos,”Alex said pleasantly. He gave Maria
what he hopedwas a reassuring smile, then stepped forward. “It’s
my fault, entirely. I’m afraid I was in such a hurry to get things
done that the lateness of the hour never occurred to me.”
“And you are…?”
“I am Alexandros Karedes. Prince Alexandros Karedes.”
The Santos woman’s eyebrows rose.
“Prince?”
“From the kingdom of Aristo. Perhaps you’ve heard of it,” he
said politely, knowing she would have. You could not read a
magazine or see a television program about the rich and famous
without hearing of places like Dubai, Monaco and Aristo.
“And you know my daughter?”
“Indeed. In fact, Maria and I are going to be spending the next
few weeks together.”
Maria gave him a look that should have turned him to stone.
“The prince means we’ll be working together.”
“Maria is making my mother’s birthday gift.”
Luz raised her dark eyebrows. “Maria? Is this what you meant
when I called you a little while ago?”
Alex looked at Maria. She glared even as color rose in her
cheeks. It had been her mother on the phone, not her lover. Why
did that please him? Whether she had a lover or not didn’t matter.
She would be his for the next four weeks. Who gave a damn if
she came home to Joaquin when the month ended?
“Sí, Mama, it was.”
He could almost see Luz mulling that over. Finally, she
stepped aside and motioned them forward. “Come inside. I don’t
want to bother the neighbors.”
Maria looked like a wild animal who wanted to escape a
trap, but she jerked her head in assent and moved past him into
the apartment.
The entry foyer was big; it led down two steps into a living
room that must have been elegant in its day but now was dimly lit
and depressing. Luz made no offer of coffee or tea; she took a chair
and when Maria hovered uncertainly,Alex took her elbow. He felt
SANDRA MARTON 65
her stiffen, knew she wanted to jerk free but she let him draw her
down beside him on a small, sagging, blanket-covered sofa.
“You see,” Alex said pleasantly, “my mother—”
“She is the queen?”
“Queen Tia. Yes. Her sixtieth birthday is next month, and—”
He launched into an explanation of the planned celebration.
The state dinner in the palace. The ball that would follow. The
presentation of Maria’s necklace to the queen at precisely midnight,
followed by fireworks. The fact that Mariawas accompanying him
to Aristo so she could consult personally with the queen and with
him, should questions arise about the design of the piece.
“You mean, my daughter will leave New York?”
“Yes,” Alex said politely, “but I can assure you—”
“Well, if it doesn’t worry her to leave me all alone, who am
I to complain? I am not well, Your Highness. Perhaps Maria has
mentioned it.”
“You’re fine, Mama. Your doctors say—”
“What do doctors know?” Luz crossed herself. “We can only
pray for the best. Besides, I suppose you’re determined to live
out this fantasy of yours.”
Alex could see a vein throb in Maria’s temple.
“Could we please have this discussion another time?” she
said, but Luz ignored her.
“Do you have children, Prince Alexandros?”
“I’m not married, Mrs. Santos,” Alex said politely, though
being polite was growing difficult.
“Well, when you do, you’ll understand that a mother’s sole
concern is for her child’s welfare. Maria’s cousin, Angela—”
“I’m sure the prince isn’t interested in Angela.”
“Angela is a wonderful girl. She has an excellent position with
an insurance company. She’s offered many, many times to
arrange for Maria to have an interview there. Why, only this
evening, I told Maria of Angela’s promotion. She’ll be earning
thirty thousand dollars a year!” Luz leaned toward her daughter.
“And I didn’t get the chance to tell you the rest. Angela’s
engaged. To her supervisor, can you imagine? She has done so
well for herself. It’s hard to believe you and she graduated high
school at the same time.”
The sofa was small. Maria’s thigh was against his. Alex
could feel her trembling.With anger?With despair? Not that it
mattered to him…
“We had different goals,” Maria said carefully. “Angela went
straight to work. I went to college.”
“And quit.”
“I didn’t quit, I changed schools. I went to the Fashion
Institute of Technology.” A touch of pride edged her words. “It
was not easy to get in.”
Luz made a face. “Such foolishness! Two years spent studying
what? Drawing? Making geegaws? And meanwhile, your cousin,
Angela, was—”
The hell with this, Alex thought, and he clasped Maria’s hand.
She tried to tug it away but he threaded his fingers through hers.
“Maria,” he said smoothly, “I think it’s time we told your
mother the truth.”
Her eyes went dark and wild. “Alex. Alex, please—”
“I admire your modesty, glyka mou,” he said softly, “but surely
your mother should know the details—of this commission.”
Maria let out a breath. Luz shrugged her shoulders.
“I know them already, Prince Alexandros. My daughter
entered a contest and lost. She’s won it now because the real
winner backed out.”
“You make it sound as if Maria entered a sweepstakes,
Mrs. Santos,” Alex said with a smile that barely softened the
tightly spoken words. “In fact, fifty of the world’s most
prestigious jewelry designers submitted sketches for my
father’s perusal. He and his ministers narrowed the field to
three but the final selection was the king’s.” He paused. “He
chose an excellent entry—but from the start, Maria’s design
was the queen’s choice.”
Maria’s eyes lit. “Was it?” she said softly.
Alex nodded. What was the harm in telling her the truth?
“The necklace your daughter creates will be photographed by
66 BILLIONAIRE PRINCE, PREGNANT MISTRESS
every major magazine. It will be featured on television news on
virtually every continent. And when the queen’s birthday celebration
ends, it will be displayed alongside the Crowns of Aristo
and Adamas, two of the most famous royal crowns in the world.”
Luz seemed to take it all in. Then she nodded and looked at
Maria.
“This is a fine opportunity, mia hija.”
“Sí, Mama. I know it is.”
“You must not squander it. Such good fortune may not come
your way again.”
Alex glanced at Maria. She had a stiff smile pinned to her lips.
He couldn’t blame her. Not that her feelings meant anything to
him, but couldn’t her mother work up a little enthusiasm? His
own mother had always been loving. Not the way mothers were
loving in the books he’d read when he was growing up, or even
in the ways he’d observed when he spent an occasional holiday
weekend with a friend from boarding school.
Tia had not tousled his hair or kissed his scraped knees; she
had not tucked him in at night or sat with him at breakfast in the
morning. He’d longed for those things as a kid but he’d understood.
She was the queen. His father was the king. His parents
had grave responsibilities; from his earliest years on, he’d been
groomed to respect that.
But Tia had applauded his every academic achievement and
sports trophy. Even Aegeus, who had always treated his children,
especially his sons, with cool removal, would have offered a
word of praise at news as important as this.
“This was more than good fortune,” he said coolly. Maria
looked at him in surprise. Hell, he’d surprised himself. “Your
daughter’s talent is the reason she won the commission.”
Maria’s counterfeit smile had given way to one that was soft
and sweet. He wanted to cup her face with his hand, taste that
sweetness, kiss her not as he had before but gently, tenderly…
A muscle knotted in his jaw.
“It’s time we left,” he said brusquely, and rose to his feet.
* * *
SANDRA MARTON 67
It had stopped snowing; the street was clear and a plow truck disappearing
just ahead, red lights blinking, was the reason.
Hans popped from the driver’s seat of the big limo and swept
the rear door open. Maria stepped in; Alex followed her.
“Where to, sir?”
What was that sound? Was Maria—was she crying?
“Sir? To the airport?”
Alex forced his attention to his driver, then dug his BlackBerry
from his pocket. There was one text message. It was from his
pilot and it was brief and to the point.
“Runways are open. Flight plan has been filed.”
“The airport,” he said briskly, and settled back in the seat.
The big car moved swiftly through the streets. Maria said
nothing; her face was turned to the window. If she’d been
weeping, she seemed to have stopped.
Alex cleared his throat.
“I forgot to leave my phone number for your mother. I’ll have
my secretary call her with it first thing tomorrow. Is there anyone
else you wish to notify?”
She shook her head.
“Not even—” He paused. Don’t, he told himself, but the need
to say it was the same as the need to touch an aching tooth, even
though you knewitwas a mistake. “Not even your friend, Joaquin?”
She swung toward him. “He is my friend,” she said fiercely,
“despite what you think. And I have my own cell phone, thank
you very much. I don’t need you or your secretary to do it for me.”
“You needn’t bite my head off. I just—I just wondered if,
perhaps—”
“Look, you did one decent thing tonight, Your Highness.
You—you tried to defend me to my mother. I suppose I owe you
my thanks for that. Just don’t—don’t spoil it.”
“I didn’t defend you. I spoke the truth. My mother loved your
design.” He hesitated. “Frankly, I agreed with her that it was the
best. Why should that be a secret?”
She lifted her chin and looked directly at him. They had just
pulled up to a traffic light. The red glow lit her lovely face with
68 BILLIONAIRE PRINCE, PREGNANT MISTRESS
color and yes, she had been crying. The delicate skin under her
eyes was swollen.
“If it isn’t a secret, why didn’t you tell me right away?”
Alex felt a quick stab of guilt, but why should he? Maria had
not been honest with him, and her lie of omission had been far
greater than his.
“I told you what you needed to know,” he said coldly. “There
was no reason to tell you anything more.”
She gave a little laugh. “Such diplomatic words, Alexandros.
Why, if I didn’t know better, I’d think you were a—” Her face
turned white.
“Maria?”
“Tell the driver to pull over.”
“What is it?”
“I’m going to be—”
Alex lowered the privacy screen and jerked his thumb toward
the curb. Hans steered to it and pulled up,Alex threwhis door open
and Maria shot past him. He was right on her heels; he caught her
by the shoulders as she bent over and was viciously ill.
“Go away,” she gasped. “I don’t want you to—”
Another spasm shook her. He could feel the violence of it and
his hands clasped her more tightly. When she was done, she
stood straight, her back still to him, her body racked with tremors.
“Maria,” he said softly. “Are you okay?”
She nodded. “I’m fine.”
She wasn’t. Her voice was thready and the trembling had increased.
Alex cursed and turned her toward him. She stood with
her head down.
“What happened?”
“I don’t know. Flu, I think. Everyone has it.”
God, she looked so fragile. Not silly, lost in that enormous and
ugly jacket, but terribly, heartbreakingly delicate.
He dug a handkerchief from his pocket and held it toward her.
She shook her head.
“Not your handkerchief. I’ll soil it.”
“Damn it, Maria,” he said, and put his hand under her chin,
SANDRA MARTON 69
lifted her face and dabbed her lips carefully with the snowywhite
linen.
She was still shaking.
Alex lifted her in his arms. “No,” she said, but he ignored her,
ducked his head, carried her inside the car, settled her close
against him and pressed the intercom button.
Hans answered immediately. “Sir?”
“Turn up the heat,” Alex said crisply. “And take us to the
nearest hospital.
“No,” Maria said, even more emphatically. “I don’t want to
go to a hospital.”
“You need a doctor.”
“For heaven’s sake, I was sick. Sick, that’s all. Flu. Or maybe
something I ate.”
“You look like you don’t eat enough,” Alex said, more sharply
than he’d intended but it was true. Holding her in his arms, he’d
realized she was as light as the proverbial feather.
“I am fine. I don’t need to be coddled.”
Yes, he thought, she did—but he knew that edge in her voice
by now, just as he knew the proud angle of her head.
“Okay. Great. No coddling. Hans?”
“Sir?”
“The airport.”
The intercom light blinked off. Maria stared straight ahead,
wrapped in mortification. Of all things to happen.To get sick in front
of this man. To have him insist on staying with her. To have him
wipe her face and now to be sitting within the circle of his arm…
“I am perfectly capable of sitting on my own,” she said coolly.
He let her move away. From the corner of her eye, she could
see him opening a mahogany compartment built into the side of
the car. Taking something from it. A bottle of water. A big white
linen napkin.
“Look at me,” he said as he poured the water on the napkin.
She looked. Their eyes met. What was in his? Pity? Damn it,
she didn’t want his pity. She didn’t want anything from him.
70 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Carefully, he began to wash her face. She jerked back. He
sighed, cupped the back of her head and went right on washing.
It felt wonderful.
When he was done, she gave him a jerky nod. “Thank you,”
she said stiffly and turned away but, once again, she could see
what he was doing from the corner of her eye. Putting the water
and napkin back in the compartment. Taking out another bottle,
this one filled with an amber liquid. Taking out a crystal tumbler.
Opening the bottle, pouring the liquid into the glass…
“Drink this.”
She swung toward him. Bad idea. Everything began to spin.
The interior of the car, Alex’s face. The glass he was holding
toward her.
“Damn it,” he said, reaching for her, “you’re as white as a sheet.”
“I’m—I’m okay. I’m not going to be sick again. I’m just a
little woozy…”
Alex’s arms swept around her. “Don’t,” she said, but she was
speaking into the hardwall of his chest as he lifted her into his lap.
He was warm. Strong. He smelled of snow and cold and of
the clean male scent she remembered, had never forgotten.
“Let go of me,” she said, and hated how her voice shook but
the truth was, she felt awful. Not sick to her stomach anymore,
just cold and shaky and awful.
“Stop arguing with everything I say and drink this.”
His tone was gruff but he held her with care. Well, of course.
He certainly didn’t want to risk having her throw up all over his
magnificent automobile.
The glass was at her mouth.
“What is it?”
“Poison,” he said, but when she looked up at him, he was
smiling. “It’s brandy.”
“I don’t—”
“Yes. I know. You don’t need brandy. Well, I do.” He took a
drink from the glass, then brought it to her lips again. “For once,
just do as I ask without giving me a tough time, okay?”
The brandy smelled wonderful. She thought of how it would
SANDRA MARTON 71
feel, warm and soothing, and of how his mouth had touched the
rim of the glass…
It was safer to think about doing as he’d commanded.
She did, and knew she’d been right. The brandy was warm
and comforting. So was the man who held her. The thought,
unbidden, unexpected, set her heart racing and she pushed the
glass away.
“That’s enough.And you can let go of me. I’m perfectly fine.”
He answered by gathering her closer. “It’s late,” he said brusquely.
“And I’ve had a long day. I think you have, too. So stop
fighting me, Maria. You’re cold and shaky and I’m not at all convinced
you don’t need a doctor.”
“I already said I didn’t.”
“Then do as you’re told. Finish the brandy, put your head
against my shoulder and maybe, just maybe, I’ll believe you.”
“You’re a—a martinet,” she said bitterly. “Did anyone ever
tell you that?”
It was such an old-fashioned word that it made him laugh.
“I’ve been called a lot of things by a lot of women, glyka mou,
but that is a first.” He sank back in the seat; she had no choice
but to sink back with him. “Now close your eyes and rest. We’ll
be at the airport soon.”
Rest? She’d won a competition that had been the goal of the
world’s best jewelry designers—and handed her life over to one
of theworld’s most gorgeous, sexiest men. Howcould she possibly
rest? Surely, the man holding her had his choice of women, a different
one every night if he wished, and yet he wanted her…
Her lashes drooped.
She couldn’t rest. Or sleep. Or…
Maria sighed, burrowed closer against him, and tumbled
into sleep.
Alex felt the tension leave her. He looked down, saw the dark
shadow of her lashes against the sculpted curve of her cheek.
The woman was impossible. Argumentative. Prickly.
Sharp-tongued.
72 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She was also beautiful and fragile and…
And, he reminded himself, she was a manipulative liar. The
sooner he had her in his bed, the better. She would not spin lies
to him there; he would not permit it. He would make love to her
until she sobbed his name, until her need for him was real, and
that would happen as soon as he had her, alone, on his plane.
But when they reached it, he carried a still-sleeping Maria
through the big cabin, to the privacy of his bedroom. Sat her on
the edge of the bed. Took off her jacket and her boots. Took off
his jacket and soggy shoes, as well.
Her eyelids fluttered but did not lift. “Alexandros?” she
murmured.
She had called him that the night they’d made love. That was
the only name he’d given her, just ‘Alexandros’. “Alex, if you
prefer,” he’d added, but not the rest.
Not that she’d needed it, he thought grimly. She had known
his identity; she had targeted him.
“Wake up,” he said coldly as he lay her back against the
pillows. She didn’t. He looked at her again. Even in sleep, she
looked exhausted. And incredibly lovely.
He lay downnext to her. Drewthe cashmere throwfrom the foot
of the bed over them both. Maria sighed in her sleep and turned
toward him.What else could he do except gather her into his arms?
SANDRA MARTON 73
CHAPTER SIX
MARIA awoke in total confusion.
Her heart thumped with terror. Where was she?
Everything about this room was wrong. The bed. The faint
light stealing in through the window. Even the feel of the silk
bed linen under her cheek, the whisper-weight of the blanket…
The pillow beside hers. Indented, as if someone’s head had
rested on it. A faint scent. Clean. Crisp. Male.
“Ohmygod,” she whispered, and shot up against the pillows.
A bad move. Her stomach did a slow roll. She bolted from the
bed, looked around wildly, saw the bathroom and barely got
there in time.
She retched until the muscles of her diaphragm ached.
Shaken and shaking, she closed her eyes and sank down on the
cold tile floor.
Easy, she told herself, just take it easy.
Seconds later, she stood, washed her face, unscrewed the top
from a small bottle of mouthwash and rinsed her mouth until the
bottle was empty.
Boneless, on legs that seemed to be made of over-cooked
pasta, she sank down on the closed commode.
She remembered it all. Alex’s arrival. The royal commission.
The awful visit to Luz, the humiliation of being sick afterward…
Most of all, the unbelievable proposition Alex had made—and
she had accepted.
SANDRA MARTON 75
Was this a hotel room?As if in answer, the floor seemed to give
a gentle dip. Not a hotel room. Thiswas his plane. They were somewhere
over the ocean and she couldn’t even remember getting on
board. Her memory took her as far as being sick in the snow. Alex
cradling her in his arms. The warming swallows of brandy.
Maria groaned and buried her face in her hands.
Had she slept with him? No. Heat flooded her body. Definitely,
no. If Alex had made love to her— Correction. If they’d had sex,
she’d remember. Besides, except for her jacket and boots, shewas
still dressed in the ratty outfit she’d worn last night.
Somehow, the thought that she’d slept between silk sheets and
beneath what was probably a cashmere blanket dressed like this
made her want to laugh.
God, she was coming apart! Aches where she’d never had
aches. Laughter that could just as easily turn to tears. Nausea
when she least expected it. Joaquin was right. She’d been
working too hard. Stress could do terrible things.
She rose to her feet. There was a stall shower. A big terrycloth
robe hanging from a hook. Shampoo and soap and—
And Alex, just outside the bedroom door.
How was she going to face him? What was she going to say?
Could she ask him if he’d slept with her? Well, not with her. In
the same bed. Not that it mattered. He had the right. Hadn’t she
agreed to share his bed, and not just for sleep?
It was a miracle he hadn’t held her to that unspoken agreement
already, but then a woman who tossed her cookies at a
man’s feet wasn’t exactly a turn-on. Not that she wanted to turn
him on. Not that she wanted him to undress her, touch her, carry
her to his bed and do more, much more than sleep next to her…
Someone knocked at the door. The knob rattled. Maria swung
around and stared as if it were a live thing about to launch an attack.
“Ms. Santos?” A woman’s voice. “Ms. Santos?”
She took a deep breath. “Yes?”
The door opened. A pleasant-faced woman of about fifty
smiled at her.
“Good morning, Ms. Santos. I’m Thalia. The stewardess. The
prince asked me to tell you we’ll be landing in a couple of hours.
He asks that you join him for breakfast.”
Maria felt her face heat. “Thank you.”
“I’ve left your bag at the foot of the bed.”
Could her cheeks get any hotter? “Fine. Thank you again.”
Thalia smiled, stepped out of the room and shut the door
behind her. Maria flew to it and turned the lock.
How could she face anyone? She’d all but died of humiliation
just now and there were other people to deal with. The pilot.
A co-pilot. Half the kingdom of Aristo, for all she knew. So
what? the logical part of her said. Common sense assured her
that Prince Alexandros had a long tradition of having women
travel with him and share his bed.
The knowledge would come as no shock to anyone.
Yes, but it came as a shock to her. She had never been a
mistress before.
The fact was, she had never been with a man before that
night two months ago. Not that His Royal Arrogance would
believe it if she told him. Not that she would tell him. Her humiliation
was already devastating enough. Why make it worse?
Far better to let him think that she was as experienced as he obviously
believed.
Why hadn’t she thought of that sooner?
Maria stripped off her clothes and stepped into the shower.
Alex had called her a liar. She wasn’t, but she could carry
things off when she had to. Hadn’t she prepped for the interview
at FIT without letting her mother know? And then there’d
been the interview itself, when she’d sat in a waiting room like
an ugly duckling lost in a bevy of swans. And years later, after
she’d won the Caligari prize and approached a buyer at a posh
Fifth Avenue store with a small box filled with earrings of her
own design…
Oh yes, she thought as she tilted her face up to the spray, yes,
she could do this. Pretend that being his sex toy for a month
meant nothing. Not a problem.
Not at all.
* * *
76 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Where in blazes was Maria?
Alex had awakened hours ago.Awakened? His mouth twisted.
Hehad not really slept.Howcould amansleep with awoman curled
against him, her breath warm and light against his throat, her hand
on his chest? Maria had curved her body into his as if she’d belonged
there. He’d told himself it didn’t affect him and it hadn’t…
For about thirty seconds.
Then, he’d gone into a full state of arousal.
He’d imagined rolling her onto her back. Undressing her.
Caressing her. Imagined her waking slowly as she felt his hands
and mouth moving gently on her flesh.
“Alexandros?” she’d have whispered, as she had that night
they’d spent together, as she had just a little while ago, when he’d
put her to bed, and he’d have said, Yes, it’s Alexandros. Say my
name again, Maria. Touch me with your cool hands. Open your
mouth so I can taste your sweetness…
That was when he’d shot from the bed.
A cold shower. A change of clothes. Then he’d left the room
without a backward glance because he hadn’t trusted himself.
He’d waited weeks for this. He wasn’t going to take her now,
when she was exhausted and sick and only half aware of him.
He wanted her wide awake when he possessed her, wanted
her eyes on his as he took what she had only pretended to give
him that first time.
His flight crew, of course, had asked no questions, nor had Thalia
when he’d told her to inform his guest that they’d be landing soon.
“Is Ms. Santos awake?” he’d asked brusquely, when Thalia
brought him coffee.
“Yessir. I gave her your message.”
Alex looked at his watch. Fifteen minutes had gone by. What
was taking her so long? Did she think she could stay locked in
the bedroom? That she could put off what would happen next?
The hell she could.
They’d land soon, his car would be waiting. He would drive
to his apartment in Ellos and take her to his bed.
He looked at his watch again. He was weary of playing her
SANDRA MARTON 77
games. He put down his coffee cup. Blotted his lips with a linen
napkin. There was still time to assert his possession now…
The door at the rear of the cabin opened. Maria stood framed
within it; her eyes met his. He saw her take a breath and then she
started toward him. The ugly sweats and boots had been replaced
by a pale gray long-sleeved sweater that fell to her hips, black
tights and pale gray ankle boots. Her hair, still damp, tumbled
around her shoulders.
His gut tightened. By God, shewas beautiful.And composed.
He had not expected that. The fact was, he wasn’t sure what
he’d expected. Tears, maybe. Pleas that he send her home. He’d
judged wrong. The look on her facewas a study in self-assurance.
“Good morning,” he said, and rose to his feet. He gestured to
the chair opposite his. She took it, plucked the napkin from under
the heavy silverware and spread it in her lap. “How do you feel?”
“I’m fine. I’m sorry about last night—”
“That you slept curled in my arms?”
“That I got sick,” she said quickly, but the tiniest bit of color
crept into her face.
So. Perhaps she wasn’t as self-confident as she appeared.
“I’m just happy a night’s sleep helped. I tried not to disturb
you when I left the bed,” he said, pouring coffee for her. He
glanced at her, to see what effect his deliberate use of the word
‘bed’ had made. None. None at all. Her expression was impersonal
again. “You were curled so tightly in my arms that I had
to disentangle us.”
There it was again. That little rush of color. She shot him a
look, then buried it in a sip of coffee. She swallowed, looked up.
The tip of her tongue peeped out; she swiped it over her lips. To
his annoyance, he felt his body stir.
“I was sure I’d wake you when I took my arm out from around
your shoulders.”
She looked straight at him. “I thought your stewardess said
we’d be landing soon.”
“A change of subject, agapimeni?” His tone was pure silk.
78 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Did you want to discuss something other than the fact that you
slept with me last night?”
“We shared the same bed,” she said, looking him straight in the
eye. “I’m sure you know the difference between that and what
people mean when they say they slept together.” Her lips compressed.
“Besides, I didn’t know I rated a change of subject. I
thought mistresses were expected to comply with the wishes of
their masters. That is what I will be, isn’t it?Your mistress? I mean,
isn’t that what one calls a woman who warms a man’s bed?”
Damn it! He was the one who could feel his face filling with
heat. What a hell of a little speech, and had she deliberately
waited until Thalia was in earshot? His stewardess had been
with him for years; if asked, he’d have said nothing could rattle
her but hadn’t her eyebrows just taken a surprised lift?
Alex tossed his napkin on the table and got to his feet.Two could
play at this game of control—but only one would be the winner.
“We’ll be on the ground soon,” he said coldly. “And then
there’ll be plenty of time for me to make my wishes clear—and
for you to make absolutely certain you comply with them.”
The last time Maria had come to Aristo, the only time, had been
in early December, the start of the Mediterranean winter.
The plane had taxied to a jet way; she’d disembarked along
with scores of other travel-weary coach passengers and sleepwalked
through the terminal to a luggage carousel where she’d
waited for her suitcase to thump its way toward her. Then she’d
headed outside and waited in line for a taxi.
Arriving in the kingdom with a prince of the Royal House of
Karedes was very different.
Alex’s jet landed and taxied to an area far from the busy
terminal. Two men wheeled a staircase to the door. The captain
and co-pilot left the cockpit and saluted as she and Alex moved
past them; Thalia dropped a little curtsy toAlex and smiled at her.
“Enjoy your stay, miss.”
Alex slid his arm around her waist. “I’ll see to it Ms. Santos
enjoys every minute.”
SANDRA MARTON 79
Was she the only one who heard the ironic undertone in his
words? She couldn’t tell; Thalia’s face showed nothing but Maria
felt a tinge of heat wash into hers.
No, she told herself fiercely, no! She would not let him take
control again. Determinedly, she shrugged free of his encircling
arm and went down the stairs.
In December, the Aristan skies had been a brilliant blue and the
day unseasonably warm. Now, in early February, the air held a
distinct chill. Just as chillingwas the sight of the uniformed chauffeur
standing at attention beside a black limousine even more
imposing than the one that had ferried them around NewYork.
A shudder went through her, and Alex immediately took off
his leather jacket and wrapped it around her shoulders.
“I don’t need that,” she said, trying to shrug it away, but he
clasped the collar, brought the edges together and, in doing so,
drew her closer.
“But you do, agapimeni,” he said, smiling though the smile
never reached his eyes. “Besides, didn’t you just tell me the first
rule a mistress must follow is compliance?”
“Don’t count your chickens before they hatch,” Maria said
coolly. “I’m not your mistress yet.”
His eyes grew darker than midnight.
“You will be, glyka mou,” he said huskily. “And very soon.”
He brushed a strand of hair from her cheek and hooked it
behind her ear. His gaze fell to her lips. Was he going to kiss her,
despite the people watching from the top of the stairs and the
chauffeur waiting beside the car?
If he did—if he did, she would stand straight and still within
his arms and give him nothing in return.
“Did you hear what I said, Maria? An hour from now, you’ll
be in my bed.”
Her pulse rocketed. It took all her strength to respond with
what she hoped was a cool smile.
“Thank you for the warning, Your Highness. It’s always
helpful to be prepared for something unpleasant.”
To her amazement, Alex laughed.
80 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Very nicely done.” His hands swept into her hair and he tilted
her face to his. “But a sad little lie.” His smile faded. “Tell me
how unpleasant it is after I have you undressed,” he whispered.
“Say it when my mouth is at your breast, when it is between your
thighs. Tell me then, glyka mou, and I might just believe you.”
She felt her nipples peak, felt the swift rush of desire spear
low in her belly. He seemed to know what effect his words had
because he bent his head and gave her a quick, possessive kiss.
“Get in the car, agapi mou,” he said, and the look of satisfaction
on his hard, beautiful face made her wonder who she hated
the most, Alex or herself.
The car moved swiftly through the streets of Ellos.
Alex was on his cell phone, talking softly as buildings flashed
by. She recognized the small hotel she’d stayed at, the busy street
where she’d first met him. The romantic restaurant he’d taken her
to, the little park where he’d kissed her.
He’d told the truth, she thought, and drew a shaky breath. He’d
have her in his bed very soon. His apartment was only a couple
of blocks away.
But the car didn’t take the turn that would have brought
them there.
Where was he taking her, then?
She threw him a glance. He’d put the phone away; he sat with
his arms folded over his chest, looking distant and formidable,
and she decided she’d sooner die than ask. Besides, what did it
matter? Maybe he had rules for this kind of thing. Or maybe he
didn’t want her in his apartment. Maybe there was another
woman there already. Or maybe he preferred to keep his women
in a hotel.
The limo swooped up a ramp and onto a highway. A sign in
both Greek and English flashed by.
To the North Coast Beaches and the Bay of Apollonia.
Beaches? Bays? Shewas a city girl. Streets, noise, traffic were
her natural habitat. Beaches and bays sounded foreign. Isolated.
“Aren’t we going to your apartment?”
SANDRA MARTON 81
She spoke without thinking, regretted it almost immediately,
but Alex had a ready reply.
“We were, but I changed my mind. I’m taking you to a place
where your compliance will be assured.”
Her heart skipped a beat. She thought of telling him he wasn’t
funny but that would be a sign of surrender, and the last thing
this man would have from her was surrender.
Her refusal to bend to his will was all she had left, and she
was intent on keeping it.
The drive took what seemed a very long time.
They had reached the bay; the sign at the exit said so but the
proof was in the spectacular view from a road that now hugged
high, curving cliffs above sand so white it looked as if it were
made of crushed pearls. Beyond that stretched a sea of deep, brilliant
blue, so beautiful it took her breath away.
All right. She had to break her self-imposed silence.
“Is that the Bay of Apollonia?”
Alex nodded. “Named for the god, Apollo. Legend says that
Virgil wrote a poem about this place some two thousand years ago.”
“Virgil? But he was Roman.”
“Aristo and its sister island, Calista, were first part of the
Greek Empire and then were ruled by Rome. You’re familiar
with Virgil?”
Maria stiffened. “I might not have had your tutors and private
schools, Alexandros, but the New York City schools provided me
with an excellent education.”
“I didn’t mean to imply…”
“Yes. You did. You don’t know a thing about me but you have
no trouble jumping to all kinds of conclusions.”
“I might say the same of you, glyka mou.”
Maria looked at him. “You mean,” she said sweetly, “you
didn’t have tutors? You didn’t go to private schools?”
“Well, no. I mean, I did—but I have to admit, I tuned out most
of what I learned in Latin III, whichwas pretty much when we dealt
withVirgil. I guess I’m just surprised you didn’t do the same.”
82 BILLIONAIRE PRINCE, PREGNANT MISTRESS
He grinned, and it instantly transformed him from cold despot
to the gorgeous, easygoing man she’d met that night two months
ago. She didn’t want that. Didn’t want to remember that night,
how he’d made her feel when he’d made love to her.
“Anyway, yes, Virgil wrote about the Bay of Apollonia. He
called it an ambrosial sea of sapphire.”
How could she not reply to that? Maria sighed and gazed out
at the bay again.
“He was right,” she said softly, “though I’ve never seen a
sapphire that magnificent. But if I did—”
“If you did?”
“I’d use it as the center stone in a ring. I’d make the setting
of twenty-four-karat gold to suggest the brilliance of the sun, and
mount the sapphire between a pair of small, perfect diamonds to
represent the sister islands of Aristo and Calista.”
“They’re not that anymore,” Alex said, a bit grimly. “The
unified kingdom of Adamas is just a memory until, if and when
the islands are somehow reunited.”
“Is that what people hope will happen?”
“It’s what King Christos hoped would happen when he gave
dominion of one island to his daughter, Anya, and the other to
my father, Aegeus.”
“Was that when Christos had the Stefani diamond split in two?”
Alex raised an eyebrow. “You’ve done your homework.”
“Did you think I designed the necklace for your mother out
of nothing? Of course I did my homework. I know the diamond
was the biggest pink diamond ever mined on Calista, that it dates
to the time of Richard the Lionheart and that it was the center of
the crown of Adamas until it was cut in half in ninety seventyfour.”
Maria flushed. “I don’t know why I’m telling you all this
when you already know it.”
The lady was full of surprise, Alex thought, watching her in
silence for a little while. Then he cleared his throat.
“What will you do with the money from the commission?”
“What will I do with it?” Her disbelieving tone suggested he’d
lost his mind.
SANDRA MARTON 83
“Yes. Surely, it’s enough to buy the perfect sapphire, the
perfect diamonds—”
“You mean, it’s enough to put a down payment on my loft.
Buy some new equipment. Pay some overdue bills. Pay some
bills for my mother, maybe even convince her to move to a nicer
place.” She gave a rueful laugh. “That’s what I’ll do with the
commission.”
“You support your mother?”
Maria gave a little shrug. “She isn’t up to working.”
“Surely, she could—”
“She doesn’t think so. And I owe her. She sacrificed everything
for me…”
“You can’t really believe that,” he said, a touch of anger in
his words.
“What does it matter? I do what must be done, Your Highness,
the same as most people, but what would you know about that,
in your world?”
“That’s unfair.”
“Is it?” Her lips stretched in a smile. “You show up at my door.
You give me the most wonderful news imaginable.”
“And that’s bad?”
“Then you tell me the only way this—this miracle will happen
is if I agree to sleep with you.”
His eyes narrowed. “Trying to get out of our deal, Maria?” He
moved quickly, covered the distance between them and caught
her by the elbows. “All I did was turn the tables,” he said in a low
voice. “You set the trap the first time. Now it’s my turn.”
Maria could feel the sting of angry tears. She didn’t want to
cry in front of him!
“Let go of me.”
“Why? Because you don’t like the truth?”
“You wouldn’t know the truth if it bit you! There was no trap!
You seduced me.”
“I seduced you the way a chicken seduces a fox! You were
good, I have to admit. I really believed you were a shy Miss
Innocent lost on the streets of a strange city.”
84 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Bastardo!” Maria hissed.
Alex slid his hands to her wrists, clamped them hard and
dragged her toward him.
“You knew who I was. You intended to use me.” His eyes
narrowed. “Now I’m going to use you.”
He bent his head and took her mouth, his kiss hard and demanding
and she hated him, hated the touch of his hands, the feel
of his mouth. Hated, hated, hated…and then she stopped thinking
and fell into the kiss.
He felt it happen. Knew the moment she let go—and then his
arms were around her, she was in his lap, his hand was under her
sweater, his mouth was feeding on hers and it was as it had been
that night, the hot need, the drowning passion, the desire to take
and take and never let her go…
His cell phone rang.
Slowly, Alex came back to the world. The car had stopped.
He cupped Maria’s shoulders, put her from him. Her eyes opened
slowly; he saw in them everything he’d seen that night. Surprise.
Desire. Even the innocence he damned well knew wasn’t real.
Angrily, he yanked the phone from his pocket and flipped it open.
“Alexandros? Are you there?”
His father’s voice buzzed in his ear. “Ne,” he said, clearing his
throat.Aegeus talked.Alex listened.Yes, he said again, yes, all right.
But his eyes never left Maria’s face. The way she was looking
at him, the way her lips were parted. He wanted to reach for her
when the call ended. She knew it; he could see it, feel it. She was
ready for him. God, yes, she was ready.
But hewasn’t a fool. Hewould be in control this time, not she.
The Mercedes slowed. Ahead, elaborate wrought-iron
security gates swung slowly open. The car moved under a long
archway of tall cedars and came to a stop in a circular drive
before a magnificent glass and cedar mansion.
“Where are we?” Maria said warily.
“Bluebeard’s castle,” Alex said wryly. “My home, Maria.
My housekeeper expects you. Go inside. See if everything is
as you wish.”
SANDRA MARTON 85
86 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“I don’t under—”
“There’s been a change of plans. I have work to do. I’ll be
back this evening. Six o’clock. We have a dinner appointment.
Be ready. I do not like to be kept waiting.”
The commands were flung at her like stones from a slingshot.
Maria lifted her chin and glared.
“I have no interest in playing games, Your Highness, or going
on pretend dates.”
A smile spread across his lips. “In such a hurry to get to bed,
glyka mou?” Her cheeks colored and he gave the kind of laugh
she knew she would never forget. “It’s hardly a date,” he said
brusquely. “My parents want to meet the winner of the royal commission.”
He wrapped his hand around the back of her neck, drew
her to him and kissed her, hard and deep. “One final reprieve,
agapimeni, and then, rest assured, you will share my bed.”
CHAPTER SEVEN
ALEX’S driver deposited Maria’s suitcase beside her, saluted
briskly and strode back to the limousine.
Wait, Maria almost said, but what would be the point? There
was something intimidating about being delivered to the massive
front doors of a mansion where she knew no one, but getting back
into the car beside a man who’d just kissed her senseless wasn’t
much of an alternative.
She could hear the purr of the big car’s engine as it went
down the drive. She took a deep breath, raised a hand toward
the bell. The doors swung open before she could touch it and a
small woman dressed head to toe in crisp black cotton stood
looking at her.
Wonderful. This had to be the housekeeper. Did she bear
more than a passing resemblance to the one in that old movie
about Young Frankenstein? Then the woman smiled, dipped a
knee, and was instantly transformed from wicked witch to a welcoming
committee of one.
“Kalimera, Keeria. Onomázome Athenia.”
“I’m afraid I don’t speak Greek—”
“Of course. Forgive me. Good morning, madam, and
welcome. I am Athenia. The prince has told me to make sure you
are comfortable.”
Did he leave the same orders for all his mistresses?
“Thank you.”
88 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Athenia clapped her hands. A manservant appeared, inclined
his head to Maria and scooped up her suitcase.
“Really,” Maria said, with a little laugh, “no one has to bow
to me. I’m not a royal or anything like that.”
“You are the prince’s guest and the lady who is to create a
beautiful gift for our beloved queen. We are honored by your
presence, keeria.” The housekeeper stepped back. “Please, won’t
you come in?”
What would happen to Athenia’s warm welcome if she knew
that Alex’s esteemed guest had also made a devil’s bargain with
him? There was no sense in thinking about it. She was here, and
she would do what had to be done.
“Thank you,” Maria said again, and stepped into a cool, slatefloored
entryway. One quick glance assured her this house would
never be confused with Bluebeard’s castle.
“Would you like something to drink? Something to eat? I
know you have had a long journey.”
Just the mention of food and Maria’s belly did a nasty little
flip-flop.
“No,” she said quickly, “no, thank you. I’m not hungry.”
“Then,would you like me to showyou to your room?”Athenia
nodded toward a spiraling staircase that seemed suspended in the
air. “Or would you prefer to see your workshop first?”
Her room? What the housekeeper meant was the prince’s
room. Unbidden, a tremor of what surely had to be apprehension
danced along Maria’s skin.
“Uh, no,” she said, a little breathlessly. “I mean…I mean, my
workshop will be here?”
“It will. I hope you will like it. His Highness gave very
specific orders but we had so little time…”
The Prince of Arrogance’s specialty, Maria thought grimly.
Handing out orders. Giving people little time to obey, much less
question. And why would she be working here? What had he
done? Put a bench in the basement? Hung a work light over it?
She’d have everything she needed, he’d said.
“If you would please come this way…?”
SANDRA MARTON 89
Maria followed the housekeeper through a series of magnificent,
high-ceilinged rooms. Despite her irritation, the artist in her
could not help but see the house’s incredible beauty.
The lifestyles of the rich and famous, she thought wryly.
Always and forever amazing.
She knew how they lived. She was a NewYorker; her life and
those of the fantastically wealthy were completely separate but,
in Manhattan, you brushed shoulders all the time even if it was
only at the Bobbi Brown counter at Saks. And if you knew
somebody who knewsomebody who knewsomebody who could
get you into a promotional party for Vogue or Vanity Fair—and
she did—you could even get up-close-and-personal glimpses of
that kind of storied existence. An old classmate from FIT, a guy
who now designed incredible floral displays, had edged her onto
a couple of those guest lists, though attending the parties had
never snagged her a client.
Still, nothing she’d seen compared to this.
Maria tried not to stare as she followedAthenia throughAlex’s
home. The mansion was spectacular but she had to give him
grudging credit. It had not been built to impress, though it surely
did, but to celebrate thewooded setting, the sapphire bay, the white
sand beach.Walls were made of glass. Almost all the rooms had
terraces or balconies, and the water from an enormous infinity
pool seemed to spill into a sea that stretched to the horizon.
Athenia led her out a pair of glass French doors. Apparently,
her workshop was not in the house. Maybe the mighty prince
thought she could make his mother’s birthday gift in the garage,
Maria thought irritably as they made their way along a flagstone
path that wound through a dormant garden.
The housekeeper turned to her and smiled.
“Your workshop, keeria.”
Maria blinked in surprise.
Ahead, in a grove of firs, stood a perfect miniature of the main
house. Wood. Glass. Soaring rooflines, terraces, white sand and
blue water a dizzying distance below.
“This is normally a guesthouse but the prince was very
specific about your needs. We worked quickly to meet them, but
if anything is not to your liking…”
Not to her liking? Maria almost laughed as they stepped inside.
The guesthouse had three rooms. A bedroom. A marble
bathroom. And a main room, big and high-ceilinged and brightly
lit, a room that had been filled with oak worktables and benches,
with shelves that held tools she had dreamed of buying but only
in a distant, far more affluent future. A quick glance revealed
heated presses, torches, hand tools and protective gear, all of it
straight out of a jewelry maker’s dreams.
And there were cabinets.
Cabinets with drawers and cubby-holes and shelves. Cabinets
that opened to reveal all the things she could possibly need to
create Queen Tia’s necklace. Waxes. Molds. Polishes. Trays of
bright gold and platinum and silver.
And one special tray that made her heartbeat quicken.
“Shall I leave you here, miss?” Athenia said.
Maria nodded. And reached for that special tray. Lined in
black silk, its small compartments burned with the fire of the brilliant
white and pink diamonds she had so carefully described in
her proposal as the only ones suitable for the queen’s gift.
The stones glittered with life.
Carefully matched white diamonds from a mine in the
Canadian Yukon, where there was no danger of them having
been involved in the blood conflicts of the world. And two magnificent
pink diamonds, so exquisite they could only be from the
fabulous mines of Calista.
Maria lifted the pink stones from their silken compartments.
She would only use one as the centerpiece of the necklace. In
her proposal, she’d pointed out that pink diamonds, that all
diamonds, had slight differences in color.
Obviously, King Aegeus had decided to provide her with two
stones so she could choose the one she preferred. The implications
of such wealth were almost beyond comprehension.
The pinks were easily forty karats each, just as she had requested.
She had determined the size she’d need by estimating
90 BILLIONAIRE PRINCE, PREGNANT MISTRESS
that the Stefani diamond, in its original form, was said to have
weighed approximately one hundred and ninety carats, meaning
it had been even bigger than the fabled Darya-ye Noor, a pale
pink diamond that had been mined in India hundreds of years ago
and then became part of the Persian crown jewels.
Thus, the half of the Stefani pink diamond now in the Aristan
crown would weigh somewhere around eighty carats, since some
material would have been lost when the stone was split. The pink
diamond that would be the focal point in the queen’s necklace
would have to be of a size to complement the one in the crown.
These incredible pink ovals would look the same to the untrained
eye, but Maria could see a slight variation in color. The
only way to choose the proper stone for the queen’s necklace
would be to check both against the pink diamond in the crown.
The palace had provided her with photos of it but no photo could
capture the soul of a diamond, or the subtleties of its color, especially
when it was half of the legendary Stefani stone.
Carefully, she returned the pink diamonds to their tray. Her
design couldn’t be changed now, nor did she want to change it, but
diamonds, born in the extreme pressure and heat of the earth’s
forming crust millions of years ago, all had their own characteristics.
Her plans needed simple refining. Nothing anyone but she
would notice.Afiligree of gold here, a millimeter less in depth there.
First, though, she had to call Joaquin and Sela and let them know
she was all right. She hadn’t had the chance to do it last night…
Better not to think about that.
She used her cell phone, left a brief message about the commission
on their voice mail, with no mention of the very personal
contract terms that involved the prince.
“I’m very happy,” she said. And, at the moment, that was the
truth. She had the perfect workshop. The best tools. And the most
magnificent diamonds imaginable.
Maria hitched her hip onto a stool, pulled a pad and pencil
toward her and began to sketch. Yes, she thought as she lost
herself in the work she loved, she could make her design even
more pleasing to the eye. And thinking about diamonds was far
SANDRA MARTON 91
safer than thinking about the man to whom she’d all but sold
herself. The man who would claim her later tonight, who would
take her to his bed, make love to her as he had all those weeks
ago. She would hold back, hold back…but, in the end, she knew
she would sob his name, open her mouth, her body to his. She
would be lost in his arms, in his strength and beauty and passion.
She forced the treacherous thoughts from her mind, put all her
energy into her ideas for the necklace and the diamonds. They,
at least, would never hurt her.
The sun shifted in the sky. She never noticed. She sketched,
erased, sketched… And yawned. Yawned again. She was tired
all the time lately. This time, at least there was a reason. It had
to be jet lag, catching up to her.
As she had done many times over the last couple of weeks,
Maria set her work aside, folded her arms on the table, lowered
her head and rested her cheek on them.
Just a few seconds, to clear the cobwebs from her brain, she
thought. Just a few seconds…
Jet lag, Alex kept telling himself. That was why he felt so
damned irritable.
Besides, it was unreasonable for his father to have demanded
a meeting now, but that was Aegeus’s way. What the king wished,
others must do. And today, this very afternoon, what Aegeus
wished was to meet with his three sons and discuss plans for the
construction of another high-rise complex in Ellos.
There was no point to such a discussion.
For one thing, the construction was already underway. For
another, Alex was in charge of the project. He had taken over development
on Aristo more than eight years ago, with Aegeus’s
grudging blessing.
“You might as well get some use out of that MBA of yours,”
he’d said, which was as close as he’d ever come to acknowledging
that his second son was now more qualified than he to oversee
the kingdom’s booming economy.
This meetingwas just a not so subtle reminder thatAegeuswas
92 BILLIONAIRE PRINCE, PREGNANT MISTRESS
still Aegeus, Alex thought as he sat at the conference table in the
king’s palace office. As if any of them could ever forget that.
“…twenty stories, Alexandros, but why not thirty?”
Alex looked up from the doodles he’d been making on the
yellow notepad before him. Aegeus’s eyes were focused on him.
His younger brother, Andreas, seated beside him, nudged Alex’s
foot with his under the table. His older brother, Sebastian, seated
opposite, raised his eyebrows.
“Didn’t you say the architect agrees that twenty stories for the
center building would be right, Alex?” he said smoothly.
“Yes,” Andreas chimed in, “twenty stories so that the view of
the harbor would not be blocked from the condominium complex
on the heights, right, Alex?”
“That’s correct,” Alex replied. Sebastian grinned. You owe us,
big time, that grin said. Well, hell. That was how it had always
been, the three brothers bailing each other out of hot water when
their father turned a stern eye on any one of them.
Aegeus looked grim, but then he looked that way most of the
time. He looked tired, too, Alex noticed, and thinner than usual.
“Are you feeling all right, Father?” he said.
The king’s eyes narrowed. “I’m feeling fine, thank you,” he
said brusquely. “Fine enough to ask a few more questions—that
is, if you can manage to keep your attention on our discussion a
bit longer.”
Alex felt a muscle knot in his jaw. “What else do you want to
know, Father?”
“Have you settled the woman in?”
“Excuse me?”
“The woman. Mary Santos. Is she settled in?”
“Her name is Maria,” Alex said carefully. “And I thought we
were talking about the Ellos convention center.”
“We were. Now we’re talking about the person who’ll make
your mother’s gift. What is she like?”
“She is, ah, she is talented.”
Talented, indeed.
“I assumed that,” his father said impatiently. “But what is she
SANDRA MARTON 93
like? I am to meet her tonight, at dinner.Will she be able to carry
on a conversation with some intelligence, or is she one of those
leftover flower children who walks around barefoot?”
Sebastian coughed. Andreas cleared his throat. Alex shot them
both looks that promised trouble when they were alone.
“She’s a designer, Father,” he said carefully. “A New Yorker.
I’m sure you’ll find her interesting and able to hold her own at
the dinner table.”
And more than able to hold her own everywhere else. In bed,
for instance, where he should have been with her, right now.
“I assume you’ve put her in a suite at The Grand Hotel.”
“No.” Alex hesitated. “I, ah, I decided to keep her at my house
at Apollonia.”
His father stared at him. So did his brothers. Damn it, Alex
thought, and felt heat rise in his face.
“Security,” he said quickly. “She’ll be working with a fortune
in diamonds.”
“Have we security problems at The Grand?”
“No, of course not. But there are so many tourists…”
“Tourists who pay a thousand Euros a night for a room are
not tourists likely to dabble in theft,” Aegeus said, his words
heavy with sarcasm.
There was a moment’s silence. Then Sebastian and Andreas
spoke at the same time.
“You can never be too sure,” Andreas said.
“Remember that incident in—where was it, Alex? Some hotel
in Manhattan?”
His brothers had redeemed themselves. “Exactly,” Alex
said. “Security is much better at my place. The gates. The
electronics. The guards. I had my guesthouse converted into a
workshop for her.”
Aegeus nodded. “Well. Well, yes. Good thinking.”
A compliment. Something rare. Of course, it was a compliment
given in response to a lie. He’d placed Maria in his home
for reasons that didn’t have a thing to do with anything but lust.
He liked women. He liked sex. He knew what desire was,
94 BILLIONAIRE PRINCE, PREGNANT MISTRESS
how anticipation could enhance the moment when a man finally
took a woman.
But he’d never behaved like this.
Demanding Maria’s compliance. Damned near forcing her to
agree to sleep with him. It had made sense, when he’d planned
it. He would use her for his own ends as she had used him—
If that was true, why was his body in this almost constant state
of arousal? He’d spent the last hours thinking about her.
Imagining her waiting for his arrival. Imagining what he would
do when he reached home.
The images, hot and raw, flooded his mind. If he didn’t do
something soon, he was going to explode.
“Alex? Are you listening to me? I said—”
“Father.” Alex pushed back his chair and got to his feet. “I’m
sorry, but I have to leave.”
Aegeus looked at him in disbelief. “You what?”
“I said I have to—”
“We have not finished discussing the convention center.”
“We finished discussing it three months ago,”Alex said crisply.
His father glared at him. “I don’t like your tone.”
“My apologies, Father. I’m exhausted, that’s all. I’ve flown
to New York and back in, what, less than twenty-four hours.”
He forced a smile. “Perhaps we can put off this conversation
until tomorrow.”
Aegeus studied his middle son, then nodded.
“Very well.” He rose from his chair. Sebastian and Andreas
immediately did the same. “Be prompt for dinner tonight, please.
All of you. Alex, tell Ms. Santos your mother and I look forward
to meeting her.”
Alex started toward the door. The king called after him.
“Alex? My initial concern about this woman, on reading her
proposal, was that she was too young and inexperienced. Now
that you’ve spent a bit of time with her, what do you think? How
does she strike you?”
Spectacularly beautiful. And spectacularly immoral. And,
Thee mou, so desirable he ached to possess her.
SANDRA MARTON 95
“I told you,” Alex said calmly. “She’s very interesting.”
Then he got the hell out of there before his father or, worse,
his brothers could ask him any more questions.
The drive home seemed to last an eternity, even though he was
at the wheel of his Ferrari and took both the highway and then
the winding road to the house at breakneck speed.
Would she be waiting for him? He’d told her to be ready by
six; he was an hour early. She might be in the bath. Or undressing,
baring her flesh to the waning afternoon light.
Such schoolboy fantasies, and completely demolished
when Athenia told him Keeria Santos was in the guesthouse.
In the workshop.
The workshop, he thought as he strode down the path to it.
Of course. The only allegiance, the only honesty she had was
to her work.
It filled him with a rage he knew had no basis in reality.
She should have been in the house. In his bedroom. Dressing
for dinner, as he’d told her to. Or waiting for him. For his touch.
For the act thatwould avenge what she had done to him weeks ago.
“Maria,” he barked as he flung open the guesthouse door.
“Maria, I told you…”
Andhe sawher.At theworkbench. Her head on her arms.Asleep.
His anger drained away. He felt something new take its place,
something he could not name and he swallowed hard, closed the
door quietly and stoodwatching her. Then, slowly, hewalked to her.
Her head was turned to the side. Her lashes formed dark crescents
against the high arc of her cheekbones; there were purple
smudges of exhaustion under her eyes.
My fault, he thought. He had walked into her life…hell, he
had bullied his way into her life, then dragged her halfway around
the world. Not that he owed her more delicate treatment. It was
just that she looked so innocent in her sleep. Her lustrous hair,
lying tumbled over one shoulder. Her translucent skin. Her lips,
delicately curved.
He could remember their taste.
96 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Not from that last kiss he’d given her hours ago, a kiss given
in rage. He remembered her taste from that night in Ellos. How
her mouth had trembled beneath his. How her sigh of surrender
had mingled with his breath. How he had groaned at the sweetness
of her.
He didn’t think. Didn’t question. Instead, he bent down,
brushed a soft, silken curl from her cheek. Put his lips to her
temple. The pink shell of her ear. The curve of her jaw.
“Maria.” Her name was a whisper. “Maria,” he said again, and
when she sighed, he squatted beside her and pressed his lips
gently to hers.
Her lashes fluttered.
He kissed her again. Her taste was honeyed. Don’t, he
thought, don’t. But what could be wrong with one more kiss?
One more sip of nectar from her soft, rosy mouth? Just one last
brush of his lips against hers. Just one… And this time, her lips
parted to his. Clung lightly to his. Her eyes opened; her pupils
were huge and dark.
“Alexandros?” she whispered, and he was lost.
Groaning, he scooped her into his arms. Brought her down
on the soft Kilim carpet. Swept his hands into her hair, lifted her
lovely face to his, and took possession of her mouth.
“Alexandros,” she sighed.
His name. Not any other man’s. His. Only his, and now her
arms were around his neck, her mouth was moving on his as he
lay her back and came down beside her.
His hands cupped her face. Her beauty stole his breath; the
smile that trembled on her lips pierced his heart.
“Yes,” he said huskily. “That’s right, glyka mou. Saymyname.”
She did, again and again until he silenced her with a deep,
hungry kiss. A cry rose in her throat. Her arms tightened around
him. Her back arched; she rose against him and he groaned again
and slipped his hand inside her black tights.
Her flesh was warm. Soft. Fragrant with the glorious scent
of arousal.
He could feel the race of his blood.
SANDRA MARTON 97
He put his lips to her throat.
She sobbed his name. Cupped the back of his head. Urged his
mouth down, down, to the uptilted thrust of her breast. To the
pebbled nipple that pressed against the softness of her sweater.
He caught the bud lightly between his teeth. Her cry pierced the
thick silence.
“Yes,” she said, “yes, yes…”
He pushed up the sweater. Sucked a nipple into his mouth. She
was lifting herself to him, burying her fingers in his hair, urging
him closer, closer…
A knock, as strident as Olympian thunder, sounded at the
door. Alex barely heard it but Maria stiffened in his arms.
“Alex,” she hissed.
“Shh, agapi mou. Never mind.Whoever is there will go away.”
The knock came again. “Your Highness?” Athenia’s voice
was thin and apologetic. “Your mother is on the phone. She asks
if you and Keeria Santos would come by a few minutes early.”
Alex pressed his forehead to Maria’s. “Yes,” he called, “all
right. Tell the queen we’ll be there as soon as we can.” He waited
until he was sure the housekeeper was gone. Until he could move
without disgracing himself. Then he sat up. “We’ll finish this
later,” he started to say, but Maria had already rolled away from
him and risen to her feet. Her face was white except for two spots
of crimson high on her cheeks.
“Is that how you get your women, Your Highness? By taking
advantage of them when they’re asleep?”
Her voice shook with indignation. Hell, he was shaking, too,
but with thwarted desire.
“You know that isn’t how it was.”
“What I know,” she said, the words laced with accusation, “is
that I woke up and found you all over me!”
He stood and faced her, caught between equal parts of anger
and frustration.
“Liar,” he said in a low voice.
She turned her back. He grasped her shoulder and swung her
toward him.
98 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“What’s the matter, glyka mou? Don’t you like it when the
tables are turned? When you’re not in control of the situation?”
“All right,” she snapped. “You made your point. You—you got
me to—to give in to you. Are you satisfied now?”
He gave a sharp, ugly laugh. “We have a long way to go until
I’m satisfied, sweetheart.”
The crimson drained from her face. “How can you do this?”
It was, he thought, an excellent question.
Despite everything, he was not a man who would ever take
an unwilling woman to bed. That was part of the problem, when
he came down to it. Maria said she didn’t want him but each time
he took her in his arms, she turned that into a lie. Or did she?
Was she still playing him? Was she using him now, even as
he was determined to use her? And how could he tell himself that
was what he was doing when the truth was he had never wanted
a woman as he wanted her and—be honest, Karedes—and
revenge or payback, whatever name he gave his supposed motivation,
had zero to do with what he felt once she was in his arms.
He turned away from her. Ran his hand through his thick, dark
hair.
He was a man who had always prided himself on logic. On
self-discipline. And right now, hell, who was he kidding? Ever
since the night he’d first met this woman, logic and self-discipline
had gone by the wayside.
Maybe it was enough to admit that he wanted her still, and
that at the end of a month she would be out of his system.
Damned right, she would, he thought grimly, and he turned and
faced her again.
“I suggest you return to the house,” he said brusquely. “One
of the maids has unpacked your suitcase. You have—” He
glanced at his watch. “You have twenty minutes to get ready and
then we leave for the palace.”
Her chin came up. “Where has your devoted slave put my
things?”
Thee mou, she enraged him! He wanted to shake her. Or strip
her naked and show her who was in charge here.
SANDRA MARTON 99
100 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Your clothes are where they belong,” he snapped. “In my
room. We have an agreement, Ms. Santos, that says you are to
fulfill your required duties in their entirety, or have you conveniently
forgotten that?”
She gave him a withering look. “How could I forget what is
sure to be the worst agreement of my life?”
It was, Maria thought, a fine line.
But the Prince of Arrogance only laughed, and that was the
sound that followed her all the way to the house.
CHAPTER EIGHT
WHAT did you wear to dine with royalty?
Probably nothing she’d packed, Maria thought unhappily as
she followed Athenia to Alex’s bedroom.
Bedroom? Could you call a room this size a bedroom? It was
bigger than her loft. Polished wood floors. Handmade rugs. A
cathedral ceiling. Skylights. A wall of glass and, beyond it, a
terrace and the pool that seemed to hang suspended over the bay.
And a bed.
A bed centered beneath the skylights, elevated on a raised
platform, covered by a black silk comforter and a sea of black
and white pillows as if it were a stage set.
“Madam will find her things hung in the dressing room.”
Maria swung toward Athenia. “Yes. I—I— Thank you.”
“Everything has been pressed, keeria, to your liking, I hope.”
“Thank you,” she said again. They seemed the only words she
could manage.
The housekeeper smiled politely and shut the door behind her.
Maria waited a couple of seconds, then turned the lock. She
leaned back against the door, shut her eyes and inhaled deeply.
It was a handsome room. Hell, it was a magnificent room.
And that bed…
Do not look at that bed, Maria. Do not even think about it.
She would not. She would shower and dress. She had twenty
minutes. Not much time, but enough. Actually, she never took
102 BILLIONAIRE PRINCE, PREGNANT MISTRESS
longer than that to get ready for a date. Except, thiswasn’t a date.
It was business. Business to be conducted at a palace.
She’d seen the palace—from the outside, anyway—the last
time she was here.
It made Buckingham Palace look small.
“That’s it,” she whispered.“Work yourself into a panic. That’s
going to be a huge help!” Spine straight, she ignored the bed and
marched across the room. This was an important night.
Indeed, it was. At the end of it, Alex was going to make love
to her.
Maria rolled her eyes. Itwas stupid to let her thoughtswander.
Of course, tonightwas important. She had the commission; now,
she had to make sure she had the hearts and minds of her clients.
Her clients. The king of Aristo and his queen. She’d come a
long way from the phony Frenchman of L’Orangerie.
The dressing room made her laugh. Add some plumbing and
most Manhattan residents would have happily called it an apartment.
And there were her things, on a rack all by themselves, surrounded
by other racks filled with men’s clothes. Alex’s clothes.
And no, she was not going to think about that now. Dinner
was everything. It had to go well.
Her clothes, as Athenia had told her, had been pressed, hung
and organized by color. Giddy laughter rose in her throat. Jeans
and jeans and jeans, T-shirts and blouses and sweaters. Organized
and pressed, and what in hell was there hanging in front of her
she could wear to a palace?
Casual, Alex had said. Easy for him to say. And to do.
What was he going to wear? And where would he shower and
dress?
Not here, and that was all that mattered. For all she knew, he
kept a complete wardrobe in each bedroom. A mistress in each,
too. Or maybe this was the way installing a new mistress was
handled. Maybe his staff was trained to move some of the
master’s clothes, just enough to get his latest conquest through
the confusion of her first night here.
Stop it, Maria thought furiously.
SANDRA MARTON 103
She was most assuredly not Alex’s conquest, she was his—
What would be the correct word? Never mind. She would not
dwell on how or why she was in his bedroom, or the implications
of it, either—or on the fact that his entire staff surely now
understood she would be sleeping with him.
A dozen other women probably had gone this route. She
lacked their experience in the art or business of being a kept
woman but instinct told her that a woman who filled that role
would not blush at such information being public.
She’d do her best not to blush, either.
Besides, Alex would not ‘keep’ her. The money for the commission
didn’t come from him. It was for the design and execution
of the queen’s birthday gift, and she would not accept so
much as a penny for anything else.
A phone rang.
Maria looked around. There it was. A small white telephone
on the wall of the dressing room. It rang again and she plucked
it from its cradle, put it to her ear and said a careful, “Hello?”
“You’re down to twelve minutes, glyka mou.”
“Alexandros?”
“I like it when you call me that.”
His voice was husky. Why did that roughness always send a
tingle along her skin?
“Alexandros!” She looked around wildly. “Where are you?”
He laughed. “Relax, sweetheart. I can’t see you—but I
know exactly what you’re doing.You’re standing in the middle
of my bedroom, trying not to look at the bed and wondering
what on earth possessed you to bring nothing suitable to wear
this evening.”
She blinked. “Wrong,” she said airily. After all, she was in the
dressing room, not the bedroom, and she’d already wasted time
trying not to look at the bed.
“Try the emerald silk dress and the black stiletto sandals.
And before you tell me you won’t wear another woman’s castoffs,
let me assure you they aren’t. The dress and shoes were
both delivered from the Chanel boutique in Ellos a couple of
hours before we arrived.” His words took on that same sexy
softness again. “I had to guess at the size, glyka mou, so I hope
I got them right. Of course, we won’t have any such difficulties
after tonight.”
Maria felt her entire body blush as she slammed the phone
back onto its cradle. How dared he buy her clothes? Did he really
think she’d wear anything he’d paid for?
There it was. The dress. And right below it, the shoes. Both
were gorgeous. The brilliant color of the dress would be perfect
with the delicately spiked heels. Exactly what she’d have bought
for an occasion like this…if she’d been in a position to spend,
what, ten thousand bucks?
She would not wear these things.
She would wear something of her own.
Black jeans. A white silk blouse. Dressy enough for dinner at
an upscale New York restaurant…but for dinner at a palace? For
what was, basically, a business meeting that was surely going to
change her life?
“Damn you, Alexandros,” she said bitterly—and knew she
had lost Round One.
She showered quickly, and never mind that the faint, clean scent
of the hand-milled soap reminded her of Alex. The shampoo had
the same effect. So what? Soap was soap, shampoo was
shampoo. She towel-dried her hair—no time for anything else—
and hurried into the dressing room.
There were more than shoes with the dress. There was a tiny
black evening purse. And undies.A black lace bra.A black lace
thong. The sheerest thigh-high nylons she’d ever seen.
She had her own underwear.
But not like this.
To hell with it.
She put on the bits of black lace, the sheer stockings. Hair loose
or up? Maria peered into the mirror. Up. The mass of dark strands
was too damp, too wild, too curly to leave loose. Finally, she
slipped on the emerald silk dress. Stepped into the black sandals.
104 BILLIONAIRE PRINCE, PREGNANT MISTRESS
And saw herself in the mirror.
He had good taste, the Prince of Arrogance, she thought wryly.
Acareer as a personal shopper could be his in the blink of an eye.
The dress was a perfect fit, demure and businesslike even as
it made the most of her slender figure. The shoes were gorgeous.
Straps that wound around her foot. Stiletto heels as thin as the
blade for which they were named.
Could he possibly know shoes were her weakness?
No, she thought. The better probability was that they were his
weakness. Maybe later tonight, he’d want her in the stilettos and
nothing besides the black lace thong…
“Oh God,” she whispered, and felt her heart rate shoot into
the stratosphere.
Jewelry, she thought numbly, because it was safer to think
about that than about what happened to her body each time she
imagined being in this room, in that bed, with the gorgeous
Alexandros. How could you hate a man and still want him?
A question for another time, not for the one minute—the one
minute she had left!
Fortunately, she’d dumped a couple of pieces of her stuff into
her handbag. A twisted gold chain? No. A shorter one, intricately
braided? No. A slender gold rope with a hunk of polished amber
knotted at the center? Yes. Perfect. Small gold hoops in her ears.
Had she forgotten anything? She certainly had. A quick swipe
of mascara. Sheer cherry lip gloss. A dab of powder on her
suddenly shiny nose.
She took a steadying breath. Another. Ready or not, she
thought, and she unlocked the bedroom door.
He was right outside it, waiting for her.
‘Gorgeous’was the wrong word to describe him. ‘Spectacular’
came closer, but it still didn’t quite cover it.
Say something, Maria told herself, but her brain was numb.
She could only look at him as he stood leaning back against the
cypress balustrade that enclosed the open loft, arms folded,
ankles crossed, the very portrait of The Male Waiting for his
Date. He wore a grey jacket, a black open-necked shirt, black
SANDRA MARTON 105
trousers and darkest brown mocs. His hair was damp; he was
freshly shaven…
He was beautiful. The in-the-flesh subject of a woman’s
dreams, except she didn’t have dreams like those. Well, not until
after that night they’d made love. Correction. That night they’d
had sex, and look where that had led.
He said nothing. Showed nothing. Slowly, slowly enough to
make her wonder if the dress didn’t look as good as she thought,
his gaze traveled from the top of her head all the way to her toes,
then back up again.
That was when he smiled. A slow, lazy, purely masculine
curve of his lips that sent shock waves through her blood.
“Just one thing…” He reached out, took the clip from her hair
and let all the wild curls tumble to her shoulders. “Perfect,” he
said softly.
She had to stop herself from returning the compliment.
Instead, she tossed her head as if it meant nothing. Damned if
that didn’t make him grin.
“Shall we?” he said, holding out his hand.
Maria ignored the offer, brushed past him and went down
the stairs.
His car was a low-slung, snarling crimson beast.
A Maserati. A Lamborghini. A Ferrari. One of those, she was
certain, but what would a born-and-bred New Yorker know?
Subway trains, yes. Automobiles, no. The only certainty was that
he drove fast, too fast, with a macho assurance that she tried not
to let impress her.
But it did.
Was there a female alive who wouldn’t be impressed by a man
so beautiful it hurt to look at him, driving a car that rumbled like
a big, predatory animal? One hand was curved over the steering
wheel. The other rested lightly on the gear shift lever.
Such competent hands. So powerful. His hands had been all
over her the night they’d met. She could still feel them, if she
closed her eyes. His fingertips playing with her nipples. His
106 BILLIONAIRE PRINCE, PREGNANT MISTRESS
thumbs gently parting her labia. Her shocked cries that had
quickly turned to sobs of ecstasy.
She felt the instant bloom of warmth between her thighs.
“Something the matter?”
His voice startled her. She looked at him and thought it was a
good thing he didn’t have X-ray vision or he’d see straight through
her clothes, see that she was wet, that her nipples were peaked.
“Maria?”
I want you, she thought dizzily, that’s what’s the matter.
“Are you worried about dinner tonight?”
No, she thought, on a faint wave of hysteria, not dinner.
“Don’t be. This is just my family.”
Dinner. She had to remember that. Hewas talking about dinner.
“Oh,” she said, and caught her bottom lip between her teeth.
Alex felt his muscles contract. Did she have to look so beautiful?
Did she have to worry her lip that way? Damn it, this was
not good. He should never have kissed her in the guesthouse.
He’d taken two cold showers before he got dressed and he was
still hard with wanting her.
What if he pulled the car over, took her in his arms and nipped
that sweet bottom lip himself? Just lightly enough to make her
moan and sigh and beg him…
“Family?” she said, and he blinked.
“Uh, yes. Family. My older brother, Sebastian. My baby
brother, Andreas. My sister Katarina—everyone calls her Kitty.
The only one missing will be Elissa. She’s in Paris.”
“So many people?”
The tip of her tongue slicked over that softly bitten, now undoubtedly
sensitive bottom lip. By the time they reached the
palace, he’d be completely out of his mind. When had this
woman assumed such power over him? It made him angry, and
his words were more harsh than he’d intended.
“Don’t tell me you’re nervous about meeting royalty, glyka
mou. After all, you did fine with me the first time out.”
She swung toward him.
“I told you, I didn’t know who you were.”
SANDRA MARTON 107
“Right. You just happened to meet me on the street and when
I suggested we go to bed, you said, hey, I have nothing else to
do, so why not?”
It hadn’t been like that and he knew it. She’d been sweetly
innocent; he’d seduced her with words, with caresses, with a need
unlike any he’d ever experienced in all his thirty-one years. Except,
it had all been a lie. She’d set him up. She had seduced him…
Hadn’t she?
“You know what, Alex?” she said, her voice shaking. “You’re
a real bastard!”
She was right. What was between them was personal and had
nothing to do with this evening’s gathering. Tonight was about
plans for the national celebration of his mother’s birthday. Affairs
of state came before everything else, a truth that had always
been part of his life.
“Okay. Let’s start over. Ask me again about who’ll be at
dinner tonight.”
Maria stared straight ahead. Alex sighed in resignation.
“You need to knowthese things, glyka mou. Howelse to prepare
for the sight of Sebastian, who stands four feet tall and weighs three
hundred pounds? Or to know that Andreas is in The Guinness
Book ofWorld Records forWorst Footballer of theYear?”
She swung toward him, as he’d hoped she would. “What?”
Alex grinned. “Don’t panic. We still tease Andreas over the
time he missed six consecutive tries in a game—but we leave out
the fact that he was only five years old at the time. As for
Sebastian…” His grin broadened. “The truth is, except for a lack
of hair anyplace but his knuckles and back, he’s not bad-looking.
Well, he’s not as handsome as I am, of course…”
He couldn’t be.
Alexwas joking, Maria knew. Still, what he’d said about being
handsome was true. He was, without question, the most beautiful
man she’d ever seen… And what did that have to do with
anything? He was still exactly what she’d called him. No-good,
self-centered and arrogant, and if she had not called him all those
names yet, she surely would before the evening ended.
108 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She sat back, folded her hands in her lap and told herself she’d
get through whatever lay ahead because she had no other choice.
The Ferrari paused before the high gates outside the palace. A
smartly uniformed soldier stepped from the guardhouse, approached,
looked in at Alex, shot straight as a ramrod and delivered
a perfect salute.
“Your Highness.”
“Stavros. It’s good to see you pretending to be a soldier again.”
Maria looked at Alex in surprise. The soldier, still saluting, went
on staring directly ahead. “Especially since we both know I can
out-run, out-shoot, out-anything you choose when we have the
chance to give it another try.”
The soldier’s lips twitched. “Your Highness is, as usual, full
of, ah, full of air. Sir.”
Alex laughed and returned the salute. “At ease, Stavros. Good
to see you back. The ankle’s okay?”
The soldier grinned. “It’s fine, sir. And your shoulder?”
“Good to go. You signed up for the next Games?”
“Absolutely, sir. And you?”
“Try and keep me away,” Alex said, smiling.
Another smart salute; the gates opened and they drove slowly
down a wide, tree-lined avenue toward the broad marble steps
that led to the front doors of the palace.
“You and that man know each other?” Maria said.
“For years. We went to nursery school together.” He smiled.
“My mother’s modernist ideas won out that time. My father
thought it was a mistake to educate me among what he tried not
to call the commoners.”
“But he didn’t—I mean, the way he addressed you—”
“What’s the problem, sweetheart? Disappointed to find out
some people don’t think of me as you do?”
He pulled up before the steps. A valet opened his door; another
did the same for Maria. Ahead, the enormous entry doors swung
open. To Maria’s surprise, she saw the world-famous King
Aegeus and Queen Tia in the doorway.
SANDRA MARTON 109
“They asked me to bring you in through the Grand Hall,”Alex
said softly as he moved around the car to stand beside her. “And
they’re greeting you themselves. We are not as formal as some
royal houses but still, this is an honor.” He offered her his arm.
“Take it,” he said softly, “and smile, or my parents will think you
hate me.And you don’t hate me, glyka mou.We both knowthat.”
“Wrong,” Maria said sweetly. “But why should I take it out
on them?”
She put her hand lightly on his arm, took a deep breath, and
let him lead her up the steps.
“…and so,” Princess Kitty said, “Alex convinced me that it was
my royal obligation to sneak into the butler’s pantry to find out
what our parents had bought us for Christmas—well, it wasn’t a
butler’s pantry anymore, of course, it was just this huge room we
store stuff in at the beach house at Kionia—has Alex told you
about Kionia? Oh, it’s this incredible stretch of land overlooking
the Strait of Poseidon that separates us from Calista, and our
house is big and old and beautiful, and it’s all very laid-back, you
know, I can go around dressed casually—”
“Sloppily, you mean,” Aegeus said, his tone harsh. “And
why are we boring our guest with talk of childish pranks done
years ago?”
The family dining room became silent. Kitty’s round, pretty
face turned crimson. “Of course. Ms. Santos, my apologies.”
“Oh, please, don’t apologize.” Without thinking, Maria
reached for the princess’s hand. “It’s lovely to hear stories like
that. My own childhood wasn’t as much fun. No brothers. No
sisters.” Suddenly, she realized that every eye was on her, and
that she was hanging onto Kitty’s hand as if it belonged to Sela
and not a princess. Flushed, she let go. “I mean—I mean, this
has been such a lovely evening…You’ve all been so—so—”
“It’s been our pleasure, Ms. Santos,” the queen said gently.
“Please, won’t you all call me Maria?”
“Maria.” Tia smiled. “I hope you intend to see some of our
island in the next few weeks.”
110 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Maria shot a glance at Alex, who was calmly drinking his
coffee. “If I have the chance.”
“I’m fascinated by the thought of such a slip of a girl designing
and making such a magnificent necklace. I understand
Alex has outfitted a workshop for you in his home on
the coast.”
“Yes.” This time, Maria didn’t dare look at Alex. “He has.”
“And is it to your liking?”
What was the sense in lying? “Very much so, Your Majesty.
In fact, it’s better equipped than my own place in New York.”
“Good. If you should need anything more—”
“Well, I do need something. A couple of things, actually…”
“Such as?”
“Some information, to start. I understand that King Christos
ordered the Stefani diamond, which had been the centerpiece of
the crown of Adamas, to be split in two.”
She could almost feel the sudden tension in the room.
“I fail to see why the history of Adamas should be under discussion,”
the king said stiffly.
Maria cleared her throat. “I don’t mean to pry, Your Majesty.
It’s only that knowing the history of the diamond will help me
in creating the necklace.”
“Nonsense. Gold and diamonds well help, not timeworn stories
about the Stefani diamond and the islands of Aristo and Calista.”
Silence. Then Maria felt Alex clasp her hand under cover of
the table and enfold it in his own.
“Maria is an artist, Father. Her creations are, in a sense, representations
of a life force—in this case, a celebration of
Mother’s birthday as well as the continuity of our people. She’s
simply trying to gain some understanding of our kingdom. Isn’t
that right, Maria?”
“Yes,” she said, staring at Alex, amazed he should instinctively
comprehend what most people did not. “Stories,
legends, history…those are some of the qualities my work is
meant to convey.”
“Well, our history isn’t very complicated,” Sebastian said
SANDRA MARTON 111
pleasantly. “The Kingdom of Adamas dates back to ancient
Rome and Greece.”
“Yes,” Maria said again. “Alex told me it did.”
“Aristo was the island from which the kingdom was ruled. It
grew wealthy on its trade routes with Greece, Turkey and Egypt,”
Andreas said. “Calista had—has—its diamond mines. Pink
diamonds. Very rare—but, of course, you know that.”
Alex squeezed her hand in reassurance.
“The Karedes family—our family—got fat and rich trading
those diamonds to Europe.” He smiled wryly. “As you can probably
imagine, the Calistans didn’t like that. My grandfather—”
“King Christos,” Maria said.
“Yes. He tried to alleviate the tension but it didn’t work, so
he announced that on his death, he’d leave Aristo to be ruled by
my father and Calista to be ruled by my father’s sister, Anya.”
“And the people accepted that?”
“What else could they do?” Sebastian said. “But Christos
always hoped for a reconciliation. Part of what he said, when he
made his decision public, was that he wished the two halves of
the Stefani diamond would someday be reunited and that when
they were, the island would also be reunited as one nation, the
nation of Adamas. We call it Christos’s Legacy.”
Kitty nodded. “But it hasn’t happened.”
“That must have been a difficult time for everyone.” Maria
looked at the king. “For you and your sister, especially, sir.”
“It’s all in the past,” Aegeus snapped. “And I fail to see a need
to go through it with a stranger.” He tossed his napkin on the
table. “You are to make a necklace for the queen, Ms. Santos,
not write our family’s history.”
“Just a minute,” Alex began, but Maria spoke first.
“My sole interest is in making the necklace as meaningful and
perfect a gift as possible, sir.” She sounded composed but Alex
recognized that distinct, don’t-screw-with-me lift of her chin. “I
regret that you don’t see it that way.”
Alex bit back a grin. His Maria had been nervous about dining
with royalty, but she sure as hell had the balls to stand up to
112 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Aegeus. His brothers were trying not to smile; his sister looked
as if she might fly from her seat, grab Maria and kiss her. The
queen gave a soft cough and covered her mouth with her napkin.
Aegeus looked as if he weren’t sure if he’d been insulted or
not. Finally he nodded, shoved back his chair and got to his feet.
The meal was over.
“Perhaps I overreacted, Ms. Santos. At any rate, a perfect gift
is my wish, too. And now, if you require nothing further—”
“Actually,” Maria said carefully, “actually, sir, I do.”
Aegeus stared at her. So did the others.You could push the king
just so far and then you had to stand back before the explosion.
“Do you, indeed?” he said coldly.
Maria looked determined but she was shaking. Enough, Alex
decided, and, despite all the eyes on her, he moved closer and
slipped his arm around her shoulders. At first, she stiffened.
Then he felt her lean into him.
“It’s—it’s a request, sir.” She took a deep breath. “I’d like to
see the Aristan crown.”
“You have seen it,” the king said coolly. “My people provided
you with photographs.”
“Photos aren’t the same as the real thing, Your Majesty.”
“Impossible. For security purposes, the crown is kept in the
royal vaults.”
“Surely, it can be taken out of the vaults, Father,” Alex said
quietly.
“There is no need.”
“Oh, but there is, sir,” Maria said quickly. “I need to be certain
the center diamond in the necklace, the big pink one, will be the
correct shade. A diamond’s true color can never be conveyed
through a photo, no matter how good the photo is.”
“You mentioned color in your proposal. That is the reason I
provided you with two large pink stones rather than one.” The
king’s lips twisted. “Surely you noticed that.”
“Of course, Your Majesty. And I very much appreciate that
thoughtful gesture.”
“Thoughtful, and expensive, Ms. Santos.”
SANDRA MARTON 113
“It was a generous thing to do, sir.” Maria drew a breath. “But
there are slight variations in the colors of the two pink diamonds.
That’s all the more reason for seeing the crown.”
“I refer you, again, to the photographs. I have been assured
that digital photos are quite accurate.”
“Not when it comes to color,” Maria said with quiet determination.
“Plus, I need to see, to touch the Aristan half of the
Stefani diamond.” She flushed. “Stones have a way of
speaking to those who work with them, sir. I know it may
sound strange—”
“Strange?” Aegeus snorted. “It would seem I was correct in
fearing this young woman might be a leftover flower child,” he
said to no one in particular, “instead of a jewelry maker.”
“Actually, Father,” Alex said coldly, “Maria is neither.” He
felt her body jerk against his. Deliberately, he drew her closer.
“She is an artist, and we are very fortunate she agreed to
create this piece.” His eyes met Aegeus’s. “I think you owe her
an apology.”
The king’s face turned red. No one spoke for what seemed an
eternity. Then the queen cleared her throat, stood and took her
husband’s arm.
“Aegeus, Alexandros. Please, let’s not spoil the wonderful
plans for my birthday celebration. I am so excited about this
necklace… Just think, Aegeus, the entire world will be watching
when you present it to me. The necklace should, indeed, be as
perfect as Ms. Santos can make it, should it not? It should glow
with the same light as the Aristan crown, especially since you’ll
be wearing the crown that night.”
Silence. A muscle knotted in Alex’s jaw. Then he nodded.
“Mother is right, Father. I’m sorry if I seemed rude, but I
spoke the truth. Maria’s talent will ensure that people everywhere
will talk of Aristo, its crown and the queen’s matching
necklace for years to come.”
The king stood as still as a statue. Then, at last, he jerked his
head in assent.
“I’ll make the arrangements. Ms. Santos, you shall have five
114 BILLIONAIRE PRINCE, PREGNANT MISTRESS
minutes with the crown and the Stefani diamond. Five minutes,
and not a second more. Is that clear?”
Maria stepped free of Alex’s encircling arm and made a
deep curtsy.
“It is, sir. And thank you. You won’t regret your decision.”
Aegeus looked at her.A shadow seemed to pass over his face.
“I hope not,” he said, and strode away.
They drove back to the mansion in silence.
The gates swung open; the Ferrari purred down the long drive.
When they reached the house, Alex turned off the engine, stepped
from the car, opened Maria’s door and thought what an amazing
woman she was.
Bright. Talented. Strong.
And lovely.
Incredibly lovely, in the moonlight.
She would be even more lovely in his bed.
Naked. Her eyes on his as he undressed. Her arms reaching
for him as he came to her and she would reach for him, he
would find a way to make her admit how much she wanted
him—and yet, at this moment, what he wanted most was to kiss
away the worried furrow between her eyes, the sad little downcurve
of her mouth.
He held out his hand. “We’re home,” he said softly.
She nodded, took his hand and stepped from the car.
“Your father will probably sendmeback to the States tomorrow.”
Alex smiled. “No danger of that,” he said as they walked to
the door. “He’s trapped. My mother, clever woman that she is,
reminded him that the world will be watching when she celebrates
her birthday.”
“Don’t try and make it sound as if I didn’t behave foolishly!”
“The word I’d use is ‘bravely.’”
“I don’t know what got into me. It’s just—”
“What got into you,” he said, turning her to him, “was all that
fiery passion you do your best to hide.”
“I don’t hide anything. I just—”
SANDRA MARTON 115
“And you do a pretty good job of it—until something comes
along and heats your blood.” He opened the door to the
sleeping house, then turned toward her again and caught a
cluster of silky curls in his fingers. “Tonight, it was the ridiculous
behavior of a king.”
“No. I mean, I only—”
“And your dedication to your art.”
“That’s—that’s nice of you to say, but I made everyone uncomfortable——”
“And me.” His voice roughened. He cupped the nape of her
neck, slipped his fingers into her hair and tilted her face to his.
“I heat your blood, agapimeni. As you heat mine.”
He bent his head and kissed her. It was a gentle kiss, the soft
whisper of his lips over hers, but it made him groan.
“Maria,” he said softly, and he felt her tremble. “Maria,” he
said again, and her arms rose, wound around his neck; she lifted
herself to him, sighed his name and when he kissed her again he
went deep. Deeper, letting the taste of her fill his senses, the feel
of her feed his soul…
And he knew, without question, that he could not, would not
hold her to the devil’s bargain they’d made.
Gently, he cupped her face and drew back. Her eyes opened
slowly; she looked up at him, her pupils dark and wide and
blurred with desire or perhaps with tears. It killed him that he
couldn’t tell the difference.
“It’s late,” he said. “Too late to discuss this tonight.” His gaze
fell to her lips. He longed to kiss her again but he wouldn’t. He
wouldn’t. He wouldn’t. “Can you find your way to your
bedroom alone?”
“But I thought— You said—”
“I know what I said.” He drew a ragged breath and then, to
hell with it, he kissed her. “I’m not a saint, Maria,” he whispered
against her mouth, “but it turns out that I’m not quite the bastard
we both thought.”
A sound that might have been a sob broke from her throat. “I
don’t understand, Alexandros. What is it you want from me?”
116 BILLIONAIRE PRINCE, PREGNANT MISTRESS
He shook his head, left her standing alone as he headed out
into the night.
He didn’t know what he wanted from her.
And that was the whole damned problem.
SANDRA MARTON 117
CHAPTER NINE
WHAT did a man do when he was obviously losing his sanity?
It had to be that because he sure as hell wasn’t into martyrdom,
Alex thought as he paced through the dark garden. Maybe
he deserved a medal. Better still, maybe he should get his head
checked by a shrink because right now, right now, instead of
burning with frustration, he could be bedding the woman he’d
brought across an ocean for expressly that purpose.
Maria had been his. His for the taking.
And he’d walked away.
“Idiot,” he said, kicking a stone out of the path.
Walked away, and for what reason? She’d been as ready for
sex as he was. She wasn’t an innocent. Nothing he’d have done
would have shocked her.
Alex glared at the house where a light still burned in his
bedroom window. He could be in the house, in that room in less
than a minute.
Forget it.
He’d made his decision. For tonight, anyway. Going back
would be an admission of weakness, never mind that he didn’t
really know what in hell he meant by that, except that he knew
it would be.
He needed sex, not Maria. That put things in perspective.
He was aroused. No problem. There were ways to deal with
it. Phone one of the numbers programmed into his cell phone.
SANDRA MARTON 119
There were half a dozen beautiful women who’d jump at the
chance to spend the night with him. Or he could drive back into
town. The bar at The Grand Hotel saw more than its share of
gorgeous women, tourists hoping for a little adventure.
Except, he didn’t want another woman, and wasn’t that a
laugh? He wanted Maria and he’d just walked away from her.
Alex kicked another stone and headed for his Ferrari.
He roared out of the gates, took the coast road at a speed that
sent him flying past the few startled drivers on the road at this
late hour. When he reached the point at which the already
narrow, winding road grew more treacherous, he floored the gas
pedal and the car careered through the turns like the thoroughbred
it was.
Maybe that would burn away the hunger thrumming
through his blood.
It didn’t.
Two hours later, he pulled through the gates of the mansion
again and skidded to a stop with Maria still in his head.
Images. Memories. Tastes and scents, all of them conspiring
against him. The softness of her skin. The honey of her mouth.
The texture of her uptilted nipples on his tongue. The scent of
her desire.
She was there, in his brain, and nothing could dislodge her.
Well, yeah. One thing could.
His body hardened like a fist.
Having her would do it. Stripping off her clothes. Baring her
body to his eyes. To his hands. His mouth. Clasping her wrists,
holding them high over her head so she had no choice but to let
him touch her everywhere until she wept with wanting him.
Then he’d sink into her. Deep, deep into her. He’d move
inside her until she screamed his name, until she came and
came and came…
A growl of anger, of desire, of something close to lunacy rose
in his throat. He crossed his hands on the steering wheel and
slammed his forehead against them. After a few minutes, he
stepped from the car and entered the house.
It was quiet. Dark. The furniture cast ominous black shadows
against the walls.
Alex’s mouth thinned as he stood in the entry foyer and stared
up at the second floor landing.
He was no knight in shining armor. He was a man who had
grown up in a world of privilege, a man who could have what he
wanted when he wanted it. Especially women. The more beautiful
they were, the more famous, the more they threw themselves
at his feet. They begged for his possession. Preened to ready
themselves for his taking, not like Maria who asked nothing of
him and had packed a suitcase full of jeans to wear in her role
as his mistress.
She looked beautiful in jeans.
And in that dress tonight, those sexy shoes, stuff he’d ordered
over the phone just figuring anything the color of emeralds would
be perfect against her dark hair and eyes…
When she’d opened that door, when he saw her… God, he’d
wanted to push her back inside the room, tumble her on the bed,
make love to her until she had no choice but to admit she’d
dreamed of this, ached for this, that she wanted him, only him…
He swung away from the staircase, marched through the silent
house to his study, poured himself a shot of brandy, slugged it
down and did what he’d been doing hours ago in the garden,
paced and paced and paced.
A sto diavolo! The hell with it! He was weary of the game. It
was time to end it.
He took the stairs two at a time, went down the hall, stopped
before the door to his bedroom, raised his fist to knock… Knock?
At his own damned door? Bad enough he’d showered and
dressed in his study, that he’d spent the last couple of hours
driving aimlessly through the night. He cursed, ripely and creatively,
grabbed the knob and turned it, ready to break the damned
door down if he had to.
It opened easily.
Maria wasn’t there. The emerald dress was crumpled on a
chair, the black stilettos were on the floor next to it.
120 BILLIONAIRE PRINCE, PREGNANT MISTRESS
The bed was untouched.
His anger vanished. Fear took its place.Wherewas she? Had she
left? Not likely. She’d have had to phone for a taxi, and a cabwould
not have been able to clear the gates without alerting Security.
What, then? Had she gone for a walk ? Alex’s mouth tightened.
She wouldn’t have done that, would she? Not at night. Not
when she didn’t know the complex layout of the gardens, the
density of the surrounding trees.
The way some of the pathways ended at vistas at the very edge
of the cliff.
No, he thought, forcing aside the ugly possibility. If she were
wandering the grounds, motion detectors would have picked her
up. Then where…?
The guesthouse!
Alex pounded down the stairs and out the door, walking fast,
running, really, his anger back and hotter than ever. Did she
think she could escape him? That he’d let her sleep there rather
than in his bed, where she belonged? Yes. There was a faint light
shining in the guesthouse window.
“Damn it, Maria,” he growled as he flung open the door, “if
you think I’m going to go on being a Boy Scout…”
The furious words died on his tongue.
She was huddled in a window seat, illuminated by the flickering
glow of candlelight. She wore jeans and an oversized
sweatshirt, her feet were tucked up under her and when she heard
his voice, she swung toward him, face pale, eyes huge and
stricken and glittering with tears.
“I’m sorry,” she said in a broken whisper. “I’m sorry for everything,
Alexandros. I should never have come here. I know
what I agreed to but I can’t do it, I can’t, I can’t.”
By then, he’d crossed the space between them and gathered
her into his arms.
“Don’t,” she said.
He ignored the plea, whispered to her in Greek the way he
might have whispered to a terrified child. He stroked her hair,
rocked her against him and she began to sob.
SANDRA MARTON 121
“I know I agreed to—to be your mistress, but I can’t do it.
Even if it means losing the commission. I can’t. I can’t. I really
thought I could but—”
“No. Of course, you can’t.” He drew her into his lap. “Shh,
glyka mou. I won’t hurt you. I could never hurt you. Please,
don’t cry.”
“I didn’t know who you were that night, Alexandros. I swear
it. I went with you because—because…I can’t explain it. I’d
never done anything like that before. I’d never even—I’d never
even—” She drew a ragged breath. “I know you won’t believe
me but—but I’d never been with a man before.”
Ah, dear Lord!
The sweet, sad little confession made him feel like a bastard—
and filled him with joy. He did believe her; the truth was, he’d
known it, deep within himself, all along. His beautiful Maria had
given him her innocence. Hell, he had taken it from her. And, of
course, she had not known who he was.
She was incapable of that kind of subterfuge.
Why hadn’t he believed her? How could he have been so
stupid? How could he have judged her by what he knew of other
women, the ones who’d tried to trap him with their lies? There
had been so many of them, starting with the Greek girl who’d
broken his heart when they’d both been kids. He’d been sure he
loved her and when she wept and trembled and told him he’d
stolen her virginity, he’d been ready to marry her—until he’d
caught her laughing with her friends at his gullibility. The Italian
debutante who said she’d die in sin if he didn’t take her as his
wife, except it turned out she’d already slept with half the young
princes in Europe. The German supermodel who’d accused him
of making her pregnant. Wiser than in the past, he’d demanded
a paternity test—and had not heard from her again.
But Maria was nothing like that. She was—she was Maria,
sweet and smart and brave, and he’d put her through hell.
“I’ll recommend someone good, someone excellent to take
my place making the necklace,” she said in a low voice . “You
can let him use my design—I owe your mother that—but—”
122 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Alex stopped the flow of words with a soft kiss.
“You owe no one anything, glyka mou. And why would I let
someone take your place?” Smiling, he thumbed a strand of
dark hair from her brow. “There is no one who could replace you,
sweetheart. Like your design, you are one of a kind.”
“But I just told you, I can’t—”
“Maria.” He framed her face with his hands. “I’m setting you
free of our agreement. You’ll stay here, create a necklace the
entire world will admire—but not because we’ve made love.” He
took a deep breath. “I want you, kardia mou. I want you so much
it hurts. But I would never take something you would not willingly
give.” His mouth twisted. “I did that to you once, and I will
never forgive myself for—”
She put her fingers lightly over his mouth.
“I gave myself to you that night, Alex. I wanted you.” She
swallowed, ran the tip of her tongue over her bottom lip. “I
want you now.”
Could a woman’s soft words make the universe tilt? “Sweetheart.
Do you know what you’re saying?”
She gave a watery little laugh. “I know exactly what I’m
saying. That’s why I can’t stay here. I want you, despite what you
think of me, and isn’t that terrible? To admit something that—
that strips me of what little pride I have left—”
He kissed her. “Hush,” he whispered.
“It’s the truth. If I had any pride, I wouldn’t have come to
Aristo with you. I wouldn’t have said I’d sleep with you.
Because—because it wasn’t only the commission, Alex, it was
being with you…”
He kissed her again. He meant the kiss to be gentle and that
was how it began but somehow her lips parted under his. The tip
of her tongue slipped into his mouth. And when she wound her
arms around his neck and dragged his face down to hers, he
reached blindly for one final bit of sanity.
“Maria,” he said against her mouth, “sweetheart, be sure. Be
very sure—”
“I’ve never been more sure of anything in my life.”
SANDRA MARTON 123
Alex groaned, swept her into his arms and carried her through
the moonlight to the bed.
This bed was not like his.
It was smaller. Simpler. It had been made from a centuriesold
olive tree and was covered in white cotton loomed in a nearby
village. It had an intrinsic, natural beauty all its own.
It was, Alex thought as he lay Maria across it, beautiful in the
same way as she, with a quiet strength and an elegance that came
from within.
“Alexandros,” she sighed, and raised her arms to him.
He went into her embrace and kissed her.
Two months ago, a lifetime ago, they had made love fiercely.
He had all but torn off her clothes in his frenzy to bury himself
inside her.
That had been sex.
Now…now, it was something more.
He kissed her again and again, until her lips were as soft as
rose petals and clung hungrily to his. He framed her face,
threaded his fingers into her hair, kissed her throat, nipped at the
tender flesh at the juncture of neck and shoulder and when she
moaned with pleasure, he could have sworn he felt his heart lift
in his chest.
Slowly, he sat her up. Drew her sweatshirt over her head and
discovered, to his delight, that she wore nothing beneath it or the
jeans that he tossed aside.
Naked, she was a moon-kissed offering to the gods.
Beautiful. Perfect. Exquisitely feminine.
Slowly, so slowly, his eyes on hers, he stroked the contours
of her body. Her breasts. Her belly. Her thighs. She sighed and
moaned and made the kinds of little sounds that told him, as much
as the sensual lift of her hips, that what hewas doing pleased her.
Still, he had to ask.
“Do you like this?” he whispered, sucking a beaded nipple
deep into the heat of his mouth. “This?” he said, kissing his way
from breast to belly. “This?” he said softly, dropping a kiss on
the soft curls at the juncture of her thighs.
124 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Alexandros,” she said, “oh God, Alexandros…”
Gently, he parted her thighs. Put his hands under her bottom.
Lifted her to him, put his mouth to the delicate cleft of her flesh,
found her with his mouth, his tongue, and her scream of joy shattered
the night.
It was almost too much for him.
He was so close to the edge. All these weeks of wanting her.
And, though it seemed crazy, all the years ofwanting her, as well.
“Alexandros,” she whispered, and he kissed her mouth, her
throat and knew that he, like Paris when he stole Helen centuries
before, had not been able to obey the rules of the civilized
world. This was what he had wanted, this woman, this lover, and
he had done whatever it took to have her.
He would have given everything for this, the honeyed taste of
her mouth. This, the sweetness of her nipples. This, the indentation
of her navel. This, the curve of her hips.
This, he thought, just this, holding her, tasting her, watching
her face as he caressed her. As he again parted the delicate petals
that protected her clitoris.
He kissed her there again. Licked her until she came again.
This time, when she cried out, she reached for him.
“Please, Alexandros,” she said, “please. Come into me.”
Quickly, he tore off his clothes. Came back to her, swore,
reached for his discarded jacket, dug into the inside pocket and
prayed he’d find a condom. He did, and he tore the little packet
open, rolled the condom on. He’d forgotten to use one the first time
he’d made love to her; he’d been too hungry, too out of control.
He was almost out of control now. That was what happened
to him, when he was with her.
He moved up her body, took her in his arms, kissed her, let
her taste the proof of their passion in his kiss. Her hands were
on him now, cool against his skin. She stroked her palms along
his shoulders, his chest, down his belly and when one hand
moved lower and almost closed around his hard length, his breath
hissed between his teeth.
“Maria,” he said in a warning whisper. “Maria, glyka mou…”
SANDRA MARTON 125
126 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She caressed him anyway, her hand moving, moving up and
down over his swollen sex, and he groaned, caught her hand and
stilled it and knew he could wait no longer.
“Look at me, agapi mou.Watch me as I make you mine.”
Her lashes lifted. Her eyes met his. He clasped both her hands.
Laced their fingers together. Held their hands to the sides and
thrust into her.
She came instantly, her body arching to his, her cries of
abandon rising into the night and still he eased forward. Deeper.
Deeper until there was no way to know where he began and she
ended, until their flesh, their souls, were one.
“Maria,” he said, “Maria, kardia mou, agapi mou…”
She wept and kissed his mouth, and as the muscles of her
womb contracted rhythmically around him, Alexandros threw
back his head and emptied himself into the sweet warmth of the
woman who now belonged to him.
To me, he thought fiercely. Only to me.
CHAPTER TEN
FOR a long moment, the world stood still.
Alex’s powerful body was sprawled over Maria’s, his face
buried in her throat, their hearts still racing, skin damp with the
commingled sweat of their passion. The night breeze, drifting
across them from the still-open door, was chill. But when Alex
began to ease away, Maria tightened her arms around him.
“Don’t go,” she murmured, and felt his lips curve in a smile.
“I’m not going very far.”
He reached for the throw at the foot of the bed, wrapped it
around them, rolled to his side and gathered her close in his arms.
“Are you all right?”
It was her turn to smile. “Yes.”
“You sure?””
“Very sure. I’m fine. I’m perfect. I’m—”
“Yes,” he said, laughing softly as he kissed her, “you are.”
Tenderly, he brushed back the tangled curls on her forehead.
“Forgive me, agapi mou.”
“For what?”
“For not making love to you this way the first time.”
She shook her head, lay her hand against his cheek. “That first
time was wonderful.”
A very male smile lit his face. “Thank you. But you were a
virgin. I should have gone slower.”
“You didn’t know.”
128 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“I should have.” He turned his face into her hand and kissed
the palm. “There was such a sweetness to you, glyka mou. Such
an innocence. The way you touched me. Responded to me.” His
hand slipped down her body, cupping her breast, then the curve
of her hip. “I’ve relived those moments a hundred times,” he said
huskily. “The feel of you.Your little cries. The way you blushed
when I undressed you.” His mouth twisted. “The way I ruined it
all with my terrible accusations.”
Maria put a finger over his lips. “Didn’t some wise man once
say that the past is best left in the past?”
Alex drew her fingertip into the heat of his mouth. “Do you
forgive me?”
“Forgive you for what?” she said, with a little smile. “I don’t
know what you’re talking about, Alexandros.”
His eyes darkened. “I love how you say my name.”
“Alexandros,” she sighed, “Alexandros, Alexandros,
Alexandros…”
Just that—the sound of her voice, the feel of her against him—
and he felt himself turning hard. “Maria,” he said, “my Maria,”
and then he was inside her again, deep inside her, and the night
enfolded them in its magical embrace.
Just before sunrise, when the grass glittered with dew, they
dressed and made their way to the house.
“Someone will see us,” Maria hissed as Alex drew her inside.
“Who could possibly see us?”
“Spoken like a true potentate,” she said, laughing up at him.
“What aboutAthenia? The cook? The maids? The rest of the staff?”
Alex swung her into his arms and carried her to his room.
“The souls of discretion, I promise.”
Well, of course. They would be. Maria’s smile dimmed just
a little. No point in being foolish about this. Other women would
have slept in Alex’s bed…
“No.”
She looked up. Alex was watching her and smiling.
“No, what?”
SANDRA MARTON 129
“No other women, sweetheart. Not here.” He could see that
she was surprised. And pleased. Crazy as it seemed, so was he.
He set her on her feet, gently pushed her back against the
closed bedroom door and framed her face with his hands. “Just
you. Which means,” he said solemnly, but with a glint of
laughter in his eyes, “we’re going to have to celebrate the
occasion. Initiate my bed properly.” He bent his head, brushed
her lips with his. “Champagne. Candles. Rose petals. How
does that sound?”
Could he feel her heart racing? Could he possibly know what
was in that racing heart, the emotions that she had spent the past
two months, the past two days trying her best to deny?
“It sounds wonderful.” She moved, just a little, enough so she
was pressed against him. “But won’t it take an awfully long time
to get all those things together?”
She saw the change sweep through him. The narrowed mouth.
The tic of a muscle in his jaw. The hint of exciting male passion
that seemed to make the beautiful structure of his face even more
pronounced.
“Maria,” he said thickly, “Dear God, Maria…”
They didn’t make it to the bed. Not then. But they did the next
time, and the next, where they made love until the Aristan sun
blazed bright and hot in the perfect blue of the sky.
When he awoke, the space beside him was empty. He sat up, the
covers falling to his waist.
“Maria?” Naked, he padded to the bathroom. The door was
locked; he heard the sound of running water and then nothing.
“Maria?” he said again, and knocked.
“I’m fine,” she called, but the weak sound of her voice was
evidence of the lie. His heart turned over. She’d been sick before,
sick again, and now… “Maria? Open the door. Please.”
There was a silence. Then he heard the lock turn. The door
swung open and he saw his Maria, standing at the sink with a
toothbrush in her hand, looking at him in the mirror. She smiled,
but her face was pale and sweaty.
“Kardia mou,” Alex said urgently, stepping behind her and
encircling her with his arms, “were you ill again?”
She nodded. “A little.”
“Maria, this has happened too often.”
“It’s just flu, Alexandros,” she said, forcing another smile.
“New York’s loaded with it.”
“This is not flu. I had flu last winter. Andreas had it, too. With
flu you’re sick and then you get better. But you—you’re not
getting better.”
“I am. Much better.”
“I will take you to my doctor.”
“Don’t be silly.”
“He will examine you, glyka mou, and prescribe an antibiotic.”
“Antibiotics don’t work against viruses, and flu is a virus.”
“Such logic,” Alex said, trying to sound angry when what he
felt was fear. She was so pale, her eyes so dark… “Come here,”
he said, and turned her and drew her close. “I don’t want you to
be sick, sweetheart. Let me take care of you.”
“I’m fine. Honestly.”
“Thee mou, you’re a stubborn woman! Very well. No doctor.”
He swung her up in his arms. “At least, come back to bed and
rest for a little while.”
He carried her to the bed and lay down with her in his arms.
Kissed her tenderly. Stroked her back. And, inevitably, as he held
her, as his body heated hers, as she burrowed against him, tenderness
gave way to desire.
“Are you sure you’re okay?” he whispered as he touched her.
“Shall I stop?”
“Don’t stop,” she whispered back, “don’t ever stop.”
And he didn’t.
She was gone again, the next time he awoke.
A knot of apprehension formed in his belly but the bathroom
door stood open and the room was empty.
He showered quickly, pulled on jeans, a white T-shirt and
mocs, and went downstairs. He could hear the radio playing
130 BILLIONAIRE PRINCE, PREGNANT MISTRESS
softly in the kitchen, turned to Athenia’s favorite music station.
She smiled at him.
“Kalimera, sir.”
“Have you seen Miss Santos?”
“Oh, yes, perhaps an hour ago. She had coffee and—”
“She was all right?”
His housekeeper raised her eyebrows. “Fine, sir. She went to
the guesthouse. To her workshop, I mean.”
The workshop. Alex ran a hand through his hair. “Of course,”
he said sheepishly.
He found her there, perched on a high stool at a workbench.
She was wearing jeans and a blue chambray shirt with the
sleeves rolled up. She’d pulled her hair back in a ponytail; her
feet were bare, one on the rung of the stool, one on the floor.
She was bent over a sketchpad, intensity in every line of her
body, and humming something he couldn’t identify other than
to be sure the tune was almost painfully off-key.
He smiled, came up behind her quietly and slipped his arms
around her.
“Kalimera, kardoula mou,” he said softly, and kissed the nape
of her neck.
She sank back against him, her head against his shoulder, her
hands covering his.
“Kalimera, Alexandros,” she said, and turned her face to his
for a kiss.
“Mmm,” he said. She tasted wonderful, of coffee and of
herself. “I missed you.”
She laughed. “I’m glad to hear it.”
Alex grinned and turned her in his arms. “Then, why were you
in such a hurry to leave my bed?”
“Oh, I wasn’t in a hurry at all!” She blushed. “I mean—”
“Such a nice compliment, glyka mou. No need to explain it
away.”
Maria linked her hands behind his neck. “I woke up and
thought of a small change I want to make in your mother’s
necklace. Nothing that will alter the design,” she said hastily,
SANDRA MARTON 131
“just a modification in the way I planned to position the central
stone. I promise, she’ll still like it.”
“She’ll love it, Maria. She thinks the design, your work, all
the pictures you sent, are brilliant.”
Her face glowed with pleasure. “I’m so glad, Alexandros! This
commission means the world to me.”
His gaze fell to her lips. “What else means the world to you?”
he said huskily.
His hands slid under her shirt, cupped her breasts. Her
breath caught; he watched her eyes turn from hazel to coffeebrown
to ink-black.
“This,” she said, covering his hands with hers, “oh, this,
this, this…”
He carried her to the bed. And as he came down beside her
and kissed her, as they undressed each other, as she kissed his
mouth, his throat, his chest, his belly and, at last, touched the tip
of her tongue to the silk-over-steel power of his erection, she
knew that what she’d just told him was only partly true.
This—touching him, kissing him, sharing his passion—did
mean the world, but only because—because…
Because she loved him.
They had brunch, what Athenia referred to as a kolatsio, a
snack, on a terrace overlooking the water. Thick, sweet Greek
coffee. Olives. Feta cheese. Slices of warm, delicious bread and
a tray of sweet cheese pastries that Alex said were called
kalitsounia kritis.
They should have been called heavenly. The pastries were delicious
and decadent and surely fattening but Maria didn’t care.
She would not worry about anything this morning, not when life
was so perfect. Even the day was perfect. Bright. Sunny. Warm.
Unusual for the time of year, Alexandros said, and nothing like
the weather they’d left behind in New York.
The truth was, nothing was like what she’d left in New York.
Not this beautiful place. And not this wonderful, gorgeous, sexy,
strong, funny, caring, intelligent man.
132 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Now, Maria, Sister Sarah would have cautioned, that’s far too
many adjectives.
Yes, Maria thought, but Sister had never met Alex.
He was seated across from her, talking about his house. He
loved it; she could see that in his animated face. He was proud
of it; she could hear that in his voice. How did you come to find
such a perfect house? she’d asked, and he’d said, with a boyish
grin, that he hadn’t found it, he’d built it.
And he had.
He’d worked along with the architect. With the builder. With
the carpenters. He’d wanted a house that blended into its surroundings,
that was spare and strong and unique.
“Like these cliffs,” he said.
Like you, she thought.
He told her that he’d lived in the palace until he’d gone away
to boarding school and then university, and, though he loved its
history and elegance, it had never felt like home. So, once he had
his MBA, he’d bought a condo in Ellos and another in New
York. Then, one weekend at the family compound overlooking
the turbulent waters that separated Aristo and Calista, the Strait
of Poseidon that Kitty had mentioned at dinner, it had suddenly
hit him that what he wanted was a place of his own, overlooking
the sea.
“I’d always loved driving along these cliffs so it seemed
natural to call a friend, a realtor, inquire about property, then
bring another friend, an architect, to see what he might suggest,
and—” Alex laughed. “Look at you, kardoula mou. Your beautiful
eyes are glazing over, thanks to my endless talk about
myself.” He reached for her hands, lifted them to his lips and
kissed them. “What I really want to talk about is you.”
She smiled. “My life isn’t anywhere near as interesting. And
my eyes aren’t glazing over. I love learning things about you,
Alexandros.”
She did. Oh, she did! She’d gone from hating him to loving
him in what seemed a heartbeat but the truth was, she’d fallen in
love with him that first terrible night.
SANDRA MARTON 133
“Still, I won’t say another word until you tell me about
Maria Santos.”
“It’s a dull— Hey,” she said, laughing as Alex, in one fast
move, rose from his chair, tugged her into his arms and settled
into his chair again but this time with her in his lap.
“Okay, then,” he said, “I’ll tell you about her. Maria Santos
was born twenty-five years ago. She was the most beautiful baby
anyone had ever seen.”
Maria began to laugh. “Alex, that’s silly!”
“What?” he said, his eyes round with innocence. “You mean,
you’re not twenty-five? What are you, then? Forty-five? Fiftyfive?
My God, you can’t be sixty—”
“I was not the most beautiful baby anyone had ever seen.”
“I’ll bet you were.”
“I was premature. Tiny. Skinny. Almost bald.”
“Beautiful,” Alex said, grinning, “just as I said.”
Maria rolled her eyes. “You’re crazy, Alexandros.”
“Crazy about you,” he said softly.
Could your heart really sing? She’d never heard such thrilling
words. Her prince. Her lover. HerAlexandroswas crazy about her.
“And I want to know all about you.”
That was wonderful, too. No one had ever wanted to know all
about her, not once in her entire life. Smiling, she pressed her
lips lightly to his.
“Okay,” she said softly, “here’s the entire, unexciting tale. I
was born in the Bronx. I went to school in the Bronx. Public elementary
and middle schools, and high school at Saint Mary’s.
Then I went to college in—”
“The Bronx?” Alex said, and smiled.
“You guessed it. Lehman College. I studied—”
“Art.”
She sighed and lay her head against his shoulder. “I studied
business. Mama’s idea, and I hated it. When everybody was
studying Word and Excel, I sketched. Back then, before I discovered
I loved working with metal and stones, I thought I wanted
to design clothes. Anyway, I stuck it out for a year. Then I did
134 BILLIONAIRE PRINCE, PREGNANT MISTRESS
what I had to do. I worked up a portfolio, arranged for an interview
at FIT—the Fashion Institute of Technology. They accepted
me, I made all the arrangements for a student loan.” She took a
breath. “Then I broke the news to Mama. I told her how hard it
was to get into FIT, I showed her my portfolio, and she said—”
“She said you had amazing talent, and that you’d be the
next—who’s that New York designer? Donna Karan?”
Maria smiled, but her smile trembled. “She said I was a
foolish girl with silly dreams.”
Alex’s arms tightened around her. “Ah, sweetheart, I’m sorry.
I should have figured…I mean, the other night—”
“No, it’s okay. Maybe it’ll help you understand why—why
she acted the way she did when you met her.” She took a deep
breath. “See, my mother never finished high school. She went to
work when she was sixteen, operating a sewing machine in the
garment center. She was determined I would not do the same,
and I couldn’t make her see that I wouldn’t end up that way.”
“And your father?”
“What about him?” she said, with a nonchalance as transparent
as glass. “He owned the company where Mama worked. He
was rich. He had a house on Long Island. He had a big car.” She
cleared her throat. “He also had a wife and kids.”
“And your poor mother had no idea…” Alex said tightly.
“She had every idea.” Maria’s voice turned brittle. “He said
he’d leave his wife and marry her—but, of course, he didn’t. And
then, when she told him she was pregnant with me, he said she
was lying. When he realized it was the truth, he gave her some
money. For an abortion, he said. But she didn’t have an abortion,
she had me instead, and he said that had been her decision, a bad
decision, and then he fired her and she never saw him again.”
Alex had gone very still. Maria bit back a groan. Whatever
had possessed her to tell him all that? She could have just told
him the first part. School. College. FIT. But the rest…Why had
she unloaded that sad, dumb story on him? She never talked
about her life. Never. Joaquin knew, but they’d grown up together.
Sela knew, but she was her best friend. No one else knew that
SANDRA MARTON 135
she was a bastard and yes, that was the right word. It was an oldfashioned
word in lots of places but in Maria’s world, the world
her mother had created and in which she had raised her, the word
still carried the smear of disgrace and dishonor.
Stupid, she told herself fiercely, how incredibly stupid, to tell
such ugly things to a man who might as well have been born and
raised on another planet.
“Well,” she said brightly, “so much for Tales from the Bronx.”
She sat forward in Alex’s lap. “This has been a lovely break,
Alex, but I have to get to work and—”
“Has he never tried to see you?”
“Who?” she said, even more brightly. “Oh, my father? No.
Why would he? I didn’t need anything from him. I wouldn’t take
anything, even if he—”
“How could a man turn his back on the woman who carried
his child? On the child herself?”
“Well, I don’t know, but—”
Alex turned her face to his, cupped it with his hands and
kissed her.
“You’re a strong, brave woman, kardia mou,” he said softly.
“And I am honored to have become your lover.”
They fell into an easy pattern, like lovers who had been together
a long time.
Not that what happened in bed lost its excitement.
It couldn’t, not when the sight of Alex coming toward her sent
Maria’s pulse skittering, not when Maria’s smile was enough to
fill Alex with such hunger that there were times he had to turn
away to keep from sweeping her into his arms and making love
to her wherever they happened to be.
He didn’t always turn away.
He made love to her in the workshop. In the garden. In the
back of the limo with the privacy screen up, bringing her to
climax with his hand high under her skirt, his mouth hot on hers.
And he made love to her in bed. The demure bed in the workshop;
the big, beautiful one in his room. They made love, and talked
136 BILLIONAIRE PRINCE, PREGNANT MISTRESS
and laughed, and worked—she in her workshop, he in his study
at the house. And they discovered all the things they needed to
know about each other.
Maria no longer felt ill. The early morning nausea was a thing
of the past. There were times she still felt exhausted but flu often
left you feeling tired; everyone said so.
The only dark moments came when she remembered that her
days with Alex were slipping away. The necklace was almost
finished, the big birthday celebration loomed on the horizon. A
week passed, then another, and the final week of her stay began.
When it ended, there would be nothing to keep her here.
Unless Alex asked her to stay. And she, who had spent her
life avoiding relationships, who had never imagined repeating
her mother’s foolish involvement with a man who was all
wrong for her…
She knew she would stay, if Alexandros asked her.
But he didn’t. Why would he? How would he? He was a
prince while she—she was a girl born into illegitimacy and raised
in poverty. She could have a place in Alex’s bed but she would
never have one in his life.
So she concentrated on completing the necklace until, finally,
she had only to set one of the fabulous pink stones in its center,
but she had to see the Crown of Aristo before she could do that.
The king kept making appointments for that to happen, then
cancelling them.
On a rainy afternoon just days before the queen’s birthday
party, Maria decided this couldn’t go on. Alex had a meeting in
Ellos. After he was gone, she phoned the palace, left a polite
message with Aegeus’s personal secretary. She had to see the
crown today, she said, or the queen’s gift might not be as perfect
as the king and she both wished.
She hung up the phone and was suddenly overwhelmed by
nausea. It took her by surprise. Apparently, she wasn’t over the
flu quite yet.
She barely made it to the bathroom, where shewas horribly sick.
When the spasms finally ended, she flushed the toilet, brushed
SANDRA MARTON 137
138 BILLIONAIRE PRINCE, PREGNANT MISTRESS
her teeth, rinsed her mouth and started for the bedroom when a
shocking wave of vertigo swept over her.
Maria stumbled and fell against the door jamb. The collision
wasn’t particularly hard but the impact was painful and hurt her
breasts. They’d grown so tender lately; even making love with
Alex, there were times the touch of his mouth on her nipples
came close to being painful…
Oh God!
Tender breasts. Nausea that seemed to have no basis. And, she
thought, biting back a moan, and a period that had not come
in…in, what? Two months? Three?
“No,” she whispered, “please, please, no…”
The phone rang. She tried to ignore it but the ringing went on
and on…
“Hello?”
It was the king’s secretary. She would be permitted to see the
crown an hour from now.
“I can’t,” Maria said, trembling as she counted back, again and
again, to the last time she’d menstruated. “How about this afternoon?
Or this evening?”
“One hour, Ms. Santos,” a commanding voice barked through
the phone, “or not at all.”
It was the king himself, and she knew he meant it.
“I’ll be there, Your Majesty,” she whispered.
She—and the illegitimate royal baby she now realized lay
cradled in her womb.
CHAPTER ELEVEN
MARIA showered quickly and dried her hair while trying not to
think about anything but the meeting with the king…
Impossible, she thought as she dropped onto the edge of the bed.
How could she be pregnant? Alex had used condoms every
time they’d made love.Well, not the very first time, almost three
months ago. Things had happened so quickly that night…
After, he’d said, “You have the right to know that I have no
diseases,” and she, embarrassed by the conversation, had thought
of telling him that she couldn’t possibly have any because she’d
never been with a man until him, but it had been easier to say
that she had none, either.
“I assume you’re on the pill,” he’d added.
Well, of course, shewasn’t. But it had been the safe time of her
cycle so she’d just nodded instead of answering and left it at that.
Maria groaned and buried her face in her hands. She thought
of all the times she’d silently wondered how her mother could
have made so many devastating mistakes. Now, she knew one
answer was that making mistakes when you were swept away
by passion was pathetically easy.
Andall the signs she’d ignored! Nausea.Wooziness. Exhaustion.
Not getting her period. That should have been the most damning
evidence of all, but she’d never been completely regular…
Maybe you didn’t see what you didn’t want to see. Maybe it
was just that simple.
140 BILLIONAIRE PRINCE, PREGNANT MISTRESS
She wanted to weep. To scream. To bang her fists against
the wall. That she, of all women, should have tumbled into the
age-old trap…
She knotted her hands in her lap. Took deep, calming breaths.
Hysteriawouldn’t change anything. Besides, therewas no time for
this now. The necklace. The summons from the king. Those things
came first. She had responsibilities. To the queen. To herself.
She dressed quickly. Black trousers, a black cashmere sweater
and, over it, a pale pink jacket. Black heels. Alex had followed
his gift of the emerald silk dress with what seemed like an endless
wardrobe, ordered from the pricey designer boutiques in Ellos
and delivered to the mansion.
“I can’t let you buy me things like this,” she’d protested, and
he’d kissed her to silence.
“I love giving you gifts, agapi mou,” he’d said, and because
she’d known he meant it, because nothing had turned out as she’d
anticipated and instead of only being in Alex’s bed she was also
in his life, she’d accepted the clothes and wore them when they
went out to dinner, to the theater, to the casino.
Now, the rack of expensive outfits in the dressing room—his
dressing room—was a mocking reminder that these were not
simply gifts, they were proof she was his mistress.
And mistresses did not get pregnant. They did not have babies.
They did not forge real, lasting relationships that led to a joint
future, especially with a man like Alex. A prince of the Royal
House of Karedes…
“Stop it,” she said sharply, and blanked her mind to everything
but the meeting with King Aegeus. She had worked years for this
kind of professional honor.
Right now, that was all that mattered.
Weeks before,Alex had arranged for his limo to be at her disposal.
“Don’t trust me to drive, hmm?” she’d said, laughing.
“Not on these roads, glyka mou,” he’d said, and she’d admitted
that was for the best. A born and bred New Yorker, she’d learned
to drive but she didn’t do it often or well.
SANDRA MARTON 141
Todaywould most certainly not have been a day to test her skill.
The driver had apparently been told where to take her. He
drove through the palace gates to a rear courtyard where an
equerry greeted her and led her through a maze of corridors to a
half-opened door in a gloomy alcove.
“Ms. Santos, sir.”
The king was seated at a small table, a velvet drawstring bag
before him. A chair was drawn up opposite his. Maria blinked.
Were these the royal vaults? She’d expected something different.
Bright lights. Security cameras. Guards. Not a small, plain,
ill-lit room.
“Your Majesty,” she began, but Aegeus cut her off with an imperious
wave. Another wave dismissed the equerry. Aegeus
pointed at the other chair.
“Ms. Santos. Sit down.”
His tone was hard, a direct contrast to his looks. She was surprised
by his pallor and the throb of a vein in his forehead.
“Are you—are you all right, sir?”
Aegeus glared at her. “Are you a physician as well as my
son’s mistress? Oh, don’t look so shocked, Ms. Santos. I’m not
a fool. I am aware of everything that happens in my kingdom.”
“Then you are aware that I’m here as the designer of Queen
Tia’s birthday gift, sir, nothing else.”
She knew she’d overstepped the boundary between commoner
and king but the last thing she would let happen today was a discussion
of her relationship with Alex.
To her surprise,Aegeus laughed. “I can understand Alex’s infatuation.
A woman with beauty and intelligence and spirit…”
His smile tilted. “What man could resist such temptation?”
Maria drew a deep breath. “Your Majesty. The crown…”
Aegeus pushed the velvet bag into the center of the table but
kept his hand protectively on it.
“Normally, it is kept in a display case along with the Crown
Jewels and, of course, the original Crown of Adamas.”
“Yes, sir. To tell the truth, I’m surprised that—”
“Your surprise does not interest me, Ms. Santos. I’ve arranged
to meet you here so we could keep things as simple and private
as possible.” He raised his wrist, pointedly looked at his watch
and then at her. “Five minutes. Then your time is up.”
Maria nodded and reached for the bag. The vein in the king’s
forehead seemed to leap as he lifted his hand and sat back.
She loosened the drawstring and lifted the Crown of Aristo
from it.
Her breath caught. The crown was magnificent.
Brilliant white diamonds shone like star fire even in the dim
light. Yes, she thought happily, yes, they’d match the ones in the
necklace perfectly, but it was the fantastic center stone that
dazzled the eye. The half of the pure pink Stefani diamond King
Christos had bequeathed to his son, Aegeus, and to the kingdom
of Aristo, dominated the crown.
“Beautiful,” Maria said softly.
Aegeus nodded. “Yes,” he said brusquely, and reached for the
crown.
“Wait,” Maria said quickly, pulling it back.
He looked up. The vein in his forehead looked even darker
than before. “You forget yourself, Ms. Santos.”
“I meant…Wait. Please, Your Majesty.”
“For what? You’ve seen what you came to see.”
“Iwant a closer look at the pink diamond, sir.To check its shade
against…” Maria took a small silk bag from the leather tote she
always carried when handling gems. She opened it, and the pair
of pink diamonds, one of which would become the centerpiece of
Tia’s necklace, tumbled onto the table. “To check it against these.”
The king hardly looked at the stones. “Either will match. The
colors are the same.”
Maria shook her head. “Actually, they’re not.”
“Of course they are. And your five minutes are—”
But Maria had stopped listening. A trickle of ice water seemed
to slip down her spine. She looked at the single light bulb in its
overhead socket.
“Is there…?” She cleared her throat. “Is there a way to get
more light in this room, sir?”
“No.”
142 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Surely we could take the crown into the display room?”
“Surely we could not,” Aegeus said coldly. “And I repeat, your
time is— What are you doing?”
Maria’s hands were trembling but she tried to stay calm. She
put the pair of pink diamonds back in their bag, put the bag in
her tote and took out a small flashlight and a jeweler’s loupe.
Quickly, she put the loupe to her eye and turned on the flashlight.
“Ms. Santos!” The king’s voice was sharp. “The Aristan
diamond is priceless. I do not want you poking at it and picking
at it and—”
“It’s a fake!” Her words seemed to explode in the confines of
the small room. She looked up, horrified. “This half of the Stefani
diamond. What’s supposed to be the Stefani diamond. It isn’t a
diamond at all.”
The king’s already pale face went paper-white. He shot to his
feet. “Give me the crown!”
“Sir. The stone is a fake. Cubic zirconium. Or something else.
It’s an excellent forgery but…” God, she was shaking like a
woman with a high fever! “Your Majesty. I have some tools in
my workshop. I can do some tests but I am sure—”
“Give me the crown!”Aegeus roared. He snatched it from her
and stuffed it into the velvet bag, but suddenly his eyes grewwide
and his face lost what little color it had.A strangled sound broke
from his throat; he clapped his hand to his heart, the bag fell on
the table and the king tumbled back into his chair.
Maria leaped to her feet, ran to the door and flung it open.
“Help!” she shouted. “Please, someone help!The king’s collapsed!”
At once, the seemingly empty corridor swarmed with people.
Maria fell back against the wall. Someone scooped everything
from the table—the loupe, the flashlight and the velvet bag—
dumped them into her leather tote, thrust the tote at her, then
grasped her arm, hurried her out of the palace and to Alex’s
waiting limousine.
It wasn’t until she was back at the guesthouse that she realized
the crown of Aristo had gone home with her.
* * *
SANDRA MARTON 143
What did you do when you had absconded with a royal crown?
Maria sat at herworkbench, staring blindly at the crown as she
tried to come to grips with all that had happened today. The
wrenching realization that she was pregnant. The horrifying discovery
that the half of the Stefani diamond in the Aristan crown
was not a diamond at all and then, the king’s collapse.
Was Aegeus dead? Had her news killed him?
She’d phoned the palace. Useless. She had the private telephone
number of the queen’s personal secretary but reached only
her voice mail. Desperate for diversion, she’d filled the time
running tests on the pink stone in the crown, praying all the
while that her initial judgment had been wrong.
She’d done a dozen tests, everything from the silly—did the
stone fog when she blew on it?—to the absolutely, completely
scientific.
She’d used an electronic tool called a diamond tester. She’d
brought it out last, as if by holding off she could avoid the truth.
The tester had been one of the things she’d brought with her from
NewYork; she hadn’t even been aware she had it with her until now.
The thing was a complex piece of equipment but it was simple
to operate. Turn it on, touch the probe at one end of it to a stone.
If the stonewas a real diamond, a green light came on. If itwasn’t…
If it wasn’t, nothing happened.
Nothing had happened, half a dozen times.
Maybe the tester wasn’t working. That had been her hope.
So she’d touched the probe to every white diamond in the
crown. To the diamonds in Tia’s almost-completed necklace. To
the two big pink stones she’d taken to the palace.
The green light blinked on each time.
Then she’d touched it to the pink stone in the Aristan crown.
Please, she’d prayed, please let the green light come on.
It didn’t.
The stone was a brilliant, beautiful fake. It would fool
anybody. Anybody but an expert.
Still, maybe she was wrong. Maybe she’d missed something.
144 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Was there a test she’d forgotten? Anything was possible, she told
herself, and reached for the phone.
Far across the ocean, Joaquin answered on the first ring.
“Maria,” he said happily, on hearing her voice. “¿Como se va?
Sela was just saying—”
“Joaquin. I need your help.”
He was not a GG—a Graduate Gemologist—as she was. The
degree had cost her a small fortune; shewas still carrying the debt.
But his depth and breadth of knowledge was excellent, and she
knewshe could trust him with this devastating news. She told him
what she’d discovered. Described the tests she’d run. Their conversation
grew complex, touched on things like heating, magnification,
trigons and dodecahedral surfaces of octahedral crystal
formations and then, finally, she took a deep breath and told him
she’d used a diamond tester.And the stone had failed that final test.
“You’re sure the tester is working properly?”
“I checked the battery. And it gave a green light, literally, to
a couple of dozen diamonds, pink and white.”
“But not this one.”
“No,” Maria said, “not this one.”
Joaquin’s sigh drifted through the telephone. “I don’t envy you,
chica.You are about to be the bearer of very bad news for someone.”
She gave a sad little laugh. “I’m afraid I already am.”
“Call me if you need me. Sela says to tell you she can do
without me for a few days. We both love you, you know that.”
She smiled. “Doing without you, even for a few days, is impossible.
I love you, too, Joaquin. With all my—”
“How cozy.”
Maria spun around. Alexandros stood in the doorway, arms
folded, legs apart, face stony and cold.
“Alexandros! I didn’t hear you come in.”
“No. Obviously not. Don’t let me interrupt you, Maria. Not
when you’re in the middle of an obviously important call.”
She said a quick “goodbye” to Joaquin and hung up the phone.
Then she slid from the stool and went to her lover. He didn’tmove.
Didn’t smile. Didn’t react at all when she put her hand on his arm.
SANDRA MARTON 145
“Your father—”
“I know all about my father.”
“Is he—is he—”
“He’s in the hospital. He had a heart attack.” Alex’s mouth
narrowed. “Thanks to you.”
“I never meant—”
“What did you say to him? Did you perhaps tell him you were
sleeping with me even though you miss your lover in NewYork?”
His mouth twisted. “No. Why would you do that when you’ve
been so careful to hide that information from me?”
“Alexandros. Listen to me. I don’t have—”
“Liar!” He caught her by the shoulders, his hands rough on
her tender flesh, and drewher to her toes, just as he had after their
first night together. “It’s the same man you were talking to that
morning three months ago.” When she said nothing, his face
contorted. “Answer me, damn you! Admit it.”
“I was talking with Joaquin, yes. But—”
“Can’t you do without him for another few days?”
“Alexandros.” Her voice broke. “You’re wrong about him.”
“I was wrong about you, you mean.”
“I told you, Joaquin works for me. He’s married.”
“What would that matter to a woman like you?”
Maria felt the insult like a knife to the heart. She jerked free
of his hands, her face white, eyes glittering with tears.
“I don’t deserve that,” she whispered.
Yes, he thought, she did. She deserved that and more. He had
taken her to his bed. Held her in his arms as she slept. Awakened
her with his kisses. He had shared his life with her these past weeks.
Fool that he was, he’d come to—to care for her. To want her more
than he’d ever wanted anything or anyone in his entire life.
He’d even imagined—he’d stupidly imagined he might keep
her with him after her work here was over.
He turned away. Composed himself. She was just another
woman. More beautiful, perhaps. More intelligent. More fun. In
the end, though, she was the same as all the others. She was with
him because of who he was. Because of the power he wielded.
146 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Because hewas a prince, not a man. He had to remember that.
She was his mistress, bought and paid for. She was not his
lover; she was not in his bed of her own free will but because he
had demanded her presence there.
He took a breath and looked at her. “What did you say to my
father?” His voice was cold. “He was fine at the start of his
meeting with you.”
“He wasn’t. He looked ill.”
“Answer the question.What did you do to make him collapse?”
Maria stared at Alex. He was looking at her as he had not done
in weeks, as if he were an autocrat and she were his to command.
His expression radiated scorn—and maybe she deserved it. She’d
slept with a man who’d made it clear he wanted her sexually, not
any other way.
And she’d fallen in love with him.
Wasn’t she repeating a pattern for which she’d vilified her
mother—up to and including the shame of becoming pregnant?
God. Oh, God! How had she let this happen? And what would
she do next? End the pregnancy? Have the baby? Raise it, alone,
as her mother had raised her? Because it was only in fairy tales
that the handsome prince married the beautiful commoner and
lived with her happily ever after.
“I’m waiting, Maria.”
She wrapped her arms around herself. Lifted her chin. Forced
herself to meet Alex’s steely gaze without flinching.
“I told him something about the Aristan crown.”
“And?”
“And, it upset him. I’m sorry for that but I was—I was
shocked myself and I just blurted it out—”
“Blurted out what? I’m not in the mood for games, Maria.”
Maria swallowed dryly. Her lover was a prince of the house
of Karedes. He had to know the truth.
“I told him—I told him the half of the Stefani diamond in the
crown wasn’t real.”
For a long moment, nothing happened. Then Alex laughed.
The sound startled her.
SANDRA MARTON 147
“You told the king that a stone that is beyond price wasn’t
real?” His laughter ended as quickly as it had begun. “And he
called you, what? A liar? A fool? An idiot? Or all three?”
“It isn’t real,” Maria said quietly. “It’s an excellent fake—but
a fake, nevertheless.”
Alex’s face darkened. “He should have had you thrown out
for spouting such nonsense!”
“Listen to me, Alex.”
“No, glyka mou, you listen to me! I don’t know what you
thought to accomplish with such a lie but—”
“The stone’s counterfeit!” Maria grabbed the crown from the
workbench along with a stack of papers and shoved all of it into
his hands. “Here’s the crown.”
“You took it out of the palace?—”
“I didn’t plan to take it but… Never mind the details. Look at
my notes. I ran endless tests. That’s why I phoned Joaquin, to see
if maybe, just maybe therewas something I missed. Therewasn’t.
The Aristan pink diamond isn’t a diamond at all!”
Alex stared at her. Then he began leafing through the papers,
quickly at first and then, as he began to absorb what she’d written,
more and more slowly. He stared at the crown. At the papers.
Finally, he looked up.
“I don’t understand. Who would have done this? And how?
The crown’s been in the vault for years.”
Maria spread her hands. “I have no answers. I only know the
diamond is a fake.”
Alex put down the papers and the crown, and ran a hand
through his hair. “You’re sure? There’s no possibility of error?”
“I’m a Graduate Gemologist,” she said softly. “I’ve appraised
lots and lots of diamonds. I even did some work with an insurance
company that involved fraud and a diamond that would
have been worth millions, had it been real.” She paused. “I
phoned Joaquin because he’s knowledgeable. And completely
trustworthy.”
Alex’s mouth thinned. “I’ll just bet he is.”
“Damn it, stop being a fool! I phoned him that morning in
148 BILLIONAIRE PRINCE, PREGNANT MISTRESS
your apartment to tell him I thought I had a good shot at winning
the competition because I knew how much it mattered to him
and his wife. And I called him now to pick his brain. He’s not
my lover. He never has been. He’s married to my best friend and
he’s my friend, too. I can trust him to keep quiet about this—or
am I wrong, thinking you don’t want the citizens of Aristo
learning the truth about the diamond from the front pages of the
world’s newspapers?”
A muscle flickered in Alex’s jaw. She was right about the need
for discretion. The diamond was priceless, not only as a stone
but as a symbol. No Aristan ruler could ever be crowned without
it. There was also his grandfather’s pronouncement, what
everyone called King Christos’s Legacy, the pledge that both
halves of the Stefani diamond would have to be joined together
in the crown of Adamas if ever the two kingdoms, Calista and
Aristo, were to be reunited.
And then, he thought, looking at Maria, then there was
Maria herself.
Maria, who had brought him the kind of joy he’d never
expected to find. What he’d told himself a little while ago was a
lie. She’d come to Aristo because he’d forced her to do so, yes,
but that was in the past. She was here now because it was what
they both wanted. He couldn’t imagine ending the day without
sharing a few quiet moments with her as they stood in each
other’s arms, watching the sun set over his beloved island.
Couldn’t imagine opening his eyes in the morning and not
finding her in his arms again.
As for Joaquin… He’d seen his Maria with him but, really, what
had he seen?A man putting his arms around a woman.A kiss, but
were either the embrace or the kiss those of lovers? Had she lifted
her face to Joaquin as she lifted hers to him? Had she drawn
Joaquin’s head down to hers? Had anything about that kiss held
the heat, the power of what happened between his Maria and him?
God, hewas a fool.Accusing her of things he knew, in his heart,
were not true. Things she would never do. He had to tell her what
she’d come to mean to him, that he didn’twant her to leave him…
SANDRA MARTON 149
“I only wish,” she said brokenly, “I just wish I’d broken the
news to your father differently. Perhaps, if I had—”
“It’s all right, glyka mou.”
“No. It isn’t. I upset him. His heart—”
“His heart is undamaged. He’s at the hospital, yes, but he’s
awake and alert.”
“Thank God,” she said, and then she began to weep.
“Ah, sweetheart.”Alex gathered her into his arms. “Don’t cry.”
She wept harder, her face pressed against his shoulder. He
could feel her tears dampening his shirt.
“Forgive me for blaming you for what happened. Discovering
the stone is fake must have been terrible.”
Maria lifted her face to his. “It was horrible. Horrible, Alexandros!
I couldn’t believe it. That was why I called Joaquin—”
“I spoke before I thought,” he said gruffly, framing her face
between his hands. “An old failing, I’m afraid. Ask my brothers.
Or my sisters.” He smiled. “They’ll tell you the same thing. I hear
something, I get upset, I react.” He clasped her chin, lifted her
mouth to his and kissed her. “Will you forgive me?”
Their eyes met, his dark with anguish, hers bright with tears.
Would she forgive him? How could she do anything else? She
loved this man with all her heart. He had been through an awful
shock about his father. She could grant him some leeway,
couldn’t she?
And she carried his child.
She had to tell him. She knew she did. They had created this
tiny life together. No matter what the consequences, Alexandros
had the right to know. She was not her mother and he was not a
clone of her father.
“Maria? Please, sweetheart, say you’ll forgive me.”
“You know I will,” she said softly, smiling through her tears.
Alex let out a long breath. “Glyka mou.We must talk.About us.”
“Yes. We do.”
“But not now.” He held her closer. “The next days are going
to be hectic. I’ll have to tell my family about the Stefani diamond.
150 BILLIONAIRE PRINCE, PREGNANT MISTRESS
We’ll have to meet with the council and decide how to handle
this. And there’s my mother’s birthday…”
“Can you postpone the celebration?”
“My father has already said it must go on as scheduled.”
“But if he’s ill…”
“This is a national celebration, sweetheart. Royal responsibility
to the people comes before everything else.” He frowned. “And
nowthat I knowthis about the diamond, Iwonder ifmyfather isn’t
concerned that his illness should not seem too important.You see,
if something should happen to him, if a new king had to be
crowned… That could not happen unless the real diamond were
found and placed in the crown. Do you understand?”
Maria nodded. Like most little girls, she’d loved fairy tales.
Now she knew, first-hand, that real kings and queens and princes
and princesses did not live such easy lives.
That fairy tales didn’t always end happily, she thought, and a
shudder went through her.
“What is it, agapoula mou?”
“Nothing,” she said quickly. “Just—I’m just thinking of how
busy everyone will be the next few days.”
“We’ll manage. I, especially, because I’ll have you beside me.”
Alex bent to her and kissed her. Whispered soft words against
her lips. Her arms crept around his neck. Now, she told herself.
Never mind that there’s no time for real talking. Let him ask you
to stay with him, and you can tell him about the baby—
His cell phone rang. He blanched as he grabbed it from his
pocket.
“Ne?” he said brusquely.
But it wasn’t the hospital, it was Andreas. The conversation
was brief. When it ended, Alex drew her against him.
“I can’t stay, sweetheart. I must meet with Sebastian and
Andreas. There are many things to discuss, and now is as good
a time as any to tell them about the diamond.”
She nodded. “Tell them, too, how sorry I am.”
“You have nothing to be sorry for, glyka mou.” His lips curved
in a smile. “But if you feel you must show contrition for some
SANDRA MARTON 151
152 BILLIONAIRE PRINCE, PREGNANT MISTRESS
imagined misdeed, I’ll consider letting you find creative ways
to do so later on.”
She laughed and kissed him, and when he whispered something
that made her blush, she kissed him again.
“Tonight,” she promised.
He gathered up the papers and the Aristan crown; he put his
arm around her as they walked to the door.
“Tonight,” he said softly.
But one night became another and then another. The mystery
of the diamond, the king’s illness, the birthday celebration on the
horizon… Alex was caught up in the politics of the palace.
There was no time for anything else.
CHAPTER TWELVE
THE day of the queen’s birthday celebration was an anomaly.
It was winter, when cool winds and rain often lashed Aristo,
but this day dawned bright and warm.
Alex hardly noticed.Hehad not been home since he’d left Maria
in the guesthouse. He’d returned the crown to the display case in
the vault and, ever since, he’d been closeted with his brothers.
They were trying to come up with answers. Where was the
missing diamond?Whohad stolen it?When?Howcould the switch
have gone unnoticed? Where did they start searching for the real
stone? Most pressing of all, how could they keep it all a secret?
And it had to be a secret. They could not permit word to get
out that the stone was gone.
On the simplest level, news like that would be humiliating.
Far more unsettling were the possible political consequences.
What if a Calistan sheikh somehow gained control of the
diamond? Could he then twist the true meaning of King
Christos’s legacy, join the Aristan stone to the one in the Crown
of Calista, and claim the right to rule both kingdoms?
It was a real possibility, one that might well destroy Aristo.
Hemissed Maria terribly. Her smile. Her quiet strength.The feel
of her in his arms.Hephoned her whenever he could: even the sound
of her soft voice was an oasis of calm in the middle of a storm.
“I miss you, glyka mou,” he told her softly.
She missed him, too. Terribly. But she understood that he was
154 BILLIONAIRE PRINCE, PREGNANT MISTRESS
needed at the palace. Sometimes, she could forget her lover was
a prince. Now, she couldn’t escape it. So, rather than burden him
with her own feelings, she did what she thought was right. She
said she missed him, too, but she was busy.
“Even if you were here,Alexandros, I couldn’t spend time with
you. I have last-minute work to do on your mother’s necklace.”
“Oh,” he said, just that one word, but he sounded disappointed.
She almost told him she was lying, that she missed him
so badly she ached, that if he came through the door she’d toss
everything aside and run into his arms…
But the last thing he needed now was a clinging female. Her
Alex, along with Sebastian and Andreas, were like jugglers
trying to keep a dozen balls in the air. Elissa had just arrived
home. She and Kitty were busy helping their mother get ready
for the party.
The king had come home from the hospital against the advice
of his doctors. The heart attack had not done any damage, true,
but they wanted other tests. Nonsense, said Aegeus. There were
affairs of state to deal with. Tia’s birthday. All the media attention
the celebration had brought. Scores of foreign dignitaries.
“I am fine,” he insisted.
Was he? The brothers thought their father looked ill.
“Actually,” Sebastian said, “he looks like hell.”
It was an accurate assessment. The king was pale. He seemed
to have shrunk in size and there was a constant sheen of sweat
on his forehead. And why did he never mention the missing
diamond? That seemed strangest of all.
“If we can just get through the celebration tonight…” Alex
said, and they all agreed. Get through tonight and then they could
institute a real if subtle search for the missing stone.
Alex was going home to shower and change. “We’ll have
half an hour alone, sweetheart,” he said when he phoned Maria,
“but we can make the most of that half hour.” He told her how
they’d do that, in explicit detail, and she gave a sexy little sigh
and said she’d be waiting.
Smiling, he flipped his cell phone shut. He needed that thirty
SANDRA MARTON 155
minutes, not just to make love to her but to tell her what he’d started
to tell her three days ago. What he should have told her weeks ago.
He didn’t want her to leave him.
Once she gave the necklace to the king, once the king presented
it to the queen, Maria would go back to New York.
He could not imagine letting that happen.
They were at the start of their relationship, not the end. In the
last month, she had become part of him. She was—she was everything
to him. Sometimes, when he held her close, he wanted
to tell her—to tell her—
“Alex?” Andreas was hurrying toward him. “Change in plans.
Last-minute stuff. Sebastian’s meeting with that guy from the
BBC, I’m going to talk to CNN. Kitty’s doing a piece with The New
York Times. Lissa was going to deal with Newsweek but Mother
needs her, something about the flowers. Can you take her spot?”
Alex looked at his watch. “I have to get home, Andreas.”
“You mean,” his brother said, smiling, “youwant to see Maria.”
“No, of course not. It’s just that my tux is at home…” Alex
sighed. “You’re right. I do.”
“Well, you’ll see her soon enough. The party starts in a couple
of hours. Let your driver pick up your tux, okay? If you don’t take
over for Lissa, we’ll be up the creek without a paddle.”
Alex hesitated, but what choice was there? He couldn’t walk
away from his duty, no matter what his own needs. He hoped
Maria would understand.
She did more than understand. She said that it was just as well,
she still had to do her nails and her hair. He said fine, he was glad
it had all worked out, but he was lying.
What he’dwanted her to saywas that she’d been longing for him.
That it was agony to know they would not have half an hour alone.
Hehad noway of knowing that Mariawas lying, too. She’d been
counting the hours untilAlex came to her, but she couldn’t tell him
that. She needed the feel of his arms around her.And then therewas
her pregnancy. She had to find the right time to tell him about it.
But when?
He was, after all, a man with all the responsibilities of a life
completely different from hers. He might see her as an exciting
lover but that was all she was, all she ever could be…
Her throat tightened.
Maybe she wouldn’t tell him about the baby. Not just yet,
anyway.
Not until the time was right.
The evening started with a flourish.
A dozen royal heralds played a trumpet fanfare at the top of
the marble steps that led into the huge ballroom. A velvet curtain
at the far end was drawn back and the queen swept in on the
king’s arm. The hundreds of guests smiled and applauded her
arrival. Every eye was on the radiant Tia.
Every eye but Alex’s.
He was waiting at the opposite end of the enormous room,
waiting and watching for Maria. Where was she?
“Alexandros,” a voice whispered, and he turned and there she
was, standing behind him, so gorgeous in a silk gown the color
of fine sherry, her dark hair tumbling down her back in a profusion
ofwaves and curls accented with tiny ruby and diamond stars
he’d had sent to her, that the sight of her almost stopped his heart.
He didn’t think, didn’t hesitate but took her hands, drew her
through the crowd and out to the terrace, took her in his arms and
kissed her.
She melted against him.
“Maria,” he said softly, “kardoula mou, you are the most
beautiful woman in the world.”
Her mouth curved against his. “And you, my prince, are the
most handsome man on the planet.”
He kissed her again. “I hoped you’d be here before the celebration
started.”
“The car,” she said, on a little laugh. “We had a flat tire. Don’t
let on that you know, Alex. Poor Alastor felt awful.”
“As he should,” Alex said, but he smiled. “Never mind.
You’re here now. That’s what matters. Did you give the necklace
to my father?”
156 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Maria nodded. “He seemed pleased with it. He said he’ll
give it to your mother at midnight.” She hesitated. “Is he all
right? He looks—”
“Terrible. I know. We tried to convince him to cancel but he
refused.” Alex gathered her closer against him. “Let’s not talk
about that,” he said softly. “Not when I have something important
to discuss with you.”
Now was the time to say that she did, too.
“Maria.” Gently, he brushed a curl from her forehead. “I know
you’re supposed to… I mean, I know we agreed you would…”
Alex groaned. “I’m making a mess of this, glyka mou. What I’m
trying to say is—”
“Your Highness! Prince Alexandros!”
An equerry was running toward them. Alex knew, before the
man said another word, that the news was of his father.
“The king?”
The equerry nodded. “He’s been taken ill, sir.”
Alex ran into the ballroom. Maria hiked up her skirt and ran
at his side.
“Where is he?”
“The throne room, sir. There’s a helicopter on its way. Your
Highness?” The equerry, running with them, caught Alex by the
sleeve just before they reached the throne room. The simple
actionwas so unprecedented that it startled even Maria. “The king
wishes to see Ms. Santos.” He swallowed audibly. “Alone.”
“Me?” Maria said, in amazement. “That can’t be.”
The Karedes family was gathered outside the closed doors of
the throne room, faces white and puzzled. When Maria hesitated,
the queen motioned her forward.
“My husband wants to see you, Ms. Santos.” Tia bit her lip.
“Please. I don’t think there’s time to waste.”
“Go on,” Alex said softly, and touched his hand to her cheek.
The doors closed behind her with an audible click.
This was Maria’s first visit to the throne room. It was not as
big as she’d imagined, the size, perhaps, of half her loft, but it
SANDRA MARTON 157
was elegant. A red carpet stretched toward a pair of ornate chairs
that stood on a raised platform but the chairs—presumably, the
thrones—were empty.
“Here,” a weak voice said.
The king was alone. He lay on a crimson velvet sofa, head
elevated on a blue silk pillow.
Maria moved slowly toward him. Her heart thumped. He’s
dying, she thought, and, as if he’d read her mind, Aegeus struggled
up against the pillow.
“I am not dead yet, Ms. Santos. Come forward.”
“Your Majesty. Your family is outside. Surely, you want to see
them—”
“You were not supposed to learn that the diamond in the
crown is false.”
Maria caught her breath. “You knew?”
The king’s face contorted. He groaned and Maria swung
toward the door to call for help but Aegeus’s fingers wrapped
around her wrist with the steely grip of command.
“My son is in love with you.”
She stared at him. “What?”
“Alexandros loves you, Ms. Santos. I’m not sure he knows it
yet, but he does.” He drew a rasping breath. “But you must not
return that love.”
Maria shook her head. “Your Majesty. Please. You’re very
sick—”
“All the more reason for you to pay attention to what I say,”
he said, a touch of the old sharpness edging hiswords. “You must
understand that there is no room in a royal’s life for love.”
“Sir. This is hardly the time—”
“A prince is not born to his mother or father, Ms. Santos, he
is born to his nation and his people. His life, from birth, is one
of responsibility. Commitment. Obligation.” Aegeus took another
labored breath. “Someday, my sons will marry. They will marry
young women born of blood as royal as theirs, young women
who understand what is expected of them.”
Maria sank to her knees beside the sofa. She could feel the
158 BILLIONAIRE PRINCE, PREGNANT MISTRESS
sting of tears in her eyes and she blinked furiously to keep them
from falling.
“I love your son,” she whispered. “And I understand he has
responsibilities. I can help him shoulder them. I can step back
when I must.”
“If you truly love him, you will give him up.”
“No. No! You can’t ask that of me. Or of him. If Alexandros
loves me—”
“His duty is to his people. To his mother. To me. A prince who
falls in love with the wrong woman can only destroy her. He can
only destroy his nation and himself. Maria. If you love my son
as you say you do, you will leave him. And you will not tell him
the reason. Alexandros must never know you love him, or that
you gave him up because you love him. You must walk away
from him, from his life, and never look back.”
Tears streamed down Maria’s face.
“You ask too much of me,” she said. “You have no right!”
“I love my country and my people. And though you may not
think so, I love my children.” The king took a long, agonizing
breath. “Alex thinks you willmake him happy but youwon’t, Maria.
Your love can only hurt him.You must, you must, set him free.”
“Your Majesty—”
The king jerked upright. His hand went to his throat; his
breath rattled though a mouth gone wide, gasping for air.
Maria sprang to her feet.
“Help,” she shouted.
“Maria,” Aegeus whispered hoarsely.
“Someone, help—”
The door swung open. Footsteps clattered against the marble
floor. And, as they did, Aegeus grabbed Maria’s hand again.
“Promise me,” he said fiercely. “Swear that you will do what
you know you must.”
Weeping, Maria stared at the king’s stricken face—and knew
he was right. She could not share Alexandros’s life. He was a
prince and she—she was nobody.
“I swear,” she said.
SANDRA MARTON 159
160 BILLIONAIRE PRINCE, PREGNANT MISTRESS
A smile pulled Aegeus’s lips back from his teeth—and then he
fell back against the pillows. His family surrounded him.The queen
sank to the floor beside him, took his hand and began to weep.
“He’s gone,” she said, “he’s gone!”
Alex gently drew her to her feet. Sebastian put his arm around
her. Andreas touched her shoulder. Kitty and Lissa bent over their
father and sobbed.
And Maria did the only thing she could. The thing Aegeus had
asked of her. The promise she had made him that she knew, in
her heart of hearts, was right.
She slipped from the room, from the palace.
From Alexandros’s life.
CHAPTER THIRTEEN
A MONARCH’S death left behind a void that must be filled quickly
for the safety and stability of the kingdom and its people.
At first, all was confusion.
Despite Aegeus’s illness, his death had been sudden. The
king’s private physicians tried every possible means to revive him
but to no avail. The Karedes family clustered around the king’s
lifeless body; the palace, filled with guests for the queen’s
birthday celebration, buzzed with rumors. Andreas comforted his
sisters. Sebastian, who as eldest son would, within hours, be
named the Prince Regent, was immediately surrounded by guards
whose duty it was to protect him, especially in times of turmoil.
Alex held his mother in his arms.
Through it all—the loss of his father, his mother’s tears, his
sister’s sobs, the stunned reactions of his brothers and his own
shock—through all that, Alex found himself looking over the
heads of those who’d crowded into the room. Where was Maria?
He needed her. And, surely, she needed him. She’d been alone
with his father at the moment of his death.
She needed his comfort. His arms.And he, God, he needed her.
A reporter and a couple of photographers had somehow
slipped into the room; two of the guards were hustling them out.
Had those guards, in error, forced Maria aside?
He was desperate to find her but Tia was distraught. He
couldn’t leave her, not until she was calmer. He told himself not
162 BILLIONAIRE PRINCE, PREGNANT MISTRESS
to worry. His Maria was smart. She was resourceful. She’d find
his car, have his driver take her home. Or she’d wait for him in
a quiet corner of the palace.
Soon, he’d be alone with her. And he’d tell her what he now
knew had been in his heart for weeks. He loved her. He adored
her. He could not imagine life without her.
He didn’t just want her to stay here, on Aristo, as his lover.
He wanted her to become his wife.
One thing about death, he thought as he led his mother from
the room. It had a profound way of making a man see what
really mattered.
And what mattered, the only thing that mattered, was Maria.
In the face of a nation’s grief and loss, tradition became its solace.
Aegeus would lie in state for three days. The Accession
Council would meet to formally name Sebastian the Prince
Regent, though by tradition coupled with the decades-old decree
of Christos, there could be no coronation of him as king until the
missing half of the Stefani diamond was returned to the Aristan
crown. The Privy Council would meet, too, so its members could
certify the succession declaration.
Andreas took on the coordination of those meetings.
Sebastian immersed himself in policy conferences. It fell to
Alex to finalize plans for the royal funeral. And yet, as he raced
home just before dawn, his thoughts were not on any of those
things. He was consumed by worries over something far more
important.
Maria.
She hadn’t beenwaiting for him in the palace, not in the public
rooms or in the royal apartments. His driver was waiting, in the
courtyard, and in response toAlex’s questions the man could only
shake his head and say that he had not seen Ms. Santos.
Alex checked his cell phone. Again. He’d already done that
a dozen times but maybe, now, she’d left a message… She hadn’t.
He’d phoned her endlessly and been connected to her voice mail,
where he’d gone from leaving messages telling her he would
SANDRA MARTON 163
break away as soon as he possibly could to increasingly terse
ones asking her to contact him.
By the time he reached the house on the bay, he was frantic.
“Maria?” he shouted as he burst through the door. “Maria?”
No answer. He ran up the stairs to his bedroom, flung open
the door. The room was dark. Empty.
“Maria,” he said again, and flew down the stairs, almost stumbling
over Athenia who stood at the bottom wearing a housecoat,
her hair in curlers.
“Your Highness. Our hearts are filled with grief. We are all
so sorry for you—”
“Yes. Thank you. Where is Ms. Santos?”
Athenia bit her lip. Shook her head. Alex cursed in frustration—
and then breathed a sigh of relief. He knew where Maria
would be. In the guesthouse. He knew her habits. She was
probably losing herself in work.
But the guesthouse, Maria’s workshop, stood as silent and
empty as his bedroom. Something about that silence made his
heart rise in his throat. He ran back to the main house, took the
steps two at a time, flung open the bedroom door, this time
switched on the light…
And knew, instantly, that Maria was gone.
The room felt cold. Not just empty but barren, as if the very life
had been stripped from it. He went to the dressing room, stepped
inside. Her suitcase was gone. The beautiful clothes he’d bought
her hung from the racks like mournful reminders of the past.
“Maria,” Alex said, bewildered. What the hell had happened?
Where was his Maria? He turned in a slow circle—and saw the
envelope propped on the bed. “Alex,” it said, and that it didn’t
say Alexandros was a statement in itself.
He picked it up. Opened it.Withdrew the note inside and read
it. It was brief. She was, she said, terribly sorry for his loss.
Though she’d only met his father a handful of times, she’d come
to respect him. She’d wanted to tell him that herself but…
The “but” made Alex’s belly knot.
But, she wrote, she knew that the king’s death meant Alex
would be immersed in the duties of a prince. She saw no reason to
burden him with concern for her, especially since she was returning
to NewYork anyway, nowthat her duties here were completed.
Her duties here.
He looked up, his face a mask of disbelief.Was that what it had
been? Had sleeping with him been part of her duties?Was leaving
him such a relief that she couldn’t have waited to say goodbye?
He read the note again. And again. Then he let out a roar of
anguished rage from a place in his soul he’d never known existed,
and tore the note into a dozen pieces.
A state funeral was not a simple thing.
Fortunately, plans for events like this had always existed.
Except for the addition of a motorcade, those plans had not
changed much since the time of the Crusades.
Aegeus lay in state for three days while his people, friends,
relatives and foreign heads of state all paid their respects.
The Sheikh King Zakari Al’Farisi represented the island of
Calista.
Zakari, a proud and ruthless man, made all the appropriate
comments to the press; he offered Tia his polite condolences.
In private talks with the Karedes princes, however, Zakari’s
words were probing as well as troubling.
He seemed to know that Aristo’s half of the Stefani diamond
was missing.
Though Alex, Andreas and Sebastian had met with their
council and agreed the mystery had to be kept secret until it was
solved, that decision was—as Andreas wryly put it—pretty much
the equivalent of shutting the stable door after the horse had
been stolen.
Clearly, the news had reached Calista. And that was dangerous.
Since Sebastian could not be crowned without the true
diamond, Alex’s worry—that it might fall into the wrong hands
and a Calistan prince could take the Aristan throne—seemed
more and more plausible.
Added to concerns of state were those of family. Lissa and
164 BILLIONAIRE PRINCE, PREGNANT MISTRESS
Kitty took their father’s death hard and clung to Andreas. Tia,
shocked by her loss, claimed Alex for solace and support.
Sebastian, now the Prince Regent, was, by custom, designated
to lead them all through the necessary formalities.
Alex had no time to think, or told himself he had no time for
it. But at night, when the hands of his watch seemed to slow to a
crawl, he lay awake in his palace rooms, despising Maria, despising
himself, telling himself what a fool he’d been to have imagined
himself in love with her because he certainly had not loved her.
Of course, he hadn’t.
He counted down the days until the formalities of mourning
would end. He had work to do, investors to meet with and
reassure that nothing would change on Aristo. He was also fully
involved in organizing the search for the missing diamond. Once
the mourning period was behind him, he’d be far too busy to
think about Maria Santos.
A lie.
Life slowly returned to normal. He was busy from early
morning until late at night.And he thought about her all the time.
What he needed was closure, to tell her, to her face, that she
had meant nothing more to him than he had obviously meant to
her, but that would mean seeking her out and he wasn’t about to
lower himself to that.
Strangely, no one in his family asked about Maria until one
morning, when his mother phoned and invited him to breakfast.
He was incredibly busy that day but he knew Tia’s grief was still
new; nothing would have made him refuse her request.
They chatted briefly about nothing special—and then, without
warning, Tia asked why Maria had gone.
“Why wouldn’t she?” Alex said, with a shrug. “She finished
your necklace. Her work was over.”
“I’m not talking about her work,” Tia said. “I’m talking about
the feelings you and she have for each other.”
“You’re wrong, Mother. We had no—”
“Alex. I’m your mother. I’m also a woman. I know love when
I see it. Maria and you were in love. So, why did you let her go?”
SANDRA MARTON 165
Alex thought of half a dozen answers, all of which would have
worked—and, instead, found himself speaking the truth.
“I didn’t,” he said in a low voice. “She left me. She enjoyed—
she enjoyed our time together but—”
“Nonsense. She loves you. I saw it. Everyone saw it.”
“The hell she did!” Alex shot to his feet. “She left the night
Father died. What was I supposed to do? Go after her? Walk out
on my duties to try and convince her not to leave me?”
“Your duties,” Tia said softly. “Yes. Such things always get
in the way.” She looked up at him. “Had you ever told her you
loved her?”
His mouth thinned. “No.”
“Perhaps,” she said carefully, “perhaps you should have.”
Yes, Alex thought, he should have. He’d known the truth, in
his heart. Why hadn’t he faced it sooner? Now it was too late.
“It’s never too late,” his mother said, and he realized he’d
spoken aloud. “Alexandros. Love is a precious gift. Don’t
throw it away.”
“How do you know that, Mother? I know you respected Father
but I don’t believe you truly loved him.” Alex drew a ragged
breath. “Hell,” he said softly, “I’m sorry. I shouldn’t have—”
“You spoke the truth, my son. I didn’t truly love Aegeus, nor
did he truly love me.” The queen’s eyes shone with unshed tears.
“And that’s exactly the reason you must not let love slip through
your fingers, Alexandros. Even a royal is entitled to happiness.”
The New York weather was harsh and uninviting.
Snow, slush, sleet and grey skies were daily companions.
The sun seemed reluctant to put in even a cursory appearance.
The weather was a reflection of Maria’s despair. She was
lonely for Alexandros, for Aristo, for the happiness she had
found with him there.
It was good that she was busy. Shops that had not wanted any
part of her in the past clamored for her designs. L’Orangerie
headed the list.
And, well, yes, there was one other good thing. A miracu-
166 BILLIONAIRE PRINCE, PREGNANT MISTRESS
lous thing that had, at first, terrified her and now made her
heart sing with joy.
Avisit to her doctor had confirmed that shewas pregnant. She
was carrying Alex’s child.A little girl, Sela had said, smiling.
“Morning sickness at the beginning of your pregnancy. And
look how high you’re carrying. Absolutely, a girl.”
Maria didn’t believe in the old superstitions but it didn’t
matter. Shewould love her baby whatever the sex, and shewould
name it for Alexandros even though he was gone from her life.
He, and Aristo, were moving forward. Sebastian was the
Prince Regent, though there was no mention of the missing
diamond. She’d followed the funeral on TV. The royal family had
looked saddened but composed. Alex had been his mother’s
strong, handsome escort.
Just seeing him had made Maria’s throat constrict.
She would never stop loving him.
But she would have his daughter to love. It saddened her that
her baby would never know her daddy but when Alexandra was
old enough, she’d tell her what a fine man he was, what a loving
man, what a good man—and never mind that he hadn’t called or
written or tried to find out how she was, where she was…
“Maria? You okay?”
She looked up at Joaquin, working at the end of the bench.
He and Sela had been wonderful. Though they knew about the
baby, they hadn’t asked questions. A good thing, too, because if
they had, she might have broken down and wept.
Tears filled her eyes and dripped onto thewax shewasworking.
“Maria?”
“Yes,” she said briskly, wiping the back of her hand across her
eyes and flashing a smile, “I’m fine. I just—I think I got a bit of
wax in my eye.”
“Want me to do that mold?”
“No. No, thanks. I’m almost finished. You know, it’s getting
late. Why don’t we call it a day, hmm?”
“Well, if that’s okay… I promised Sela I’d pick up some stuff
from the Chinese market on the way home.”
SANDRA MARTON 167
“Better get going, then, before the market closes.”
Joaquin nodded, cleaned up his end of the workbench, then
put on his coat. He kissed her cheek and she managed to keep the
tears from building again until the door closed behind him.
Why was she weeping? She’d wanted to end things cleanly.
To keep Alex from contacting her. And she’d succeeded.
She just had to stop crying every five minutes. Sela said itwas
her hormones. It wasn’t. It was her inability to accept that she
would never lie in her lover’s arms again, but she’d sooner have
died than admit that to Sela or even to herself because it wasn’t
true, itwasn’t true, she had awonderful, fulfilling life nowand—
Someone knocked at the loft door.
Maria grabbed the edge of herwork-apron and wiped her eyes.
Had Joaquin forgotten something? Why didn’t he use his key?
Unless it was a reporter. They were still driving her crazy, hoping
for an interview about life on Aristo and the death of its king.
The knock came again. She sighed, smoothed down her apron,
fixed a polite smile to her lips and marched to the door.
“Yes?” No answer. Maria rolled her eyes. “Look, I’ve said I
won’t do interviews so whoever you are—”
“Open the door, Maria.”
Her heart leaped. No. It couldn’t be.
“Maria. Did you hear me? Open this door.”
She shook her head, as if Alex could see her. “Go away,” she
said in a shaky voice.
“I’m not going anywhere. Either you open this door or I’ll
break it down.”
He would, too. He was angry—she could hear it in his voice,
and she remembered what his anger had been like that night he’d
first come here.
Bam! The door, heavy as a chunk of steel, shuddered under
the blow.
“I don’t want to see you.” Maria licked her lips. “Joaquin is
here. He says—”
“He says I’ve been a fool. And he’s right.”
Maria stared at the door. “You talked to Joaquin?”
168 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Just now. On the stairs.” Alex’s voice softened; she had to
put her ear to the door to hear him. “He’s been a good friend
to you.You’re lucky to have him to turn to. Maria, glyka mou,
let me in.”
She swallowed hard. Then she undid the bolt and opened the
door.
“I don’t want to talk to you,” she started to say, but the sight
of her Alexandros, so tall, so powerful, so much the lover she remembered,
stole the words away. To her horror, her eyes flooded
with the tears she’d fought only minutes before. She couldn’t let
him see her cry, she couldn’t, she told herself, and she slapped
her hands against the door and started to push it closed.
Alex was too quick. He jammed his shoulder between the
door and its frame and pushed. Maria staggered back, the door
swung open and he stepped into the loft.
He’d had plenty of time to consider how he would handle this
meeting. The flight from Aristo had taken longer than usual. Bad
weather had meant putting down at Charles de Gaulle Airport in
Paris for a few hours. Just as well, he’d thought. The delay had
given him extra time to decide what to say.
He’d come up with a list of questions.A little speech, though
he tried not to call it that, in which he’d let Maria know that a
woman did not simply walk out on him without explanation.
He would be cautious in expressing his feelings, never
mind his mother’s insistence that Maria and he were in love.
The sad truth, as Tia had admitted, was that his mother didn’t
know a damned thing about love. If Maria loved him, why had
she left him?
A woman who loved a man didn’t walk out on him without
so much as a handshake.
He wasn’t so sure about loving her, either. Why would a man
love a woman who’d abandoned him? Who was so independent?
Why would he want her back in his life?
Logical, all of it. The trouble was, the closer he’d come to her
street, the harder his heart had beaten. All his hours of planning
and doubt had dissolved like cotton candy in the rain. And when
SANDRA MARTON 169
he’d bumped into a man on the stairs, he’d known instinctively it
was Joaquin—and known, just as instinctively, that the guy knew
who he was, too, and wanted nothing more than to flatten him.
He could hardly blame him.
Hell, itwas what he’d have done if the situation were reversed.
The men had taken a long look at each other.
“Are you the prince?” Joaquin had finally growled. At Alex’s
nod, the other man’s mouth had thinned. “She loves you, you
jerk. And you don’t deserve her.”
Alex had grinned. Then he’d put his hand out.
“You’re right,” he’d said, and after a few seconds Joaquin had
smiled. They’d shaken hands. Then Joaquin had stepped aside
and Alex had continued up the stairs to Maria’s door when terror
had stopped him cold. Certain of everything, sure of nothing, he
had resorted to anger…
And then he’d come to his senses.
He would do whatever it took to win his Maria’s heart…and,
looking into her eyes, he knew, with a rush of fierce joy, that her
heart had always been his for the taking.
Maria loved him. He loved her. And he’d be damned if he’d
lose her again.
So, in the end, there were no questions, no speeches, no
doubts. There was only a man, baring his soul by stepping
forward and opening his arms to a woman. And—thank you,
God—there was the woman, his Maria, giving a little cry and
throwing herself into his embrace.
He kissed her. Kissed her for a very long time. Her mouth.
Her eyes. Her hair.
“Why did you leave me?” he said.
“Because I have no place in your life,” she said, returning each
kiss, each caress, each sigh.
“I love you. You are my life.”
Her heart soared, but she shook her head. “I can’t be.”
“Do you love me?”
How could she lie to him? How could she deny what burned
in her heart?
170 BILLIONAIRE PRINCE, PREGNANT MISTRESS
“Yes,” she said softly, “I love you. I adore you, Alexandros.
But I can’t be part of your life. I—I’m not cut out to be a mistress.”
“Of course you aren’t,” Alex said, in that imperious way that
she’d learned to love. “You’re going to marry me and be my wife.”
His words were more precious than any of the diamonds in
the Aristan crown. She knew she would cherish them forever,
even if what he’d just told her could never happen.
“I can’t marry you,” she whispered.
“Because?”
“Because you’re a prince. You have obligations. Duties.”
“I have nothing, unless I have you, glyka mou. You are my
heart. My joy. My love.”
Oh, how easy it would be to give in. To say, ‘Yes, I’ll marry
you…’ But she couldn’t. She loved her Alexandros too much to
ruin his life.
“Alexandros, listen to me.Your father’swords to me were true.”
Alex’s eyes darkened. “What are you talking about?”
“The night he died—Aegeus said—he said I was wrong for
you. That if I loved you, I had to leave you. He said—”
“Was that why he wanted to see you?” Alex’s tone was harsh.
“To tell you to go away?”
“No. Yes. It was more than that. He said he wanted the best
for you.”
“You are the best for me, glyka mou.”
“Also—also, I think he knew something about the diamond.
I think—I think he had something to do with switching the fake
for the real one.”
“I don’t care about that diamond right now,” Alex said fiercely.
“All that matters is us. What we feel for each other, the life we’ll
create together… What?”
Maria was laughing. Or maybe she was crying. He couldn’t
tell; he only knew that something in what he’d just said had
affected her.
And then, he knew.
Slowly, he clasped her shoulders. Held her just far enough
away so he could look at her from head to toe. She looked dif-
SANDRA MARTON 171
172 BILLIONAIRE PRINCE, PREGNANT MISTRESS
ferent. Her face was fuller. So were her breasts. And, under her
denim work-apron, he could see the delicate but clear convexity
of her belly.
It all came together. Her nausea. Her exhaustion. And now,
these physical changes that made her even more beautiful.
“Maria.” He could feel the smile starting to stretch across his
lips. “Maria, my heart, my soul, are you pregnant?”
She stared at him. She could lie. She could say, no, of
course not…
“Yes,” she said softly.
Alex grinned. Then he gathered her in his arms and rained
kisses on her face.
“Pregnant,” he said, as if he were the first man in the world
ever to hear such news. “My God, sweetheart, we’re pregnant!”
He held her inches from him, his eyes searching hers. “Say the
words, Maria. Tell me that you love me as I love you, and that
you will do me the honor of becoming my wife.”
Maria thought of how far they had come, of a time her
Alexandros would have demanded to know if he was really the
father of the baby in her womb. She thought of how he had
crossed the ocean to claim her. She thought of King Aegeus’s
warning, and how cold and empty the life he’d foreseen for his
son now seemed.
“Alexandros,” she said, because if life wasn’t worth risks,
what was the point? “Alexandros. I love you. And it is you who
do me honor, my beloved, by asking me to marry you.”
Alex gave her a solemn look. “Is that,” he said carefully, “a yes?”
Maria laughed, though she was crying again, this time tears
of joy that streamed down her face.
“Yes,” she said, “yes, yes, yes!”
Her Alexandros kissed her. Then he kicked the door shut,
swept her into his arms, and carried her to the bed.
* * * * *
The Kingdom of Adamas: a Turbulent History
The islands of Calista and Aristo have always been a temptation
to world powers. Initially this was because of their excellent positions
for trading and the agricultural potential of Aristo’s
luscious, fertile land. The discovery of diamonds on Calista in
the Middle Ages made the kingdom a target for invaders.
The kingdom passed through the hands of many foreign powers
throughout the ages. Originally part of the ancient Greek empire,
Adamas then came under the control of Rome from 150 BC
onwards. Following the fall of the Roman empire approximately
400 years later, the islands were annexed to Byzantine control.
It was not until Richard the Lionheart seized Adamas in the
twelfth century that the family of Karedes, local island nobility,
was installed on the throne. When the republic of Venice briefly
took control in the fifteenth century, the Karedes dynasty continued
to rule as mere figureheads.
There followed a period of struggle for the royal family. The
Ottoman Empire claimed the islands in the sixteenth century and
they were forced into an exile that lasted nearly 200 years. When
the Turks finally sold the islands to the British in 1750, the royal
family was reinstated but the kingdom did not gain its independence
until 1921.
The death of King Christos in 1974 marked the end of the
kingdom of Adamas. The islands have functioned under separate
rule ever since.
The Stefani Diamond
Diamonds have been prized since the dawn of human history for
their unique qualities. The jewels were first discovered in India
in 800 BC, and brought to Europe by Alexander the Great five
hundred years later.
In 1477, Mary of Burgundy became the first known recipient
of a diamond engagement ring, given to her by the Archduke
Maximilian of Austria. This begins the history and tradition of
diamond engagement rings.
The Koh-i-Noor and the Hope diamonds were brought to
Europe in 1631. In 1792, the Hope Diamond was stolen from the
French crown jewels during the French Revolution. In 1851,
The Koh-i-Noor diamond was re-cut to 105 carats for Queen
Victoria (Empress of India). This famous diamond is part of the
British crown jewels.
In the medieval period, a beautiful pink diamond was discovered
on Calista, and used in the Karedes crown to symbolize the
power of the Karedes’s rule. The jewel became known as the
Stefani (“Crown”) diamond. It quickly took on a deeply symbolic
role in the kingdom of Adamas. Believing that their power
resided in the stone, the Karedes family vowed that it would
never leave their hands. They thought that if the jewel was lost,
their kingdom would fall. The existence of this diamond fueled
treasure hunters’ dreams for centuries, but no other diamond of
any size was found on Calista until the 1940s.
In 1972, faced with increasing tension in his kingdom
between the islands of Aristo and Calista, and with family
pressure, King Christos announced that after his death the two
islands would split. In the presence of his children Anya and
Aegeus, witnessed by the court, Christos declared,
“You will rule each island for the good of the people, and
bring out the best in your kingdom, but my wish is that
eventually these two jewels, like the islands, will be
reunited. Aristo and Calista are more successful, more
beautiful and more powerful as one nation—Adamas.”
After King Christos died in 1974, the Stefani diamond was
split in two to form two stones for the coronation crowns of
Aristo and Calista and fulfill the ancient charter.
About the Author
Sandra Marton wrote her first story when she was seven years
old and began to dream of becoming a writer. Today, Sandra is
the author of more than thirty romance novels. Readers around
the world love her strong, passionate heroes and determined,
spirited heroines.
Sandra has won a Holt Medallion for Best Single Title and
the Romantic Times BOOKreviews Reviewers’ Choice Award
for Best Mills & Boon Modern Romance of the Year twice.
Sandra sees her greatest goal as a romance writer is to provide
readers with characters and stories that offer hours of pleasurable
escape from the stresses of everyday living. When she’s not
busy writing, Sandra likes to hike, read, explore out-of-the-way
restaurants and travel to faraway places. She is active in many
organizations including Novelists Inc., the Nature Conservancy,
the Humane Society and the Audubon Society.
The mother of two grown sons, Sandra lives with her husband
in a sun-filled house in a quiet corner of Connecticut where she
alternates between extravagant bouts of gourmet cooking and
take-out pizza.
Turn the page for our exclusive interview
with Sandra Marton!
We chatted to Sandra Marton about the world of THE ROYAL
HOUSE OF KAREDES. Here are her insights!
Would you prefer to live on Aristo or Calista? What appeals to
you most about either island?
The two islands are both fascinating, but I would much prefer to
live on Aristo. I love the contrast between the sophistication of
Ellos and the wildly beautiful cliffs that wind above the Bay of
Appollonia, the gentle climate, the white sand beaches kissed by
the sea. My husband and I took a wonderful trip to Greece a few
months ago; all the time we were on the island of Santorini, I
kept thinking how easily beautiful Santorini could be Aristo.
What did you enjoy about writing about THE ROYAL HOUSE
OF KAREDES?
I was very fortunate to write the launch book for the series. That
gave me the chance to give substance to Aristo and to the Karedes
family. Breathing life into Aegeus and Tia, the princes and
princesses was great fun. I especially loved creating my hero,
Alexandros. I’ve always thought the lives of royals must be hard,
all that balancing of personal needs with public demands, and here
I had the chance to show some of the inner conflicts that are part
of a royal’s existence. I loved creating Maria, too. Because I was
born and raised in New York, I was able to give her a background
with at least some similarities to mine. Like Maria, I attended its
schools. I went to university there. My first job was in the part of
Manhattan where I put Maria’s loft and, believe me, her feelings
on riding a crowded subway car were a mirror of my own.
How did you find writing as part of a continuity?
It was great fun but also a challenge. I had to be sure to introduce
my secondary characters in ways that wouldn’t be a problem for
the other authors, and to describe places (the palace, for example)
so that the other authors could “see” it as clearly as I did. I’ve
always loved writing my own miniseries because I love the scope
and depth a writer gets from dealing with ongoing characters and
intertwining plots. Writing BILLIONAIRE PRINCE, PREGNANT
MISTRESS gave me that same feeling.
When you are writing, what is your typical day?
Hmm. Let’s see. The maid draws my bath, brings me my morning
coffee… Oh, if only! Seriously, my working day isn’t a glamorous
one. I get up anywhere from six to seven-thirty, shower,
put on what I think of as my summer outfit (shorts, T-shirt, thong
sandals) or my winter garb (jeans, T-shirt, sneakers), head down
to the kitchen where my husband, bless him, is generally already
brewing the coffee. Breakfast is toast and coffee with the
morning news playing in the background. Then I give my
husband a kiss, head for my office, turn on my computer, play a
few rounds of Spider Solitaire—it’s addictive—and settle in to
work. I take a break somewhere between noon and one o’clock.
My husband’s office is upstairs and he comes down and joins me
for lunch. Then it’s back to work until he calls me on the intercom
and reminds me—by then, I’m lost in what I’m writing—that it’s
time for a glass of wine. Depending on my mood, I’ll either put
together an easy meal—not much cooking goes on when I’m
working!—or he’ll grill something on the deck, or we’ll go out
somewhere local for supper. After that, I generally curl up beside
him on the sofa to read, catch the eleven o’clock news, and then
stagger off to bed.
Where do you get your inspiration for the characters that you write?
I’m an inveterate people-watcher. Plop me down in a café in
NewYork or San Francisco, Paris or Athens, and I’m content. I’m
very aware of people’s body language and facial expressions.
Those things communicate a lot to me. Many of my characters
have come to life through my observations of complete strangers
who have no idea I’m taking mental notes! Friends sometimes
think I’ve based my characters on them. I never do. If I know
someone well, I can’t see them as anything but themselves, if that
makes sense.
What did you like most about your hero and heroine in this
continuity?
I love creating Presents heroes, men who are strong and loving, protective
and powerful, and maybe just a little bit arrogant.
Alexandros is all those things. He’s also a man accustomed to not
showing his feelings. I find that, always, a special challenge for a
heroine. And my Maria is, I think, the perfect woman for
Alexandros. She’s independent, spirited and tough in the best
possible meaning of the word. She, too, has learned to keep her
emotions in check. That’s why the passion they discover in each
other’s arms is so exciting. It changes them, forces them to examine
their own true needs, their own true desires. Fate has created Alex
and Maria for each other. For me, that fierce sense of destiny is what
Presents, passion and enduring love are all about.
What would be the best—and worst—thing about being part of
a royal dynasty?
The best would probably be the good one can do as a royal. The
worst would surely be the public exposure demanded of royals.
I tried to convey some of that in Alexandros’s story.
Are diamonds really a girl’s best friend?
It depends on the girl. A serious answer? No, absolutely not.
When you get down to basics, what women—all women—want
is happiness. And happiness comes from much more precious
things than diamonds. Good friends. Good health. A loving
family… Above all, the love of one special man. Those are the
qualities that make a woman’s life full and rich. The glow of
diamonds is lovely but the glow in your lover’s eyes when he sees
you means far, far more.
HPHALO
We hope you enjoyed being swept
off your feet with this book from the
Harlequin Presents line!
Meet sophisticated men of the world and
captivating women in glamorous, international
settings, available every month from
Harlequin Presents wherever books are sold,
including most bookstores, supermarkets,
discount stores and drugstores.
Seduction and Passion Guaranteed
60EBOOKENDR
From passion, paranormal, suspense and
adventure, to home and family,
Harlequin has a romance for everyone!
Harlequin has
a romance for every mood™!
Look for all the variety Harlequin has to offer
wherever books are sold, including
most bookstores, supermarkets,
discount stores and drugstores.
Visit
www.HarlequinCelebrates.com
to choose from a variety of
great series romance stories
that are absolutely FREE to download!
(Total approximate retail value $60.)
A Romance for Every Mood™!
Passion
SMP60PASSIONR2
Harlequin Presents®
Intense and provocatively
passionate love affairs set
in glamorous international
settings.
Harlequin® Blaze™
Fun, flirtatious and steamy
books that tell it like it is,
inside and outside the
bedroom.
Silhouette Desire®
Rich, powerful heroes and
scandalous family sagas.
Look for these and many other Harlequin and Silhouette
romance books wherever books are sold, including most
bookstores, supermarkets, drugstores and discount stores.
A Romance for Every Mood™!
Suspense and Paranormal
SMP60SUSPENSER2
Harlequin Intrigue®
Breathtaking romantic suspense.
Crime stories that will keep you
on the edge of your seat.
Silhouette® Nocturne™
Dark and sensual paranormal
romance reads that stretch
the boundaries of conflict and
desire, life and death.
Silhouette® Romantic Suspense
Heart-racing sensuality and the
promise of a sweeping romance
set against the backdrop of
suspense.
Look for these and many other Harlequin and Silhouette
romance books wherever books are sold, including most
bookstores, supermarkets, drugstores and discount stores.
A Romance for Every Mood™!
Home and Family
SMP60HOMER3
Harlequin® American Romance®
Lively stories about homes,
families and communities like
the ones you know. This is
romance the all-American way!
Harlequin® Superromance®
Unexpected, exciting and
emotional stories about
homes, families and
communities.
Silhouette® Special Edition
A woman in her world—living
and loving. Celebrating the
magic of creating a family and
developing
romantic
relationships.
Look for these and many other Harlequin and Silhouette
romance books wherever books are sold, including most
bookstores, supermarkets, drugstores and discount stores.
A Romance for Every Mood™!
Romance
SMPNASROMANCER2
Harlequin® Romance
The anticipation, the thrill of
the chase and the sheer rush
of falling in love!
Harlequin’s officially licensed
NASCAR series
The rush of the professional
race car circuit; the thrill of
falling in love.
Harlequin® Historical
Roguish rakes and rugged
cowboys capture your
imagination in these stories
where chivalry
still exists!
Look for these and many other Harlequin and Silhouette
romance books wherever books are sold, including most
bookstores, supermarkets, drugstores and discount stores.
A Romance for Every Mood™!
Inspirational Romance
SMP60INSPIRATIONALR2
Love Inspired®
Contemporary inspirational
romances with Christian
characters facing the challenges
of life and love in today’s
world.
Love Inspired® Historical
Travel back in time and
experience powerful and
engaging stories of romance,
adventure and faith.
Love Inspired® Suspense
Heart-pounding tales of
suspense, romance, hope
and faith.
Look for these and many other Love Inspired romance
books wherever books are sold, including most bookstores,
supermarkets, drugstores and discount stores.
Look for Harlequin, Silhouette
and Love Inspired books
WHEREVER
BOOKS
ARE SOLD
including most bookstores,
supermarkets, discount stores
and drugstores.
60HARSIL09R
ISBN-13: 978-0-373-12835-8
BILLIONAIRE PRINCE, PREGNANT MISTRESS
First North American Publication 2009.
Copyright © 2009 by Harlequin Books S.A.
Special thanks and acknowledgment are given to Sandra Marton for her
contribution to The Royal House of Karedes series.
All rights reserved. Except for use in any review, the reproduction or
utilization of this work in whole or in part in any form by any electronic,
mechanical or other means, now known or hereafter invented, including
xerography, photocopying and recording, or in any information storage
or retrieval system, is forbidden without the written permission of the
publisher, Harlequin Enterprises Limited, 225 Duncan Mill Road,
Don Mills, Ontario, Canada M3B 3K9.
This is a work of fiction. Names, characters, places and incidents are
either the product of the author’s imagination or are used fictitiously,
and any resemblance to actual persons, living or dead, business
establishments, events or locales is entirely coincidental.
This edition published by arrangement with Harlequin Books S.A.
® and TM are trademarks of the publisher. Trademarks indicated with
® are registered in the United States Patent and Trademark Office, the
Canadian Trade Marks Office and in other countries.
www.eHarlequin.com
For MM, my very own hero, yesterday, today and forever.
And for MIM, whose spirit of adventure is an inspiration.
I love you both.
Recycling programs
for this product may
not exist in your area.
eISBN-13: 978-14268-5365-4
An unusual book of euphemisms
Linda Berdoll
Very Nice Ways to Say
VERY BAD THINGS
Very Nice Ways to Say
Very Bad Things
w
w
g
g

Nice Very
ways
to say Very
Things
AnUnusual Book of Euphemisms
by
Linda Berdoll
Copyright © 2003, 2007 by Linda Berdoll
Cover © 2007 by Sourcebooks, Inc.
Internal design © 2003 Carol Sue Hagood
Internal graphics © 2003 Carol Sue Hagood and Johnny Alvarez
Sourcebooks and the colophon are registered trademarks of Sourcebooks,
Inc.
All rights reserved. No part of this book may be reproduced in any form or
by any electronic or mechanical means including information storage and retrieval
systems—except in the case of brief quotations embodied in critical
articles or reviews—without permission in writing from its publisher,
Sourcebooks, Inc.
Published by Sourcebooks Hysteria, an imprint of Sourcebooks, Inc.
P.O. Box 4410, Naperville, Illinois 60567-4410
(630) 961-3900
Fax: (630) 961-2168
www.sourcebooks.com
Originally published in 2003.
Library of Congress Cataloging-in-Publication Data
Berdoll, Linda
Very Nice Ways to Say Very Bad Things / Linda Berdoll
p. cm.
ISBN-13: 978-1-4022-0885-0
ISBN-10: 1-4022-0885-5
1. English language—Euphemism. 2. English language—Jargon.
3. English language—Terms and phrases. I. Title.
PE1449.B4435 2007
427—dc22
2006100787
Printed and bound in the United States of America.
WC 10 9 8 7 6 5 4 3 2 1
ISBN-13: 978-1-4022-2983-1
ISBN-10: 1-4022-2983-6
Contents
Aspersions, brickbats, carping, cavil, censure, denunciation,
disparagement, reproach, opprobrium, reproof, stricture,
vitriol, epithets, and vituperation
Censuring the Inherent Fool: The Lost Art 3
Shakespeare, Addressing Individual
Mounds of Foul, Undigested Lumps
of Donkey Entrails 19
Sacre Bleu: Profanities and
Expletives 21
Oaths and General
Vituperation 27
Silent Disparagement
(The Bird and His Friends) 30
1v
Circumlocution 33
Worshipping at the Shrine
of Bacchus 49
Fits, disease, ill health, infirmity, breakdowns, affliction, ailment,
attacks, bugs, collapse, complaint, confinement, convalescence,
disability, disorder, disturbance, dose, failing health, flu,
indisposition, malady, malaise, prostration, seizure, syndrome,
a bit of unwell, and what’s been going around
Indisposition 55
Going to Europe with Ralph
and Earl in a Buick 56
In the Privy 65
Afflicted by Time’s Wing’d
Chariot 69
53
v
Gender specific activity, femininity, manhood, manliness,
masculinity, sexuality, womanhood, womanliness, intercourse
between animate beings, coition, coitus, copulation, fornication,
generation, intimacy, lovemaking, magnetism, procreation,
relations, reproduction, sensuality, sexuality
Bewitched, Bothered
and Betwattled 75
The Long Carbine 80
Dallying, Firkytoodling
and Finkdiddling 83
Sex “Sain et Sauf” 102
Men Behaving Badly 107
73v
A Pea in the Pod 109
Misbegotten 110
Unknown to
Man 112
Trafficking with
Oneself 114
Daft, mentally strange, barmy, unzipped, batty, berserk,
insane, bonkers, cracked, loony, crazed, cuckoo, demented,
deranged, peculiar, erratic, flaky, fruity, idiotic, insane,
lunatic, mad, maniacal, nuts, potty, psycho, touched,
unbalanced, unglued, unhinged, wacky 135
The Gazelles are in
the Garden 137
’N What? 142
Acknowledgements 144
v
spersions, brickbats, carping, cavil,
censure, denunciation, disparagement, reproach,
opprobrium, reproof, stricture, vitriol, epithets,
and vituperation
Zounds, I have been bethumped by words.
—Shakespeare
You clod of puke-stocking,
roastmeat for worms!
ab
Censuring the
Inherent Fool:
The
Lost Art
Few would argue that some behavior is so abhorrent, it
demands redressing. Regardless of justification—idiot
drivers, impudent clerks, adolescents who have been
spawn by the devil—we as a society simply cannot condone
smacking the offender upside the head. (Admittedly,
we institute this decision partly in deference to decorum,
but also in the distinct possibility that said transgressor
might be packing heat.) Since throwing the bric-a-brac can
become prohibitively expensive, our only alternative is to
let fly with a few choice words.
That acknowledged, it is miserably apparent that standards
about what is said or heard in public have become
remarkably lax. What comes out of the mouths of babes
nowadays would have once made a fishwife blush. Not that
we deplore vehement notification of character flaws, but
3
4
Granted, no one can hold a verbal candle to Will Shakespeare,
but with a few carefully tailored ripostes, one might
just leave the miscreants of society flummoxed, if not actually
chastened.
chucking stock profanities about does not exhibit the exercise
of intellect to which we aspire. To wit:
You stupid, fat fuck
Famous mob boss
You show yourself highly fed and lowly taught.
Shakespeare
or:
5
Aspersions, brickbats, carping, cavil, censure,denuncia-
He’s a most notable coward, an infinite and
endless liar, an hourly prose-breaker, the owner
of no one-good quality.
Shakespeare
b
Hos and Hounds
Call Him a Rat;
Just Don’t Call Him a Mouse.
Once, was one to imply a man less than a
gentleman, one would have to meet him
at dawn accompanied by one’s seconds.
The current vogue of anti-heroes
appears to have reversed such a
notion. Calling a man a rogue,
scoundrel, heel or even humping
dog will not necessarily be
an insult. Therefore, with
honor now discounted, the
male character flaws vulnerable
for attack are intellect,
cuckoldry, wimpiness and
penis size.
He is not only dull in himself, but
is the cause of dullness in others.
Samuel Foote
Homo-Boobus
To properly vilify the cabbage-headed oaf, we must, unfortunately,
blaspheme the animal kingdom*—polecat,
skunk, swine, baboon, (particularly effective with a British
inflection) varmint, goose, or donkey. However, if one
calls him a capon (a de-knackered chicken), one has hit a
triple—not only is he a graceless lout, but also a eunuch—
and unless he was in 4H, unlikely to comprehend the slam.
7
Aspersions, brickbats, carping, cavil, censure,denuncia-
*(In that today few people understand that an ass is actually a four-legged
animal, not the gluteal area surrounding one’s anus, we omitted it.)
:
He has no such brain as ear-wax.
Shakespeare
Dullard, dim bulb, dolt, lobberhead, or flap-doodle are
inherent fools. A lurdane or sluggard is not only a fool, but
a lazy fool. The particularly cantankerous ignoramus is a
devil child, demon rogue, archfiend, churl, Mephistopheles,
or carcass fit for dogs. If one wanted to cover all the
8
bases, there is Lusus naturae, which is Latin for freak of
nature. To clarify the subtle difference between a jerk and
a dunce, one must remember not to credit insult that can
be more appropriately explained by stupidity.
cThe Two-timed
Not so very long ago if a man found his wife in bed with
another man and took a shotgun to them both, it was ruled
justifiable homicide. Hence, it might be wise to make certain
there is a clear avenue for escape before one goes
rattling this particular cage. If one does have the moxie
to do it, there is only one way to go. To quote Pulp Fiction,
c
You met your wife’s wit
brickbats, carping, cavil, censure, denunciation,dispar-
9
one has to “get medieval on his ass.” To do so effectively,
one must become intimately familiar with terms as old as
the middle ages.
We begin with the word cuckold, which many believe
originates with the French word for cuckoo bird. This conclusion
is apparently due to that dirty bird’s penchant for
depositing, then abandoning, its monstrous egg into some
unsuspecting little wren’s nest for it to hatch, then attempt
to feed. History has writ cuckoldry a shooting offense, giving
us to understand quite clearly that men do not want another’s
cuckoo baby in their nest. In that the cuckoo egglayer
and proprietress of said nest are both female should
Shakespeare
going to your neighbor’s bed.
throw a monkey wrench in
this entire affronted manhood
stuff, but as far as we can
determine, it has not.
The derivations of most of
our terms for cheating appear
to be some convolution of the
definition for horn—hard
protuberance, e.g. penis, and
cornu—horn-shaped anatomical
characteristic. Indeed,
there was a mythical beast
called a bicorn, which, legend
says, used to eat husbands
who had unfaithful
wives (as to why these victims
of infidelity were the
ones preyed upon, our crack
team of researchers have
been unable to ascertain).
Then there is the Greek legend of Artemis who caught Actaeon
peeking while she was bathing and turned him into a
stag, thereupon causing his own hounds to eat him—which
maybe served him right.
10
A man does not look
behind the door unless he has
stood there himself.
Du Bois
Hence, the poor cuckold is doomed to suffer, not only his
wife’s infidelity, but being taunted as a cornuto or buck’s
face (has horns, you know), suffering the forked plague,
prey to the bicorn, or, get this, wearing Vulcan’s badge:
The roof of Vulcan, her, by many a gift
Seduced, Mars won, and with adult’rous lust
The bed dishonour’d of the King of fire.
Cowper—The Odyssey of Homer
One must concede that in issuing the jibe, Vulcan’s badge,
it could be misconstrued. A certain element of the population
may not understand that in this context, Vulcan pertains
to the God of Fire and has nothing whatsoever to do
with Star Trek.
A wittol is aware he is being cheated on and puts up with
it (what was Camilla Parker Bowles’ husband’s name anyway?).
If he is aware and enraged, he is horn-mad. If he is
cheating on her, she is a cuckquean and usually The Last
to Know. If the correspondent in this affair is a man, he is,
indeed, Actaeon. His female counterpart is an inconstant,
faithless sore in the side of a man and, no doubt, a wanton
hussy. The entire activity is, quite aptly, named cornucopia
—horn of plenty (we suppose, because there is plenty of
horniness going on).
ATTENTION: It is imperative that when one
inflicts any of the above abuse, it must be done with
extreme superciliousness, else its just not gonna work.
11
brickbats, carping, cavil, censure, denunciation,dispar-
What a candy-ass!
13
carping, cavil, censure, denunciation, disparagement, The Invertebrate
In cockfighting, a white tail feather
among the plumage of a gamecock denotes
inferior breeding and therefore a
less combative rooster. When calling a
person’s courage into question, the accusation
of showing a white feather
may now seem a bit obscure, but for
centuries, it was tantamount to saying
“what a candy-ass.”
In common parlance a cur is a mongrel
dog, but its second definition dating
also from the thirteenth century, is
coward. From the Middle Ages comes
recreant, which as an adjective describes
a begging of mercy (we understand
not an uncommon occurrence
during those times) and by token, one
who does so, a coward.* In the first
half of the 18th century, funk meant
“a state of paralyzing fear,” hence one
who funks is, too, a coward. As to how
and why this term was usurped by the
music industry in the ‘70s remains a
mystery, but it will arbitrarily remove
the word funky from possible cowardly
insults.
*If faced with being drawn and quartered we are
not certain who among us would not go down
screaming like a woman bringing forth child.
He led his regiment
from behind,
He found it less
exciting.
W.S. Gilbert
14
Therefore, the terms that imply the lack of
stalwartness of someone’s innards are: lilylivered,
yellow-bellied, spineless, faint, or
chicken-hearted, pantywaist, or a gutless
wonder. One might avoid wimp and big
baby—they lack imagination. Woody Allen
says he is not a hypochondriac, but an
alarmist. That makes our list, as does
milquetoast, caitiff, craven, dastard, or
poltroon. Save sissy-britches or wienie for
when one has to pull out the big artillery.
VFYI: We note a rectal sub-category
as it relates to the frightened. First, there is the
pucker factor, which refers to the degree of fear that
causes one’s sphincter to tighten. Contrarily is the
green heron or shitepoke which, when startled into
flight, defecates. It goes without saying that whatever
category one may find oneself in when, say, one’s aircraft
plummets or the IRS makes inquiries, should remain
between oneself and one’s laundress.
He Who Is Not Nick-Named Tripod
There was an old man named Ringer,
Who was seducing a beautiful singer.
He said with a grin,
“Now, I’ve got it in.”
Said she, “You mean that’s not your finger?”
Cowardice is distinguished from panic by
the inability to suspend the imagination.
Of the euphemisms we uncovered for a man less favored
by nature (hung like a chicken, pencil-dick and bugfucker),
we can only recommend under-endowed and
three-inch fool, so this entry will be blessedly small (no
pun intended).
The Five-Letter Womanr
:
She was a woman of mean understanding,
Little information and uncertain temper.
Jane Austen
Historically the most effective means to rebuke any woman
was to disparage her virtue (that or possibly her fashion
sense). Nowadays, un-virtuous and unladylike are probably
as useless as insults go as un-gentlemanly. Yet however
ubiquitous its use, we can agree that calling a disreputable
female a bitch (or even puppy’s mama) is not only
common, but an insult to female dogs. Harpy,
harridan, slattern, or shrew may be vintage, but
they are just pithy enough for general reproach
of shrill, hateful behavior. When faced with an
irredeemably cantankerous woman,
she may well be the Devil’s Sister.
(If she appears to find this in any way
complimentary, a keen sense of self
preservation might suggest one run
like a cheap pair of pantyhose.)
carping, cavil, censure, denunciation, disparagement,
16
rFoul Sluts
Even if the succubus
that one’s brother intends
to marry is a fornicatress
that has seen
more pricks than a
dartboard, we encourage
one not to refer to
her as a slut, tramp,
hussy, trollop, roundheeled
floozy, or dirtylegged
Jezebel. One
might get away with
“she’s been around the
block more times than
the Good Humor man”
to others, but unless he
actually asks your opinion,
one might do well
to refrain from comment
at all.
Other analogies for that
woman who has been
laid on every flat rock
in three counties include
the town pump
or any noun that can
I can remember
when the air was clean
and sex was dirty.
17
be ridden: bicycle, hobby horse, barber’s chair, ferry,
hackney, taxi, etc. A badger is a loose woman who is particularly
ill-scented.
Disclaimer: This information is offered only for elucidative
purposes.
VFYI: If one believes that a woman is of accommodating
morals and decides to say so publicly, one has
bewhored her (or, depending on one’s ‘hood, possibly
beho-ed her). Be certain that she doesn’t mind
the advertisement or have your affairs in order, for it
is said: “Hell hath no fury like pussy with a pistol.”
Dog City
If while mentally cruising some parallel
universe, one believes it a good
idea to slander a masculine woman,
at least have the good sense to avoid
calling her a diesel-dyke or hell pig.
Virago, beldame, trolleymog, daggletail,
or buffarilla mean precisely
the same thing and their relative obscurity
may offer one just enough
time to elude being beaten to a pulp.
VNOTE: We have been told
that if one is in a Spanish-speaking
country, it is also advisable not to
compliment a strong woman by
calling her macha.
w
At a loss for words, hockey puck?
Quote Shakespeare.
BULLETIN: The unparalleled king of insults
is not Don Rickles.
As has certainly not passed one’s notice, Shakespeare marshals
up gems of abuse that would whoosh right over the
average boor’s head. Therefore, appropriating The Bard’s
18
Y’wanna piece
of me, sweetie?
19
Shakespeare, Addressing Individual Mounds
of Foul, Undigested Lumps of Donkey Entrails:
for those of the female persuasion
Hag of hell, fat chuff, latten bilbo (brass shackles),
painted maypole, long-tongued babbling gossip,and
Amazonian trull.
For men who have fallen out of one’s favor
False hound, untutored churl, rank weed, insolent cracker, unlettered
small-knowing soul, odoriferous stench, pigeon-egg of
discretion, dilatory sloth, homely swain, clod of wayward marl,
dunghill groom, puke-stocking, improvident flea, ronyon (mangy
or scabby creature), roastmeat for worms, princox (fop), cacoethes
(one with insatiable desire, usually disreputable), mad mustachio’d
purple-hued maltworm, prick-eared cur of—(fill in the
name of town, school, or neighborhood the cur claims as home),
and whoreson.
for one’s boss
Old feeble carrion, scolding crookbank, embossed carbuncle,
white-livered-red-faced prince of fiends, cacodemon (evil spirit),
maggot pie, execrable wretch, beef-witted, or sodden-witted
implorer of unholy suits.
to verbally backhand group obnoxiousness
You rabble of vile confederates, herd of boils and plagues, petty
spirits of region low, strangely visited people, foul and pestilent
congregation of vapors, college of witcrackers, dissolute crew, or
base lackey peasants.
words to one’s own needs will serve a dual purpose. It
confounds the ignorant and catches the erudite off guard.
Hence:
cavil, censure, denunciation, disparagement, reproach,
Under certain circumstances, profanity provides a
relief denied even to prayer.
Mark Twain ab
21
Sacre Bleu:
Profanities and
Expletives
w
The “F” ing Word & Other Intensives
Veritable, sure enough, or bona-fide are perfectly respectable
intensifiers when one needs, well, emphasis. Unfortunately,
fucking seems to be the hands-down pejorative of
choice in modern society. This being the case, we believe a
little historical perspective couldn’t hurt . . .
No matter how many people believe it true, it is highly unlikely
that the word “fuck” is an acronym of For Unlawful
Carnal Knowledge or that other old chestnut, Fornicate
Under Consent of the King. Eric Partridge believed it
evolved from the German word ficken for “to strike.” Like
most, he found the word objectionable. He, however, categorized
it along with words that he considered sadistic representations
of the male’s part in copulation: clap, strike,
thump, nail, and, yes, bang.Webster’s offers the derivations,
fokken (Dutch, to breed) or fokka (Swedish, to copulate).
Others suggest the French word foutre, to thrust, and even
firk (English 1600’s), to beat or to lash. However it originated,
it has been in use and considered a vulgarity the better
part of a millennium. As an intensive, Webster’s calls it
meaningless. There are those who would disagree.
To avoid inciting an affronted swoon
by the more sensitive souls of society,
acronyms have been embraced in place
of a number of phrases that include
the “f” word. Specifically, we have
GFY, which instructs one to do something
anatomically impossible (Go
Fuck Yourself ); GFU, a moron (General
Fuck-Up); and NFW, an implausibility
(No Fucking Way). Related
acronyms include SNAFU, a cynical
expectation of any situation in which
the military is involved (Situation
Normal, All Fucked-Up); FUBAR,
unrecognizably mussed (Fucked-Up
Beyond All Recognition); and there is
the sarcastic BFD (Big Fucking Deal).
Additionally, when one has been indisputably wronged, one
has been RF—Royally Fucked (also known as the king’s
elevator—the royal shaft). Just for the record, a flying
fuck is what one does not give, not airborne copulation.
And abso-fucking-lutely means beyond a shadow of a
doubt.
22
w
Merde
The four-letter word for defecation has been in use for
eons—which allows that antiquity does not necessarily dictate
grand lexicon. It is possible to avoid the vulgarity of
the word shit completely, as feces, manure, and dung all
mean the same thing. (Small point of interest: feces refer to
human waste, manure and dung, animal.) Other selections
tend to be polysyllabic but are colorful—meadow dressing,
bovine excrement, horse apples, corral confetti, etc.
Granted, if one is discussing political matters, it may be impossible
to avoid using (or even shouting) bullshit. However,
if one does not want to compromise decorum completely,
that can be shortened to B.S. Or, call it hogwash,
heifer dust, or lip-gloss. Bull-chips might do in a pinch
but, in all probability, not what pops out of one’s mouth
when faced with ultimate doom (at which time one will
most likely be up Shit Creek). Indeed, sources report that
when the black boxes are recovered from airplane crash
sites, invariably the last words on the tape are “Uh-oh,”
“Fuck!” and “Oh, shit. ”
Of course, one can use the French, merde or
speak of “a short French expletive” which
would in fact allow one to perform a rather impressive
circumlocutory hat trick, a euphemism
for a euphemism for euphemism.
When one finds it necessary to point out the
limitations of another’s character via the
23
cavil, censure, denunciation, disparagement, reproach,
alimentary canal, it is our position that it is preferable to
enlist mock Latin such as excrementum cerebellum vincit
rather than call someone a shit-head.
Other expressions that would benefit such translation are:
shit list (a mental note of personae non gratae); the shitty
end of the stick (the bad end of a bargain—often known
as the shaft); to shit or get off the pot (or fish or cut bait).
To shit in high cotton is to have attained a higher standard
of living. But not knowing shit from Shinola—well, that
means . . . owing to stupidity one cannot tell feces from
shoe polish. Someone whose continued presence is an
24
annoyance sticks like shit to a shovel. Alternatively,
shit on wheels reflects an over-inflated opinion of oneself.
(We, however, could in no way determine how one could
deign this to be a self-compliment).
Shit a brick technically means discharging a copious and
compacted bowel movement, but colloquially it refers to
accomplishing the impossible. Lastly, to be so angry as to
perform said impossibility is engaging in a shit-fit (also
known as pitching a bitch). Certainly there are Latin
instructors standing by to assist us.
w Vexed
As ancient a word as is piss, it was not until the last century
that humankind found use for it beyond the single
verb or noun. Nowadays, if one is pissed off, one is actually
choleric (and undoubtedly with one’s panties in
a bunch or knickers in a knot). Shakespeare expressed
it thusly: “You do me the most insupportable vexation.”
Other urinary-based euphemisms and their more civilized
translations: full of piss and vinegar (effervescent), piss
away (squander the inheritance—leaving oneself without a
pot to piss in), and piss blood (work with extreme diligence).
A piss-ass is a worthless individual (occasionally called an
arseworm), to engage in a pissing match is an endeavor
that is certain to be unproductive, and if one is piss-poor,
one is monetarily disadvantaged (e.g., without cable).
25
cavil, censure, denunciation, disparagement, reproach,
26
Full a’ piss and
vinegar, ye’ are!
Piss ugly is extremely unattractive
and if piss-faced, one
is overly medicated by alcohol.
The heretofore unheard of, pissed as
a newt has come to our attention. As we
have not personally been confronted by an
outraged salamander, we are uncertain of the
etymology or history of this term. We can only
labor under the supposition that in this situation,
“pissed” does mean vexed, for we believe one even less
likely to come across a drunken newt than a mean one.
VNOTE: The colorful late U. S. Vice President, John
Nance Garner is oft quoted as saying the office of Vice
President was not worth a bucket of warm spit. Those who
knew the man insist he didn’t use the word, “spit.”
Oaths and General
Vituperation
If a potty mouth forsakes stock curses and lets fly with the
likes of Jumpin’ Jehosaphat, just imagine the stunned
silence. Likewise, pshaw, Land a Goshen, Lord love a
duck, criminey, Ye Gods and little Fishes, pish-tosh, My
Well, bugger my giddy aunt.
Great Aunt Gussie—or as Great Aunt Gussie might say,
hells bells and panther tracks! While we understand these
oaths are insufficiently obscene for some, calling someone a
pinhead instead of a fuck-head will neither get one ticketed
nor beat like a one-legged step-child.
w
TheAbodeof theWickedDead
Down, down to hell;
and say I sent thee thither
Shakespeare
Technically hell is the nether realm of
the devil in which the damned suffer
everlasting punishment. In other words, a real sticky
wicket. The word in and of itself is not naughty. Nevertheless,
everyone knows (or at least suspects) that damning
someone to it is considered a blasphemy.
Hence, it is not surprising that an entire cottage industry
of euphemistic splendor has erupted from that word’s
roots. Indeed, the lengths to which people go to say it without
“saying it” is quite remarkable: Hades, Hail Columbia,
blue blazes, Cain, tarnation, Sam Hill, Kingdom Come,
You Know Where, or any place that implies “down there,”
the hot place, netherworld, lower regions, etc. Although
28
the Victorians gave us heck, perdition has been a suitable
alternative since the 14th century. Today we seldom hear
the once popular Go to Helen B. Happy. A shame, really.
Hellacious is a multi-purpose adjective that can mean either:
exceptionally powerful, remarkably good, extremely
difficult or extraordinarily large. To be hell-bent is
recklessly determined come hell or high water.
w
Doggone and Up Yours
Oh darn, dang, confound, consarn, dagnab, dash, blank,
or blast followed by it, are all euphemistic replacements for
the word, damn. All denounce someone or something as
evil. Truly genteel society frowns on these seemingly benign
adjectives as well as bloody, bleeding,
blamed, all-fired, dad-gummed, dratted,
and cotton-picking.
At one time, a curse was serious business.
No one took lightly being consigned
by another to hell, which may
be why Go to the Devil morphed
into Kiss My Ass. Go jump in the
lake or take a long walk off a short
pier are only nicer sounding ways to
tell someone to fold it five ways and
shove it where the sun don’t shine.
censure, denunciation, disparagement, reproach,oppro-
29
Silent Disparagement
(The Bird and His
Friends)
Although many think of it as contemporary,
digitus infamis or digitus impudicus (infamous
or indecent finger) as a phallic symbol has
been referenced in literary works as early as
ancient Rome. Mad-as-a-hatter Caligula was
rumored to hold up his middle finger for supplicants
to kiss.
There is the obvious suggestion of genitalia
in fist and extended middle finger, but we
have heard that during early warfare, captured
enemy archers had their fingers removed so they
couldn’t draw a bow. Therefore, holding up two fingers
(index and middle) backwards to one’s enemy signified
one could still do them damage. One could premise
that’s pretty much saying “f--- you.”
From the fight scene in Romeo & Juliet, which
commenced with the snapping of thumbnail under
the front teeth, to Texas A&M’s upraised “Gig ‘Em
Aggies” thumb, we see the ultimate insult can be insinuated
by other than extended middle finger. In the Arab world,
palm down, middle finger waggling downward means the
same as raised middle finger in the West.
31
censure, denunciation, disparagement, reproach,oppro-
VFYI: The little finger offered as
suggestion of a, shall we say, modestly
proportioned male part is not of modern
origin. Seek ye the Bible. I Kings 12:10.
32
To express general disrespect, there is the Cock a Snook,
also known as Ann’s Fan or Pulling Bacon, which is the
thumb on nose, fingers waving. To grasp one elbow and
raise a fist is one of the commonest insult
found worldwide, but is not universal.
That title must go to displaying
one’s naked backside. Anthropologists
say mooning predates Braveheart and,
loosely translated, meant “eat shit.”
After the fall of Bagdad, we saw Iraqis
beating the tar out of portraits and statues
of Saddam Hussein with their
shoes, revealing to westerners one of the strongest insults
of their culture—that of sticking the sole of your shoe in
someone’s face.
As there are any number of variations of armpit, bicep, fist,
finger, thumb, nose, crotch and spit . . . maneuvers to express
disrespect in different cultures, if one must hail a cab,
say, in Greece or New York, do so with all due caution.
Circumlocution
Because we often toss them about willy-nilly, we may forget
that euphemisms serve a greater purpose than merely
keeping the ladies at a garden party from glaring at us over
the top of their spectacles. A glib turn of phrase can spare
wounded feelings, a few mincing words keep lawyers at bay.
Until the Victorian era, however, the euphemistic mother
lode had not really begun to be mined. Once Queen Victoria
was on her throne and her minions on high alert, there
was little that couldn’t be accused of having a sexual, and
therefore, evil, connotation. Everything had to be renamed.
Hence, a bull became a cow’s spouse and one’s buttocks,
sit-upons. One can only imagine how dicey it must have
been sitting down at Sunday dinner for some poor soul
trying to ask for a specific piece of chicken.
w
A Woman of Expansive Sensibilities
Paphos was an ancient city of Cypress known for worshipping
Aphrodite. The well-traveled, or at least well-read,
33
Victorian men found it quite sly to call a prostitute or her
doings, Paphian. Further 19th century circumlocution favored
demimondaine, academician, abbess, courtesan,
Fille de jolie (fun gal) or nymph du pave (streetwalker). The
term of choice for those whose professions or predilections
sought to save her soul: she was a fallen woman.
One rarely hears of a lady of certain description or
painted woman anymore, but one would have to be pretty
obtuse not to understand the meaning. A little more recent
is woman of the night, streetwalker, naughty girl, and
commercial sex worker. A quick check of our Yellow Pages
did not uncover services by call girls. However, escort,
model, and actress listings are numerous and offer “discreet
billing.”
As for the specific establishment where these shenanigans
take place, a century ago it was referred to as a leaping
academy, vaulting school, disorderly house, knocking
shop, or chamber of commerce. However dated that
sounds, one must agree that today’s snake ranch or slut
hut is not much of an improvement. Granted, whorehouse
is to the point, but just a tad crude. If compelled to speak
of it, polite society might say it is a brothel, place of accommodation,
bagnio, or seraglio. Or, depending on your
frame of mind, house of ill-repute.
Furthering the subject, we proffer that he who pimps prostitutes
is not a pussy peddler, hole-toller, buttock broker,
vent renter, or crack salesman, but a panderer, procurer,
or the French, sounteneur, and with or without pimpmobile,
undoubtedly, a louse (editorially speaking).
For he who thinks he is pulling the wool over by describing
she who is gyrating upon his lap as an exotic dancer rather
Dividing the spoils?
35
denunciation, disparagement, reproach, opprobrium,
than a stripper, be advised, he can go one step further in
self- (or wife) delusion. Employing the term ecdysiast is
even more oblique. Although ecdysis does sound like a
moderately uncomfortable medical procedure, it is actually
the molting or shedding of a skin like a snake.
w
Men Much Taken With Wenching
As lechery appears to be an
accepted major by college-age
males, modern vernacular has responded.
Nowadays, he who pursues
such activities with undue
vigor is a walking hard-on. If the
little stud muffin has seen more
tail than a toilet seat, when the
dean writes home of his expulsion,
he may be described as of
distempered blood and duteous
to vices. (Well, maybe if he was
at Oxford.)
We can call him a debaucher,
libertine, flesh-monger, incubus,
Lothario, insatiate, or roué,
but it doesn’t make him any less
36
irredeemable. Of course, if he insists he is a man of the
world by way of visiting three county fairs and a goatfucking,
he undoubtedly is a bon viveur.
w
The Prevaricator
Liar, Liar, Pants on Fire
At one time, to question a
person’s honesty was no trivial
matter. Such was its consequence;
one dare not bandy
the word liar about. Therefore,
the more innocent prevaricator
was often accused
of only spinning a windy or
embroidering the truth. A
mountebank would lie like
a rug, and a charlatan was crooked as a barrel of snakes.
To piss in someone’s pocket means one is feeding him a
pack of lies.
A lie can travel halfway around the world
while the truth is putting on its shoes.
Mark Twain
37
denunciation, disparagement, reproach, opprobrium, Honest
Bob’s
Used
Wagons
If you seek the finest for
the least, Honest Bob
can procure it for you.
38
It’s hard to believe that a man is telling the truth
when you know that you would lie
if you were in his place.
H.L. Mencken
Current euphemisms such as terminological inexactitude
and economy with the truth dilly-dally about. When last
we checked, thou shalt not bear false witness against one’s
neighbor was still the ninth commandment. So “I misspoke”
won’t cut it with the keeper of the pearly gates.
A lie is an abomination to the Lord
and a very present help in trouble.
Adlai Stevenson
Although not technically lying, pettifoggery fits into this
category for general unscrupulousness. Since the 16th century,
it has described the antics of two or more lawyers
haggling unceasingly about minute matters thereby inflating
their client’s bill—thus proving the old axiom about the
more things change, the more they stay the same.
Q. Why do lawyers wear neckties?
A. To keep the foreskin from crawling up their chins.
disparagement, reproach, opprobrium, reproof,stricture,
w Tuft Hunters and Suck-ups
In English colleges such as Oxford, the aristocrats wore
special tassels (tufts) on their mortarboard hats to denote
their status. The more obsequious among the student body
sought them out, ergo—tuft-hunters. To most, these truly
annoying suck-ups are sycophants.
If overtaken by an undeniable need to
publicly decry this character flaw, one
might whip out one’s French dictionary
and sniff “leche-cul” (butt-kisser) right in
the servile flatterer’s face. Once out of high
school, however, it is advisable to sling
more derogatory comments such as
bootlicker and brown-noser behind the
back. If you do cast this particular stone,
understand that bootlicker is associated
with the habit of kissing the feet of kings
and therefore conveys a modicum of respectability (only
barely). However, it is often overlooked that brown-noser
refers to the result of smooching another part of the anatomy.
Shakespeare called them all puling pickthanks.
40
41
disparagement, reproach, opprobrium, reproof,stricture,
Clothes make
the man. Naked
people have little
or no influence
on society.
Mark Twain
w
In the Altogether
By definition, if one finds oneself in dishabille, one is carelessly
attired. In truth, that French term is often nothing
less than an outright accusation of misconduct. Not only
has one been cited for having one’s clothes in a muss, but
also by having them become that way because one has
been fooling around. Standard advice: Gather whatever
dignity one is able to muster, deny everything and make
a brisk exit.
If caught without a stitch in the great outdoors, one is au
natural. If indoors and can strike a pose, one is nude. In
the case of being stark-ballock-naked and in a compromising
situation, one is nekkid.* No defense—beat a hasty retreat
without the bugle call.
w
The Part that Goes Over the Fence Last
It is never necessary to use that three-letter word regardless
whether it has been in use since the 12th century. Nor
the four-letter one short for buttocks. Just say buttocks.
Or bottom, behind, or rear end for heaven’s sake. If that
is just too simple and one feels the need to express oneself
more floridly, we suggest posterior, derriere, ampersand,
parts behind, prat (hence, pratfall), differential, fanny,
fleshy part of the thigh, blind cheeks, bum, or tushy.
However we would like not, one hears, of course, of
ying-yang, wazoo, and poop-chute. Or if you prefer the
cloak of Latin, gluteus maximus.
42
*This southern colloquialism, often preceded by “buck,” is differentiated
from naked thusly: “Naked” means you do not have any clothes on.
“Nekkid” means you don’t have any clothes on and you’re up to no good.
There once was a woman from Mass
Who had an enormously large ass
when asked does it wiggle
she replied with a giggle
No, but it occasionally does pass gas.
43
reproach, opprobrium, reproof, stricture, vitriol, epithets
w
The Endomorph
He sweats to death,
And lards the lean earth as he walks along.
Shakespeare
“A goodly bulk,” Shakespeare
also called it. But
even on those rare occasions
when an absolute description
is unavoidable,
however ample the avoirdupois,
we believe buffalobutt,
barge-assed, or
hopper-hipped are unnecessarily
mean. Weightadvantaged
would be discreet.
With corpulent,
obese, or endomorphic,
one gets the broad-beamed
picture. Callipygian or
Rubenesque are downright
complimentary.
It is a long-held defense for
having an amply fleshed
mate that one is assured of
optimum warmth in the winter
and shade in the summer.
Conversely, lore tells of a
guy of disturbingly low
reproach, opprobrium, reproof, stricture, vitriol, epithets
morals and poor initiative who only dates fat girls because he
figures, “They don’t have much willpower.”
We are seeking out the purveyors of these stories in order
to exact retribution.
VNOTE: Any abuse is allowable if it is indemnified by
the “bless her heart” clause. The only criteria for its application
is that one can either claim Southern heritage or
manage a credible Southern drawl when it is employed:
“That girl is so fat, bless her heart, if she sat on a bug it
would fossilize in five minutes.”
Clarification: In the South, a boy or a girl is anyone under
the age of 60.
w
Ill-Favored by Nature
Whether or not a person looks like they fell from the ugly
tree and hit every branch on the way down, one certainly
would not want to make this observation within their earshot.
If it becomes necessary to describe an unprepossessing
person to a third party and one does not want to be
Never try to teach a pig to sing,
it wastes your time and annoys the pig.
Proverb
out-right deceptive, said person might be described as
unlovely, disagreeable to the eye, or a bit homely. Do
avoid butt-ugly at all costs (impolitic remarks have a nasty
history of payback).
The Paper Bag Rule
If only one paper bag over the head is necessary to keep
from frightening children, one is uncomely. Two paper
bags, admittedly hard-featured. Three paper bags, o.k.,
butt-ugly. If the person in question is a close friend or relative,
said person is plain but has a good personality.
Postscript: If one would chew off an arm in the morning to
escape undetected from a one-nighter who looked all right
when they said, “Last Call,” that person is Coyote Ugly
(owing to a coyote’s supposed willingness to chew off a
limb to get out of a steel-jaw trap).
46
The Lord prefers common-looking people.
That is the reason why he made so many of them.
Abraham Lincoln
In our western regions, if one looks a bit worse for the
wear, one has been rode hard and put up wet. If this colloquialism
needs explaining, then it would be wise not to try
to work that dog won’t hunt into the conversation either.
w
Short Pockets
A small-statured person is not sawed-off nor suffering from
duck’s disease (short legs), but is vertically-challenged,
abbreviated, a bit close to the ground, compact, diminutive,
petite, slight, undersized, wee, or not tall. Alternately
one with exceedingly long legs may have high pockets and
run like a dromedary with the staggers, but it would be
kinder to describe him as lean, lanky, or rangy. She is statuesque,
unless, of course she is a carbon copy of Olive Oyl.
If this is the case, one might want to disregard bony, emaciated,
scrawny, living stick, or skeletal and rely on slender
or a bit spare.
w
Buck-Toothed
As to why the French describe someone with protruding
teeth as dents a l’anglaise, we shall, in the name of diplomacy,
not look to the British throne.
47
opprobrium, reproof, stricture, vitriol, epithetsand
Bacchus hath drowned
more men than Neptune
Dr. Thomas Fuller
And ale for
my steeds!
Worshipping at the
Shrine of Bacchus
w
Killing a Few Brain Cells
Webster’s first definition for bibulous is “highly absorbent,”
which is probably why its second definition describes one
who over-imbibes on alcoholic substances. Over-imbibers
are also: besotted, befuddled, bleary-eyed, blotto, soused,
bombed, Bosco Absoluto, adrip, afloat, wall-eyed, cupshot,
lit, likkered up, walking on rock socks, or stinking
drunk.
An oenophile is a lover of wine. With the addition of a
prepositional phrase such as “of legendary proportions,”
said drinker is a wino.
The difference between a drunk and an alcoholic
is that drunks don’t have to attend all
those meetings.
How we identify inebriates today is not half so eloquent as
did our forefathers. Their excessive quaffers were called
49
belch-guts, bibblers, biled owls, bloaters, boosey-cocks,
bubbing-culls, cadgers, fuddle-caps, fuddlers, groghounds,
gullions, guttles, large-heads (a hands-down
favorite), lick-spigots, lick-wimbles, moist-uns, plonkdots,
squiffs, and tosspots.
The productive drunk is the bane of moralists.
Anon
50
An alcoholic is someone you don’t
like who drinks as much as you.
Dylan Thomas
The trouble with jogging
is that the ice falls
out of your glass.
w Paying for It
crapulous \ kra-pye-les\ adj [LL crapulosus,
fr. L crapula intoxication, fr. Gk
kraipale ] (1536) 1 : marked by intemperance
esp. in eating or drinking
2 : sick from excessive indulgence in liquor
If not behind the wheel, intemperance can
be relatively benign. Indeed, a crying jag
is embarrassing but hardly lethal. Be forewarned,
even wearing beer goggles (optically
impaired by drink) can get a limb
chewed off (see coyote ugly). In one’s
armor (fighting drunk) is the best way to
get ass-whupped.We don’t even want to
talk about the infamous brewer’s droop (also known as
whiskey dick). While the morning after one may be spitting
feathers, visited by the brown bottle flu implies a trip
to Europe with Ralph and Earl in a Buick.
I always keep a stimulant handy in case
I see a snake, which I also keep handy.
W.C. Fields
The beezie-weezies sound kind of cute. If you have
them it means an array of colorful visitors from the
animal kingdom have come to call on you (pink
elephants, blue devils, red spiders, a black
dog, or snakes—of any hue—in one’s boots).
51
reproof, stricture, vitriol, epithets and vituperation
They are also synonymous with the screaming meemies, a
term a tad more accurate. But the presence of either means
the delirium tremens or DTs have invaded. And, if on the
wagon is not something the afflicted has yet contemplated,
clearly, the time is at hand.
VFYI: St. Bibiana, 4th century Spanish
Patron Saint of hangovers
52
You’re not drunk if
you can lie on the floor
without holding on.
its, disease, ill health, infirmity, breakdowns, affliction,
ailment, attacks, bugs, collapse, complaint, confinement,
convalescence, disability, disorder, disturbance, dose,
failing health, flu, indisposition, malady, malaise,
prostration, seizure, syndrome, a bit of unwell, and
what’s been going around
I need to see the
Duchess of York.
Indisposition
They do not fall under the canopy of saving face, litigation,
nor feelings. No, these situations have to be the reason
euphemisms were invented in the first place.
w
Gastro-Intestinal Disorder
Few of life’s miseries have escaped schoolyard ridicule,
occasionally even put to rhyme. Therefore,
it is not surprising that lower intestinal
disturbance inspired at least one school-age
ditty—“When you’re sliding into home and your
pants are full of foam, diarrhea, diarrhea . . .”
There is an array of frank terms that describe
not the bowel disorder itself, but the
rapid response it necessitates. Hence, far
too often we hear the runs, quickstep,
sprints, trots, scoots, scatters, etc. Yet,
admittedly, any of these are preferable to
excusing oneself to company by declaring
onset of the screaming shits. Additionally,
if on one’s vacation one has an attack of the
turistas, assigning specific ethnic blame
Going to Europe with Ralph and Earl in a Buick
If one is sick to one’s stomach, we believe that is all the
information one needs to share. Throwing up or vomiting
are also perfectly good descriptive terms. It has been
our experience, once that announcement has been made,
everyone pretty much gets out of your way on the way to
the lavatory.
We reduce ourselves to the indelicacy of delineating regurgitation
euphemisms for no other reason than it is an absolute
playground for onomatopoetic words such as gurk,
urp, and barf. With one’s head stuck down the big white
phone, one can talk to Earl, Ralph, or Cousin Sis, call
Hughie or cry Ruth.
Invariably, the most colorful are offered up by friends of the
vomitee recounting the entire event to avid listeners: flash
the hash, flay the fox, feed the fish, drive the Buick,
bow to the porcelain altar, hug the throne, toss
tacos, woof cookies, laugh at the carpet, launch
one’s lunch, de-food, bestow a Technicolor yawn,
heave Jonah, blow beets, park a custard, or go see
the Duchess of York.
Evidently, there is bovine sub-category provision for the escalation
of vomiting: to bison (be nauseated), yak (very
nauseated), or water buffalo (throw up one’s toenails).
57
Fits, disease, ill health, infirmity, breakdowns, affliction, ailment, such as Montezuma’s revenge, Dehli-belly, Mexican twostep,
Spanish squirts, Botswana bop, or Cairo crud does
nothing to improve international goodwill.
Let’s face it, unless one is sitting on the edge of an examining
table wearing nothing but a gaping hospital gown, “I
am unwell,” is pretty much all anyone needs to tell.
w
Pussyfooting around The Curse
When OTR (on the rag) or having that time
of the month, few occurrences engender more
verbal pussyfooting (again, no pun intended) than
women’s troubles.
Victorian ladies suffered from domestic affliction. So general
a term, however, could mean either the sink is stopped
up or one’s husband is a cur. Today we seldom hear of the
flowers, floods, vapors, wretched calendar, or high tide.
While weathering feminine complaint, then as now, not
only can one entertain the general or fly the red flag, one
can have the painters in, a wet weekend, endure wallflower
week, or a visit from Aunt flo. When the British
have landed (wearing red coats), the Captain is at home
and it is BENO time (there’ll be no fun).
Inevitably, the onset of one’s menstrual period requires
covering the waterfront by the wearing of a sanitary
58
product. It is preferable to specify
perineal pad or tampon by brand
name (Kotex, Tampax, etc.), else one
is left with a hopeless number of riding
analogies: the cotton bicycle, red stallion,
white sling, white horse, or
fanny mattress.
From a male point of view, this item is
identified as peter-cheater or manhole
cover which, while applicable, are in
poor taste. Pleasure garden padlock
sounds oh-so-refined, but we haven’t conjured
an occasion when this, as a topic of
general conversation, was.
w
Crawling Creatures
When once only an accusation one screamed at the opposite
sex at recess, cooties have become a renewed nuisance,
not only to school children, but to the population in general.
(There are those who blame this phenomenon entirely
on the hippie generation.) One would think such progress
would have birthed a parallel vocabulary. That seems not
the case. Euphemisms for pediculosis, while dated, are
interesting: light troops, active citizens, bosom chums,
familiars, walking dandruff, intimate friends, and seam
squirrels.
59
VNOTE: Lobby lice are found in hotels,
but of the two-legged variety, not eight.
60
Genital or crab lice are crotch pheasants and pants rabbits.
Lice are chats, hence, technically, a chatty person is
not loquacious, but slovenly.
That nightly admonition to not let them bite not withstanding,
few of us ever encounter bedbugs anymore. To the
Victorians, they were a fact of life, yet a troubling conundrum.
The more fastidious citizens of society refused to utter
the word “bug” because of its unfortunate connotation
(see The Love That Durst Not Speak Its Name). Hence,
the pesky critters were known as gentlemen in brown,
B-flats, or Norfolk Howard (which may or may not reference
either the War of the Roses or Flodden Field—far too
obscure for a non-Anglophile to ascertain).
w
Social Diseases
Disgraceful disorders refer specifically to gonorrhea
(the clap) and syphilis (the pox). Other substitutes are:
bad blood, nasty complaint, bone ache, foul disease, delicate
taint, pintle fever, fire down below, forget-me-not,
Venus’ curse, and infinite malady. Historically, however,
such misfortune appears to have incited unlimited opportunity
to disparage various ethnicities: French measles,
Neapolitan favor, Spanish gout, Irish mutton, and
Rangoon itch.
61
w Foul Emanations
There once were two men in black suits
who had trouble controlling their poots
At lunch one finally said
As the other nodded his head
We should switch now from beans to fruits
Breaking Wind
Should one befoul the
air with an unduly emphatic
noise, one has
committed a rouser.
If one got by, it was a
blind-fart also known
the acronym SBD—
silent but deadly. Anything
in between is
a backfire, backdoor
trumpet, bad powder,
buck-snort, or bathtub
bubble. In addition,
a whistle
britches can suffer
butter’s revenge or
pocket thunder.
ailment, attacks, bugs, collapse, complaint, confinement,convalescence,
62
Under these audible
circumstances (if the dog
is unavailable to blame),
someone might have stepped
on a frog, talked German
(supposed guttural reference),
cut a rusty, sliced the
cheese, or shot rabbits. If
any of these aforementioned
indiscretions occur and the
offender does not know to
look suspiciously at others,
then that person does not
deserve to inhabit polite society.
As already observed,
when one is beset by gastrointestinal
disorder, there is
little discretionary reaction
time. We shall assume any
sullying of the air, too, is
inadvertent, giving all transgressors
(you know who you
are) blanket clemency.
This is the rankest
compound of villainous smell that
ever offended nostril.
Shakespeare
63
convalescence, disability, disorder, disturbance, dose, failing
There once was a wonderful wizard
who had a great pain in his gizzard
So he ate wind and snow
at 50 below
and farted a forty day blizzard.
VFYI: Breaking wind was actually a great party trick
in the Renaissance. Even Dante wrote of a fartiste
who made a trumpet of his ass. At the turn of last
century, a French nightclub performer, Joseph Pujol,
reportedly plied his artistry in the Moulin Rouge. Although
known to play O Solo Mio on the ocarina, his
tour de force was an anal rendition of Claire de Lune.
Dog Breath
If one’s breath is strong enough to carry coal, could fell a
horse at twenty paces, or smells like the Chinese army
has walked through one’s mouth in their sweat socks,
one has halitosis.
Some small woodland creature sneaked
into his mouth and used it as a latrine.

65
In the Privy
w
Calls of Nature
In Elizabethan time, the place of ease was known as a jakes,
this was eventually corrupted to ajax. Derivation of another
more oft used term for the facilities, the loo remains under
disagreement. Some like l’eau (French for water), others
insist it lieu (as in “place”). Nonetheless, euphemisms for
the room that contains a toilet can fall into two categories.
In the first, based on the concept of contrary connotation,
we have bank, chapel, coffee shop, commons, counting
house, cottage, library, office, parliament, Spice Island,
or the temple.
The less verbally discriminating, however, relieve themselves
in a bog, cacatorium, can, compost hole, dilberry
creek, dunny, forakers, john, necessarian, place where
one coughs, siege-house, or stool of ease. In most places
in Europe, one seeks the W.C. (water closet), which seems
infinitely more reasonable than in America’s restroom
(where one may sit but does not necessarily rest).
VFYI: Yes, the story is apparently true, there actually
was a Thomas Crapper who invented a flush toilet.
w Wring Out One’s Socks
Our study has revealed a vast disparity
between the number of euphemisms for
male urination (lots) compared to those
for female (zilch). This may well fall to
the unquestionably finer sensibilities prevalent amongst the
lady-folk. Either that or if one sits to release one’s bladder,
it is a solitary, quiet event. There is very little associated
activity once one has made certain the toilet seat is down.
But he who has a penis with which to pee can even write
his name in the snow—well, for argument’s sake, we suppose
a woman could do it, but it would take a while.
Men can also take the snake for a gallop, siphon the
python, shake hands with the bishop, point Percy at the
porcelain, or train Terrance on the terracotta after which
they can shake the dew off the lily.
Either sex could give the Chinaman a music lesson, but in
that few use china pots in which to tinkle anymore, it is
generally obsolete.
As an exit excuse to relieve themselves, men go water the
horses, feed the goldfish, see how high the moon is, kill a
snake, chase a rabbit, drain the radiator, or check the ski
rack.Women seem to just go to the “Ladies” to powder
their noses (albeit a bit nonsensically, in pairs).
66
VNOTE: There was a hunt-themed restaurant that
initiated some baffled head-scratching among their patrons
by labeling their respective restrooms, Pointers and
Setters.
67
flu, indisposition, malady, malaise, prostration, seizure, syndrome,

Afflicted by Time’s
Wing’d Chariot
Be kind to your children, they will choose
your nursing home.
For those of us middle-aged (assuming everyone lives to
be 110), a person of maturity has the dwindles, is a bit
forward at the knees, long in the tooth, white-topped,
blue-haired, rusting out, old as the hills, in one’s dotage,
and no spring chicken, whiling away their time in God’s
waiting room.
CAUTION: Make very certain the senior citizen of
whom one speaks is deaf as a post before one utters any
of these little nuggets. Else, the person upon whom one
remarks is always distinguished.
He is alive, but only in the sense
that he can’t be legally buried.
Geoffrey Madan (subject of the observation unknown)
70
w
The Bucket Kick’d
Are there any grander occasions to pull out all the stops,
euphemistically speaking, than speculating on just where
the dearly departed’s place of eternal rest will be? The
late-lamented could land in Abraham’s bosom, be church
triumphant, called to a higher service, or, less optimistically,
stoking Lucifer’s fires. Non-ecumenically, a quietus
or an exitus could have occurred.
Better judgment would insist (at least insofar as the eulogy)
one avoid calling the deceased either worm food or
buzzard meat.
Pardon My Dust
Dorothy Parker’s epitaph by
Dorothy Parker
There appears to be a paradoxical
inclination by the bereaved
to insist said worm
food to action when they have
had a mortality experience
(a term popular with the mortuary
profession). Hence, we
hear the dearly departed may
suck grass, grin at daisy
roots, buy the farm, give up
the ghost, pay nature’s debt,
pull a cluck, cash in one’s
chips, fold one’s hand, coil
one’s rope, drop off the hook,
71
slip the cable, sun one’s moccasins, take the long count,
jump the last hurdle, drop the cue, ride off on the last
round-up, or answer the ever-lasting knock.
The report of my death was an exaggeration.
Mark Twain, after reading his own obituary, June 2, 1897
Suicide is our way of telling God,
you can’t fire me—I quit.
72
One of the funniest of Monty Python’s routines involved
the return of a dead parrot, “Maybe he’s just shagged out
after a long squawk—no, he’s bleeding demised, ceased
to be, bereft of life, joined the choir invisible . . .”
One can go wearing the Q (the death face rather coarsely
delineated by comics—tongue lolling out the corner of the
mouth), feet first, toes up, eyes closed, heels foremost,
face turned to the wall, on one’s shield, in a box, or in
repose . . . whence one goes to the bone orchard.
ender specific activity, femininity, manhood,
manliness, masculinity, sexuality, womanhood,
womanliness, intercourse between animate beings,
coition, coitus, copulation,
intimacy, lovemaking, G
magnetism, procreation, relations, reproduction, sensuality, sexuality
fornication, generation,
Bed is the poor
man’s opera.
Italian Proverb
74
A hard man is
good to find.
Mae West
Bewitched, Bothered
and Betwattled
R
Overborne by Desire
Few, if any, still believe that only the male gender suffers
from the pangs of lust. If proof be needed, the phenomena
of Valentino, Elvis and Chippendale’s dancers provide full
support for the theory that sexual appetence is an equal
opportunity employer. Yet, regardless how prevalent its
use, we again point out that the word horny, via horn,
comes from a root word pertaining to the erect penis.
Therefore, for absolute accuracy, a woman may be just as
lustful, dissolute, concupiscent, lascivious, libidinous,
salacious, appetent, licentious, ribald, prurient, wanton,
or humpy as a man, but, unless born a hermaphrodite, or
completed gender reassignment, she will not be horny.
Those terms describing the throes of excess cupidity can
be gender specific and—however we wish they not—the
examples that come to mind for men are pussy simple,
cunt-struck and betwattled. Although a woman may have
75
hot pants or be cocksmitten, we prefer to say either is
confounded by love (more likely confounded by lust, but
it is not our place to proselytize). The unmistakable (and
most conspicuous) concomitant of desire, however, is
borne by the male: Penis in erectus.
F Temporary Priapism
Although it might initially sound like a Viagra high, a
priapism (named after Priapus, a Greek and Roman god
of male generative power) is a medical condition that manifests
itself by an unrelenting erection which is quite painful
76
A stiff prick has
no conscience.
Ancient Proverb
and—here’s the catch—is unrelieved by sexual gratification.
We will remark only upon the temporary kind.
Unlikely as it is to be referenced in one of Martha Stewart’s
fine books, for procreative (or recreational) purposes
everyone will agree that an erection is A Good Thing.
However, if the little devil rears its head when copulation
is merely on the mind but not imminent, it might prompt
some explaining—something we did not find indexed by
Miss Manners either.
If a rise in one’s Levi’s is espied by someone peripheral to
the action, we advise the male in question to adopt an air of
innocence and complain of an involuntary biological reaction.
Genital tumescence, virile reflex, and male arousal
are equally non-accusatory terms. All are preferable to hat
rack, blue-veiner, clothes prop, tent peg, live rabbit,
proud meat, horn colic, bit of a stiff, or sporting some
wood—even if one is ready to dig post holes with it.
VNOTE: Many men consider an inadvertent hard-on
(an expression we do not endorse) as an unwitting condition
and maintain, therefore, that they should not be held
accountable for that over which they hold no control (see
The Unruly Member).
Certainly beyond one’s sway is morning pride, which, for
exonerative purposes, can be identified as matutinal erection.
Indeed, if the male can convey an appropriately
sleepy-headed look, this excuse is good until noon. If one’s
nocturnal erection is inexplicably relieved during the
night, one has shot the bishop.
Gender specific activity, femininity, manhood, manliness, masculinity,
77
78
Another actual affliction is erethism, an abnormal irritability
or responsiveness to stimulation. Erethism (it too comes
from Greek, but we did not find any reference to the god of
crankiness) is an actual disorder, which does give marginal
credibility to the otherwise questionable assertion by some
men that for arousal they need no more inducement than a
stiff breeze. One could propose either of these ailments as
reason for undue . . . excitement, but both are a bit obscure.
We suggest one assert oneself as constitutionally inclined
to passion. It sounds a bit Edwardian, but far better
than randy as a goat.
Beware: If one needs to call upon this explanation
while wearing nothing more than a trench coat, it is
probable the police will look upon one’s suffering
unsympathetically. The docket sheet will read lewd
conduct, however, not weenie-wagger.
Is that a gun in
your pocket—or are you
just glad to see me?
Mae West
F
Humbled in the Act of Love
Alternately, if the male member remains flaccid regardless
of encouragement, he is suffering from orgiastic impotence.
He has not only failed in the furrow, he has no
money in his purse, lead in his pencil, ink in his pen, nor
toothpaste in his tube. When his ability is thus compromised,
he is slack in his matrimonial duty or leaving the
pillow unprest. The culprit is itself deadwood, a dangling
participle, dolphin, flounder, lob-cock, half-mast, flat
tire, hanging Johnny, or Mr. Softy.
femininity, manhood, manliness, masculinity, fornication, generation,
79
The Long Carbine
Whether one is endowed with a howitzer or peashooter,
guns are, and always have been, phallic symbols.
In the 17th century, flintlock guns had a hammer, a flint to
produce a spark, a lockpan that held the priming powder
and a main charge behind the
musket ball. When the hammer
was released, it hit a small flint
rock igniting a spark that lit the
priming powder, and if all went
as planned, then exploded the
main charge. Sometimes this prehigh-
tech procedure backfired
and the priming powder flashed
but did not ignite the main charge.
Hence a flash in the pan, but no
shot was produced. If one had
game (or the enemy) in one’s
sights but needed time to aim,
the hammer could be partially
cocked. If the gun fired while
in this position, it went off halfcocked—
no doubt a quite vexing
and dangerous occurrence.
80
81
manhood, manliness, masculinity, fornication, generation, sexuality,
We recount all of this seeming arcane information only to
provide background to fully understand the following:
If one achieves an erection but one’s intention is thwarted by
a premature ejaculation, one has gone off half-cocked, fired
in the air, shot in the bush, misfired, or has experienced
a flash in the pan. Hanging fire occurs when the priming
powder initially failed to ignite the main charge. This term
has come to be synonymous with indecision, not as some insist,
a lengthy orgasm. These expressions have been bandied
about for both sexual and non-sexual purposes for centuries.
When we study their origins, they do make perfect sense.
Sometimes a cigar is just a cigar.
Attributed to Sigmund Freud
As much as it sounds as if it should be, we all know to
peter-out is not necessarily a sexual innuendo. In fact, the
dictionary definitions for peter are as follows: (1) to diminish,
(2) to become exhausted, (3) a vulgar name for one’s
penis, and (4) one of the twelve apostles. (Insomuch as
one’s penis (3) diminishes (1) when it becomes exhausted
(2), we will conclude that other than that the Apostle Peter
(4) must have had one, he is irrelevant to this discussion).
The French word pete means to explode weakly (also an
expulsion of intestinal gas). Peter dans la main means literally,
to come to nothing. The Dictionary of Word Origins says
that peter-out originated with miners in the mid-1800’s (an
explanation of which, trust us, is even less relevant than
the Apostle Peter). Regardless, what we do know is that to
peter-out means to give out—be spent—and usually not
with a bang (so to speak). Lest one’s lover be unconsoled,
we suggest it is time to explore The French Arts.
If one can get it up, but is sterile—firing blanks, or engaging
in a dry bob, one is improcreant.
82
Agriculturalsidebar
For those unaware, when a horse
and a donkey mate, their offspring
is a mule, a hybrid. This hybrid cannot
reproduce; hence, one occasionally
hears an improcreant male
referred to as a mule.
Dallying,
Firkytoodling, and
Finkdiddling
If one has the Jones for another, as a rule, one dares not
jump their bones without first introducing oneself. Under
the right circumstances, small talk can be dispensed with,
but it is reasonable to
insist that if copulation
is the goal, at least
a little foreplay is in
order. This is known
as canoodling.
Engaging in tonsil
hockey, chewing face,
and cow kissing means,
in baseball analogy,
one has arrived at first
base. Copping a feel
above the waist is a
double. The digital
investigation of a
83
female’s privates (otherwise known as down there) is a bit
for the finger and progression to third base (a hand down
a man’s pants is probably third base also, but one seldom
hears women using sports analogies). Reasonably, physical
congress is going all the way, because one has scored
( yes, scored) a home run.
If one ruts on another without penetration or relocation of
one’s clothing, it is called a dry hump and runners do not
advance to home. Male students of an Ivy League school
perform the Princeton rub and, we are told, no one scores
(bats, if not balls, being optional).
r
Making the Beast with Two Backs
To have a legion of suitable euphemisms for doing the big
nasty at one’s fingertips, there is a very simple formula.
Merely select any combination
of an adjective
from the first column
and noun from the
second. Voila!
84
Adjective Noun
Physical congress
Carnal knowledge
Intimate necessities
Capital embrace
Amorous favors
Connubial attention
Passionate connection
Fulfilling arrangements
Horizontal relief
Illicit affections
Nocturnal pleasures
Conjugal union
Voluptuous combat
Loving consummation
Secret deed
Lewd rites
Naughty conflict
Night association
Nuptial coupling
When in want of euphemistic eloquence, however, we often
rely on Latin as in in coitu or actus coitus. Shakespeare’s
words, however, are positively rhapsodic: The very lists of
love, to make one’s heaven in a lady’s lap, dance on one’s
heels, lay it to one’s heart, or behind door work. Indeed,
having an enseamed or fortunate bed sounds a skosh classier
than parking the pink mustang up a side street. Granted,
Shakespeare was capable of circumlocutory stretch. Groping
for trout in a peculiar river means fishing in a private
stream, which means knowing someone in the Biblical
sense. Knowing someone in the Biblical sense means
manliness, masculinity, fornication, generation, sexuality, womanhood 85
86
that one is engaging in an act of generation, androgyny
(no, not Boy George—at least not necessarily), original
sin, shame, darkness, or, in the words of Delbert McClinton,
plain ol’ makin’ love.
R We Are Not Amused
In Victorian times, illicit love referred to sexual shenanigans
between anyone outside of holy wedlock. So heinous
was such activity, it was called criminal conversation (a
more stupefyingly cryptic euphemism we have yet to behold).
If it became necessary to allude to such foul doings
in print, even so ambiguous a term as criminal conversation
was far too explicit. It was abbreviated as crim. con.
or even c.c., a ruse that was not particularly effective.
Sociologists explain
that sexual shenanigans
inside holy wedlock
were frowned on as
well, but we understand
that authorities
were seldom able to
actually make arrests
for it.
H Wreak One’s Passion
If our ancestors’ sensibilities in print were euphemized into
total obscurity, private conversations were not held to the
same standard. We note that the designation for the act of
c.c. was then, as now, often preceded
by an acquiring verb: getting,
having, copping, grabbing, nabbing,
snatching, fetching, wanting,
and occasionally, begging for any
noun used to specify the female pudendum.
Typical of these nouns are
cookie, nooky, ass, beaver, bird,
jam, pork, bob, snug, box, bull’s
eye, buns, booty, cat, pussy, crack,
cranny, crotch, down, flesh, hole,
fur-pie, honey-pot, kitty, lap, milk,
monkey, mouse, muff, naughty,
oyster, poontang, rump, squirrel,
twat, twittle, and you-know-what
(often times further identified adjectively
as a bit of). All are, even
now, substituted for the detested
“c” word.*
masculinity, fornication, generat ion, sexuality, womanhood, womanliness,
87
*Although it has been in use since the
14th century, we offer less sensibility stunning
euphemisms for the monosyllable
under the heading The Little Man and
His Boat.
On occasion, this vast array of monosyllabic choices causes
an exceedingly aroused male to abandon assigning what he
wants a specific name. He may eliminate the use of a noun
altogether and just says some (get some, want some, etc).
However brainless this sounds, we have to give credit here.
Although man is an unwitting party, Webster’s does define
the word some as “being an unknown, undetermined, or unspecified
unit or thing.” This suggests that when in rut, the
male of the species does not like to restrict his possibilities.
p To Spend
If referring to the male orgasm, shot his wad, blown off,
spooged, or popped his nuts are unnecessary. He has experienced
sexual reflex. He can also have eventuation,
emissio seminis, or effect emission
(although we admit the latter does
sound a bit like a NASA instruction).
For women, making the chimney
smoke seems not overly graphic, but
again, we prefer the French, petit
mort meaning little death. Come,
spend, and get off do refer to orgasm
for both sexes, but then, so does
climax.
88
O
The Loving Spoonful
That which is ejaculated, is just that, ejaculate. Baby batter,
duck butter, man oil, bull gravy, gism, guma glue, buttermilk,
and love juice are a bit too . . . icky.We prefer it called
semen, reproductive fluid, or even sexual discharge.
Leukorrhea is the whitish viscid discharge from the vagina
known more delicately as fleurs blanches or white flowers.
(In the 19th century, wags said this condition was occasioned
by young ladies who read overly explicit French
novels.) Those less discerning call it the twitters. Only the
outright crude would pronounce her dripping for it. Under
the heading of untoward
euphemisms,
we find snail trails
(the resultant traces
of vaginal secretion
on a woman’s leg) and
pecker tracks (dribbles
of semen on trousers—
or, occasionally, a blue
dress).
fornication, generation, sexuality, womanhood, womanliness, intercourse
Prithee, summon
ye laundress.
The cheesy sebaceous matter that collects around the
glans penis and the foreskin or the clitoris and labia minora
is neither gnat bread nor crotch cheese. It is smegma
(granted, a word that sounds only marginally better than
crotch cheese). In the fairer sex, this is understood to be
too much cheese on the taco and an indication of better
attention to personal hygiene.
90
Between two evils,
I always pick the one
I never tried before.
Mae West
r
Bow at the Altar of Eros
One should try every experience once,
excepting incest and folk-dancing.
Arnold Bax
In some societies, anything other than the missionary position
( figura veneris primi) between a churched couple is
not only considered kinky but illegal. The ancient legend
of The Dragon and St. George notwithstanding, doing it
dorsally (coitus a la vache) or doggy-style will get you jail
time. But, if the woman is on her back (star-gazing) and
the man on top (beating the bunny), evidently the kitchen
table is just fine so long as the kids are asleep.
Chinese fashion denotes a sexual arrangement found in
ancient woodcuts revealing the male and the female facing
each other in an entwined X configuration. This reference
is tainted by its association with some rather tasteless jokes
proposing it necessary to copulate with an Asian woman in
such a fashion because her pudendal crevice is supposedly
horizontal.
Q. Why don’t Frenchmen like to eat flies?
A. Because they can’t get their little legs apart.
generation, sexuality, womanhood, womanliness, intercoursebetween
91
R
Different Strokes
There once was a girl from Vancouver
Whose mouth had the strength of a Hoover;
When she turned it on high,
A week would pass by,
Before anyone could remove her.
Q. What is the definition of trust?
A. Two cannibals giving each other a blow job.
In popular vernacular, a blow job, talking to mike, polishing
the helmet, giving head, or lipstick on the dipstick
denotes achieving sexual satisfaction through oral stimulation.
Either sex can be a goot-gobbler, piccolo player,
peter-eater, lick spigot, dicky-licker, or mouth-whore
(one particularly well-practiced is said to be able to suck
the chrome off a trailer hitch). Introduce the use of Latin
and a modicum of class drapes any non-orthogenital sex
act no matter how indelicate: penilingus, fellatio, or irrumatio
(root word, to suck). We add that a female who fellates
is a fellatrice.
Talking to the canoe driver, nose painting,
sneezing in the basket, yodeling in
the canyon, gamahuching, whistling in
the dark, eating at the Y, having a hairburger,
mouth music, lickety-split, and
muff-diving mean cunnilingus. (As this activity is not new,
it is not surprising that those mutton-chop sideburns were
92
called depth markers long before the sixties.) Loop de
loop, 69, or soixante neuf is mutual oral-genital stimulation.
Two women involved in such activity are engaged in
bird-washing.
Granted, shooting the beaver (or, squirrel, or moon)
could pertain to a night out with one’s favorite scent
hound, but most likely it references female exhibitionism.
Alternately, a female-attired male prostitute hides his
candy (and occasionally said candy is discovered by an unamused
client). Goober-grabber usually means a forward
woman. We note however, that goobers can be grabbed by
either sex. Sadly, we can offer no socially acceptable euphemisms
for these last entries.
sexuality, womanhood, womanliness, intercourse between animate 93
Are you a lesbian?
Are you my alternative?
O
The Love That Durst Not Speak Its Name
or the Unmentionable Vice
Paedictcato, crimen innomentatum, coitus in ano—gay
sex is still called buggery in some circles. Also, concubitus
cum persona ejus dem sexus, alternative proclivities, or
same-sex-oriented. (Although the receiver in this act has
been called a pillow-biter, we find that distinction to be
pan-sexual.)
94
95
womanho od, womanliness, intercourse between animate beings, coiti In the unlikelihood one finds it necessary to note the sexual
inclinations of another, we hold that it is best to cleanse one’s
euphemistic repertoire of the terms Nancy-boy, friend of
Dorothy’s (referencing Parker, not the Wizard of Oz—
although one’s confusion on this point might be understandable),
confirmed bachelor, light in the loafers, playing
for the pink team, pouff, and no bull-fighter. Sissybritches
and pantywaist, along with manhole inspector,
or rump ranger should also be avoided. Gunsel (derivation
of the German for gosling) is passably PC if the person
to whom one refers is also a jerk.
Daisy (along with daffodil and pansy) has long been used
as an insulting term for a gay man. That notwithstanding,
participation in a daisy chain would involve a group of
like-minded individuals engaging in simultaneous, er, sexual
acts.
VFYI: It was not until the second decade of the
twentieth century that the word faggot came to be
a disparaging term for a male homosexual. In the
middle ages, faggot meant a bundle of sticks, and,
understandably, fag eventually became slang for
cigarette. In the 1800’s, it also meant to work hard
or become exhausted (by having to cut all that fire
wood?). Its contemporary connotation possibly stems
from its use to describe the tradition of young English
boarding school boys who act as a servant to older
students. The rumors about what actually occurs in
single-sex schools when hormones rage undoubtedly
fanned the fires of this particular vulgarism.
96
As for her, she may be somewhat butch, but rug-muncher,
lesbyterian, and bull-dyke are understandably offensive.
(For further aspersions on masculine women, see Dog City.)
o
Switch Hitting
If one has taken a position of non-discrimination about the
sexual orientation of one’s romantic encounters, one is referred
to on the street as ambisextrous, double-gaited,
batting and bowling, AC-DC, trolling both sides of the
stream, driving a two-way street, or buttering both sides
of the bread.We, however, insist said person is influenced
by an amphi-genous inversion.
w Fetishes and General Freakiness
A widow whose singular vice
was to keep her late husband on ice.
Said “It’s been hard since I lost him-
But, Ill never defrost him!
Cold comfort, but cheap at the price.”
A hundred years ago the learned did not call pornography
smut, but facetiae or curiosa. By the twentieth century, an
abnormal interest in obscene material was known as coprophilia,
although more technically that is the use of feces
for sexual excitement. Those who revel in such activity are
indelicately referred to as fecal-freaks, kitchen cleaners,
and felch queens. To felch means to perform anilingus*
(tookus-lingus to our less somber fellow beings). A pound
cake-queen enjoys being defecated on. Anyone engaged in
this base pastime (beyond the age of two) is taking part in
a Boston Tea Party. An associated activity, golden showers,
is more formally termed micturation. (Our informawomanliness,
intercourse between animate beings, coition, coitus,
97
*If one needs it spelled out, anilingus is to the anus what cunnilingus is
to the cun . . . er, vagina.
tion notwithstanding, polite society prefers that if these . . .
entertainments exist, they pass unremarked.)
Apparently, there is a “queen” designation to fit every
manner of sexual diversion. For instance, one who is
under the spell of a foot fetish (equus eroticism)
is known as a shrimp-queen (referring to the
shape of the toes), enjoys sex in the great
outdoors: a green-queen, or in public:
a tea room-queen.
Once: a philosopher, twice: a pervert!”
Voltaire
(turning down his second invitation to an orgy)
If generously lubricated by cooking oil, your run-of-themill
orgy is a Mazola party. Team cream, gang bang, round
pound, and bunch punch appear self-explanatory—this
poetry occurring, undoubtedly, when one trots out one’s
pussy to pull a train, or choo-choo.
98
Said the masochist to the sadist: “Beat me, beat me!”
Replied the sadist: “No.”
Originally, ascetics employed ritual floggings to induce discipline
through strict self-denial by means of mortification
of the flesh. Evidently, somewhere in this mix of flogging
and religious frenzy, sexual titillation reared its ubiquitous
head. Pious monks in robes metamorphosed into leatherclad
dominatrices wielding wicked-looking whips. Indeed,
if one has not actually experienced it, one has certainly
heard of being whipped into a frenzy.
intercourse between animate beings, coition, coitus, copulation,fornication,
99
My problem is reconciling my gross habits
to my net income.
Attributed to Errol Flynn
There is bondage and sado-masochism. In bondage one
finds erotic stimulation either in exacting or under subjugation.
One who finds sexual satisfaction by inflicting pain on
another is a sadist—not slap artist nor fire-queen (not to
be confused with a flaming-queen who is a relatively benign,
if outrageously flamboyant, male homosexual).
Flagellation (not to be confused with flatulence—although
it is noted that foundered horses used to be flogged in order
to force them to pass gas) can be inflicted either by a
willing compatriot or one’s own hand. Indeed, a masochist
gets off by being beaten literally or figuratively. And while
most people know that sadism found its name by way of
the infamous Marquis DeSade, few realize masochism
100
was named for the German novelist, Leopold von Sacher-
Masoch who evidently also wrote of what he lived.
VNOTE: There is the term Zooerasty, but we decided
that there is some information we just do not want to have.
Clearly, however, limerick writers have been much amused
with the notion.
The Right Reverend Dean of St. Just
Was consumed with erotical lust
He buggered three men
Two mice and a hen
And a little green lizard that bust.
between animate beings, coition, coitus, copulation, fornication, generation,
101
Sex “Sain et Sauf ”
One may hear of peek-freaks and peer-queers, but they
are voyeurs. If one gets one’s jollies by listening, one is an
ecouteur.
102
103
animate beings, coition, coitus, copulation, fornication, generation, w
Dishearten
If a man is sexually aroused and then brought to satisfaction
by manual manipulation of his member by another, he
has been brought down by hand, a more circumspect term
for a hand-job, spitting white, or upshot. (The person
who supplied the hand is a peter-beater who caught an
oyster otherwise known as “sweetheart.”)
No Glove—No Love
It has been some time since one would hear of a condom
wearer as fighting in armor. So ancient is the term, it very
nearly predates the invention of circular protection itself.
It may surprise some to know that condoms were in use as
early as Elizabethan times although they did not become
common (and then only in the large cities) until two centuries
later.
These early sheaths were often made of material such as
sheepskin (ouch), fish bladder (yuck) and eventually, rubber.
Indeed, Casanova is said to have bragged of owning a
pretty little linen ditty (un petit linges) with a delicate drawstring
ribbon—the upper class does have its privileges.
Some of these early protectors were said to bear the portraits
of famous persons—the significance of which does
not immediately leap to mind. Perhaps a modern equivalent
is a lunch box bearing the image of Mighty Morphin’
Power Rangers—but we suppose we digress.
Once these handy little devices were perfected, euphemisms
did abound—keeping down the census, taking a dry run,
or wearing a cheater.Wearing a fearnought, lace curtain,
diving suit, head gasket, nightcap, catcher’s mitt, rubber
cookie, overcoat, raincoat, saddle, shower cap, lifepreserver,
washer, party hat, Dutch cap, phallic thimble,
or, less preferably, a cum drum could reference either a
condom, or a diaphragm. A pussy butterfly is an intrauterine
device or IUD.
Most commonly, however, a protective sheath has been
called, variously, a French, Italian, Spanish, or American
letter. (We did not find indication that it has been called an
English letter. Hmmm.)
Malthus was an English curate. The Maltusian
Theory posits that population tends to increase
geometrically and resources or means of subsistence,
arithmetically. Hard-liners believed that
unless procreation was checked by moral restraint
or even disaster (pestilence, famine, or
war) unrelenting poverty and its resultant human
degradation would inevitably result. This doctrine
is often cited as an excuse for the use of
birth control, hence, labeling any non-procreant
sex, Malthusian.
A vasectomy means not ever having
to say you’re sorry.
w
Sex a Cappella
The rhythm method may indeed be chancy, but it is still a
bit graceless to refer to it as playing Vatican roulette. Riding
bareback means intercourse without protection. On
occasion, condom-less sex is called making faces in that it
often leads to producing babies.
VFYI: As the male anatomy is limited in its ability
to produce semen rapidly, when a man is fortunate
enough as to enjoy what is called a triple header, a
fourth round might result in his coming air.
NOTE: In England if someone tells you to keep your
pecker up, it is indeed an expression of encouragement.
However, not necessarily what one imagines. In Great
Britain, pecker can mean chin.
NOTE, PART 2: There was a big hoo-haw over there when
the American movie Free Willy premiered because willy
does not mean chin in England.
NOTE, PART 3: If fortune has smiled upon one to the extent
that one has somehow missed seeing any of the Austin
Powers’ movies, please understand, in Great Britain, a
shag is not a 70’s haircut.
beings, coition, coitus, copulation, fornication, generation, intimacy, 105
My wife doesn’t understand me.We’re only staying
together for the kids. I’ve never done this before. I only
shoot blanks. If you get pregnant, honey, I’ll take care of you.
I’ll respect you in the morning. I promise it won’t come in your mouth.
107
The check’s in the mail. I’ll call you.
Men Behaving
Badly
Although snatch can be either verb or noun, in either
sense it usually refers to rapid copulation. Irish foreplay,
sometimes known as brace yourself Bridget, is essentially
a Wham Bam, Thank You Ma’am with a lilt. Jewish foreplay,
we have been told, involves only extended pleading.
A flyer can be either prone or vertical. If an upright quicky,
it is also known as a knee-trembler. Occasionally this excursion
doesn’t even entail intercourse. To cop a feel is either
with or without permission and is usually performed
nose open (for those unfamiliar with animal husbandry—
it describes an eager bull) possibly with it in his hand
(which clearly does not).
If it need be said, we
endorse neither the
above behavior nor
the euphemisms, we
only offer them for
elucidative purposes.
108
He would fuck a snake if someone
would hold its head.
109
One need not call a gestating woman knocked up*. She
is enceinte, fecund, or expecting. While we do not find
preggers offensive, round-wombed, about to find pups,
apron-up, one is up the spout, a lap full, or a bun in the
oven do not offer the proper respect as does experiencing
a blessed event.
*Again, this is a phrase to use judiciously in England. For there if a
woman has been knocked up it means she has merely experienced someone
rapping on her door. Whether or not that leads to a shotgun wedding
is beyond our polite speculation.
A Pea in the Pod
110
Misbegotten
The word bastard was once solely a comment on one’s
happenstance of birth. Some say the word came from the
French, fils de bast or packsaddle child.Born on the wrong
side of the blanket (the foul event producing said infant
taking place other than within the marriage bed) or born
in the vestry (left on the church steps) were other roundabout
ways of disparaging one’s heritage. Also, a counterfeit,
wood colt, stall whimper, nullius filius, or side-slip
(as in oops). Producing a child out of wedlock to Shakespeare
was to tender a fool. More specifically in the chain
of generations, a child born out of wedlock to a mother
Only if its
done right.
Is sex dirty?
who was of illegitimate birth herself, was said to be a bellbastard.
Today, the word bastard, as used in this sense is so out of
fashion as to be irrelevant. Indeed, most probably all these
euphemisms have been overridden by “DNA test” and
“child support payments.”
beings, coition, coitus, copulation, fornication, generation, intimacy,
111
112
Unknown to Man
In historical romance novels, a virgin (depending on the
genre) was either picked, plucked, ruined, trimmed,
deflowered, or devirginated. When a sweet young thing
succumbed to seduction, she was persuaded to venery.
Until then she was a chaste treasure, virgin patent, or
rosebud, possibly remaining that way by wearing iron
knickers. Digital investigation of the vagina comes under
the heading of heavy petting. For
anyone in need of exact terminology—
in legalese, carnal knowledge
is the slightest penetration of the
vulva. If one is messing with jailbait,
one is looking at jail time.
Even if she spends more time on
her knees than a priest, she is still
technically a virgin—a demi-vierge—
never having gone all the way.
Indeed, if the results of recently
published, if somewhat unscientific, polls are to believed,
by today’s standards, a fellatio generating enough suction
to suck a golf ball through a garden hose would not be
considered a sex act. We believe this is now known as
The Clinton Exculpation.
In scholarly tomes,
one may come across
the term, claustrum
virginale. This reference
is a bit of a
stretch in that claustrum
is one of the
four basal ganglia in
each cerebral hemisphere
that consists
of a thin lamina of
gray matter separated
from the lenticular
nucleus by a layer of
white matter (whew).
However, we believe
it is more likely that
it is derived from
claustral—cloister.
coition, coitus, copulation , fornication, generation, intimacy, lovemaking,
113
114
Trafficking
with Oneself
95% of people masturbate.
The other 5% just lie about it.
If there are no obliging friends about and one does not care
to avail oneself of commercial outlets (brothels), genital
stimulation via phallengetic motion may be the only alternative
for one’s sexual . . . disposal. Still condemned by
many, the solitary vice has managed not only to survive,
but flourish, even under the threat of blindness, insanity,
hairy palms and your mother’s (and the Church’s) wrath.
There was a young fellow from Yale
Whose face was exceedingly pale.
He spent his vacation
In self-masturbation
Because of the high price of tail.
Many believe masturbation is synonymous with Onanism.
In the Bible, however, Onan’s sin, scholars insist, was not
coitus, copulation, fornication, generation, intimacy, lovemaking,magn
115
masturbation at all but coitus interruptus. (Regardless, he
was slain by God for this heinous sin and let that be a lesson
to us all.) This misunderstanding may well have been a
deliberate Victorian manipulation of the scriptures laying
the groundwork for generations of adolescent guilt-trips.
Additionally, as Onan was a guy, these guilt-trips were
taken primarily by young men. The Victorian rationale
was that all womankind (wives, daughters, mothers—i.e.
any female a Victorian man was not trying to seduce) were
chaste of mind and body. Indeed, most doctors of the era
persisted with the fallacy that ladies were devoid of sexual
desire; therefore, the possibility of these women having an
orgasm with or without the aid of a penis did not exist.
Hence, it is the masculine population whose indiscriminate
nocturnal . . . twiddling came under intense scrutiny and
abject condemnation. So strong was the shame, we still
hear self-pleasuring condemned as genital pollution, selfabuse,
and the sin of youth.
Wanking may be the more oft-used term, but for those
formal occasions, digitally oscillating one’s penis or selfinduced
penile regurgitation would be preferable. In
French, se branler, se crosser, se faire les cinq doigts de la
main, se passer un poignet, or la veuve poignet (branler—to
shake, crosser—to club, faire—perform, cinq doigts—five
fingers, passer—happen, poignet—wrist, veuve—widow—
you do the math.) If merely fiddling with the equipment,
one is playing pocket pool.
As the dogma surrounding this abhorrent act is so intense
and the deed is fraught with such euphemistic eloquence, it
is an absolute necessity to be generous in recounting them.
w
Take Herman to the Circus
The chasm separating proper Victorian sensibility and that
of the unwashed masses about who was and was not the
116
117
Master of One’s Domain (a term “Seinfeld” did not invent)
can be succinctly defined. For every shaming euphemism,
we find dozens that are unrepentant (and if not actually
poetry, one can appreciate the rhyme): bleed the weed,
bang your wang, shake the snake, ram the ham, rope the
Pope, spank the frank, squeeze the cheese, stroke the
bloke, crank one’s whank, flog the log, lube the tube,
wanker the anchor, hone the cone, strain the vein, pump
the stump, torque the fork, thump the pump, tickle the
pickle, jerkin’ the gherkin, yank your plank, tease the
weasel, fist your mister, punchin’ the munchkin and
make the scene with a magazine.
Some are, if not poetic, at least alliterative: burp the baby,
cuddle the Kielbasa, fondle the fig, punish Percy in the
palm, smash the stake, hug the hog, stir one’s stew,
strangle the stogie, slap pappy, bash, beat, or bop the
bishop, pummel the priest, wave the wand, whip one’s
copulation, fornication, generat ion, intimacy, lovemaking, magnetis m,
wire, paddle the pickle, bang the banjo, dash one’s
doodle, grip the gorilla, and prime one’s pump.
There are dated euphemisms—get the German soldiers
marching, have a date with Rosy Palms, polish one’s
helmet, phone the czar, take Herman to the circus, feed
the ducks, clean one’s rifle, give a one gun salute, and
choke the sheriff and wait for the posse to come; and
contemporary—adjust your set, go on Pee Wee’s little
adventure, boot up the hard drive, paint a small Jackson
Pollock, stretch the turtleneck, play the single-string
air guitar, feed the Kleenex, tweak your twinkie, do
the Han Solo, romance the bone, choke Kojak, play
Uno, R2 your D2, upgrade your hardware, test fire
the love-rocket.
There once was a man named McGill,
Whose acts grew exceedingly ill,
He insisted on habits,
involving white rabbits,
and a bird with a flexible bill.
The animal kingdom is not only not exempt, it is well represented:
gag the maggot, lope the mule, wax the dolphin,
burp the worm, look for ticks, and corral the tadpoles.
Granted the prominent feature of euphemisms for
this activity is hardly political correctness, a few examples
would outright enrage PETA: club the baby seal, violate
the hedge-hog, flog the dolphin, suffocate the trout,
pound the pup, pump the python, or choke the chicken.
118
119
I swear, you could ask your
class if they’d had sex with goats
and the next thing you’d hear
is somebody asking,
‘Define sex.’
Overheard from a College Professor
Everything else falls into the category of one-night stands:
come to grips with oneself, climb Mount Baldy, audition
the finger puppets, beat the bald-headed bandit, do the
five-finger solo, iron the wrinkles, make the bald man
puke, shake hands with the unemployed, rough up the
suspect, summon the genie, butter the corn, kill the
snake, seed the rug, paint the ceiling, dig for change,
fire the flesh musket, frost the pastries, mangle the
midget, unload the gun, varnish the flagpole, and cane
the vandal.
While amusing, none of the above makes quite the statement
as does address Congress.
120
w
Searching for Spock
Nymphomaniacal Jill
Tried a dynamite stick for a thrill
They found her vagina
Way over in China
And bits of her tits in Brazil
Contrary to popular opinion, self-pleasuring (even, experts
say, obsessive self-pleasuring) is not unique to the male of
the species. We note that masculine verbal images involve
all manner of phallic symbols such as guns and beasts.
Those feminine (aside from allusions to small furry creatures)
are quite dissimilar. (These are offered, we swear,
only for their sociological enlightenment value.)
There are culinary references—preheat the oven, baste
the beaver, grease the skillet, butter the muffin, skim the
cream, sort the oysters, stir the cauldron, roll the dough,
and stuff the taco. In the aforementioned animal category—
caress the kitty, dunk the beaver, feed the bearded clam,
fan the fur, make the kitty purr, roll the mink, floss the
cat, and check the foxhole. Specific to the feminine sex
too are dig for one’s keys, candle bashing, apply lip gloss,
dust the end table, air the orchid, do one’s nails (also
soak in Palmolive), get a stain out of the carpet, part the
petals, polish the pearl, do something for chapped lips,
gusset typing, unclog the drain, ride side-saddle, paddle
the pink canoe, wake the butterfly, and work in the garden.
fornication, generation, intimacy, lovemaking, magne tism, procreatio 121
122
Fare un ditolino (Italian: to do a little finger)
Specific to self-digitation is to drown the man in the boat,
circle the knoll, flip through the pages, grope the grotto,
leglock the pillow, null the void, read Braille, stroke the
furnace, surf the channel, play the silent trombone, do
the two-finger slot rumba, play solitaire, check one’s oil,
tickle one’s fancy, tiptoe through the two-lips, check the
status of the I /O port, bury the knuckle, and search for
Spock. Oh yes, females can rhyme too: scratch the patch,
scuff the muff, itch the ditch, and rubbin’ the nubbin.
Not specific to women, but peculiar to them is the polymorphously
perverse orgasm where the entire body, not
123
generation, intimacy, lovemaking, magne t ism, procreation, relations, just its genitals, is a source of erotic pleasure. Without
direct clitoral stimulation, some women say psychic
orgasm can be achieved. All one needs is either a risqué
novel or a full bladder. No doubt, euphemisms have been
coined just for such occurrences, but we are still digesting
the information.
w
Pokin’ the Pucker
The word dildo has been in use since the 16th century, but
there is evidence the item was created not long after Eve
herself. The ancient Greeks called them godemiches, the
French, bijoux indiscrets. Made of glass or velvet, they
were also known as paprilla, cazzi, consolateurs, and bienfaiteurs.
Whatever one’s position on the use of mechanical
devices to achieve sexual fulfillment, we can agree that
today’s battery-powered vibrators are
a vast improvement over 18th century
ladies’ penchant for turkey necks
(headless, carcass-less turkey necks
we pray, else it’s a whole other story).
Today, if a woman employs an indiscreet toy too . . . enthusiastically
upon herself, the medical community refers
to it as a picket fence injury. Masturbatory mishaps by the
male of the species seem to revolve around where, shall we
say, the evidence might happen to land and is rarely lethal
(notwithstanding Portnoy’s worry about the bathroom light
bulb). As far as we have evidence, that story about the
young man whose investigation of the erotic effects of a
vacuum cleaner hose left him with a severely elongated
penis, is only an urban legend.
Although many believe that inflatable dolls are a recent
phenomenon, we understand that there was such a thing for
horny sailors called a Dutch sea wife. How anatomically
correct these dolls were remains unascertained, but a
Dutch husband was a bed bolster.
124
Were it not for imagination, a man
would be as happy in the arms of a
chambermaid as of a Duchess.
Samuel Johnson
w
Fleshly Treason
What men call gallantry and the gods, adultery
Is much more common where the climate’s sultry
Lord Byron
Let us make this abundantly clear: adultery is what others
commit. However, if one’s own shoes find themselves under
another’s bed, one has suffered an error of the blood.
A spouse’s infidelity is grounds for divorce (if not an
exchange of gunfire). If one wanders a bit (“I was thinking
of you the whole time, honey”), it begs forgiveness.
In the 1900s, those guilty of facile morals committed a
marriage breach by engaging in illicit embraces. Now, if
intimacy, lovemaking, magne t ism, procreation, relations, reproduction,
125
one’s affections stray, it is called offshore drilling, parallel
parking, or extra curricular activity. In any case, one
is likely to be the recipient of a folded piece of paper announcing
one’s imminent matchruptcy, dewife-ing, or
splitting of the sheets spelled D-I-V-O-R-C-E and the
lawyers won’t be kind. To remain faithful to one’s vows,
however, is to keep league and truce.
If one is not in a committed relationship and simply
screwing around, it is an affaire d’ amour (which is French
for one night stand). Hence, one is in an irregular situation,
breaking the pale, and indecorously familiar.
126
French does offer us very specific nuggets of circumlocutory
gold. Cinq-a-sept refers to a customary afternoon period
for quick assignations, hence the slang, un petit cinqa-
sept—a matinee; Le demon de midi—demon at noon:
mid-life crisis or middle-aged men or women with
eighteen-year-olds. (Ever notice that you never see someone
living solely on social security sporting arm candy?)
Skin off old dead horses is to marry one’s mistress.
Q. How are a redneck divorce and a tornado alike?
A. Somebody’s gonna lose a trailer.
Of a man who was very unhappy in marriage and remarried
immediately after his wife died, Samuel Johnson observed
that it was “triumph of hope over experience.”
w
Family Jewels
Baubles, Bangles and Beads
In polite company, they are privates, urogenital concern,
apparatus, loins, or vitals. Indeed, sports announcers tend
to identify this area as the groin or lower abdomen, apparently
believing that to broadcast a more accurate “Ouch,
that shot to the nuts had to hurt!” is not FCC-becoming.
In less discreet society, the virilia are called doodads, marriage
tackle, peculiar members, nads (short for gonads),
lovemaking, magne t ism, procreatio n, relations, reproduction, sensuality,
127
wares, Adam’s arsenal, stick and bangers, lingam (from
the Kama Sutra), credentials, testimonials, pencil and
tassels, and master of ceremonies.
Whirleygigs, baubles, jinglebangers, whenneymegs,
clangers, clappers, and bangers betray a great deal of fascination
by the male of the species with their own apparatus.
Regardless how noisy they all sound, none have, as far as is
known, ever actually made any audible noise (not counting
that poor man with testicular cancer whose doctor experimented
by replacing his with ball-bearings—they didn’t
rust but his scrotum drooped abysmally).
Not unexpectedly, there is a specific term for the relaxation
of scrotum—the whiffles. There is an equally curious name
for the foreskin—whickerbill.
w
The Unruly Member (My Body’s Captain)
There was a man from Ghent
Who had a penis so long it bent
It was so much trouble
That he kept it double
And instead of coming he went.
Only the female pudendum rivals the membrum virile for
euphemistic grandiloquence such as purple helmeted warrior
of love. Are anthenaeum, Aaron’s rod, carnal stump,
128
Q. What did Adam say to Eve?
A. Stand back, I don’t know how big this thing gets!
husbandman of nature, lance of love, man-root, torch of
cupid, or dribbling dart of love too overwrought? May
we suggest swaydangle, larydoodle, tallywacker, flapperprick,
or bean-tosser (a term of which one dares not guess
a derivation). The basis for bald-headed hermit and oneeyed
trouser snake seem far less obscure.
The ancient term for the male appendage was yard. This,
mercifully, was when this word meant a stick or rod, not
thirty-six inches. Almost as ancient is man’s inclination to
give his appendage a pet name. This selection often reflects
the esteem (or lack thereof ) in which said member is held
by its owner—Big Steve, Pile-Driver, merry-maker,
General Custer, He Who Must Be Obeyed, Old Faithless,
or Sleeping Beauty. For pomposity, one cannot top
plenipo, an abbreviation of plenipotentiary, which means
“a diplomatic agent invested with full power to transact
business.” We also hear Tommy, Dick, Harry, Willie,
Giorgio, Percy, and Peter (which reminds us that it was
Groucho Marx who observed that actor Peter O’Toole’s
name was a penis euphemism times two).
The terms bayonet, bazooka, blade,
brachmard, dagger, dirk, gun, sword, and
weapon reflect the already acknowledged
phallic/weapon imagery.
One can, of course, always call it a penis.
130
131
w
The Upright Wink
or The little man and his boat
Once described by some wit as the factotum (that which
controls all things), “it” is also demesnes (female domain).
If one cannot bring oneself to say the word vagina, try
hypogastric cranny (which will serve the purpose, no
doubt, of completely bewildering one’s audience). More
easily interpreted, if not particularly concise, we have yoni,
love’s sweet quiver, delta of Venus, tufted love mound,
Alter of Hymen, Adam’s own (beg pardon?), nonnynonny,
the Ace of Spades (clearly presaged the advent of
Nair) and the cabbage garden (which does explain that
inane story about where babies came from).
There once was a woman from China,
Who went to sea on a liner,
She slipped on the deck,
And twisted her neck,
And now can see up her vagina.
The mons veneris has generated a whole school of hirsute
appellations, namely: bearded clam, bearskin, brush
brakes, bush, belly whiskers, thicket, down, nature’s
veil, fleece, fluff, motte, and beaver (which alone accounts
for the derivations beaver-den, beaver-flick, beaver hunt,
beaver pose, beaver-retriever, and the ever-popular
beaver-shot). The entire range of pussy metaphors would
require corresponding redundancy and offers no particular
revelations apart from the clearly onomapoetic term kweef
which can most briefly be described as a pussy-fart.
There was a young lady named Brent
With a cunt of enormous extent
And so deep and so wide
The acoustics inside
Were so good you could hear when you spent
132
There is a rather indiscreet tale of a man who claims to have encountered
a vaginal cleft so commodious that upon intromission he
discovered “another bugger looking for his hat.”
w
Nature’s fonts
I once knew two sisters whose breasts
They exposed to their thunderstruck guests
A policeman was called
And the young chap, enthralled
Ogled, but made no arrests.
The female breasts. Or appurtenances,
bosom, bust, front, mammary
glands, mammilla, teats,
balcony, big brown eyes, headlights,
lung warts, love bubbles, baloobas,
bazookas, bazoongies, garbonzas,
gazongas, kajoobies, toraborahs,
maracas, lollies, diddies, or bodacious
ta-tas often encased in an
over-the-shoulder-boulder-holder
or flopper stopper.
Wasn’t it one of the “friends” on TV
who observed that it was God’s plan
magnetism, procreation, relat ions, reproduction, sensuality, sexualit 133
that men didn’t have boobs because, if they did, “they’d
never get anything done”?
I knew a young lady named Claire,
Who possessed a magnificent pair,
Or that’s what I thought,
Till I saw one caught,
On a thorn and begin losing air.
134
beings, coition, coitus, copulation, forni cation, generation, intimacy, lovemaking, 135
136
aft, mentally strange, barmy, unzipped,
batt y, berserk, insane, bonkers, cracked,
loony, crazed, cuckoo, demented, deranged, peculiar,
erratic, flaky, fruity, idiotic, insane, lunatic, mad, maniacal, nuts, potty,
psycho, touched, unbalanced, unglued, unhinged, wacky
D135-
We are all born mad.
Some remain so.
Samuel Becket
The Gazelles are
in the Garden
w
Cerebrally Challenged
As spoken by a Texan, it is not “ig-nor-ant,” but “ig-nernt,”
thus altering the meaning from “unlearned” to “too stupid
to live.” Our exploration of euphemisms for ignorance will
be specific to the second definition.
Stupidity is the deliberate cultivation of ignorance.
William Gaddis
Obtuse, dull, imperceptive, opaque, stolid, unintelligent
—in other words, a peckerhead, one whose intellect is rivaled
only by garden tools, got off the Clue Bus a couple
137
They say that the difference between
genius and stupidity is that
genius has its limits.
of stops early, is dumber than a box of hair and couldn’t
pour water out of a boot with instructions on the heel.
The cerebrally challenged can occasionally be identified
by physical characteristics: narrow between the eyes,
green as a gourd, room temperature IQ, wanting in the
upper story, more nostril hair than sense, numb nuts,
mouth-breather, and flat Peter (trampled penis syndrome).
138
Statistics say that one out
of every four Americans suffers from
some form of mental illness. If your
three best friends look okay,
then it’s you.
If one has too much yardage between the goal posts, one
is likely smart as bait, not the sharpest tool in the shed,
the brightest crayon in the box, nor shiniest bulb on the
Christmas tree. If one is as fat as a hen in the forehead
one is in want of understanding, thick as a plank, dense
as a post, dumber than a bag of hammers, experiencing a
leak in the think-tank, or suffering a crop failure and is
definitely officer material.
w
Whiff of the March Hare
Insanity doesn’t only run in my family, it actually gallops.
Anon.
w
Non Compos Mentis
Although lunatic is a perfectly good word to describe
someone who is absolutely crackers, this loss of reason is
rarely addressed head-on. Circumlocution often implies the
139
Daft, mentally strange, barmy, unzipped, batty,berserk,
afflicted is not properly wound, as in wandered, unhinged,
or unglued.
When someone is dotty, it is not unusual to allude to something
lacking; therefore they are—
A few bricks shy of a load.
Couple of bubbles off plumb.
Several fries short of a happy meal.
One midget shy of a Fellini movie.
Two clowns short of a circus.
One Fruit Loop shy of a full bowl.
One taco short of a combination plate.
A few feathers short of a whole duck.
A few beers shy of a six-pack.
A few peas short of a casserole.
Or:
Both oars aren’t in the water.
Only 50 cards in the deck.
Wants for some pence in the shilling.
Cheese slid off one’s cracker.
All one’s dogs aren’t barking.
One’s elevator doesn’t go all the way to the top.
Lights are on but no one is at home.
No seeds in the pumpkin.
Doesn’t have all his cornflakes in one box.
The wheel’s spinning, but the hamster’s dead.
Her sewing machine’s out of thread.
His antenna doesn’t pick up all the channels.
His belt doesn’t go through all the loops.
Missing a few buttons on the remote control.
140
No grain in the silo.
Receiver is off the hook.
All foam, no root beer.
w
A Stranger to Reason
If absolutely nuts, one is playing
with the squirrels, has walnut
storage disease, or is several nuts
short of a full pouch. In the pinball
game of life, if one’s flippers
are a little further apart than
most, one is dicked in the nob,
blinky (milk about to sour), off
one’s napper, has a slate loose, is
damp in the attic, one’s slinky is
in a kink, one’s skylight leaks, or
one’s drawers are left open.
Of course, if one is wealthy anything
queer one says or does is
merely eccentric.
141
insane, bonkers, cracked, loony, crazed, cuckoo,
142
VCautionv
Even a fool can be right
once in a while.
’N What?
2 Or 5
Ode to Dan Rather
Lower’n the rent on a burning building
Jumpier’n virgin at a prison rodeo
Emptier’n a eunuch’s underpants
Colder’n a well digger’s ass
Stiffer’n a preacher’s prick at a wedding
Tighter’n the bark on a tree
Smoother’n snot on a doorknob
Happier’n a coon on an ear of corn
Awkward’n a cow on skates
Lower’n a snake’s belly
Slicker’n owl shit
Clumsier’n a pig on ice
Smaller’n a bar of soap after a hard day’s wash
Colder’n a copper toilet seat in the Klondike
Deafer’n an adder
Fuller’n a tick
Hotter’n a fresh fucked fox in a forest fire
Happier’n a man who spent the day sorting
out his concubine collection
Sicker’n a pizzened pup
Finer’n bee’s wings
Noisier’n skeletons fucking on a tin roof
Uselesser’n pantyhose on a pig
Panickier’n a pig in a packing plant
Jittery’n than a long-tailed cat in a room
full of rockers
Grinnin’ like a possum eatin’ cactus
Happier’n a baby in barrel of tits
Busier’n a dildo in a harem
Icier’n the shady side of a banker’s heart
Madder’n a wet hen
Slowr’n a wet week
Happier’n clams in high water
Happier’n a puppy with two peters
6
143
demented, deranged, peculiar, erratic, flaky, fruity
The author acknowledges the Dover Publications Pictorial
Archive series, and the satirical graphics of Cruikshank,
Gillray, Hogarth, and Rowlandson.
problems. They use the past to reflect the present in hopes of
resolving its crises. Their novels explore how political history
is shaped by individuals or how it shapes them in turn.
Related Historical Events: Like the American Revolution,
the French Revolution was launched in the spirit of rational
thought and political liberty. But these ideals of the 18thcentury
Enlightenment period were soon compromised when
the French Revolution devolved into the “Terror”—a violent
period of beheadings by the very citizens who overthrew the
tyrannous French monarchy. The French Revolution cast a
long shadow into 19th-century Britain, as industrialization
seemed to divide the English population into the rich and
poor. Many people feared the oppressed working class
would start an English Revolution, but a series of political
compromises and wake-up calls like Dickens’s A Tale of Two
Cities helped to avert the potential crisis.
Extra Credit
Serial fiction: Like many of Dickens’s novels, A Tale of Two
Cities was first published in installments in his magazine All
the Year Round. Many Victorian novels were first published in
serial parts and then later collected into books.
American favorite: Since its publication, A Tale of Two Cities
has always been Dickens’s most popular work in America.
Background Info
Key Facts
Full Title: A Tale of Two Cities
Genre: Historical novel
Setting: London and Paris
Climax: Sydney Carton’s rescue of Charles Darnay from
prison
Protagonist: Charles Darnay
Antagonist: French revolutionaries; Madame Defarge
Point of View: Third person omniscient
Historical and Literary Context
When Written: 1859
Where Written: Rochester and London
When Published: 1859
Literary Period: Victorian era
Related Literary Works: Sir Walter Scott pioneered the
genre of historical fiction. In novels like Waverley, Scott
places fictionalized characters against a war-time historical
tableau. Scott also uses a narrator who alternately explains,
editorializes, preaches, and jokes, like Dickens’s own
characteristic narrative voice. Historical fiction evolved with
works like George Eliot’s Middlemarch with its multiple
plot lines and realistic psychological detail. Scott, Dickens,
and Eliot all use historical fiction to examine contemporary
Author Bio
Full Name: Charles Dickens
Date of Birth: 1812
Place of Birth: Portsmouth, Hampshire, England
Date of Death: 1870
Brief Life Story: Born to a naval clerk, Dickens moved
with his family to London at age 10. When his father was
briefly imprisoned for debt, Charles worked long days at a
warehouse. He left school at age 15, but read voraciously
and acquired extensive knowledge through jobs as a law
clerk, court reporter, and journalist. As a novelist, Dickens
was successful from the start and quickly became the most
famous writer in Victorian England for his unforgettable
characters, comic ingenuity, and biting social critique. He also
enjoyed huge popularity in America where he made several
reading tours. He worked tirelessly, producing a magazine
Household Words (later All the Year Round) and cranking out
still-famous novels including Oliver Twist, Bleak House, Great
Expectations, and David Copperfield. Dickens had ten children
with his wife Catherine Hogarth, but their marriage was never
happy and Catherine left him after Dickens had an affair with
the actress Ellen Ternan. Dickens died in 1870 and is buried in
Poets’ Corner of Westminster Abbey.
The year is 1775. On a mission for his employer, Tellson’s Bank,
Mr. Jarvis Lorry travels to Dover to meet Lucie Manette. On
his way, Mr. Lorry receives a mysterious message and replies
with the words “Recalled to life.” When they meet, Mr. Lorry
reveals to Lucie that her father, Dr. Alexandre Manette, who
she thought was dead, is still alive. Dr. Manette had been secretly
imprisoned for 18 years in the Bastille, but his former servant
Monsieur Defarge, who now owns a wine shop in Paris that
is a center of revolutionary activities, has smuggled Dr. Manette
out of prison and hidden him in the store’s attic. Meanwhile, Defarge’s
wife, Madame Defarge, secretly encodes the names of
the Revolution’s enemies into her knitting. Mr. Lorry and Lucie
arrive in Paris to find Manette compulsively making shoes in a
dark corner—prison has left him insane. Lucie lovingly restores
him to himself and they return to London.
The year is 1780. In London, Charles Darnay stands trial for
treason as a spy. Lucie and Dr. Manette attend, having met Darnay
during their return from France. The defense lawyer is Mr.
Stryver, but it is his bored-looking associate, Sydney Carton,
who wins the case. Carton points out how much he himself resembles
Darnay in order to ruin the main witness’s credibility.
In France, the wealthy aristocracy wallows in luxury and
ignores the suffering poor. Marquis St. Evrémonde recklessly
runs over and kills a child with his carriage. At his castle,
he meets his nephew Charles Evrémonde (a.k.a. Darnay) who
has returned to France to renounce his family. That night, the
Marquis is murdered in his sleep.
Back in England, Charles, Stryver, and Sydney Carton all
frequently visit Dr. Manette and Lucie. Mr. Stryver plans to
propose to Lucie, but Mr. Lorry warns him that his proposal is
unlikely to be accepted. Carton also admires Lucie; he tells her
how she makes him believe that, despite his ruined past, he still
has a shred of goodness deep within him. Charles obtains Dr.
Manette’s permission to marry Lucie, but Manette refuses to
learn Charles’s real name until the wedding day. On the wedding
day, Dr. Manette relapses into his shoe-making madness after
discovering that Charles is an Evrémonde. Mr. Lorry helps him
recover. Charles and Lucie soon have a daughter of their own.
The year is 1789. Defarge leads the peasants in destroying
the Bastille. He searches Dr. Manette’s old cell and finds a letter
hidden in the chimney. The new Republic is declared, but its
citizens grow extremely violent, imprisoning and killing aristocrats.
Charles’s former servant, Gabelle, writes a letter from
prison asking for help. Charles secretly leaves for Paris and is
immediately taken prisoner. Mr. Lorry travels to Paris on bank
business and is soon joined by Lucie and Dr. Manette. Because
of his imprisonment, Dr. Manette is a local hero. He uses his
influence to get Charles a trial, but it takes over a year. Every day
Lucie walks near the prison hoping Charles will see her. Charles
is finally freed after Dr. Manette testifies. But that very night, he
is arrested again on charges brought by Monsieur and Madame
Defarge.
Miss Pross and Jerry Cruncher have come to Paris to
help. On the street, they run into Miss Pross’s brother, Solomon
Pross, whom Jerry recognizes from Charles’s English trial as
John Barsad. Sydney Carton also shows up and, threatening to
reveal Barsad as a spy, forces his cooperation to help Charles.
At Charles’s second trial, Defarge produces Dr. Manette’s
letter from the Bastille, which explains how the twin Evrémonde
brothers—Charles’s father and uncle—brutalized a peasant
girl and her brother, then imprisoned Manette to protect themselves.
Charles is sentenced to death and sent back to prison.
Realizing his letter has doomed Charles, Dr. Manette loses his
mind. That night, Carton overhears Madame Defarge at her wine
shop plotting against Lucie and her daughter in order to exterminate
the Evrémonde line. It is revealed that Madame Defarge
was the sister of the peasants the Evrémondes killed.
Carton conspires with Mr. Lorry to get everyone in a carriage
ready to flee for England. With Barsad’s help, Carton gets into
Charles’s prison cell, drugs him, and swaps clothes with him.
Barsad drags the disguised Charles back to Mr. Lorry’s carriage,
which bolts for England. Madame Defarge shows up at Lucie’s
apartment, but Miss Pross blocks her way. The two scuffle.
When Madame Defarge tries to draw her pistol, she accidentally
shoots herself. The blast deafens Miss Pross for life.
On his way to the guillotine in place of Charles, Carton
promises to hold hands with a young seamstress, who has
been wrongly accused. He dies knowing that his sacrifice was
the greatest thing he’s ever done.
Plot Summary
Characters
Charles Darnay (a.k.a. Charles Evrémonde) – Renouncing
the terrible sins of his family, the Evrémondes, Charles
abandons his position in the French aristocracy to make his own
way in England. Charles believes in the revolutionary ideal of
liberty, but is not a radical revolutionary. Instead, he represents
a rational middle ground between the self-satisfied exploitation
practiced by the old aristocracy and the murderous rage
exhibited by the revolutionaries. Charles has a heroic sense of
justice and obligation, as shown when he arranges to provide for
the oppressed French peasantry, and later endangers himself
in coming to Gabelle’s aid. However, Charles is also deluded
in thinking he can divert the force of history and change the
Revolution for the better. Similarly, Charles constantly overlooks
Sydney Carton’s potential and must learn from his wife, Lucie,
to have faith in Carton. Charles represents an imperfect but
virtuous humanity in whose future we must trust.
Dr. Alexandre Manette – An accomplished French physician
who gets imprisoned in the Bastille, and loses his mind. In his
madness, Manette embodies the terrible psychological trauma of
persecution from tyranny. Manette is eventually “resurrected”—
saved from his madness—by the love of his daughter, Lucie.
Manette also shows how suffering can become strength when
he returns to Paris and gains a position of authority within the
Revolution. Manette tries to return the favor of resurrection when
he saves Charles Evrémonde at his trial. However, Manette
is ultimately a tragic figure: his old letter from the Bastille seals
Charles’s fate. Falling once more into madness, Manette’s story
implies that individuals cannot escape the fateful pull of history.
Lucie Manette – The daughter of Dr. Manette, and
Charles’s wife. With her qualities of innocence, devotion, and
abiding love, Lucie has the power to resurrect, or recall her
father back to life, after his long imprisonment. Lucie is the
novel’s central figure of goodness and, against the forces of
history and politics, she weaves a “golden thread” that knits together
the core group of characters. Lucie represents religious
faith: when no one else believes in Sydney Carton, she does.
Her pity inspires his greatest deed.
L I T C H A R T S GET LIT www. L i tChar t s . com
TM
TM
A Tale of Two Cities
Sydney Carton – In his youth, Sydney Carton wasted his
great potential and mysteriously lost a woman he loved. Now
he’s a drunk and a lawyer who takes no credit for his work.
Carton has no hope for his life. Only Lucie understands his
potential for goodness. In his selfless dedication to her and her
family, Carton represents the transformative power of love. His
self-sacrifice at the end of the novel makes him a Christ figure.
By saving Lucie’s family, Carton redeems himself from sin and
lives on in their grateful memory.
Monsieur Defarge – The former servant of Dr. Manette,
Defarge uses his Paris wine shop as a place to organize French
revolutionaries. Like his wife, Madame Defarge, Defarge
is fiercely committed to overthrowing tyranny and avenging
injustice. Yet Defarge always retains a shred of mercy, and
does not participate in his wife’s plot to kill Lucie. This quality
of mercy makes Defarge a symbol for the failed Revolution,
which ultimately loses sight of its ideals and revels in the
violence it causes.
Madame Defarge – The wife of Monsieur Defarge, Madame
Defarge assists the revolutionaries by stitching the names of
their enemies into her knitting. Madame Defarge wants political
liberty for the French people, but she is even more powerfully
motivated by a bloodthirsty desire for revenge, hoping to
exterminate anyone related to the Evrémondes. Where Lucie
Manette is the embodiment of pity and goodness, Madame
Defarge is her opposite, a figure of unforgiving rage. Over the
course of the novel she emerges as a kind of anti-Christ, completely
devoid of mercy, and as such comes to symbolize the
French Revolution itself, which soon spun out of control and
descended into extreme violence.
Marquis St. Evrémonde – Charles’s uncle and a cruel
French aristocrat committed to preserving the power of the
French nobility. He and his twin brother exemplify the tyrannical
and uncaring aristocracy. When the Marquis is murdered,
his corpse is a symbol of the people’s murderous rage.
Mr. Jarvis Lorry – An older gentleman who works for
Tellson’s bank, Lorry is a model of loyalty and discretion. Lorry
hides his emotions under the cover of “business,” but he works
hard to save the Manettes and to encourage Charles to become
Lucie’s husband.
Mr. Stryver – A lawyer who defends Charles Darnay.
Stryver, as his name implies, only cares about climbing the
professional ladder.
Jerry Cruncher – By day, an odd-job man for Mr. Lorry.
By night, a “resurrection man”—robbing graves to sell body
parts to sketchy doctors. He complains about his wife’s praying
because it makes him feel guilty about his secret activities,
but by the end of the novel he decides to give up his secret job
and endorses praying, a sign that he hopes to be resurrected
himself through the power of Christ.
John Barsad (a.k.a Solomon Pross) – Barsad was born
Solomon Pross, brother to Miss Pross, but then became a
spy, first for the English, then later for the French government.
He is an amoral opportunist. In England, he accuses Charles
Darnay of treason.
Jacques Three – “Jacques” is the code name for every male
revolutionary; they identify themselves by number. Jacques
Three is a cruel, bloodthirsty man who represents the corruption
of the Revolution’s ideals. He controls the jury at the
prison tribunals.
The Vengeance – A peasant woman from Paris and Madame
Defarge’s ultraviolent sidekick. Like Madame Defarge
and Jacques Three, The Vengeance enjoys killing for its own
sake, not for any reasonable political purpose.
The mender of roads (the wood-sawyer) – A French
working man who represents how average people become seduced
by the worst, most violent qualities of the Revolution.
Gabelle – A servant of Charles Evrémonde who carries
out Charles’s secret charities. Gabelle is jailed simply by association
with the aristocracy, showing how justice flies out the
window during the Revolution.
Roger Cly – A spy and colleague of John Barsad who faked
his death to escape prosecution.
Miss Pross – The long-time, devoted servant of Lucie Manette.
She is Solomon Pross’s sister, and hates the French.
Monseigneur – A powerful French aristocrat.
Tyranny and Revolution
Much of the action of A Tale of Two Cities takes place in Paris
during the French Revolution, which began in 1789. In A Tale
of Two Cities, Dickens shows how the tyranny of the French
aristocracy—high taxes, unjust laws, and a complete disregard
for the well-being of the poor—fed a rage among the commoners
that eventually erupted in revolution. Dickens depicts this
process most clearly through his portrayal of the decadent
Marquis St. Evrémonde and the Marquis’ cruel treatment of
the commoners who live in the region under his control.
However, while the French commoners’ reasons for revolting
were entirely understandable, and the French Revolution was
widely praised for its stated ideals of “Liberty, Equality, and Fraternity,“
Dickens takes a more pessimistic view. By showing how
the revolutionaries use oppression and violence to further their
own selfish and bloodthirsty ends, in A Tale of Two Cities Dickens
suggests that whoever is in power, nobles or commoners, will
fall prey to the temptation to exercise their full power. In other
words, Dickens shows that while tyranny will inevitably lead to
revolution, revolution will lead just as inevitably to tyranny. The
only way to break this cycle is through the application of justice
and mercy.
Secrecy and Surveillance
Everybody in A Tale of Two Cities seems to have secrets:
Dr. Manette’s forgotten history detailed in his secret letter;
Charles’s secret past as an Evrémonde; Mr. Lorry’s tightlipped
attitude about the “business” of Tellson’s Bank; Jerry
Cruncher’s secret profession; and Monsieur and Madame
Defarge’s underground activities in organizing the Revolution.
In part, all this secrecy results from political instability. In the
clash between the French aristocracy and revolutionaries, both
sides employ spies to find out their enemies’ secrets and deal
out harsh punishments to anyone suspected of being an enemy.
In such an atmosphere, everyone suspects everyone else, and
everyone feels that they must keep secrets in order to survive.
Through the secrets kept by different characters, A Tale of
Two Cities also explores a more general question about the
human condition: what can we really know about other people,
including those we’re closest to? Even Lucie cannot fathom
the depths of Dr. Manette’s tortured mind, while Sydney
Carton remains a mystery to everybody. Ultimately, through
Lucie’s example, the novel shows that, in fact, you can’t ever
know everything about other people. Instead, it suggests that
love and faith are the only things that can bridge the gap between
two individuals.
Fate and History
Madame Defarge with her knitting and Lucie Manette
weaving her “golden thread” both resemble the Fates, goddesses
from Greek mythology who literally controlled the
“threads” of human lives. As the presence of these two Fate
figures suggests, A Tale of Two Cities is deeply concerned with
human destiny. In particular, the novel explores how the fates
of individuals are shaped by their personal histories and the
broader forces of political history. For instance, both Charles
and Dr. Manette try to shape and change history. Charles
seeks to escape from his family’s cruel aristocratic history
and make his own way in London, but is inevitably drawn “like
a magnet” back to France where he must face his family’s
past. Later in the novel, Dr. Manette seeks to use his influence
within the Revolution to try to save Charles’s life from the
revolutionaries, but Dr. Manette’s own forgotten past resurfaces
in the form of an old letter that dooms Charles. Through
these failures of characters to change the flow of history or to
escape their own pasts, A Tale of Two Cities suggests that the
force of history can be broken not by earthly appeals to justice
or political influence, but only through Christian self-sacrifice,
such as Carton’s self-sacrifice that saves Charles at the end
of the novel.
Sacrifice
A Tale of Two Cities is full of examples of sacrifice, on both a
personal and national level. Dr. Manette sacrifices his freedom
in order to preserve his integrity. Charles sacrifices his family
wealth and heritage in order to live a life free of guilt for his family’s
awful behavior. The French people are willing to sacrifice
their own lives to free themselves from tyranny. In each case,
Dickens suggests that, while painful in the short term, sacrifice
leads to future strength and happiness. Dr. Manette is reunited
with his daughter and gains a position of power in the French
Revolution because of his earlier incarceration in the Bastille.
Charles wins the love of Lucie. And France, Dickens suggests
at the end of the novel, will emerge from its terrible and bloody
revolution to a future of peace and prosperity.
Yet none of these sacrifices can match the most important
sacrifice in the novel—Sydney Carton’s decision to sacrifice his
life in order to save the lives of Lucie, Charles, and their family.
The other characters’ actions fit into the secular definition of
“sacrifice,” in which a person gives something up for noble reasons.
Carton’s sacrifice fits the Christian definition of the word.
In Christianity, God sacrifices his son Jesus in order to redeem
mankind from sin. Carton’s sacrifice breaks the grip of fate and
history that holds Charles, Lucie, Dr. Manette, and even, as the
novel suggests, the revolutionaries.
Resurrection
Closely connected to the theme of sacrifice is the promise of
resurrection. Christianity teaches that Christ was resurrected
into eternal life for making the ultimate sacrifice (his death)
for mankind. Near the end of A Tale of Two Cities, Carton
remembers a Christian prayer: “I am the resurrection and the
life.” As he goes to the guillotine to sacrifice himself, Carton
has a vision of his own resurrection, both in heaven and on
earth through Lucie and Charles’s child, named Sydney Carton,
whose life fulfills the original Carton’s lost potential. Yet
Carton’s is not the only resurrection in the novel. After having
been imprisoned for years, Dr. Manette is “recalled to life” by
Lucie’s love. Jerry Cruncher, meanwhile, works as a “resurrection
man” stealing body parts from buried corpses, but by
the end of the novel he gives it up in favor of praying for a holier
resurrection of his own.
Imprisonment
In the novel, the Bastille symbolizes the nobility’s abuse of
power, exemplified by the unjust imprisonment of Dr. Manette
by Marquis St. Evrémonde. Yet the Bastille is not the
only prison in A Tale of Two Cities. The revolutionaries also
unjustly imprison Charles in La Force prison. Through this parallel,
Dickens suggests that the French revolutionaries come to
abuse their power just as much as the nobility did.
The theme of imprisonment also links to the theme of history
and fate. For instance, when Charles is drawn back to
Paris because of his own past actions, each checkpoint he
passes seems to him like a prison door shutting behind him.
Themes
In LitCharts, each theme gets its own corresponding color,
which you can use to track where the themes occur in the
work. There are two ways to track themes:
Refer to the color-coded bars next • to each plot point
throughout the Summary and Analysis sections.
• Use the ThemeTracker section to get a quick overview of
where the themes appear throughout the entire work.
www. L i t C h a r t s . c om 2 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Summary and Analysis
Book 1, Chapter 1: The Period
“It was the best of times, it was the worst of
times…” The year is 1775, a time that the narrator
describes through a set of contradictions: wisdom
and foolishness, belief and disbelief, optimism and
doubt, light and darkness, hope and despair. The
narrator compares this historical era to his own
present moment in Victorian England.
The contradictions listed in the
opening of the novel portray
1775 as an age of profound
transition, full of promise and
threat. The comparison to Dickens’s
Victorian times establishes
the novel’s use of the past to
comment on the present.
In France, the government spends wildly and hands
out harsh sentences to anyone connected with a
crime, no matter how minor. In England, burglars
infest the cities—even the Mayor of London gets
robbed—and not even frequent hangings can stop
the wave of crime.
The narrator extends the
potential similarities between
revolutionary France and England.
Because of their injustices,
both governments are sowing
the seeds of discontent and
political radicalism.
The narrator tells an allegory of the Woodman and
the Farmer—figures of the coming revolution who are
silently at work. But the royalty in both England and
France believe in their divine right to rule and don’t
notice the gathering storm.
The Woodman stands for Death
and the Farmer for Fate. Both,
the narrator implies, will harvest
the awful products of the monarchy’s
political mistakes.
Book 1, Chapter 2: The Mail
On a stormy night in late November 1775, the mail
coach from London slogs its way toward Dover. Three
passengers sit in the carriage. Everyone is suspicious
of each other. When he hears an approaching horse,
the coach driver stops the carriage: it’s a messenger
seeking one of the passengers, Mr. Jarvis Lorry of
Tellson’s Bank. Mr. Lorry recognizes the man as Jerry
Cruncher, who works odd-jobs for Tellson’s.
The port city of Dover was the
main port for passage between
England and France. The road
from London to Dover, battered
by storm and fraught with
suspicion and highwaymen, represents
the worsening political
conditions in both countries.
Jerry gives Mr. Lorry a note that reads “Wait at
Dover for Mam’selle.” In reply, Mr. Lorry tells Jerry
to return to Tellson’s with the message: “Recalled
to life.” The coach drivers overhear the mysterious
message but can make nothing of it. Neither can
Jerry, though he worries that “recalling to life” would
be bad for his other work.
“Recalled to life” sets up the
theme of resurrection. At both
ends of the book, someone
liberates another person from
prison and saves them from
the grave. Jerry’s odd thought
establishes the mystery of what
his other work might be.
Book 1, Chapter 3: The Night Shadows
The narrator reflects on the strangeness of the human
condition: how we are all mysteries to each other. No
matter how close, we always remain alienated from
each other by our unique individualities.
One of the main themes in all
of Dickens’s work is the search
for mutual understanding and
human sympathy.
Half asleep in the mail coach, Mr. Lorry dreams of
wandering through the inner vaults of Tellson’s Bank
and finding everything safe. He also dreams that he
“was on his way to dig someone out of a grave.” In
his dream, he sees a cadaverous man who has been
buried alive for 18 years. Mr. Lorry asks the man if he
cares to live, then also asks over and over if the man
will “come and see her?” Sometimes the man cries
out that seeing “her” would kill him, at other times
that he must see her immediately.
Mr. Lorry’s dream foreshadows
Dr. Manette’s situation. Lorry’s
questions about whether the
man “cares to live” and whether
he wants to see “her,” link the
idea of Manette’s potential return
to life with a woman, suggesting
that it is love that will
return him to life. The dream
of digging up someone from a
grave also foreshadows Jerry’s
other job as a grave robber.
Book 1, Chapter 4: The Preparation
In Dover, Mr. Lorry takes a room at the Royal George
Hotel. The 17-year-old Lucie Manette arrives that
same afternoon, having received vague instructions
to meet a Tellson’s Bank employee at the Royal
George Hotel regarding some business of her “long
dead” father. Though he describes his news as just
a “business matter,” Mr. Lorry struggles with his
emotions as he explains the “story of one of our
customers”—Lucie’s father, Dr. Manette.
Mr. Lorry works like a secret
agent for Tellson’s Bank. He
uses the cover of “business” to
assist in political activities (like
freeing Dr. Manette). But he
also uses “business” rhetoric
to hide his feelings and protect
others’ emotions, even when
explaining a father’s history to
his daughter.
20 years ago, Dr. Manette, a renowned doctor,
married an English woman and trusted his affairs
to Tellson’s Bank. One day, Manette disappeared,
having been jailed by the authorities and taken to a
secret prison. Rather than tell Lucie the truth, Lucie’s
mother told her that her father was dead. Lucie’s
mother herself died soon afterwards, and Mr. Lorry
took Lucie from Paris to London.
Lucie learns her own and her
father’s real history—her father
suffered imprisonment at the
hand of a tyrannical government.
Lucie’s history makes her a
figure who connects the “two cities”
of Paris and London, and in
A Tale of Two Cities, characters
cannot escape their histories.
Mr. Lorry braces Lucie for a shock: her father is not
dead. He has been found, though he’s a shell of his
former self. Manette is now in the care of a former
servant in Paris, and Mr. Lorry tells the astonished
Lucie that he and she are going to go to Paris so that
she can “restore [her father] to life.” Lucie’s servant,
the loud and red-haired Miss Pross, rushes in and
shouts at Mr. Lorry for upsetting Lucie. Mr. Lorry
asks her to travel with them to France.
Though freed from jail, Manette
is still imprisoned by his traumatic
history. It now becomes
clear that Lucie is the woman
whom Lorry in his dream hoped
could save Manette. Miss Pross
is a stereotypical British servant,
brash, devoted to her mistress.
Book 1, Chapter 5: The Wine-shop
Outside a wine shop in the poor Parisian suburb of
Saint Antoine, a cask of wine accidentally falls and
breaks in the street. Everyone in the area scrambles
to drink the runoff: cupping their hands, slurping it out
of gutters, licking it off the fragments of the broken
cask. It turns into a game with dancing and singing in
the streets. The wine has stained the ground, stained
people’s skin and clothes. Someone jokingly uses the
spilled wine to scrawl the word “Blood” on a wall.
This scene is an extended
metaphor for how people
transform into a frenzied mob.
It foreshadows the blood to be
spilled in the Revolution. The
writing on the wall alludes to
the Biblical story (in Daniel)
of Belshazzar’s feast where a
disembodied hand prophesied
the fall of his empire.
The color-coded bars in Summary and Analysis make it easy to track the themes through the
work. Each color corresponds to one of the themes explained in the Themes section. For instance,
a bar of indicates that all six themes apply to that part of the summary.
Wine
Defarge’s wine shop lies at the center of revolutionary Paris,
and throughout the novel wine symbolizes the Revolution’s intoxicating
power. Drunk on power, the revolutionaries change
from freedom fighters into wild savages dancing in the streets
and murdering at will. The deep red color of wine suggests that
wine also symbolizes blood. When the Revolution gets out of
control, blood is everywhere; everyone seems soaked in its
color. This symbolizes the moral stains on the hands of revolutionaries.
The transformation of wine to blood traditionally
alludes to the Christian Eucharist (in which wine symbolizes
the blood of Christ), but Dickens twists this symbolism: he uses
wine-to-blood to symbolize brutality rather than purification,
implying that the French Revolution has become unholy.
Knitting and the Golden Thread
In classical mythology, three sister gods called the Fates controlled
the threads of human lives. A Tale of Two Cities adapts
the classical Fates in two ways. As she knits the names of her
enemies, Madame Defarge is effectively condemning people
to a deadly fate. On the other hand, as Lucie weaves her
“golden thread” through people’s lives, she binds them into
a better destiny: a tightly-knit community of family and close
friends. In each case, Dickens suggests that human destinies
are either predetermined by the force of history or they are tied
into a larger pattern than we as individuals realize.
Guillotine
The guillotine, a machine designed to behead its victims, is
one of the enduring symbols of the French Revolution. In Tale
of Two Cities, the guillotine symbolizes how revolutionary chaos
gets institutionalized. With the guillotine, killing becomes
emotionless and automatic, and human life becomes cheap.
The guillotine as a symbol expresses exactly what Dickens
meant by adding the two final words (“or Death”) to the end
of the French national motto: “Liberty, Equality, Fraternity, or
Death.”
Shoes and Footsteps
At her London home, Lucie hears the echoes of all the footsteps
coming into their lives. These footsteps symbolize fate.
Dr. Manette makes shoes in his madness. Notably, he always
makes shoes in response to traumatic memories of tyranny, as
when he learns Charles’s real name is Evrémonde. For this
reason, shoes come to symbolize the inescapable past.
Symbols
Symbols are shown in red text whenever they appear in the Plot
Summary and Summary and Analysis sections of this LitChart.
www. L i t C h a r t s . c om 3 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
The jubilation fades and the street returns to its sad,
dirty, impoverished condition. The people are sick
and aged, beaten down by hunger.
Hunger and want are the
conditions that fuel the revolutionary
fire.
Monsieur Defarge, the owner of the wine shop,
enters his store. From her position behind the
counter, his wife, Madame Defarge, silently alerts
him to the presence of Mr. Lorry and Lucie. Defarge
ignores them, instead lamenting the condition of the
people with three men, all of whom go by the name
“Jacques” (a code name used by revolutionaries in
France).
The code name “Jacques” does
double service: because it is a
common name, it both hides
identity and also implies that
this revolution is of the people.
Lucie and Lorry’s presence in
Defarge’s wine shop indicates
that Defarge is Manette’s
former servant.
Once the “Jacques” have left, Mr. Lorry speaks with
Monsieur Defarge. Defarge leads Mr. Lorry and
Lucie up to his attic. The room is dark and kept locked
for the sake of the inhabitant, Monsieur Defarge
explains. Lucie leans on Mr. Lorry for support. Defarge
opens the door and they see a white-haired man in
the corner stooped over a bench and making shoes.
Because his mind was unoccupied
in prison, Dr. Manette
compensated by making shoes
to occupy his hands. Now, even
though he is free, he can’t
escape the prison of his own
mind, so he continues to make
shoes.
Book 1, Chapter 6: The Shoemaker
The shoemaker is dressed in tatters. When Defarge
asks him his name, he replies “One Hundred and Five,
North Tower.” Mr. Lorry then asks the shoemaker if
he recognizes anyone. The shoemaker seems as if he
does for a moment, but his face quickly clouds over.
Dr. Manette suffered so greatly
in prison that his identity was
virtually erased. He knows himself
only by the room number in
the Bastille, the prison in which
he was held.
Lucie approaches, with tears in her eyes. The
shoemaker asks who she is. Noticing her blonde hair,
he removes a necklace he wears and reveals a scrap
of paper containing some golden threads of hair—
stray hairs from his wife, which he has kept all these
years as a spiritual escape from his imprisonment.
Overcome by emotion, Manette struggles to
recognize his daughter. Lucie rocks Manette’s head
on her chest like a child. She promises him that his
agony has ended, and gives thanks to God.
Lucie’s golden hair reminds Manette
of his wife’s golden hair.
These hairs, from before and
after Manette’s incarceration,
form a kind of bridge over his
years in prison. These are the
“golden threads” with which Lucie
weaves a better fate for her
family. Cradling Manette, Lucie
is like a mother and Manette
her child—a metaphor for
Manette’s new life ahead.
Mr. Lorry and Defarge arrange for their immediate
departure. Before he leaves, Manette asks to bring
along his shoemaking tools. With Defarge escorting
them, the group is able to get past the barricades
in the street and reach a carriage. Mr. Lorry asks Dr.
Manette if he wants to be recalled to life. Dr. Manette
replies, “I can’t say.”
Dr. Manette’s desire to keep his
tools close at hand indicates
that his emotional trauma still
lies close to the surface. Dr.
Manette’s statement, “I can’t
say,” indicates that he doesn’t
yet totally believe in the possibility
that he could escape his
traumatic past.
Book 2, Chapter 1: Five Years Later
The year is 1780. The narrator describes Tellson’s
Bank in London as an old, cramped building with
ancient clerks. The bank has business interests
connecting England and France. Encrusted by
tradition and unwilling to change, the bank seems
much like England itself.
The bank is a symbol of
England and France. Like the
tradition-encrusted bank, each
of these countries has problems
with the institutions they’ve
inherited, such as the monarchy.
In his cramped apartment in a poor London
neighborhood, Jerry Cruncher yells at his wife for
“praying against” him, which he insists is interfering
with his work as an “honest tradesman.”
Jerry’s dislike of praying and
insistence that it interferes with
his business, implies that his
work as an “honest tradesmen”
makes him feel guilty.
Jerry and his son then go to work—they sit outside
Tellson’s waiting for odd jobs from the bank. On this
day, word emerges from the bank that a porter is
needed. Jerry hurries inside. Jerry’s young son, left
alone outside, wonders why his father’s boots are
muddy and his fingers stained by rust.
The stains of guilt on Jerry’s
conscience are represented
by the mud and rust from his
nocturnal work, which is as of
yet still unrevealed.
Book 2, Chapter 2: A Sight
One day, Jerry Cruncher is sent to await Mr.
Lorry’s orders at the Old Bailey Courthouse, where a
handsome young gentleman named Charles Darnay
stands accused of treason. Jerry enters the court and
pushes through the crowd gathered to see the trial.
The spectators stare at Darnay, and one onlooker
excitedly predicts that the accused will be convicted
and then brutally drawn-and-quartered.
The sadistic appetites of this
English crowd are similar to
those of the French mob in
Book 1, chapter 5. The title of
the chapter, “A Sight,” indicates
that these people come to the
trial for the fun of it, hoping not
for justice but for the spectacle
of violence.
Charles, who stands accused of being a French
spy, is defended by two lawyers: Mr. Stryver and
the insolent and bored-looking Mr. Carton. When
Darnay glances at a young woman and her father
sitting nearby (Lucie and Dr. Manette), word flashes
through the crowd that these two are witnesses
against Darnay. Nonetheless, Lucie’s face radiates a
compassion that awes the spectators.
The compassion in Lucie’s face
indicates that she does not
want to condemn Charles, even
though she is a witness for the
prosecution. This foreshadows
Charles’s final trial in Paris,
when Dr. Manette, contrary to
his intentions, dooms Charles.
Book 2, Chapter 3: A Disappointment
The Attorney General prosecuting the case demands
that the jury sentence Charles to death. He calls
a witness, the “unimpeachable patriot” John
Barsad, whose testimony implicates Charles as a
spy. However, on cross-examination Stryver reveals
Barsad to be a gambler and brawler and a generally
untrustworthy witness. Stryver similarly is able to
raise questions about the motivations of another
witness, Roger Cly, Charles’s former servant.
The prosecuting attorney
foreshadows the later prosecutors
in France who will bend
the truth to seek an execution.
Ironically, Charles is accused
of spying while John Barsad
and Roger Cly (who are later
revealed to be actual English
spies) are presented as “unimpeachable”
witnesses.
Mr. Lorry, Lucie, and Dr. Manette are each called to
testify: they had all met Charles aboard ship on their
way back from Paris five years earlier. Lucie explains
how Charles helped her care for her father, swaying
the jury in Charles’s favor. But she then accidentally
turns the court against Darnay. How? First she admits
that Charles was traveling with other Frenchmen and
carrying lists. Second she mentions Charles’s joking
comment that George Washington’s place in history
might one day match that of England’s King George III.
Another irony: as will be
revealed later, Charles’s “suspicious”
activities are actually his
humanitarian efforts to help his
impoverished tenants in France.
He is putting himself in danger
to help others. His comment
about George Washington
(who was leading the American
Revolution at the time) indicates
that he has revolutionary
sympathies.
Later, while Mr. Stryver is unsuccessfully crossexamining
a witness who has been called to identify
Charles, Carton hands Stryver a note. After reading
from the note, Stryver forces the court to notice the
striking resemblance between Charles and Carton,
shattering the witness’s credibility.
Besides serving an important
role in the plot, the uncanny
resemblance between Carton
and Charles links them and
sets them up as doubles to be
compared and contrasted.
The jury goes to deliberate. Carton continues to look
bored, stirring only to order help when he notices
Lucie start to faint. Finally, the jury returns from its
deliberations with a verdict of not guilty.
Carton’s boredom identifies him
as uninterested in the world
and empty. Only Lucie seems to
interest him.
Book 2, Chapter 4: Congratulatory
After the trial, Charles kisses Lucie’s hands in gratitude
and thanks Stryver for his help. Dr. Manette is now
a distinguished citizen of London. He can still become
gloomy, but this occurs only occasionally because
Lucie serves as a “golden thread” linking him to his
life before and after his imprisonment. Stryver, Dr.
Manette, and Lucie depart in a carriage.
Though Lucie’s love and compassion,
her “golden thread,”
have returned Dr. Manette’s
to life, his grip on sanity is still
tenuous, only as strong as a
thread of hair.
A drunk Sydney Carton emerges from the shadows.
His shabby clothes and impertinent manners offend
Mr. Lorry, who departs. Carton and Charles go out
to dinner at a tavern, where Carton slyly asks Charles
whether being tried for his life is worth the sympathy
and compassion he now gets from Lucie. Annoyed,
Charles comments on Carton’s drinking. In response,
Carton says, “I am a disappointed drudge, sir. I care for
no man on earth, and no man on earth cares for me.”
After Charles leaves, Carton curses his own reflection
in a mirror and then curses Charles, who reminds him
of what he might have been.
Carton’s lack of manners and
shabby looks show that he
doesn’t care much about life.
His bitter comments about the
compassion Charles receives
from Lucie show that Carton
craves Lucie’s pity. His words
also suggest that Carton only
saved Charles because he
wanted to help Lucie. Carton
curses Charles because their
resemblance forces Carton to
consider his own life, which was
ruined by some past experience.
www. L i t C h a r t s . c om 4 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 5: The Jackal
The narrator describes Mr. Stryver as an ambitious
man starting to climb the professional ladder. Due to
his problem distilling information, he partnered with
Sydney Carton, who now secretly does all the work
for Stryver to win his cases. If Stryver is a lion in court,
Carton is a cunning jackal behind the scenes.
As his name implies, Stryver
“strives” to get ahead in the
world. He is uninterested in
sacrifice because he is only out
for himself.
After leaving the tavern where he dined with Charles,
Carton joins Stryver in his apartment. To stay awake,
he wraps a wet towel around his head and works
through a pile of legal documents. Stryver watches.
Carton willingly makes himself
a slave to Stryver’s legal work.
He is sacrificing his potential
for no reason, which is a kind
of suicide.
Afterwards, Stryver and Carton drink and talk.
Stryver comments on Carton’s moodiness and lack
of direction, which have been evident since their days
at university. Carton responds that he lacks Stryver’s
ambition, and must live in “rust and repose.” Stryver
changes the subject to Lucie’s beauty. Carton mocks
her as a “golden-haired doll,” but Stryver senses
Carton’s true feelings might be different.
This exchange reveals an
important part of Carton’s
character and history. He is
always working for others, never
seeking the credit, as Stryver
would. Carton’s denials about
his interest in Lucie don’t even
convince Stryver.
After leaving Stryver, Carton stumbles home
through the grey dawn, imagining for a moment a city
of hope, full of love and grace. But it passes and he
cries into his pillow, resigned to his miserable life.
Carton’s vision is of a celestial
city in heaven. But in his current
state of empty self-pity, he can
only glimpse it for a moment.
Book 2, Chapter 6: Hundreds of People
Four months pass. Mr. Lorry visits Dr. Manette and
Lucie at their home. Lucie has decorated the house
beautifully, but Mr. Lorry notices that Manette’s
shoe-making workbench is still in the house.
The beautiful house symbolizes
the Manettes’ return to life, but
the presence of the workbench
indicates that Manette is not
yet completely free of his past.
Dr. Manette and Lucie are out, though. Mr. Lorry
speaks with Miss Pross, who comments on and
dismisses all the suitors who constantly call on Lucie.
She adds that her brother, Solomon Pross, is the
only man good enough for Lucie. Lorry remains silent,
though he knows Solomon is a cheat and scoundrel.
Mr. Lorry then asks if Dr. Manette ever uses his
workbench or speaks about his imprisonment. Miss
Pross responds that Dr. Manette does not think about
his traumatic years of imprisonment.
Miss Pross’s comments
introduce her brother, while
Lorry’s skepticism establishes
that Solomon is not all that he
seems—he’s really a spy. Dr.
Manette’s silence about his
imprisonment and insistence
on keeping his shoe-making
workbench show that he has
not resolved his traumatic past:
he’s still hiding from it.
Lucie and Manette return. Charles arrives to visit
moments later. Charles tells them of his recent trip
to the Tower of London, where a workman recently
realized that what he had thought were someone’s
initials carved into a wall (“D.I.G.”) were actually
instructions: beneath the floor, they found the ashes
of a letter. Dr. Manette nearly faints at this story.
Charles’s story foreshadows
what will be discovered in Dr.
Manette’s old cell: his carved
initials and a letter telling his
story. Dr. Manette almost faints
because he can’t face his past
and senses the letter’s danger,
whether consciously or not.
Sydney Carton also visits. Sitting out on the veranda
as a storm approaches, Lucie tells him that she
sometimes imagines that the echoes of the footsteps
from the pedestrians below belong to people who will
soon come into their lives. Carton says it must be a
great crowd to make such a sound, and says that he
will welcome these people into his life.
The storm and footsteps
symbolize the oncoming French
Revolution. Carton’s comment
is prophetic: in the end, he
welcomes the Revolution into
his life and sacrifices himself to
the Revolution to save Lucie.
Book 2, Chapter 7: Monseigneur in Town
The scene cuts to Paris and the inner sanctum of
Monseigneur, a powerful French lord. He drinks
some hot chocolate with four richly dressed servants
to help him. Monseigneur is surrounded by luxury, by
state officials who know nothing of state business but
everything about dressing well. Every aristocrat there
seems disfigured by the “leprosy of unreality.”
The hot chocolate exemplifies
the nobility’s self-indulgent and
foolish focus on personal comforts.
They are so out of touch
with the hard realities of the
common people in France that
the narrator compares their
disconnection to a disease.
One sinister lord with a pinched nose, the Marquis
Evrémonde, leaves in a huff that the Monseigneur
did not treat him a bit more warmly. He takes out
his anger by having his carriage speed through the
streets, scattering the commoners in the way.
The Marquis cares only about
power. Feeling snubbed by the
Monseigneur, he makes himself
feel powerful again by taking it
out on the commoners, whom
he clearly cares nothing about.
The carriage runs over and kills a little girl. As a tall man
wails over his dead daughter, the Marquis scolds the
people for not taking care of their children and tosses
the man a gold coin. As his carriage pulls away, the
coin sails back in: Monsieur Defarge threw it back.
Furious, the Marquis screams that he will “exterminate
[the commoners] from the earth.” He drives away
while Madame Defarge looks on, knitting.
The girl’s death is a metaphor
for the brutality of tyranny.
Defarge throwing the coin back
shows how tyranny inspires
revolution, creating a situation
where both sides want to
destroy the other. For his actions
against the commoners, the
Marquis gets his name knitted
into Defarge’s register of death.
Book 2, Chapter 8: Monseigneur in the Country
Returning through the village he rules and has taxed
nearly to death, Marquis Evrémonde stops to
question a mender of roads who the Marquis had
noticed staring at his passing carriage. The man
explains that he saw someone hanging on beneath
the carriage who then ran off into the fields.
The stowaway represents how
the Marquis is bringing his own
troubles home to roost. The
trouble is spreading from the
cities through the country.
The Marquis drives on, passing a shoddy graveyard.
A woman approaches the carriage and petitions the
Marquis for help for her husband who has recently
died of hunger, like so many others. The Marquis
dismissively asks the women if she expects him to
be able to restore the dead man to life or to feed
everyone? The woman responds that all she wants is
a simple grave marker for her husband, so he won’t
be forgotten. The Marquis drives away.
The Marquis fails to realize
that he does have the power
to feed the people. But it would
require sympathizing with them
or even sacrificing some of
his prosperity and power. The
Marquis’s lack of pity contrasts
with Lucie’s compassion. Unlike
the Marquis, she has the power
to restore someone to life.
Book 2, Chapter 9: The Gorgon’s Head
At his luxurious castle, the Marquis Evrémonde waits
for the arrival of his nephew, Charles Evrémonde
(a.k.a. Charles Darnay) from London. Charles explains
he has been questing for a “sacred object,” but that
he’s run into trouble. The Marquis dismisses him, but
complains that the power of the French aristocracy
has waned. They used to hold the right of life and
death, and ruled by fear and repression.
The object of Charles’s sacred
quest is Lucie. Charles’ “trouble”
in winning her love is his
aristocratic background. Notice
also the contrast between Lucie
and the aristocracy: she has the
power to restore life, while the
French nobility rule through the
power of taking life away.
Charles responds that the Evrémondes have lost
their family honor by injuring anyone who stood
between them and pleasure. He adds that when his
mother died, she commanded him to have mercy
on the people. He renounces his family name and
property, which he says is cursed, and explains that
he will work for a living in England. The Marquis
scoffs at his nephew’s “new philosophy,” tells him to
accept his “natural destiny,” and goes to bed.
The “new philosophy” of the
Enlightenment, which inspired
both the American and French
Revolutions, held that all people
are born equal, that no one
has a natural right to rule. Yet
rather than facing his past,
Charles tries to run from it by
renouncing his family and living
and working in England.
As the morning dawns, the expressions on the
castle’s stone faces seem to have changed to shock.
Bells ring and villagers gather to share urgent news:
the Marquis has been found dead with a knife in his
chest and a note signed “Jacques.”
The stone faces represent the
old institution of the nobility,
shocked at the unthinkable: a
challenge to their power. Yet the
murder also shows that despite
their ideals, the revolutionaries
are as bloodthirsty and revengedriven
as the nobles.
Book 2, Chapter 10: Two Promises
A year passes. Charles now makes a passable living
in London as a French teacher. Charles visits Dr.
Manette. During the visit, Charles tells Dr. Manette
of his deep love for Lucie. Dr. Manette at first seems
frightened by the news, but relaxes when Charles
promises that he intends not to separate them, but to
share the Manettes’ home and bind Lucie closer to her
father. Dr. Manette suspects that Stryver and Carton
are also interested in Lucie, but promises to vouch for
Charles’s love for Lucie should Lucie ever ask.
Charles has sacrificed his
wealth and aristocratic heritage
to try to win Lucie’s love. Since
only Lucie’s love keeps Dr. Manette
sane, any threat to their
bond makes him worry. Charles
understands this and promises
that his relationship to Lucie
won’t interfere with Lucie’s
relationship with Manette.
Charles thanks Dr. Manette for his confidence in
him, and wants to return the favor by sharing a secret
of his own: his real name. But Manette suddenly
stops him. He asks Charles to tell him on the morning
of his wedding, not before. That night, Lucie returns
and finds her father again making shoes.
Dr. Manette must have a hunch
that Charles is an Evrémonde.
By stopping Charles from
revealing the truth, he continues
to try to repress his pain. But he
is not entirely successful, as his
return to shoemaking shows.
www. L i t C h a r t s . c om 5 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 11: A Companion Picture
That same night, as Sydney Carton plows through
heaps of legal papers, Mr. Stryver announces
that he intends to get married. He chastises Carton
for acting strangely around people, especially the
Manettes. Stryver explains how he works to get along
with people, which gets him ahead in the world.
Stryver is not an evil character,
but he is selfish. All his actions
are focused on getting ahead.
He would never consider sacrificing
any of his hard-earned
success for any reason.
Because Carton had previously (though insincerely)
insulted Lucie, Stryver breaks the news to him
carefully: he plans to marry her. Stryver thinks she’s
a “charming creature” and will improve his home and
professional standing; besides, she would be lucky to
marry a man of such rising distinction. Carton drinks
harder and says almost nothing. Stryver worries
about Carton and tells him to get married, to settle
down with some wealthy woman.
Stryver wants Lucie for all
the wrong reasons: she’ll be a
trophy wife who will help him
professionally. This contrasts
with the feelings of profound
love that both Charles and
Carton feel for Lucie. Stryver
thinks that Carton can find
redemption on an earthly path,
like getting married for money.
Book 2, Chapter 12: The Fellow of Delicacy
On his way to Lucie Manette’s house to propose,
Mr. Stryver passes Tellson’s Bank and decides
to drop in on Mr. Lorry. When Stryver tells him of
his plans, Mr. Lorry stiffens and advises him not to
proceed. Stryver is stunned and insulted. Mr. Lorry
clarifies that he knows Lucie’s likely answer. But
Stryver cannot believe that any girl could refuse him.
Stryver thinks the world
revolves around him, that
everyone must believe in the
virtue of pursuing earthly
rewards, at which he excels. But
Mr. Lorry has a sense that Lucie
has different goals and a more
profound destiny.
Mr. Lorry asks Stryver to wait while he visits the
Manettes to see about Stryver’s chances. Stryver
agrees and returns home to think it over. When Mr.
Lorry arrives with the expected bad news, Stryver
has already decided to drop it. He explains that Lucie
shares the “vanities and giddiness of empty-headed
girls” and that he’s better off without her.
Stryver convinces himself he
never wanted Lucie. But his
insult about Lucie is so far off
that it shows his foolishness. A
selfish materialist like Stryver
will never deserve or receive the
rewards of love and restored life
that Lucie can provide.
Book 2, Chapter 13: The Fellow of No Delicacy
Although his awkward social skills obscure it, Sydney
Carton loves to visit the Manette house. After
Mr. Stryver informs him that he’s given up his
plans to propose, Carton visits Lucie for a private
conversation.
Carton’s earlier insults of Lucie
were just a show. From his visits,
it is clear that he loves and
admires her for her compassion
and goodness.
Lucie is astonished when Carton breaks into tears
over his wasted life during the visit. She asks if she
can help him, if she can persuade him to live a better
life. Carton says no, that his life was over long ago. But
Lucie responds that she believes he has it in him to live
a much worthier life, and that she can help him.
Carton’s past is a mystery. Not
unlike Dr. Manette, Carton has
been imprisoned in his own
depression since some trauma
in his youth. Even he does not
anticipate the great deeds he is
capable of, but Lucie does.
Carton tells Lucie he loves her, that she is “the last
dream of [his] soul.” But that even if she loved him
back, he would probably just make her miserable.
Carton asks only one thing: for Lucie to confirm that
there is still something in him to pity, some shred of
humanity to sympathize with. She does and Carton
tells Lucie he would do anything, even give his own
life, for her and the family she loves.
Prophetic words. Carton’s soul
dreams of Lucie’s pity, of being
forgiven and welcomed by her
boundless compassion. Carton
sees this compassion as the
most important thing in the
world, and with the strength he
derives from Lucie’s faith, he
would do anything to protect it.
Book 2, Chapter 14: The Honest Tradesman
Outside of Tellson’s Bank, Jerry Cruncher sees an
approaching funeral procession. An angry crowd
harasses the drivers of the hearse with shouts of
“Spies!” Cruncher learns the hearse carries the body
of Roger Cly, a convicted spy against the English.
The English crowd threatening
the spies foreshadows the
French mob that, in later
chapters, will actually lynch its
enemies in public.
Jerry follows the mob, which roughs up the drivers
and takes over the procession. They drive into the
country and bury Roger Cly with mock ceremony.
Then they start carousing, busting up local pubs until
the police intervene.
The mobs’ anger at the spy
Roger Cly escalates into a
general zest for mayhem,
foreshadowing the French
revolutionaries who lose sight
of their ideals in their thirst
for blood.
Back at home, Jerry once again complain’s about his
wife’s praying. His son, Young Jerry, asks his father
about where he goes at night. Jerry tells his son that
he goes fishing, as Mrs. Cruncher knows.
Mrs. Cruncher knows Jerry’s
secret, which is why she prays:
she feels guilty about Jerry’s
secret occupation.
That night, Young Jerry sneaks out after his father,
whose “fishing gear” includes a crowbar and ropes.
He follows his father to the grave of Roger Cly, and
watches his father start digging, then runs in terror,
with visions of Cly’s coffin chasing after him.
Jerry is a grave robber! Jerry,
who “fishes” for dead bodies,
represents a perversion of
Jesus, who was described as a
fisher of men. .
The next morning, frustrated that Cly’s body had
been missing, Jerry Cruncher furiously rebukes
his wife for her praying and intervening in the work
of an “honest tradesman.” Later, Young Jerry asks
his father what a “resurrection man” is and says
he would like to be one when he grows up. Jerry is
worried, but also a little proud.
Cly’s missing body will play an
important part in the plot in
later chapters. A “resurrection
man” (grave robber) perverts
the idea of resurrection. Rather
than bringing the dead back to
life, resurrection men sell stolen
body parts to doctors.
Book 2, Chapter 15: Knitting
One day, Monsieur Defarge enters his shop with
the mender of roads and takes him to the attic
with the three “Jacques.” The mender of roads
tells his story: he had watched a man clinging to the
underside of Marquis Evrémonde’s carriage, and
about a year later saw soldiers escort the same man,
who was accused of killing the Marquis, to prison. A
petition to save the man’s life was presented to the
King and Queen, but to no avail. The man was hung
on a gallows above the village fountain. The mender
of roads explains how the corpse cast a long and
frightening shadow.
In presenting a petition, the commoners
are working within the
established political structure:
accepting the nobles as rulers
and making an appeal to their
mercy. But the nobles squander
their chance to show mercy, and
hang the murderer as a warning.
The effect is the opposite: the
dead man’s shadow represents
the commoner’s desire for revenge
and revolution. By showing
no mercy the nobles give up any
chance of receiving any mercy.
Defarge sends the mender of roads outside and
consults with the Jacques. Jacques Three, hungry
for blood, agrees with Defarge that the Marquis’s
castle and the entire Evrémonde race should be
exterminated. Another Jacques points to Madame
Defarge’s knitting, which lists in its stitching the
names of everyone the revolutionaries mean to kill.
Just as the Marquis would
exterminate the people, those
people would exterminate him.
In other words, the revolutionaries
are just as blood-minded as
the corrupt and brutal aristocracy
they seek to overthrow.
Several days later, Monsieur and Madame Defarge
take the mender of roads to Versailles to see a
procession of the King and Queen. The mender of
roads, overwhelmed with excitement, shouts “Long
live the King!” Defarge thanks the man for helping to
keep the aristocrats unaware of the people’s rage.
The mender of roads exemplifies
the fickle mob, who crave spectacle
above all else. One minute
he’s working for the Revolution,
the next he’s overcome with joy
at seeing the king. The Defarges
exploit people like him.
Book 2, Chapter 16: Still Knitting
When the Defarges return home that evening, they
receive information that an Englishman named John
Barsad has been sent to spy on them. Madame
Defarge promises to add his name to her knitting.
Defarge admits to his wife that he’s tired and
doubts the Revolution will come during their lives.
Madame Defarge counters that the Revolution is like
an earthquake: it builds slowly, but when it comes
it releases catastrophic damage. She says she is
content to wait, and will act when necessary.
For all his revolutionary zeal,
Monsieur Defarge also has
some sympathetic human
attributes. Madame Defarge, on
the other hand, is tireless and
merciless. Her comment suggests
just what the Revolution
will be like when it comes: not
a controlled political action with
rational goals defined by political
ideals, but a vengeful riot.
John Barsad enters the shop the next day. In
conversation with the Defarges, Barsad comments
on the plight of the people, trying to get the Defarges
to reveal their revolutionary sympathies. Wise to his
scheme, the Defarges reveal nothing.
John Barsad the spy has already
been spied upon. Suspicion and
surveillance are in full swing.
Barsad changes tactics. Knowing that Defarge was
once Dr. Manette’s servant, he mentions that Lucie
is now married to Charles Darnay—who is in reality
the nephew of the Marquis Evrémonde. After
watching the impact of this news, Barsad leaves.
Because Charles and Lucie bring
together opposite sides of the
French political divide—nobility
and daughter of a revolutionary
hero—their marriage provokes
anger on both sides.
Defarge is in disbelief. He feels a deep anxiety when
Madame Defarge adds Charles’s name to her
knitting.
To Defarge, human connections
still mean something. To Madame
Defarge, all aristocrats
must die, no matter what.
www. L i t C h a r t s . c om 6 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 17: One Night
Lucie spends the last night before her wedding to
Charles with her father. She asks Dr. Manette if he
believes that her marriage will bring them closer. Dr.
Manette assures her that he wants to see her fulfilled,
and couldn’t live with himself otherwise.
Dr. Manette clings to Lucie for
his emotional security. But he
does the noble thing and risks
his mental health in order to
ensure her happiness.
For the first time, Dr. Manette talks to Lucie about
his imprisonment in the Bastille. He tells her that
while there, he passed the time by imagining how his
unborn daughter would grow up. Would she know
nothing about him, or think about her lost father and
weave his memory into the family of her own?
Manette’s thoughts about living
on in his daughter’s memory
after death hint at Carton’s reward
for his sacrifice at the end
of the novel: a legacy carried on
by Lucie’s future family.
Late that night, Lucie sneaks downstairs to check
on her sleeping father. Dr. Manette’s face is deeply
worn from his trials, but he is peacefully asleep.
Manette’s peaceful face is
“imprisoned” in a worn body,
hinting that he won’t be able to
escape his past quite so easily.
Book 2, Chapter 18: Nine Days
On the day of the wedding, Charles Darnay and Dr.
Manette speak privately. When they emerge, Mr.
Lorry notices that Manette looks deathly pale, though
he had looked perfectly normal before the meeting.
Charles has just revealed his
secret to Dr. Manette: he is
an Evrémonde. Somehow this
means something to Manette.
After the wedding, Charles and Lucie leave for their
honeymoon in Wales. The plan is for Dr. Manette to
join the newlyweds after nine days. But after Lucie
leaves, Mr. Lorry notices that Dr. Manette seems
absent-minded. By that evening, Manette is lost and
incoherent, making shoes again in his room. Mr. Lorry
and Miss Pross keep an anxious watch over him, and
decide not to tell Lucie in hopes that Dr. Manette will
improve. He doesn’t improve for nine days.
The discovery that Lucie has
married an Evrémonde pushes
Manette back into his old shoemaking
mania. These events link
the Evrémondes to Manette’s
years in prison, though just
what role the Evrémondes
played in Manette’s imprisonment
remains unclear.
Book 2, Chapter 19: An Opinion
On the tenth day, Mr. Lorry wakes to find Dr. Manette
reading as if nothing has happened. Discovering that
Dr. Manette has no memory of the past nine days,
Mr. Lorry carefully tries to figure out what caused the
relapse by asking Dr. Manette’s opinion about the
medical case of a “friend” who’s daughter Mr. Lorry
cares about. Nonetheless, Manette quickly seems to
suspect what’s going on.
Ever a model of discretion, Mr.
Lorry avoids mentioning anything
that happened directly. Dr.
Manette is still hiding from his
past, even when discussing it.
Mr. Lorry very discreetly describes Dr. Manette’s
situation, never using Manette’s name. He asks what
might have caused the relapse and how he might help
to prevent another one. Dr. Manette replies that it
would be far too painful for the “patient” to tell anyone
his secrets, but surmises that something must have
recently reminded the patient of his past trauma. He
then assures Mr. Lorry that the worst should be over,
and that only something extraordinary could upset
the patient’s mind again.
Dr. Manette represses his
traumas, which remain hidden
until they violently erupt. This
is a metaphor for the French
Revolution itself—the nobles
suppressed the commoners until
a revolt erupted. Dr. Manette
now knows the truth about
Charles’s past, but doesn’t
entirely remember his own.
Mr. Lorry then explains that this “friend” has a
hobby, “blacksmith work,” that may be associated
with the trauma. He wonders if the blacksmith tools
should be removed. Looking worried, Dr. Manette
answers that if manual labor helped the man get
through the trauma, he should be allowed to keep
the tools. Eventually Dr. Manette agrees that the
tools should be removed, but only if these tools are
removed while the patient is elsewhere at the time.
Dr. Manette needs these tools
like a child needs a security
blanket. His inability to face
losing the tools, or even to be
present when they are taken
away, is another example of
Manette’s persistent avoidance
of his traumatic past, whether
conscious or not. But he is still
willing to sacrifice the tools.
That night, after Manette has left to join Lucie and
Charles, Mr. Lorry and Miss Pross remove the
shoemaker’s tools and destroy the bench. Feeling as
guilty as murderers, they burn or bury everything.
Notice how the burying of Dr.
Manette’s work bench parallels
and contrasts with Jerry’s digging
up of dead bodies.
Book 2, Chapter 20: A Plea
The first person to visit Lucie and Charles after they
return from their honeymoon is Sydney Carton.
Carton apologizes for his drunkenness during past
encounters, and asks for Charles’ friendship. Carton
declares himself a worthless man, but says he has a
favor to ask: would Charles mind if he occasionally
visited his house? Of course not, Charles replies.
The novel foreshadows that
Carton, as the first to meet
the married couple, will be
especially important to Charles
and Lucie’s life as a family. For
his part, Charles is just being
polite, humoring Carton out of
his sense of obligation to him.
At dinner that night, Charles comments to Lucie,
Manette, Mr. Lorry, and Miss Pross about Carton‘s
careless and reckless behavior. Later that night in
their room, Lucie suggests that Charles was too
judgmental toward Carton. She asks Charles to have
faith in Carton, who she believes has a wounded heart
but is nevertheless capable of doing tremendous
good. Charles blesses Lucie for her compassion and
promises to have more sympathy for Carton.
Unlike Charles, Lucie has a
deep sympathy and compassion
for Carton’s pitiful soul. Even
though she hardly understands
his behavior, Lucie has faith.
Her prediction about Carton
foreshadows the incredible
sacrifice that Carton will make
for the Manette family.
Book 2, Chapter 21: Echoing Footsteps
Years pass. Lucie weaves her “golden thread”
of positive influence through the family. She often
sits by the parlor window and ponders the echoing
footsteps rising from the street below. She gives birth
to a daughter, Lucie, who particularly likes Sydney
Carton. Her second child, a son, dies in childhood.
As the political situation starts
to unravel in France, Lucie
weaves her domestic community
more tightly together in
London. Her daughter, like her,
has an innocent belief that
Carton is a good man.
In the year 1789, distressing “echoes” arrive from
France. Mr. Lorry confides in Charles that the Paris
office of Tellson’s Bank has been flooded with anxious
aristocrats trying to save their property.
Charles sacrificed his property
to try to escape his family’s
past. Aristocrats who hung on to
their wealth have now lost it.
The scene cuts to Defarge’s wine shop, now the
center of a revolutionary maelstrom. The streets are
thronged with dingy, angry people, armed with guns,
knives, or any weapon they can get their hands on.
The dirty angry revolutionaries
show that the Revolution will
be more about revenge than
Enlightenment ideals.
Defarge leads this army to the Bastille. Madame
Defarge rallies the women, swearing they can kill
as well as the men. After fierce fighting, the Bastille
surrenders and the people swarm inside to free the
prisoners. Defarge and Jacques Three demand
that an older officer show them “One hundred and
five, North Tower.” There, they find Dr. Alexandre
Manette’s initials “A.M.” and search the room.
The taking of the Bastille was
one of the major early events of
the French Revolution. It’s anniversary
is still celebrated as the
French Independence Day. Note
Madame Defarge’s bloodthirstiness.
Manette’s initials on the
wall recall Charles’s story about
the Tower of London.
Returning to the Bastille courtyard, the crowd swarms
the old officer and stabs him to death. Madame
Defarge takes her long knife and slices off his head.
Seven prisoners are freed. Seven prison guards are
killed and their heads are stuck on pikes.
The exchange of the seven
prisoners with seven guards
suggests that power may have
switched sides, but that nothing
has really changed. Madame
Defarge’s beheading of the
guard foreshadows the guillotine.
Book 2, Chapter 22: The Sea Still Rises
Madame Defarge, now the leader of the female
revolutionaries, sits in the wine shop with her
second-in-command, a stocky woman whose violent
acts have earned her the name The Vengeance. No
spies dare come into this neighborhood anymore.
If Madame Defarge represents
Fate, her assistant reveals
exactly 0what kind of Fate
is in store: angry and violent
vengeance in response to years
of tyranny and oppression.
Monsieur Defarge returns with news that an old
aristocrat, who once said that starving people should
just eat grass, tried to fake his own death but has
now been caught. Anger swells—a revolutionary mob
rushes from the neighborhood to the courts building.
The mob overwhelms the officials, captures the old
aristocrat, then drags, beats, and stuffs his mouth with
straw. Finally, they hang him from a lamppost.
The story of the murdered
aristocrat alludes to the famous
story of Queen Marie Antoinette
who, when told that the
starving people had no bread,
replied “Let them eat cake.”
The statement exemplifies cruel
snobbery, but the response is
out of proportion to the offense.
Afterwards, the commoners return home, eat their
“scanty suppers,” play with their kids, and make
love. Back at the shop, Defarge tells his wife that he
is happy the Revolution has finally come. “Almost,”
Madame Defarge replies.
The scenes of the commoners
at home highlights that the
vicious mob is made up of ordinary
people. Madame Defarge’s
comment shows her insatiable
appetite for revenge.
www. L i t C h a r t s . c om 7 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 23: The Fire Rises
While at work in the ruined countryside of France, the
mender of roads encounters a shaggy but powerful
man. Addressing each other as “Jacques,” they
confirm that something will happen “tonight.”
“Jacques” keeps cropping up
everywhere, suggesting how the
revolutionary cause is taken up
again and again by new people.
In the dark courtyard of the castle of Marquis
Evrémonde, four torch-bearing figures appear.
Soon, fire rages through the castle—its stone faces
look tormented and are lost in flame. The inferno
becomes a pillar of fire surging high into the sky.
The stone faces symbolize the
ancient French nobility, which
gets decimated by the Revolution.
The burning castle is a
symbol of the failing aristocracy
and the commoners’ revenge.
A man from the castle rushes into the village
screaming for help to put out the fire and salvage the
valuables in the castle. The crowd of villagers refuses
to budge.
Now the nobility is asking the
people for help, when for so
long they refused to listen to
the people’s appeals for aid.
Later, the villagers surround the house of Monsieur
Gabelle, the government “functionary” in charge of
the area. He is forced to hide on his roof, but is able
to come down in the morning. The narrator explains
that other functionaries in other areas aren’t so lucky,
and that fires are burning all over France.
Though Gabelle is not an
aristocrat himself, he works for
the government. His association
with the aristocrats is enough
for the revolutionaries to distrust
and want to harm him.
Book 2, Chapter 24: Drawn to the Loadstone Rock
It’s now 1792. In the three years that have passed,
there have been battles and bloodshed. The French
nobility has scattered. Many French aristocrats have
become emigrants, fleeing France for London where
they gather at Tellson’s Bank for news.
Though both London and
Paris teetered on the edge of
revolt at the beginning of the
novel, only France has fallen
into revolution.
Inside the bank, Charles is trying to talk Mr. Lorry
out of his latest mission: going to the Paris branch of
the bank to protect whatever bank documents he can.
The aged Mr. Lorry is apparently the youngest clerk at
the bank, and he plans to take Jerry Cruncher for
protection. He will leave that night.
Charles may have democratic
sympathies, but Tellson’s Bank is
invested in old money and aims
to preserve it. This makes Mr.
Lorry’s political and moral positions
in the book ambiguous.
Just then, Mr. Lorry is given a letter addressed to
the “Marquis St. Evrémonde.” Not knowing such a
person, he asks the assembled French nobles. They
declare the man a coward who betrayed his noble
family. Though insulted, Charles does not respond.
Instead, he tells Mr. Lorry that he is an acquaintance
of the Marquis and will deliver the letter.
Although the nobles are wrong
about him, Charles has not
demonstrated to France what
kind of man he is. Because he
ran from his past rather than
confronting it, the nobles and
the commoners despise him.
The letter is from Gabelle. He was arrested, brought
to Paris, and charged with treason for helping an
emigrant, Charles Evrémonde. Gabelle writes that
the peasants neither know nor care that he in fact
was trying to help them, working on Charles’s orders.
He begs Charles to come save his life.
Gabelle was trying to help the
commoners on Charles’s behalf.
But the revolutionaries no longer
care about the truth. They just
want to kill aristocrats. Charles
now gets an opportunity to
restore Gabelle to life.
Charles realizes that he must go to Paris. His sense
of justice obliges him to help Gabelle. He also thinks
he can do something to stop the Revolution’s terrible
violence and urge the people toward mercy. The
narrator describes Charles as being drawn to Paris as
to a Loadstone Rock (a naturally magnetic rock).
Charles wrongly thinks one man
can influence history, or sway
the mob. In fact, the reference
to the magnetic “loadstone”
suggests that even the choice
to return is not really Charles’s
own, that his past has fated him
to go back.
Charles gives Mr. Lorry a reply to send to Gabelle:
Evrémonde will come. Charles packs secretly, writes
a letter each to Lucie and Dr. Manette, and without
telling them leaves for France the following night.
Charles thinks he can do this
on his own, not realizing that
he will also magnetically pull
Lucie and Dr. Manette back to
Paris as well.
Book 3, Chapter 1: In Secret
Charles arrives in France and finds things very
different from when he left. At each village and
checkpoint, he is subjected to the sneering of
revolutionaries dedicated to what the narrator calls
the new republic of “Liberty, Equality, Fraternity, or
Death.” Charles feels each gate close behind him like
a prison door.
Themes of imprisonment
and fate merge as Charles is
gradually locked into his journey
to Paris. The narrator’s addition
of the words “or death” to the
motto of the Revolution shows
its ideals have been perverted.
Three soldiers accompany Charles to Paris as
his “escort.” Upon arriving in Paris, they deliver
Charles—whom they now call their “prisoner”—to
Monsieur Defarge. Charles demands to know under
what charges he is held, and is told that new laws
against emigrants have been passed. Defarge quietly
asks him why he ever returned to France in this, the
age of “La Guillotine.“ Charles asks Defarge to help
him. Defarge refuses.
As he gets closer to Paris,
Charles goes from free man to
escorted suspect to prisoner,
though he has done nothing.
Defarge refuses to help Charles,
but he shows some sympathy.
The revolutionaries invoke the
guillotine as if it’s a saint: bloodthirsty
violence has replaced
religious compassion.
Defarge conducts Charles to the prison of La Force
with a note for the jailor saying “In secret.” The jail is
full to bursting with aristocrats who welcome Charles
with incredible politeness and sympathize with his
fate. Charles is jailed in a solitary cell in a tower. He
realizes he has been virtually left for dead. Charles
paces off the dimensions of the room again and
again: “five paces by four and a half.”
Defarge helped free Dr.
Manette from his secret
imprisonment, but now Defarge
secretly jails Manette’s son-inlaw.
The Revolution has become
a tyranny. Charles paces to
deal with the isolation of imprisonment,
just as Dr. Manette
turned to making shoes.
Book 3, Chapter 2: The Grindstone
Mr. Lorry arrives at the Paris branch of Tellson’s
Bank. It sits next to the former house of a grand
French noble that has been converted into an armory
for the revolutionaries. In the courtyard there’s a
large grindstone.
The house’s transformation
symbolizes the Revolution:
formerly representing the
excesses of the nobility, now the
house represents the revenge
that excess inspired.
Mr. Lorry is stunned when Lucie and Dr. Manette
rush in. They left London immediately after reading
Charles’s letters. Dr. Manette’s fame as a Bastille
prisoner has granted him access and information,
and he has learned that Charles has been imprisoned
at La Force.
In his return to Paris, Dr.
Manette represents redemption
through suffering. He’s been
restored to his former life, and
suffering has earned him political
power within the Revolution.
Noises outside draw them to the window. Half-naked
men covered in blood are turning the grindstone to
sharpen swords. Frenzied, blood-smeared women
pour wine into the men’s mouths. The mob runs
howling into the streets with their weapons.
The revolutionaries are
described as uncivilized savages,
engaged in some terrible ritual.
Note the wine-blood connection
and the intoxication of violence.
Mr. Lorry whispers to Dr. Manette that the mob
has gone to kill the prisoners at La Force. Horrified,
Manette runs out to the mob. Manette and the
remaining revolutionaries rush to La Force as the mob
cries out, “Help for the Bastille prisoner’s kindred in
La Force!”
It is not enough for the
revolutionaries to imprison their
enemies. They must kill them.
Manette, though, uses the political
power he gained from his
sacrifice to save Charles.
Book 3, Chapter 3: The Shadow
Feeling it necessary to separate Tellson’s Bank from
his own personal business, Mr. Lorry finds an
apartment for Lucie and her family, and leaves Jerry
Cruncher with them to act as guard. On the way
back to Tellson’s Mr. Lorry is stopped by Monsieur
Defarge, who brings news that Charles is safe, a
note for Lucie from Dr. Manette, and instructions for
Lorry to let Defarge in to see Lucie.
Mr. Lorry keeps his two worlds
as separate as possible, but
is deeply committed to both.
It is unclear if Defarge has
tampered with this letter, but
certainly at this moment he is
acting as a secret agent for the
Revolution.
On their way to the apartment, Mr. Lorry and
Defarge are joined by Madame Defarge, who is
knitting, and The Vengeance. Defarge tells Lorry
that, in order to be able to protect Lucie, Madame
Defarge must see and remember Lucie’s face.
That Madame Defarge is
knitting shows that she’s
planning to add Lucie’s name
to her list of victims. “Safety”
and “security” are words the
power-hungry use to mask their
real intentions.
In the apartment, Lucie reads the note from Charles:
he is fine, and under Dr. Manette’s protection.
She gratefully kisses one of Madame Defarge’s
hands, but Madame Defarge coldly withdraws to
her knitting. Lucie pleads for Madame Defarge
to help Charles, to use her influence as a “sisterwoman.”
Madame responds that she has seen so
many women suffering for imprisoned husbands that
Lucie’s predicament doesn’t mean much. After they
leave, Lucie tells Mr. Lorry that Madame Defarge
seems to throw a shadow over all her hopes.
This crucial meeting between
the two key female characters
reveals a lot about each: Lucie
has compassion even for this
terrible woman and asks for
her pity; Madame Defarge
shows she is no “sister-woman”
but is a cold messenger of
death. Madame Defarge is
meant to be a frightening
perversion of femininity, while
Lucie, with her goodness and
compassion, is the model of it.
www. L i t C h a r t s . c om 8 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 4: Calm in the Storm
After four days, Dr. Manette returns. He tells Lorry
that 1100 defenseless prisoners have been murdered,
convicted by a self-appointed Tribunal. The Tribunal
also nearly condemned Charles to death, but Dr.
Manette was able to sway the crowd and Charles was
returned to his cell.
Although Charles’s trial in
England was unfair, the French
Tribunal is depicted as even
more monstrous, a total sham
of justice. Dr. Manette seems
to have brought Charles back
to life.
Dr. Manette has been invigorated by his newfound
authority. He believes his suffering has become
strength and power, capable of breaking Charles
out of prison. Having earned the respect of the
revolutionaries, he has been made the inspecting
physician of a number of prisons, including La Force. In
this new role, he can protect Charles. However, as time
passes, he cannot seem to get Charles freed.
The novel implies that through
suffering comes redemption,
and that faith can empower
people to break the pull of
fate of history. Yet even Dr.
Manette’s political power is not
enough to free Charles.
A year goes by. The Revolution gains in force. The
King and Queen of France are beheaded. As the
revolutionaries grow stronger, their courts zealously
prosecute people, guilty or not. Suspicion reigns. Civil
freedoms disappear.
After the Republic was declared
in France in 1792, the “Reign
of Terror” began: a period of
spying, fear, and escalating
numbers of executions.
The guillotine becomes an institution, and guillotines
can now be found in the streets all over Paris. The
narrator says that in Paris the guillotine has come to
replace the Cross as an idol for worship.
The guillotine, a tool to make
it easier to execute people by
beheading, has become a sacrilegious
idol in place of Christ.
This signals that compassion, in
France, is dead.
Book 3, Chapter 5: The Wood-sawyer
Through it all, Lucie tries to keep a normal English
household to relieve her mind. Dr. Manette reassures
her that he can save Charles. He suggests that she
walk near the prison at a place where Charles might
see her from the window of his cell in order to boost
Charles’s spirits. Lucie does just that, everyday, rain,
shine, or snow.
Lucie fits the classic Victorian
stereotype of female strength
through domesticity (“the angel
in the house”) and selfless
dedication to her husband. Just
as Dr. Manette will unwittingly
doom Charles later, he dooms
Lucie with his advice here.
As Lucie stands at her spot on the street each day,
a wood-sawyer—formerly a mender of roads—
who works nearby always says hello. As he cuts his
wood, the wood-sawyer jokes that he is guillotining a
little family. Though the wood-sawyer unnerves her,
Lucie is always polite and friendly to him.
The mender of roads has
transformed into a man drunk
on the violence of the Revolution.
His sawing represents the
potential executions of Charles,
Lucie, and their daughter.
One snowy day, as Lucie stands outside the prison,
she sees a crowd of people dancing to a popular
revolutionary song. Lucie is horrified by their savage
movements and screams.
Another intense depiction of
revolutionaries as crazed savages
who worship the violence
of the Revolution.
Moments later, Dr. Manette appears. He tells Lucie
that Charles’s trial will be held tomorrow, and
promises her that all will work out well. Lucie kisses
her hand in farewell to Charles as she departs, just
as Madame Defarge comes around the corner.
Manette and Madame Defarge salute each other.
For Lucie, her kiss is a gesture
of love toward her husband.
For Madame Defarge, it’s a
crime of commiserating with an
enemy of the state. But Defarge
is not yet ready to make her
play against Dr. Manette.
Book 3, Chapter 6: Triumph
A rowdy, bloodthirsty crowd gathers for the trial of
“Charles Evrémonde, called Darnay.” Defarge and
Madame Defarge sit in the front row. Madame
Defarge is knitting away. Charles is sentenced
to death as an emigrant, despite the fact that the
law was passed after his imprisonment. The crowd
screams to cut off his head.
This is a court not of justice but
of unchecked political passions.
Charles’s sentence is, in fact,
a travesty of justice—the law
shouldn’t even apply to him.
The crowd does not care about
justice, though. It just wants the
spectacle of his execution.
In his testimony, Charles explains that he actually
isn’t an emigrant: he gave up his aristocratic title and
property, then worked as a French tutor and married
a French woman: Lucie Manette. He says that he
returned to France to save the life of a citizen of the
Republic: Gabelle.
Charles finally explains who
he is to the French people.
By swearing that he is still
a Frenchman, Charles offers
himself as a positive, non-violent
role model for change.
Gabelle, who had been forgotten in prison before the
trial, takes the witness stand and confirms Charles’s
story. Then Dr. Manette testifies, praising Charles’s
character and republican ideals.
Gabelle was left for dead.
Imprisonment is like the grave.
Dr. Manette once again tries
to use political tactics to free
Charles.
The jury votes to acquit Charles. The boisterous
crowd now celebrates Charles as a patriot and carries
him through the streets in celebration.
Charles goes from death row to
a public parade, floating on the
fickle allegiance of the mob.
When she sees Charles, Lucie faints with joy. In their
apartment, she thanks God, then her father, who
declares, “I have saved him.”
Dr. Manette’s political influence
seems to be enough to save
Charles after all.
Book 3, Chapter 7: A Knock at the Door
The next day, Manette remains confident and proud
at having saved Charles, but Lucie continues to fear
for her husband’s safety because so many other
innocent people have been imprisoned and killed. For
safety’s sake, they keep no outside servants, using
only Jerry and Miss Pross. Miss Pross vehemently
and regularly voices her distaste for the French.
Lucie’s worries counter Dr.
Manette’s confidence in his political
power. As Lucie suspects,
everyone in France succumbs to
the Reign of Terror. Miss Pross
embodies the inherent English
distrust of the French.
That afternoon, as Miss Pross and Jerry are out on
errands, Lucie hears footsteps on the stairs outside
the apartment. Then there is a knock at the door. Four
armed revolutionaries enter and declare that Charles
Evrémonde is again the prisoner of the Republic.
In the revolutionary Republic,
laws can change in an instant
as the new people in power
begin to abuse it. The footsteps
in the hall echo the footsteps
Lucie used to hear in England.
Dr. Manette tries to intervene, but the soldiers tell
him that he must make sacrifices if the Revolution
demands it. Still, out of respect for Manette, the men
explain that evidence for the charge comes from
three people: Monsieur and Madame Defarge, and
one other, whom they refuse to name.
The Revolution demands that
the revolutionaries be willing
to sacrifice the lives of others,
even family members, without
question. Manette’s political
power can’t stand up to the
pull of fate and history or to
the Revolution’s all-consuming
desire for blood.
Book 3, Chapter 8: A Hand at Cards
While they’re out on their errands, Miss Pross
screams when she recognizes her brother, Solomon
Pross, disguised as a French republican. Solomon
tells her to be quiet, or else she’ll get him killed. Jerry,
meanwhile, also thinks he recognizes this man, but
can’t quite remember his name.
As an unthinking English patriot,
Miss Pross has never questioned
her brother’s integrity,
but as this chapter will show,
he’s a traitorous opportunist in
an ugly political world.
Sydney Carton, appearing out of nowhere, tells
Jerry the name he is trying to remember: John
Barsad. Having arrived in Paris a day earlier, Carton
explains, Carton chanced upon and recognized
Barsad from Charles Darnay’s English trial. Carton
also learned that Barsad was serving as a French
government spy working in the prisons.
Dickens’s novels are often filled
with extreme coincidences, such
as Carton and Barsad’s sudden
appearances. Though one can
guess that Carton came to
Paris out of concern for Lucie.
Carton and Jerry escort John Barsad to Tellson’s
Bank, where Mr. Lorry also recognizes him. Carton
says he has a plan to help Charles. He then blackmails
Barsad, threatening to reveal him as a spy of the
French government and as a former English spy, both
of which would enrage the revolutionaries. Carton
then reveals that he has seen Barsad associating with
another known English spy: Roger Cly.
Because Carton has nothing to
lose, he can play the dangerous
game of counter-intelligence.
Carton wants to save Charles
in part for Charles’s sake, but
to a larger extent because of
his feelings for Lucie. Recall his
promise to Lucie at the end of
Book 2, Chapter 13.
Barsad grins: Cly is dead, he says. He then takes
out a certificate of burial and says he buried Cly
himself. To everyone’s surprise, Jerry angrily objects
that Barsad had placed “shameful impositions upon
tradesmen,” and then reveals that Cly’s body wasn’t
in his coffin. Barsad realizes he’s caught and agrees
to help. Carton takes him into an adjoining room
to talk.
Jerry’s secret job as a “resurrection
man” saves the day!
But note that it takes being
caught in a lie to get Barsad to
help Charles. There is no honor
among spies. And Barsad has
no concept of sacrificing himself
to a higher cause.
www. L i t C h a r t s . c om 9 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 9: The Game Made
After a while, Barsad leaves and Carton explains
to Mr. Lorry that if Charles is convicted, Barsad
will smuggle Carton into Charles’s cell. Refusing to
explain anything more, Carton asks that Lucie be
told nothing about the plan. He then asks if Mr. Lorry
is satisfied with his long life. Mr. Lorry replies that,
nearing the end, he feels closer again to his life’s
beginning. Carton says he knows the feeling. Mr.
Lorry gains a new respect for Carton.
Carton’s exchange with Lorry
suggests that Carton plans to
sacrifice himself and expects
to die. As always, he works for
other people without taking
credit, but this time he works
for a greater cause. Mr. Lorry’s
sense of returning to the beginning
takes on a religious tone
with Carton: he will be reborn
in heaven.
Carton visits a pharmacy and buys a mysterious
packet of drugs that the chemist warns are very
potent. All night, Carton wanders the streets of Paris.
As he walks, he remembers a prayer the priest spoke
at his father’s funeral: “I am the resurrection and the
life, saith the Lord: he that believeth in me, though he
were dead, yet shall he live: and whosoever liveth and
believeth in me, shall never die.”
The prayer Carton remembers
comes from the story of Jesus
and Lazarus, whom Jesus
resurrects in the Bible (John
11:25). The line says that Jesus
will resurrect and give eternal
life not only to Lazarus, but to
anyone who believes in him.
As he continues to walk, he encounters a young girl,
whom he helps across the street. She kisses him, and
once more Carton remembers the prayer.
Carton is showing compassion
to others, and receiving blessings
(the kiss) in return.
Carton arrives at the courthouse the next morning
for Charles’s trial, where Jacques Three is the
head of the jury. As the trial begins, the prosecutor
announces who brought the charges: Defarge,
Madame Defarge, and Dr. Alexandre Manette.
Like the wood-sawyer, Jacques
Three enjoys political executions.
As in Charles’s first trial,
Manette is again forced by
fate and history to serve as a
witness for the prosecution.
The court erupts in chaos. Manette objects that he
never denounced Charles. The judge silences him.
Defarge then takes the stand and explains how,
during the storming of the Bastille, he searched
Manette’s old cell and found a letter hidden in the
chimney. The judge asks that it be read aloud.
Manette’s hidden letter recalls
Charles’s story about the Tower
of London. It represents all
the trauma and revenge that
Dr. Manette has repressed,
consciously or unconsciously.
Book 3, Chapter 10: The Substance of the Shadow
Defarge explains that Dr. Manette wrote the letter
while in the Bastille to explain how he ended up in
prison. He then reads the letter. Walking home one
night in 1757, Dr. Manette was taken into a carriage
by two men, identical twins. From their coat of arms,
he learned that they were Evrémondes: Charles’s
father (who was then the Marquis) and his uncle (who
became the Marquis after Charles’s father died, and
was murdered in Book 2, Chapter 9).
The letter tells the story of
Manette’s imprisonment. The
twin Evrémonde brothers
epitomize the selfishness and
cruelty of aristocratic power.
They take what they want,
when they want, by whatever
means necessary.
The men took the doctor to see two patients: one,
a beautiful young woman deliriously calling out for
her family, and the other, a peasant boy with a stab
wound in his chest. As Manette treated the boy,
the boy told him that the young woman was his
sister. After she married, the two aristocrats decided
they wanted her for themselves. So they forced her
husband to endure impossibly hard work until he
died. Then they took her away and raped her.
The Evrémondes don’t
recognize the individual rights
of peasants, the sovereignty of
marriage, or the sacredness
of female sexuality, which was
a huge deal in Dickens’s time.
They are the worst example
of aristocratic tyranny, and, as
such, they embody many of
the reasons the commoners
revolted.
The peasant boy and young woman’s father died
upon hearing the news. The boy then sent his
younger sister to a distant, secret place, and, seeking
revenge, snuck into the Evrémondes’ castle. He
confronted one of the Evrémondes, who stabbed
him. The boy soon died, but before he did he cursed
the Evrémondes by marking the air with a cross of his
own blood. The young woman died within a week.
The nobles then offered Dr. Manette some gold in
return for his silence, but he declined and returned
home, disgusted with all he had seen.
The curse seals the fate of the
Evrémonde brothers. While
Charles did not know this story,
he sensed his family’s dark
past when he renounced it in
Book 2, Chapter 9. Dr. Manette
refused to be bought off by the
Evrémondes, despite the danger
of such an action. He sacrificed
his freedom to preserve his
integrity.
The next day, the wife of the Marquis (and Charles’s
mother) visited Dr. Manette. Hearing what had
happened, she hoped to find and help the surviving
sister of the abused peasant family. She told her little
boy Charles that he must someday repay this injured
girl. Unfortunately, Manette didn’t know where the
girl was.
That surviving sister, as future
events in the novel will show,
is Madame Defarge. Ironically,
Charles has pledged himself to
help this girl, while she blindly
seeks revenge and does everything
in her power to kill him.
Dr. Manette soon sent a letter to the authorities
detailing the crimes of the Evrémonde brothers.
But the Marquis intercepted and burned Manette’s
letter. He then sent Manette in secret to the Bastille.
Manette ends his letter from prison with a curse on
the Evrémondes.
Manette tried to condemn
the Evrémondes officially and
failed—just as he does now,
having tried to use his political
influence to save Charles. Both
governments are corrupt. His
curse seals Charles’s fate.
Incensed at the actions of the Evrémondes, the jury
sentences Charles to death. The crowd goes wild.
Just days before the crowd
cheered Charles as a patriot.
Book 3, Chapter 11: Dusk
As the crowd celebrates Charles’s conviction in the
streets, John Barsad, who is escorting Charles back
to his cell, lets Lucie her embrace her husband for
the last time. Charles says farewell and asks her to
kiss their daughter. Lucie tells him she feels that they
will not be long separated and will meet in heaven.
Lucie has some kind of serene
connection to the next world.
If their love isn’t possible in
the world, it will be renewed
in heaven. Note how well
positioned Barsad is to smuggle
Carton into Charles’s cell.
Devastated, Dr. Manette tries to apologize to
Charles. But Charles stops him, and instead thanks
him, acknowledging all that Dr. Manette must have
suffered to offer his own daughter back into the
Evrémonde family he justifiably hates.
Like Dr. Manette, Charles also
had a horrific secret past, of
which he was unaware, come
back to haunt him. He cannot
escape the curse on his family.
Lucie faints. Carton carries her to a carriage and
escorts her home. There, he instructs Dr. Manette
to use any remaining influence to try to save Charles.
Dr. Manette hurries away. However, once he’s gone,
Carton and Mr. Lorry confess they have no hope.
Carton is just distracting Dr.
Manette; he knows that politics
are no longer of any use.
Something stronger is necessary
to break the grip of fate, history,
and the Revolution.
Lucie’s daughter begs Carton to help. Carton
embraces her and, before he leaves, kisses the
unconscious Lucie and whispers, “A life you love.”
As his farewell implies, Carton’s
goal is to give Lucie and her
family a happy life. He is willing
to sacrifice himself for that.
Book 3, Chapter 12: Darkness
Sydney Carton decides to make sure he is seen
around Paris. He eventually wanders into a wine
shop—Defarge’s wine shop. Defarge and Madame
Defarge marvel at his physical resemblance to
Charles, but have no idea who he is.
Carton wants to make sure
that it is known that there is
someone who looks just like
Darnay walking free on the
streets of Paris.
Carton eavesdrops on a conversation between
Defarge, Madame Defarge, The Vengeance, and
Jacques Three, in which Madame Defarge plots to
exterminate the Evrémonde line—including Lucie and
Lucie’s daughter. She says that she and the woodsawyer
will testify against Lucie for sympathizing
with a prisoner. Jacques Three promises a conviction.
Monsieur Defarge, however, hesitates, and suggests
that poor Dr. Manette has suffered enough.
The bloodthirsty juries of
the Revolution need only the
slightest suspicion to convict
someone. Jacques Three’s promise
indicates that there is no
justice, and that the trials are
shams. Monsieur Defarge’s pity
for Manette makes Madame
Defarge’s utter mercilessness
stand out even more starkly.
Madame Defarge responds by revealing her history
with the Evrémondes: she is the missing sister of
the peasant family whom the Evrémonde brothers
abused and killed. She vows to carry out her brother’s
dying curse. She barks at Defarge that he can tell
wind and fire where to stop, “but don’t tell me.”
Jacques Three and The Vengeance are thrilled.
Madame Defarge exceeds the
forces of nature. She is a the
terrifying product of tyrannous
cruelty. She symbolizes all
of the people abused by the
aristocrats, and her vengeance
is the embodiment of the
Revolution.
Carton hurries home. Soon, Dr. Manette returns
too, begging for his shoemaker’s bench. Shocked,
Carton and Mr. Lorry realize that Dr. Manette has
lost his mind. Carton instructs Mr. Lorry to gather
everyone’s passports, including Carton’s, and leave
the next day before Madame Defarge’s accusations
make it impossible for them to leave France. Then
Carton says farewell, blesses Lucie, and leaves.
A key tipping point: the curse
against Charles cannot be
stopped, and Dr. Manette’s
insanity is now permanent. After
failing to save Charles, Manette
reverts to his own fate as a
traumatized prisoner. Carton
takes control of things, setting
up his final plan.
www. L i t C h a r t s . c om 10 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 3, Chapter 13: Fifty-two
In the prison, 52 people, including Charles, await
execution that day. Charles writes a final letter to
Lucie, in which he says that he did not know about
her father’s history and that he believes Dr. Manette
was unaware of his damning letter. Charles writes
much the same to Dr. Manette. He also writes to Mr.
Lorry, but never thinks to write to Carton.
Neither Charles nor Dr.
Manette were aware of their
real legacies. The don’t control
their own destinies. Charles
has underestimated Carton
before. The fact that Carton is
under no obligation to make
his sacrifice only increases its
symbolic power.
Suddenly John Barsad opens the cell door and lets in
Carton. Carton tells Charles to start changing clothes
with him. Then Carton dictates a letter for Charles to
write, in which he asks “someone” to remember him
and is grateful to have the chance to prove himself.
In Book 2, Chapter 4 Carton
envied Charles. Now he
becomes Charles by literally
sacrificing his identity to save
Charles’s life. The “someone” in
the letter is Lucie.
As Charles writes, Carton waves the packet of drugs
under his nose. Charles passes out. Carton finishes
swapping their clothes and Barsad carries Charles,
now disguised as Sydney Carton, back to Mr. Lorry.
Charles has been helpless to
stop history, and is not just passive,
but actually unconscious,
during his escape.
Soon the guards arrive and take Carton, whom they
think is Charles Evrémonde, out to join the other
condemned prisoners. A young woman, who was
wrongly accused and convicted, asks him if she can
hold his hand. Suddenly, the women realizes that
he is not Evrémonde. “Are you dying for him?” she
asks. “And his wife and child,” Carton replies. Carton
promises to hold the woman’s hand until the end.
The young girl reveals how
corrupt and merciless the
republic’s tribunals are. Her innocence
also lets her recognize
Carton for who he is: a figure
of Christ, giving his life to save
others. Holding Carton’s hand
suggests how the girl’s faith will
sustain her.
At the Paris barricade, guards check the papers of the
passengers in a carriage: Mr. Lorry, Dr. Manette,
Lucie, and “Sydney Carton,” who is unconscious.
They wave the carriage through.
Just as Mr. Lorry smuggled the
infant Lucie out of Paris, he now
transports these mostly helpless
passengers to safety.
Book 3, Chapter 14: The Knitting Done
At the shop of the wood-sawyer, Madame Defarge
holds a secret conference with Jacques Three and The
Vengeance. Madame says that she no longer trusts
Monsieur Defarge, and that they must exterminate
the Evrémondes themselves. Jacques Three swears
that his jury will condemn Lucie, and fantasizes about
the blond hair and blue eyes of Lucie’s beheaded child
at the guillotine. The wood-sawyer and Madame
Defarge promise to testify against Lucie.
Lucie kissed her hand to
the prison as a gesture of
loyalty and compassion. But the
revolutionaries see it as an act
of treason. The revolutionaries
have given up all human feeling
and mercy, as is shockingly
apparent in Jacques Three’s sick
fantasy about murdering an
innocent girl.
Madame Defarge strides through the streets like
a tigress, a woman without pity, armed with a knife
and loaded pistol. She heads to Lucie’s apartment,
hoping to strengthen her case by catching Lucie
insulting the Revolution in her grief.
Madame Defarge combines the
figures of Fate and Death. She
is terrifying and inhuman. She
represents death as opposed to
resurrection, murder as opposed
to sacrifice.
At the apartment, Jerry Cruncher and Miss Pross
get ready to leave in their own carriage. Jerry swears
that he will give up grave robbing, and states that his
opinions about praying have changed. He adds that
he hopes Mrs. Cruncher is praying right then.
Jerry gives up his work as a
“resurrection man” because
that job belongs to Christ. With
death (Madame Defarge) on
the move, Jerry turns to religion
to save him.
Jerry leaves to make arrangements. Soon after,
Madame Defarge arrives at the apartment and
demands that Miss Pross let her see Lucie. Miss
Pross refuses to budge from Lucie’s bedroom door.
Madame Defarge tries to shove her aside, but
Miss Pross grabs her. During the ensuing struggle,
Madame Defarge grabs for her pistol. But as she
grabs the weapon it accidentally goes off, killing her.
Miss Pross flees the apartment in terror. She meets
up with Jerry and discovers that she has permanently
lost her hearing.
Lucie kissed Madame Defarge’s
hands and asked for mercy.
That failed. Now, the faithful
English servant Miss Pross
wrestles with a faithless French
former servant turned revolutionary.
Madame Defarge’s accidental
suicide shows how the
revolutionaries sow the seeds of
their own destruction. In fact, as
the Reign of Terror progressed,
many French revolutionaries
died under their own guillotines.
Book 3, Chapter 15: The Footstep Die Out Forever
Three carts rumble through the Paris streets carrying
the condemned prisoners to the guillotine. Some
onlookers, used to the spectacle, are bored. Others
gather to see Charles Evrémonde and insult him.
This alludes to Christ’s journey
to the crucifixion, during which
Christ was also harassed and
insulted by spectators.
The Vengeance is in the crowd. She has been saving
a front-row seat for Madame Defarge and holding
her knitting. She bitterly regrets that her friend will
miss the festivities.
Madame Defarge is separated
from her knitting: the grip of
fate has been broken.
The young woman is scheduled to be beheaded
by the guillotine just before Carton. She thanks
Carton for helping her stay composed, and says he
must have been sent to her from Heaven. Carton
tells her to focus only on him and to have no fear.
When her time comes, they kiss, and she calmly
goes to the guillotine. Carton is next. He says “I am
the resurrection and the life.” Carton ascends the
platform, his face looking serene and prophetic, and
the guillotine crashes down on his head.
As Christ comforted his fellow
prisoners on the cross, Carton
also comforts the girl, urging
her to look past the suffering of
politics toward a heavenly future.
With such faith, the condemned
have no fear. Carton’s prayer
suggests that they will live
forever. His serene face implies
the certainty of his salvation
and resurrection, brought about
through faith.
The narrator describes Carton’s final thoughts. He
recognizes that Barsad, The Vengeance, and all
the “new oppressors” will die by the guillotine they
now celebrate. Yet he is also sure that Paris will rise
up from its ashes, struggling to be free. He sees a
vision of Lucie with a new son, named after him,
who will live a successful and prosperous life. He also
sees Dr. Manette restored to health, and Mr. Lorry
leaving all his considerable wealth to the Manette’s
and then passing tranquilly away. And Carton knows
he is blessed and treasured by all these people. The
novel ends with Carton’s final thoughts, “It is a far, far
better thing that I do, than I have ever done; it is a far,
far better rest that I go to than I have ever known.”
In Carton’s vision, the revolutionaries
who showed no mercy
will not receive any, just like the
aristocracy before them. The
novel makes the case for mercy,
in particular Christian mercy, as
a vital force to counteract the
tendency of the powerful toward
tyranny, and suggests that
France will eventually find this
balance. For his selfless sacrifice,
which alone could break the
grip of fate and history, Carton
is resurrected not just in heaven
but also through Lucie’s son,
who lives out Lucie’s hope that
Carton would live a better life.
Important Quotes
Book 1, Chapter 1 Quotes
It was the best of times, it was the worst of times, it was the
age of wisdom, it was the age of foolishness, it was the epoch
of belief, it was the epoch of incredulity, it was the season
of Light, it was the season of Darkness, it was the spring of
hope, it was the winter of despair, we had everything before
us, we had nothing before us, we were all going direct to
Heaven, we were all going direct the other way.
Book 1, Chapter 3 Quotes
A wonderful fact to reflect upon, that every human creature is
constituted to be that profound secret and mystery to every
other. A solemn consideration, when I enter a great city by
night, that every one of those darkly clustered houses encloses
its own secret; that every room in every one of them
encloses its own secret; that every beating heart in the hundreds
of thousands of breasts there, is, in some of its imaginings,
a secret to the heart nearest it!
Book 1, Chapter 5 Quotes
The children had ancient faces and grave voices; and upon
them, and upon the grown faces, and ploughed into every
furrow of age and coming up afresh, was the sign, Hunger. It
was prevalent everywhere. Hunger was pushed out of the tall
houses, in the wretched clothing that hung upon poles and
lines; Hunger was patched into them with straw and rag and
wood and paper; Hunger was repeated in every fragment of
the small modicum of firewood that the man sawed off; Hunger
stared down from the smokeless chimneys, and started
up from the filthy street that had no offal, among its refuse,
of anything to eat. Hunger was the inscription on the baker’s
shelves, written in every small loaf of his scanty stock of bad
bread; at the sausage-shop, in every dead-dog preparation
that was offered for sale.
Book 1, Chapter 6 Quotes
If you hear in my voice … any resemblance to a voice that
once was sweet music in your ears, weep for it, weep for it! If
you touch, in touching my hair, anything that recalls a beloved
head that lay on your breast when you were young and free,
weep for it, weep for it! If, when I hint to you of a Home that
is before us, where I will be true to you with all my duty and
with all my faithful service, I bring back the remembrance of a
Home long desolate, while your poor heart pined away, weep
for it, weep for it! — Lucie Manette
Book 2, Chapter 2 Quotes
The sort of interest with which this man was stared and
breathed at, was not a sort that elevated humanity … The
form that was to be doomed to be so shamefully mangled,
was the sight; the immortal creature that was to be so butchered
and torn asunder, yielded the sensation. Whatever gloss
the various spectators put upon the interest, according to
their several arts and powers of self-deceit, the interest was,
at the root of it, Ogreish.
www. L i t C h a r t s . c om 11 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book 2, Chapter 4 Quotes
Only his daughter had the power of charming this black
brooding from his mind. She was the golden thread that united
him to a Past beyond his misery, and to a Present beyond
his misery: and the sound of her voice, the light of her face,
the touch of her hand, had a strong beneficial influence with
him almost always.
Book 2, Chapter 5 Quotes
Waste forces within him, and a desert all around, this man
stood still on his way across a silent terrace, and saw for
a moment, lying in the wilderness before him, a mirage of
honourable ambition, self-denial, and perseverance. In the
fair city of this vision, there were airy galleries from which the
loves and graces looked upon him, gardens in which the fruits
of life hung ripening, waters of Hope that sparkled in his sight.
A moment, and it was gone. Climbing to a high chamber in a
well of houses, he threw himself down in his clothes on a neglected
bed, and its pillow was wet with wasted tears.
Book 2, Chapter 7 Quotes
But, the comfort was, that all the company at the grand hotel
of Monseigneur were perfectly dressed. If the Day of Judgment
had only been ascertained to be a dress day, everybody there
would have been eternally correct. Such frizzling and powdering
and sticking up of hair, such delicate complexions artificially
preserved and mended, such gallant swords to look at, and
such delicate honour to the sense of smell, would surely keep
anything going, for ever and ever. … with the rustle of silk and
brocade and fine linen, there was a flutter in the air that fanned
Saint Antoine and his devouring hunger far away.
Book 2, Chapter 8 Quotes
Expressive signs of what made them poor, were not wanting;
the tax for the state, the tax for the church, the tax for the lord,
tax local and tax general, were to be paid here and to be paid
there, according to solemn inscription in the little village, until
the wonder was, that there was any village left unswallowed.
Book 2, Chapter 9 Quotes
“Repression is the only lasting philosophy. The dark deference
of fear and slavery, my friend,” observed the Marquis,
“will keep the dogs obedient to the whip, as long as this roof,”
looking up to it, “shuts out the sky.”
Book 2, Chapter 10 Quotes
He had loved Lucie Manette from the hour of his danger. He
had never heard a sound so sweet and dear as the sound of
her compassionate voice; he had never seen a face so tenderly
beautiful, as hers when it was confronted with his own
on the edge of the grave that had been dug for him.
Book 2, Chapter 13 Quotes
For you, and for any dear to you, I would do anything. If my
career were of that better kind that there was any opportunity
or capacity of sacrifice in it, I would embrace any sacrifice for
you and for those dear to you. Try to hold me in your mind, at
some quiet times, as ardent and sincere in this one thing. The
time will come, the time will not be long in coming, when new
ties will be formed about you […] O Miss Manette, […] when
you see your own bright beauty springing up anew at your feet,
think now and then that there is a man who would give his life,
to keep a life you love beside you! — Sydney Carton
Book 2, Chapter 16 Quotes
Another darkness was closing in as surely, when the church
bells, then ringing pleasantly in many an airy steeple over
France, should be melted into thundering cannon; when the
military drums should be beating to drown a wretched voice,
that night all potent as the voice of Power and Plenty, Freedom
and Life. So much was closing in about the women who
sat knitting, knitting, that they their very selves were closing
in around a structure yet unbuilt, where they were to sit knitting,
knitting, counting dropping heads.
Book 2, Chapter 18 Quotes
Nothing would induce him to speak more. He looked up, for
an instant at a time, when he was requested to do so; but, no
persuasion would extract a word from him. He worked, and
worked, and worked, in silence, and words fell on him as they
would have fallen on an echoless wall, or on the air.
Book 2, Chapter 20 Quotes
My husband, it is so. I fear he is not to be reclaimed; there
is scarcely a hope that anything in his character or fortunes
is reparable now. But, I am sure that he is capable of good
things, gentle things, even magnanimous things. – Lucie
Book 2, Chapter 21 Quotes
The sea of black and threatening waters, and of destructive
upheaving of wave against wave, whose depths were yet unfathomed
and whose forces were yet unknown. The remorseless
sea of turbulently swaying shapes, voices of vengeance,
and faces hardened in the furnaces of suffering until the
touch of pity could make no mark on them.
Book 2, Chapter 22 Quotes
The raggedest nightcap, awry on the wretchedest head, had
this crooked significance in it: “I know how hard it has grown
for me, the wearer of this, to support life in myself; but do
you know how easy it has grown for me, the wearer of this, to
destroy life in you?” Every lean bare arm, that had been without
work before, had this work always ready for it now, that it
could strike. The fingers of the knitting women were vicious,
with the experience that they could tear.
Book 2, Chapter 23 Quotes
With the rising and falling of the blaze, the stone faces
showed as if they were in torment. When great masses of
stone and timber fell, the face with the two dints in the nose
became obscured: anon struggled out of the smoke again, as
if it were the face of the cruel Marquis, burning at the stake
and contending with the fire.
Book 2, Chapter 24 Quotes
Like the mariner in the old story, the winds and streams had
driven him within the influence of the Loadstone Rock, and
it was drawing him to itself, and he must go. Everything that
arose before his mind drifted him on, faster and faster, more
and more steadily, to the terrible attraction. His latent uneasiness
had been … that he who could not fail to know that he
was better than they, was not there, trying to do something to
stay bloodshed, and assert the claims of mercy and humanity.
Book 3, Chapter 1 Quotes
Not a mean village closed upon him, not a common barrier
dropped across the road behind him, but he knew it to be
another iron door in the series that was barred between him
and England. The universal watchfulness so encompassed
him, that if he had been taken in a net, or were being forwarded
to his destination in a cage, he could not have felt his
freedom more completely gone.
Book 3, Chapter 2 Quotes
As these ruffians turned and turned, their matted locks now
flung forward over their eyes, now flung backward over their
necks, some women held wine to their mouths that they
might drink; and what with dropping blood, and what with
dropping wine, and what with the stream of sparks struck out
of the stone, all their wicked atmosphere seemed gore and
fire. The eye could not detect one creature in the group free
from the smear of blood.
Book 3, Chapter 4 Quotes
Above all, one hideous figure grew … the figure of the sharp
female called La Guillotine. It was the popular theme for jests; it
was the best cure for headache, it infallibly prevented the hair
from turning grey, it imparted a peculiar delicacy to the complexion,
it was the National Razor which shaved close: who kissed
La Guillotine, looked through the little window and sneezed into
the sack. It was the sign of the regeneration of the human race.
It superseded the Cross. Models of it were worn on breasts from
which the Cross was discarded, and it was bowed down to and
believed in where the Cross was denied.
Book 3, Chapter 5 Quotes
No fight could have been half so terrible as this dance. It was
so emphatically a fallen sport—a something, once innocent,
delivered over to all devilry—a healthy pastime changed into
a means of angering the blood, bewildering the senses, and
steeling the heart. Such grace as was visible in it, made it the
uglier, showing how warped and perverted all things good by
nature were become.
Book 3, Chapter 6 Quotes
Looking at the Jury and the turbulent audience, he might have
thought that the usual order of things was reversed, and that
the felons were trying the honest men.
Book 3, Chapter 8 Quotes
Miss Pross recalled soon afterwards, and to the end of her
life remembered, that as she pressed her hands on Sydney’s
arm and looked up in his face, imploring him to do no hurt to
Solomon, there was a braced purpose in the arm and a kind
of inspiration in the eyes, which not only contradicted his light
manner, but changed and raised the man.
Book 3, Chapter 9 Quotes
“I am the resurrection and the life, saith the Lord: he that
believeth in me, though he were dead, yet shall he live: and
whosoever liveth and believeth in me, shall never die.”
Before that unjust Tribunal, there was little or no order of procedure,
ensuring to any accused person any reasonable hearing.
There could have been no such Revolution, if all laws,
forms, and ceremonies, had not first been so monstrously
abused, that the suicidal vengeance of the Revolution was to
scatter them all to the winds.
Book 3, Chapter 10 Quotes
The boy’s eyes, which had been fixed on mine, slowly turned
to the looker-on, and I saw in the two faces that all he said
was true. The two opposing kinds of pride confronting one
another, I can see, even in this Bastille; the gentleman’s, all
negligent indifference; the peasants, all trodden-down sentiment,
and passionate revenge. — Dr. Manette
Book 3, Chapter 14 Quotes
There were many women at that time, upon whom the time
laid a dreadfully disfiguring hand; but, there was not one
among them more to be dreaded than this ruthless woman,
now taking her way along the streets … imbued from her
childhood with a brooding sense of wrong, and an inveterate
hatred of a class, opportunity had developed her into a
tigress. She was absolutely without pity.
Book 3, Chapter 15 Quotes
Along the Paris streets, the death-carts rumble, hollow and
harsh. Six tumbrils carry the day’s wine to La Guillotine. All the
devouring and insatiate Monsters imagined since imagination
could record itself, are fused in the one realisation, Guillotine.
… Crush humanity out of shape once more, under similar hammers,
and it will twist itself into the same tortured forms. Sow
the same seed of rapacious license and oppression over again,
and it will surely yield the same fruit according to its kind.
www. L i t C h a r t s . c om 12 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Book,
Chapter
Themes
Backstory
–– Dr. Alexandre Manette is secretly imprisoned in the Bastille by Marquis St. Evrémonde and his twin brother.
–– Mr. Jarvis Lorry smuggles Dr. Manette’s daughter, the infant Lucie, to safety in London.
1,1
–– It’s 1775 and the King and Queen of France govern their country harshly. The allegorical figures of Fate and Death
stalk the land.
1,2 –– Mr. Lorry travels to Dover in a mail coach. Jerry Cruncher delivers a note to him, and Mr. Lorry replies with the
words “Recalled to life.”
1,3 –– Mr. Lorry has a dream about digging someone up from the grave.
1,4 –– In Dover, Mr. Lorry meets Lucie Manette and explains that her father, Dr. Manette, has been found.
1,5 –– Mr. Lorry and Lucie meet Monsieur Defarge at his wine shop. He takes them to the attic where Dr. Manette is making shoes in a corner.
1,6 –– Dr. Manette gradually recognizes Lucie. Mr. Lorry helps them all leave Paris.
2,1 –– Jerry Cruncher waits for jobs outside Tellson’s Bank. He complains about his wife’s praying.
2,2 –– In London, Charles Darnay stands trial for treason. His defense lawyers are Mr. Stryver and Sydney Carton. Lucie and Dr. Manette are witnesses for the
prosecution.
2,3 –– Carton undercuts an accusing witness when Carton points out how much he himself looks like Charles. The jury acquits Charles.
2,4 –– Carton takes Charles to a tavern and drunkenly, insultingly prods him about Lucie Manette.
2,5 –– Carton works through the night to do all of Stryver’s legal work.
2,6 –– Dr. Manette and Lucie move into a house. They receive frequent visits from Mr. Lorry, Charles, and Carton.
2,7 –– In Paris, Marquis Evrémonde runs over a little girl with his carriage as the Defarges look on.
2,8 –– A mender of roads tells the Marquis he saw a man clinging onto his carriage. Driving on, the Marquis spurns the petition of a woman wanting a gravestone
for her husband.
2,9
–– Charles Evrémonde visits the Marquis and renounces his family’s name and property.
–– The sleeping Marquis is murdered in his bed; a note on the knife reads “Jacques.”
2,10 –– A year passes. Charles obtains Dr. Manette’s permission to marry Lucie, but Dr. Manette refuses to learn Charles’s real name.
2,11 –– Stryver tells Carton that he will propose marriage to Lucie Manette.
2,12 –– Mr. Lorry strongly advises Stryver not to propose to Lucie, and Stryver changes his mind.
2,13 –– Carton visits Lucie to express his love and his hope that she might pity him. Carton promises his life to her.
2,14 –– Jerry Cruncher watches a funeral procession for the spy Roger Cly. That night, Jerry digs up the coffin to steal the body, but the body is missing.
2,15 –– The mender of roads tells the Defarges about Marquis Evrémonde’s murder, and how the criminal was later caught and hanged.
2,16 –– John Barsad tries to spy on Monsieur and Madame Defarge. The Defarges are shocked to learn that Lucie Manette has married an Evrémonde.
2,17 –– Lucie spends the evening before her wedding with Dr. Manette and promises to become closer to him.
Theme Key
Tyranny and Revolution
Secrecy and Surveillance
Fate and History
Sacrifice
Resurrection
Imprisonment
ThemeTrackerTM
The LitCharts ThemeTracker is a mini-version of the entire LitChart. The ThemeTracker provides a quick timeline-style rundown of all the important plot points and allows you to track the themes throughout
the work at a glance.
www. L i t C h a r t s . c om 13 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
2,18 –– Charles privately tells Dr. Manette his real name. When the couple leaves for a honeymoon, Dr. Manette relapses into making shoes.
2,19 –– Dr. Manette snaps out of it. That night, Mr. Lorry and Miss Pross destroy the shoemaker’s bench and bury the tools.
2,20 –– Carton meets Charles and Lucie on their return. Later, Charles notes that Carton is awkward and strange, but Lucie demands that her husband give Carton
a chance.
2,21
–– Time passes. Lucie gives birth to a daughter (“Lucie”) and a son who doesn’t survive.
–– In 1789, Defarge leads the people to destroy the Bastille. He searches Dr. Manette’s old cell. Madame Defarge beheads the prison warden with a knife.
2,22 –– The Defarges lead another mob to seize and execute an aristocrat who insulted the people.
2,23
–– The castle of Marquis Evrémonde is burned down. No one helps. More arson and fighting occur across the nation.
–– A mob pursues Gabelle who hides on his roof.
2,24 –– The year is 1792. Mr. Lorry receives a letter for Evrémonde from Gabelle who has been imprisoned and asks for help. Charles leaves for Paris the next night.
3,1 –– After arriving in France, Charles is escorted by soldiers to Paris, taken prisoner, and jailed in secret.
3,2 –– Mr. Lorry, Lucie, and Dr. Manette arrive in Paris. Bloodthirsty revolutionaries are on a rampage killing their prisoners. Dr. Manette rushes out and saves
Charles.
3,3 –– Mr. Lorry gets Lucie an apartment. The Defarges bring a note from Dr. Manette. The cold Madame Defarge terrifies Lucie.
3,4 –– Dr. Manette’s reputation in Paris grows, but Charles is not freed. A year passes and the guillotine becomes a popular form of capital punishment.
3,5 –– Every day, Lucie walks near the prison where Charles might see her. She is watched by the wood-sawyer and Madame Defarge.
3,6 –– At Charles’s trial, Gabelle and Dr. Manette testify in his favor. Charles is freed and celebrated as a patriot.
3,7 –– Charles is reunited with Lucie, but that night he is arrested again on charges brought by Monsieur and Madame Defarge, as well as one other unnamed
person.
3,8 –– Jerry Cruncher and Miss Pross run into Solomon Pross in the streets. Sydney Carton shows up and identifies him as John Barsad. Carton threatens to
reveal him as a spy unless he cooperates to help Charles.
3,9
–– Carton wanders the Paris streets and buys a packet of drugs. A prayer runs through his head: “I am the resurrection and the life.”
–– At Charles’s trial, the prosecutor brings charges from the Defarges and Dr. Alexandre Manette.
3,10
–– Monsieur Defarge reads from Dr. Manette’s letter he discovered in the Bastille. The Marquis Evrémonde and his twin brother secretly imprisoned Manette
to hide their crimes against a peasant family.
–– Charles is convicted and sentenced to be killed within 24 hours.
3,11 –– Lucie and Charles say their goodbyes. Dr. Manette freaks out. Carton and Mr. Lorry quietly confess they have no hope for a political solution, but Carton
implies that he has a plan.
3,12
–– At the wine shop, Carton overhears Madame Defarge plotting to convict Lucie. Madame Defarge is the missing daughter of the peasant family persecuted
by the Evrémondes.
–– Carton instructs Mr. Lorry to get a carriage ready to leave for England.
3,13
–– John Barsad lets Carton into Charles’s prison cell. Carton drugs Charles, swaps clothes with him, and makes Barsad carry Charles back to Mr. Lorry, who
immediately leaves with everyone.
–– As Carton is taken to the guillotine, another prisoner, an innocent girl, asks to hold Carton’s hand until the end.
3,14 –– Madame Defarge goes to Lucie’s apartment. She scuffles with Miss Pross and accidentally shoots herself.
3,15 –– Carton is executed by the guillotine. Before he dies, he realizes that his sacrifice is the greatest thing he’s ever done and has a vision of his resurrection
through Lucie’s son, who will one day be born and named after him.
www. L i t C h a r t s . c om 14 Copyright © 2009 LitCharts. All rights reserved.
L I T C H A R T S GET LIT TM
TM
Installation Guide
Sun™ ONE Application Server
Version 7, Enterprise Edition
817-2146-10
September 2003
Sun Microsystems, Inc.
4150 Network Circle
Santa Clara, CA 95054 U.S.A.
Copyright © 2003 Sun Microsystems, Inc. All rights reserved.
THIS SOFTWARE CONTAINS CONFIDENTIAL INFORMATION AND TRADE SECRETS OF SUN MICROSYSTEMS, INC. USE,
DISCLOSURE OR REPRODUCTION IS PROHIBITED WITHOUT THE PRIOR EXPRESS WRITTEN PERMISSION OF SUN
MICROSYSTEMS, INC. U.S. Government Rights - Commercial software. Government users are subject to the Sun Microsystems, Inc.
standard license agreement and applicable provisions of the FAR and its supplements. Use is subject to license terms.
This distribution may include materials developed by third parties.
Sun, Sun Microsystems, the Sun logo, Java, Sun™ ONE, the Java Coffee Cup logo and the Sun™ ONE logo are trademarks or
registered trademarks of Sun Microsystems, Inc. in the U.S. and other countries.
UNIX is a registered trademark in the U.S. and other countries, exclusively licensed through X/Open Company, Ltd.
This product is covered and controlled by U.S. Export Control laws and may be subject to the export or import laws in other
countries. Nuclear, missile, chemical biological weapons or nuclear maritime end uses or end users, whether direct or indirect, are
strictly prohibited. Export or reexport to countries subject to U.S. embargo or to entities identified on U.S. export exclusion lists,
including, but not limited to, the denied persons and specially designated nationals lists is strictly prohibited.
________________________________________________________________________________________
Copyright © 2003 Sun Microsystems, Inc., 4150 Network Circle, Santa Clara, California 95054, Etats-Unis. Tous droits réservés.
CE LOGICIEL CONTIENT DES INFORMATIONS CONFIDENTIELLES ET DES SECRETS COMMERCIAUX DE SUN
MICROSYSTEMS, INC. SON UTILISATION, SA DIVULGATION ET SA REPRODUCTION SONT INTERDITES SANS
L’AUTORISATION EXPRESSE, ÉCRITE ET PRÉALABLE DE SUN MICROSYSTEMS, INC. Droits du gouvernement américain,
utlisateurs gouvernmentaux - logiciel commercial. Les utilisateurs gouvernmentaux sont soumis au contrat de licence standard de
Sun Microsystems, Inc., ainsi qu aux dispositions en vigueur de la FAR (Federal Acquisition Regulations) et des suppléments à
celles-ci. L’utilisation est soumise aux termes de la Licence.
Cette distribution peut comprendre des composants développés pardes tierces parties.
Sun, Sun Microsystems, le logo Sun, Java, Sun™ ONE, le logo Java Coffee Cup et le logo Sun™ ONE sont des marques de fabrique ou
des marques déposées de Sun Microsystems, Inc. aux Etats-Unis et dans d’autres pays.
UNIX est une marque déposée aux Etats-Unis et dans d’autres pays et licenciée exlusivement par X/Open Company, Ltd.
Ce produit est soumis à la législation américaine en matière de contrôle des exportations et peut être soumis à la règlementation en
vigueur dans d’autres pays dans le domaine des exportations et importations. Les utilisations, ou utilisateurs finaux, pour des armes
nucléaires, des missiles, des armes biologiques et chimiques ou du nucléaire maritime, directement ou indirectement, sont
strictement interdites. Les exportations ou réexportations vers les pays sous embargo américain, ou vers des entités figurant sur les
listes d’exclusion d’exportation américaines, y compris, mais de manière non exhaustive, la liste de personnes qui font objet d’un
ordre de ne pas participer, d’une façon directe ou indirecte, aux exportations des produits ou des services qui sont régis par la
législation américaine en matière de contrôle des exportations et la liste de ressortissants spécifiquement désignés, sont
rigoureusement interdites.
3
Contents
About This Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Who Should Use This Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
How This Guide is Organized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Using the Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Documentation Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
General Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Conventions Referring to Directories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Product Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
For More Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Chapter 1 Preparing to Install . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Installation Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Installation Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Application Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Administration Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Java 2 Software Development Kit (J2SE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Sun ONE Message Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Sample Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Always-On Technology Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
High-Availability Database (HADB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
HADB Management Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Load Balancer Plug-in . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Installation Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Graphical Interface Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Command-Line Interface Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Silent Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Distribution of the Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Installation Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Platform Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Configuration 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Configuration 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Configuration 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Solaris Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Solaris 8 Patch Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Shared Message Queue Broker Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Hardened Solaris Operating Environment Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
General Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
High-Availability Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Topology Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Space Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Web Server Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Shared Memory Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Remote Access Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Accessing the Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Chapter 2 Installing Enterprise Edition Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
About Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Installation Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Installation Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
Installation Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Installing Application Server Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Installing the Load Balancer Plug-in . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Installing in Silent Mode (Non-Interactive) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Creating the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Syntax for Creating the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Example Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Modifying the Installation Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Installing in Silent Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Chapter 3 Preparing for HADB Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Configuring Shared Memory and Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Setting Up Host Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Setting up RSH for HADB Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Setting Up SSH for HADB Administration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
SSH Requirements and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Installing SSH for Solaris 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Configuring SSH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Setting Up the User Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Setting Up Administration for Non-Root . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Using the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
How the clsetup Command Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5
How the Input Files Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
What the clsetup Command Accomplishes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Commands Used by the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
clsetup Requirements and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
Editing the clsetup Input Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
The clinstance.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
The clpassword.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
The clresource.conf File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Running the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Cleanup Procedures for the clsetup Command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Chapter 4 Post-installation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Starting and Stopping the Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Using the Command-line Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Using start-domain and stop-domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
Using start-instance and stop-instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Getting Helpful Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Using the Administration Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Creating Domains and Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Web Services Client Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Stopping and Starting the HADB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Stopping the HADB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Starting the HADB After Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Chapter 5 Uninstalling the Enterprise Edition Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
About Uninstalling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstallation Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstalling the Application Server Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Uninstalling in Silent Mode (non-interactive) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Chapter 6 Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
About Logs and Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
J2SE Installation/Upgrade Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Incompatible J2SE version---cannot upgrade. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Failure to install J2SE reported through install log file. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Forgotten User Name or Password . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Forgotten Admin Server Port Number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Connection Refused for Administration Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Server Won’t Start: CGI Error Occurs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Set Limits on File Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
On Solaris: Change Kernel Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
6 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Uninstallation Failure Cleanup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
Appendix A Installation Cheatsheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
1. Fulfill the installation requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
2. Install the software components. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3. Complete the high-availability installation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4. Complete the post-installation tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7
About This Guide
This Installation Guide provides instructions for installing the Sun™ Open Net
Environment (Sun ONE) Application Server 7, Enterprise Edition product.
The following topics are addressed here:
• Who Should Use This Guide
• How This Guide is Organized
• Using the Documentation
• Documentation Conventions
• Product Support
• For More Information
Who Should Use This Guide
This manual is intended for system administrators, network administrators,
evaluators, application server administrators, and developers who want to install
the Sun ONE Application Server software.
This guide assumes you are familiar with the following:
• Installation of enterprise-level software products
• UNIX® operating system
• Client/server programming model
• Internet and World Wide Web
• High-availability and clustering concepts
How This Guide is Organized
8 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
How This Guide is Organized
This guide contains the following documentation components:
• Chapter 1, “Preparing to Install” on page 15—Provides information on the
installation components, installation methods, and requirements for installing
Sun ONE Application Server 7, Enterprise Edition software
• Chapter 2, “Installing Enterprise Edition Software” on page 31—Provides
instructions for installing the Sun ONE Application Server 7, Enterprise
Edition software components. Includes instructions for performing a
non-interactive silent installation.
• Chapter 3, “Preparing for HADB Setup” on page 53—Provides instructions for
configuring shared memory, and setting up host communications and the user
environment for the high-availability configuration.
• Chapter 4, “Post-installation Tasks” on page 81—Describes additional tasks
you may need to perform during or after installing the Sun ONE Application
Server software.
• Chapter 5, “Uninstalling the Enterprise Edition Software” on
page 87—Provides instructions for uninstalling the Sun ONE Application
Server 7 software. Includes instructions for performing a non-interactive silent
uninstallation.
• Chapter 6, “Troubleshooting” on page 93—Provides information on logging as
well as solutions to problems you may encounter during or after installation or
uninstallation.
• Appendix A, “Installation Cheatsheet” on page 101—Provides a checklist of
the summarized tasks of installing the Sun ONE Application Server Version 7,
Enterprise Edition software.
Using the Documentation
The Sun ONE Application Server 7, Enterprise Edition manuals are available in
Portable Document Format (PDF) and Hypertext Markup Language (HTML) on
the documentation CD that is distributed with the product.
The following table lists tasks and concepts described in the Sun ONE Application
Server manuals. The left column lists the tasks and concepts, and the right column
lists the corresponding manuals.
Using the Documentation
About This Guide 9
Application Server Documentation Roadmap
For information about See the following
Late-breaking information about the software and the documentation Release Notes
Comprehensive, table-based summary of supported hardware, operating system, JDK,
and JDBC/RDBMS.
Platform Summary
Sun ONE Application Server 7 overview, features available with each
product edition
Product Overview
Diagrams and descriptions of server architecture, benefits of the Sun ONE
Application Server architectural approach
Server Architecture
New enterprise, developer, and operational features of Sun ONE
Application Server 7
What’s New
How to get started with the Sun ONE Application Server 7 product. Includes
new features, architectural overview, and sample application tutorial.
Getting Started Guide
Installing the Sun ONE Application Server software and its components, such as
sample applications, the Administration interface, and the high-availability
components. Instructions for implementing a basic high-availability configuration are
included.
Installation Guide
Evaluating your system needs and enterprise to ensure that you deploy Sun
ONE Application Server in a manner that best suits your site. General issues
and concerns that you must be aware of when deploying an application
server are also discussed.
System Deployment Guide
Best practices for HTTP session availability that application architects and
developers can use
Application Design Guidelines
for Storing Session State
Creating and implementing J2EE applications intended to run on the
Application Server 7 that follow the open Java standards model for servlets,
Enterprise JavaBeans™ (EJBs™), JavaServer Pages (JSPs), and other J2EE
components. Includes general information about application design,
developer tools, security, assembly, deployment, debugging, and creating
lifecycle modules. A comprehensive Application Server glossary is included.
Developer’s Guide
Creating and implementing J2EE web applications that follow the Java™
Servlet and JavaServer Pages™ (JSP™) specifications on the Application
Server 7. Discusses web application programming concepts and tasks, and
provides sample code, implementation tips, and reference material. Topics
include results caching, JSP precompilation, session management, security,
deployment, SHTML, and CGI.
Developer’s Guide to Web
Applications
Using the Documentation
10 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Creating and implementing J2EE applications that follow the open Java
standards model for enterprise beans on the Sun ONE Application Server 7.
Discusses Enterprise JavaBeans™ (EJB™) programming concepts and tasks,
and provides sample code, implementation tips, and reference material.
Topics include container-managed persistence, read-only beans, and the XML
and DTD files associated with enterprise beans.
Developer’s Guide to
Enterprise JavaBeans
Technology
Creating Application Client Container (ACC) clients that access J2EE
applications on the Application Server 7
Developer’s Guide to Clients
Creating web services in the Sun ONE Application Server environment Developer’s Guide to Web
Services
Java™ Database Connectivity (JDBC™), transaction, Java Naming and
Directory Interface™ (JNDI), Java™ Message Service (JMS), and JavaMail™
APIs
Developer’s Guide to J2EE
Services and APIs
Creating custom NSAPI plugins Developer’s Guide to NSAPI
Information and instructions on the configuration, management, and deployment of
the Sun ONE Application Server subsystems and components, from both the
Administration interface and the command-line interface. Topics include cluster
management, the high-availability database, load balancing, and session persistence.
A comprehensive Application Server glossary is included.
Administrator’s Guide
Editing Sun ONE Application Server configuration files, such as the server.xml
file
Administrator’s Configuration
File Reference
Configuring and administering security for the Sun ONE Application Server
operational environment. Includes information on general security,
certificates, and SSL/TLS encryption. HTTP server-based security is also
addressed.
Administrator’s Guide to
Security
Configuring and administering service provider implementation for J2EE™
Connector Architecture (CA) connectors for the Sun ONE Application
Server 7. Topics include the Administration Tool, Pooling Monitor, deploying
a JCA connector, and sample connectors and sample applications.
J2EE CA Service Provider
Implementation
Administrator’s Guide
Migrating your applications to the new Sun ONE Application Server 7
programming model, specifically from iPlanet Application Server 6.x and from
Netscape Application Server 4.0. Includes a sample migration.
Migrating and Redeploying
Server Applications Guide
How and why to tune your Sun ONE Application Server to improve
performance
Performance Tuning Guide
Information on solving Sun ONE Application Server problems Troubleshooting Guide
Application Server Documentation Roadmap (Continued)
For information about See the following
Documentation Conventions
About This Guide 11
Documentation Conventions
This section describes the types of conventions used throughout this guide:
• General Conventions
• Conventions Referring to Directories
General Conventions
The following general conventions are used in this guide:
• File and directory paths are given in UNIX® format (with forward slashes
separating directory names).
• URLs are given in the format:
http://server.domain/path/file.html
In these URLs, server is the server name where applications are run; domain is
your Internet domain name; path is the server’s directory structure; and file is
an individual filename. Italic items in URLs are placeholders.
• Font conventions include:
? The monospace font is used for sample code and code listings, API and
language elements (such as function names and class names), file names,
path names, directory names, and HTML tags.
? Italic type is used for code variables.
Messages that you may encounter while running Sun ONE Application
Server 7. Includes a description of the likely cause and guidelines on how to
address the condition that caused the message to be generated.
Error Message Reference
Utility commands available with the Sun ONE Application Server; written in
manpage style
Utility Reference Manual
Using the Sun ONE Message Queue software. The Sun ONE Message
Queue documentation at:
http://docs.sun.com/db?p
=prod/s1.s1msgqu
Application Server Documentation Roadmap (Continued)
For information about See the following
Product Support
12 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? Italic type is also used for book titles, emphasis, variables and placeholders,
and words used in the literal sense.
? Bold type is used as either a paragraph lead-in or to indicate words used in
the literal sense.
• Installation root directories for most platforms are indicated by install_dir in
this document.
• Instance root directories are indicated by instance_dir in this document, which
is an abbreviation for the following:
default_config_dir/domains/domain/instance
Conventions Referring to Directories
By default, when using the Solaris 8 and 9 package-based installation, the
application server files are spread across several root directories. These directories
are described in this section.
• install_dir refers to /opt/SUNWappserver7, which contains the static portion of
the installation image. All utilities, executable files, and libraries that make up
the application server reside in this location.
• default_config_dir refers to /var/opt/SUNWappserver7/domainswhich is the
default location for any domains that are created.
• install_config_dir refers to /etc/opt/SUNWappserver7/, which contains
installation-wide configuration information such as licenses and the master list
of administrative domains configured for this installation.
Product Support
Use your early access support process for any product or documentation issues
and for submitting defects.
If you have general feedback on the product or documentation, please send this to
appserver-feedback@sun.com.
If you have problems with your system, contact customer support using one of the
following mechanisms:
• The online support web site at:
http://www.sun.com/supportraining/
For More Information
About This Guide 13
• The telephone dispatch number associated with your maintenance contract
Please have the following information available prior to contacting support. This
helps to ensure that our support staff can best assist you in resolving problems:
• Description of the problem, including the situation where the problem occurs
and its impact on your operation
• Machine type, operating system version, and product version, including any
patches and other software that might be affecting the problem
• Detailed steps on the methods you have used to reproduce the problem
• Any error logs or core dumps
For More Information
Useful information can be found at the following Internet locations:
• Sun ONE products and services information
http://www.sun.com/service/sunps/sunone/index.html
• Sun ONE developer information
http://wwws.sun.com/software/product_categories/application_development.html
• Sun ONE learning solutions
http://wwws.sun.com/software/training/
• Sun ONE product data sheets
http://wwws.sun.com/software/
• Sun Microsystems product documentation
http://docs.sun.com/
For More Information
14 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
15
Chapter 1
Preparing to Install
This chapter explains the Sun Open Net Environment (Sun ONE) Application
Server 7, Enterprise Edition software components, the scope and limitations of
your installation choices, and the system requirements for the Application Server
environment.
The following topics are addressed here:
• Installation Roadmap
• Installation Components
• Installation Methodology
• Distribution of the Product
• Installation Requirements
• Accessing the Documentation
Read the Sun ONE Application Server Release Notes for any late-breaking installation
information.
For more information about configuring the Sun ONE Application Server software
after installation, refer to the Sun ONE Application Server Administrator’s Guide.
The following location contains helpful information, including Technical Notes,
Forum discussions, tools and utilities, and product downloads:
http://wwws.sun.com/software/products/appsrvr/home_appsrvr.html
Installation Roadmap
16 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Installation Roadmap
Implementing the functionality of the Enterprise Edition of Sun ONE Application
Server 7 is not a simple process. The complexities of the high-availability database
(HADB), clustering, failover, and load balancing are different for each possible
scenario and for each installation.
The roadmap in Table 1-1describes the high-level tasks that are required to fully
implement the Sun ONE Application Server 7, Enterprise Edition software. The
right column provides the location of instructions for the task.
In addition to this high-level roadmap, the summarized installation steps are
presented in a checklist format in Appendix A, “Installation Cheatsheet.”
Table 1-1 Installation Roadmap
Step Description of Task Location of Instructions
1 Decide on your high-availability
configuration and set up your systems.
System Deployment Guide
2 Verify that Enterprise Edition requirements
are met.
“Installation Requirements” on page 23
Platform Summary
3 Install the software components. “Installing Enterprise Edition Software” on page 31
4 Set up shared memory for the HADB hosts. “Configuring Shared Memory and Semaphores” on
page 53
5 Set up communication for the HADB
management client using SSH or RSH.
“Setting Up Host Communication” on page 55
6 Set the environment variables for the HADB
management client.
“Setting Up the User Environment” on page 63
7 Set up a basic cluster. “Using the clsetup Command” on page 65
8 Start the application server instances. “Starting and Stopping the Server” on page 81
9 Install the load balancer plug-in. “Installing the Load Balancer Plug-in” on page 44
10 Set up the loadbalancer.xml file. Administrator’s Guide, Configuring Load Balancing
11 Tailor your high-availability setup. Administrator’s Guide, HADB Configuration
Administrator’s Guide, Session Persistence
12 Administer the installed cluster. Administrator’s Guide, Cluster Management
Installation Components
Chapter 1 Preparing to Install 17
Sun ONE Application Server 7, Enterprise Edition documentation is located on the
documentation CD that accompanies the product.
Installation Components
The Sun ONE Application Server Version 7, Enterprise Edition product is made up
of the following software components that work together to create the Application
Server platform:
• Application Server
• Administration Client
• Sun ONE Message Queue
• Java 2 Software Development Kit (J2SE)
• Sample Applications
• Always-On Technology Components
Application Server
This component includes the core components of the Sun ONE Application Server
software and is dependent on the J2SE component. Refer to What’s New and the
Product Overview documents for a more in-depth explanation of the features of Sun
ONE Application Server 7, Enterprise Edition.
Administration
The Administration interface and the command-line interface are automatically
installed when you install the Application Server component. When the
Administration interface has been started, the initial page of the Application Server
graphical interface is displayed.
• Admin Server—Provides administration facilities (one Admin Server per
domain).
• Administration interface—Graphical interface used for performing server
administration tasks. Also called the Admin Console.
Installation Components
18 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Command-line interface—Performs the same tasks as the Administration
interface. A number of high-availability commands are available with this
release. Refer to the Application Server Administrator’s Guide for instructions on
using these commands.
• Multiple administrative domains—This mechanism allows different
administrators to create and manage their own sets of application server
instances.
Both the graphical and command-line administration clients allow you to manage
and configure your servers and the applications hosted on them, as well as help
you deploy your applications.
Full instructions for using the administration tools are contained in the Sun ONE
Application Server Administrator’s Guide, the Administration interface online help,
and the asadmin and hadbm man pages.
Administration Client
The administration client is the separate command-line component of the
Application Server. It is installed automatically when the Sun ONE Application
Server component is installed and is dependent on the J2SE component.
You can choose to install the command-line version of this client separately on a
machine where the Application Server is not installed. Do this by selecting the Sun
ONE Administration Client component instead of the Sun ONE Application Server
component during installation.
Java 2 Software Development Kit (J2SE)
The Sun ONE Application Server product requires the J2SE 1.4.1_03 and leverages
the performance and feature improvements that are part of the 1.4 platform.
During an installation, you can choose to reuse a J2SE component that is already
installed on your system as long as the J2SE version is correct.
NOTE The Sun ONE Application Server 7 product is only certified to work
with J2SE 1.4.1_03 from Sun Microsystems. Third-party J2SE
development kits, even with appropriate version numbers, are not
supported.
Installation Components
Chapter 1 Preparing to Install 19
The J2SE is installed here by default: /usr/j2se
Sun ONE Message Queue
The Sun ONE Message Queue, Platform Edition software is a production
implementation of the Java Messaging Service (JMS) 1.0.2 specification. It is
automatically installed when you install the Application Server software.
The Platform Edition of Sun ONE Message Queue differs from the Enterprise
Edition in that Platform Edition does not have the following Message Queue
features:
• Support for multi-broker message services
• HTTP/HTTPS connections
• Secure connection services
• Scalable connection capability
• Multiple queue delivery policies
For further information, the Sun ONE Message Queue has its own documentation
set that can be found at the following location:
http://docs.sun.com/db?p=prod/s1.s1msgqu
Sample Applications
The Sun ONE Application Server Version 7, Enterprise Edition product includes
over sixty sample applications that are available when you install the Application
Server software. This component is dependent on the Application Server
component.
All samples come with the source, schema, Ant build scripts, and EAR files. These
sample applications are categorized as follows:
• Technology samples—Introduce you to various technical aspects of the Java™
2 Platform, Enterprise Edition (J2EE™) specification as well as the value added
features of the Sun ONE platform. High-availability samples are included.
• Interoperability samples—Provide more detailed views on how these
technologies come together on the Application Server platform.
The sample applications are installed here:
Installation Components
20 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
install_dir/samples
More information about the samples can be found here:
install_dir/samples/index.html
Always-On Technology Components
The Sun ONE Application Server 7, Enterprise Edition includes the Always-On
Technology, which supports multi-tiered, multi-machine, clustered application
server deployments. In Enterprise Edition, the web tier supports load balancing
and application traffic partitioning using a web server plug-in.
Various topologies for the Always On Technology are discussed in the Sun ONE
Application Server System Deployment Guide. For instructions on configuring and
administering high availability for the Application Server, refer to the Sun ONE
Application Server Administrator’s Guide.
The following installation components provide the basis for the Always-On
Technology:
• High-Availability Database (HADB)
• HADB Management Client
• Load Balancer Plug-in
High-Availability Database (HADB)
The Application Server provides a transactional, highly-available and
highly-scalable session state persistence infrastructure. Application Server uses the
HADB to store session information.
For additional information on this component, refer to the HADB Configuration
chapter in the Sun ONE Application Server Administrator’s Guide.
HADB Management Client
The HADB management client is the command-line interface for the HADB. A full
set of utilities is available for performing HADB configuration, runtime
management, and monitoring.
Instructions for using the utilities are contained in the Sun ONE Application Server
Administrator’s Guide, the hadbm man pages, and the asadmin session persistence
man pages.
Installation Methodology
Chapter 1 Preparing to Install 21
Load Balancer Plug-in
The load balancer is responsible for taking incoming HTTP requests and
distributing them across the instances in the cluster. The load balancer also makes
it possible for sessions to fail over to new instances when an instance becomes
unavailable, and for a user to quiesce an instance prior to taking it offline.
The Application Server high-availability load balancer plug-in is an enhanced
version of the HTTP reverse proxy plug-in. In addition, third-party load balancers
can be used. This component is dependent on a pre-installed web server.
Supported web servers are listed in the Sun ONE Application Server Platform
Summary.
For additional information on this component, refer to “Installing the Load
Balancer Plug-in” on page 44 and the Configuring Load Balancing in the Sun ONE
Application Server Administrator’s Guide.
Installation Methodology
The Sun ONE Application Server can be installed or uninstalled using the
command-line interface or the graphical interface. You can install interactively
using either the graphical or command-line interfaces, or you can use silent mode
to replicate an installation scenario on one or multiple machines.
Partial and incremental (subsequent) installations are supported. Using either of
the interactive methods, you can do a partial installation which can be followed by
any number of incremental installations. For silent mode, you can do a partial
initial installation, but any subsequent installations must be done using an
interactive method.
The installation program or uninstallation program checks for component
dependencies and does not allow you to install or uninstall components without
their dependent components.
The following sections explain the various installation methods:
• Graphical Interface Method
• Command-Line Interface Method
• Silent Mode
Installation Methodology
22 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Graphical Interface Method
If you choose to use the graphical interface for installation, you are provided with a
set of interactive graphical dialogs.
To invoke the installation program using the graphical (default) method:
./setup
To invoke the uninstallation program using the graphical (default) method:
./uninstall
Command-Line Interface Method
If you choose to the use the command-line interface, the installation steps are the
same as for the graphical-interface installation, but a graphics-capable display is
not provided.
To invoke the installation program using the command-line method:
./setup -console
To invoke the uninstallation program using the command-line method:
./uninstall -console
If you are using Telnet to access a remote server, you can use the command-line
interface to install the product in an interactive fashion.
Silent Mode
You can use silent mode to perform a scripted installation based on the presence of
a parameter file that was created during an interactive installation. In silent mode,
the Application Server software is installed or uninstalled without any interaction
with you. By referring to the installation configuration file, the components that
were installed or uninstalled in the interactive model are automatically installed or
uninstalled on one or multiple servers.
NOTE For a hardened Solaris operating environment, you must use the
command-line method. To start the installation program in a
hardened environment, you will need to perform the steps in
“Hardened Solaris Operating Environment Requirement” on
page 26.
Distribution of the Product
Chapter 1 Preparing to Install 23
Instructions for using silent mode are contained in “Installing in Silent Mode
(Non-Interactive)” on page 48 and “Uninstalling in Silent Mode (non-interactive)”
on page 90.
Distribution of the Product
The Sun ONE Application Server 7, Enterprise Edition software is available on a
CD-ROM. The Enterprise Edition license is automatically installed with the
product and doesn’t expire. No other licenses can be transitioned to the Enterprise
Edition license.
The package-based model installs the components as packages. By default, the
installation locations are spread across three directory roots:
• /opt/SUNWappserver7 contains the static portion of the installation image. All
utilities, executables and libraries of the Application Server software reside in
this location.
• /etc/opt/SUNWappserver7 contains installation-wide configuration
information such as licenses and the master list of administrative domains
configured for this installation.
• /var/opt/SUNWappserver7/domains is the default area under which
administrative domains are created.
Installation Requirements
This section lists the requirements that must be met before installing the Sun ONE
Application Server 7, Enterprise Edition product.
• Platform Requirements
• Solaris Requirements
• General Requirements
• High-Availability Requirements
NOTE Only product patches and upgrades affect /opt/SUNWappserver7.
Installation Requirements
24 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Platform Requirements
Table 1-2 through Table 1-4 summarize the Sun ONE Application Server 7,
Enterprise Edition requirements for the various high-availability configurations.
Configuration 1
Table 1-2 describes a three-machine type of configuration:
• * Machine 1—Web Server
• ** Machine 2—Application Server instance 1, HADB Node 1
• *** Machine 3—Application Server instance 2, HADB Node 2
Configuration 2
Table 1-3 describes a two-machine type of configuration:
• * Machine 1—Web Server/Application Server (1 Admin Server instance, 1
Application Server instance)
• ** Machine 2—Application Server instance 2 (1 Admin Server instance, 1
Application Server instance), 2 HADB Nodes
Table 1-2 Platform Requirements for Configuration 1
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
96 MB 128 MB 250 MB 500 MB
2 ** Solaris 8, 9
for SPARC
32 and
64 bit
512 MB
(256 MB for
AppServ; 256
MB HADB)
768 MB 500 MB
(250 MB for
AppServ;
250 MB for
HADB)
750 MB
3 *** Solaris 8, 9
for SPARC
32 and
64 bit
768 MB 500 MB
(250 MB for
AppServ;
250 MB for
HADB)
750 MB
Installation Requirements
Chapter 1 Preparing to Install 25
Configuration 3
Table 1-4 describes a single-machine type of configuration:
• * Machine 1—Web Server/Application Server (1 Admin Server instance, 2
Application Server instances), 2 HADB Nodes
You can check your operating system version using the uname or showrev
command. Disk space can be checked using the df -k command. RAM can be
checked using the prtconf or top commands.
For the latest information about supported directory servers, web servers, web
browsers, and so on, refer to the Sun ONE Application Server Platform Summary.
Solaris Requirements
The following Solaris-specific requirements must be met:
• Solaris 8 Patch Requirements
• Shared Message Queue Broker Requirement
• Hardened Solaris Operating Environment Requirement
Table 1-3 Platform Requirements for Configuration 2
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
352 MB 640 MB 500 MB 1 GB
2 ** Solaris 8, 9
for SPARC
32 and
64 bit
768 MB 1 GB 750 MB 1 GB
Table 1-4 Platform Requirements for Configuration 3
Machine Config
uration
Operating
System
Archite
cture
Minimum
Memory
Recommend
ed Memory
Minimum
Disk Space
Recommende
d Disk Space
1 * Solaris 8, 9
for SPARC
32 and
64 bit
992 MB 1.5 GB 1.128 GB 1.75 GB
Installation Requirements
26 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Solaris 8 Patch Requirements
For Solaris 8 systems, the following Solaris patches must be installed:
• 109326-06
• 108827-26
• 110934-02
These patches are available individually from the patch finder page here:
http://sunsolve.sun.com/pub-cgi/show.pl?target=patches/patch-access
Shared Message Queue Broker Requirement
If your machine has an active installation of the Solaris 9 bundled version of the
Application Server software, and you install the unbundled version of the server,
the Message Queue broker for these application server installations will be shared.
Therefore, if you fail to uniquely name your domains and instances, you may
receive the following errors when starting up the second instance with the same
domain or instance name:
SEVERE: JMS5024: JMS service startup failed
SEVERE: CORE5071: An error occured during initialization
To avoid these errors, see JMS Support in the Sun ONE Application Server
Administrator’s Guide.
Hardened Solaris Operating Environment Requirement
Hardening means customizing existing services or functions so as to improve the
overall security of the platform. The hardening process generally includes tasks
such as disabling unnecessary services, strengthening ownership and permissions
on objects, and enabling miscellaneous security functions such as non-default
logging and auditing. A hardened operating system usually doesn't allow
GUI-based applications to be run in the environment.
NOTE Solaris 8 systems should have the “Sun recommended patch cluster”
installed. The patch cluster includes the three required patches listed
in this section and is available under “Recommended and Security
Patches” here:
http://sunsolve.sun.com/
Installation Requirements
Chapter 1 Preparing to Install 27
The following two libraries are required to install and use Sun ONE Application
Server 7, Enterprise Edition in a hardened Solaris operating environment:
• libC.so.5
• libCrun.so.1
These libraries can be obtained by installing the SUNWlibC (Sun Workshop
Compilers Bundled libC) package which is part of the Solaris distribution in the
end-user package cluster (not in the core).
General Requirements
The following additional requirements should be met before installing the Sun
ONE Application Server 7, Enterprise Edition product:
• Removing previously-installed Sun ONE Application Server 7 software—If
there is previously-installed Sun ONE Application Server 7 software on the
target machine, you must remove it using the uninstallation program before
starting installation.
• Available ports
? You’ll assign one for the Admin Server and another for the HTTP server
default instance during installation.
? The installation program will detect used ports and assign two others for
you: Sun ONE Message Queue (by default, 7676), and IIOP (by default,
3700). If either of these default port numbers are in use, the installation
program will assign the next available port (for example, 7677 or 7678, and
so on).
? Additional ports will be needed for the HADB servers. Refer to the HADB
configuration chapter in the Sun ONE Application Server Administrator’s
Guide for guidelines.
• Root privileges—You must have root privileges on your target machine.
NOTE Solaris 9 bundled installations or non-package-based evaluation
installations do not affect the Enterprise Edition installation
program, so they do not need to be removed from your system.
However, port conflicts must be resolved.
Installation Requirements
28 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Single installation—You can have only one installation per machine, however,
you can have multiple instances running within the same installation.
High-Availability Requirements
The following requirements are key to setting up your high-availability
environment:
• Topology Planning
• Space Considerations
• Web Server Installation
• Shared Memory Setup
• Remote Access Setup
Topology Planning
Before you install the Sun ONE Application Server 7, Enterprise Edition software,
you will need to decide on product topology, that is, which component will be
hosted on which available system. The Sun ONE Application Server and the HADB
server can generally be hosted in two ways:
• Application Server and HADB server node hosted on the same system
• Application Server and HADB server node hosted on separate systems
In both cases, at least two systems per component are needed to achieve high
availability.
The installation program enforces explicit component dependencies, but will not
otherwise limit combinations of product components that can be installed on a
particular machine. As a result, the number of possible product topologies is quite
large.
Details on the various topologies that can be implemented for the Always On
Technology are discussed in the Enterprise Edition of the Sun ONE Application
Server Operational Deployment Guide.
Installation Requirements
Chapter 1 Preparing to Install 29
Space Considerations
Data devices should not be filled beyond 50% of capacity because additional space
is needed to refragment the HADB. If refragmentation fails, it might be because
devices are too full and there is not enough space. If devices are running at 80% or
90% of capacity and refragmentation fails, the HADB will need to be cleared,
meaning that all data removed from the database and the session schema.
It is important to monitor the space on the devices using the hadbm deviceinfo
command. When device capacity exceeds 50%, additional nodes should be added.
Refer to the Sun ONE Application Server Administrator’s Guide and the Sun ONE
Performance Tuning Guide for information and instructions.
Web Server Installation
Before you start the installation process, your web server must be installed on any
machine where you are going to install the load balancer plug-in.
Currently-supported versions include the following:
• Sun ONE Web Server 6.0 SP6
• Apache Web Server 1.3.27
For installing the Sun ONE Web Server, refer to the iPlanet WebServer Installation
Guide at this location:
http://docs.sun.com/db/prod/s1websrv
Instructions for installing the plug-in are contained in “Installing the Load Balancer
Plug-in” on page 44.
Shared Memory Setup
You will need to configure shared memory on the HADB hosts before you can set
up the HADB. This can be done before or after installing the high-availability
components. Refer to “Configuring Shared Memory and Semaphores” on page 53
for instructions.
Remote Access Setup
Before you can set up the HADB, you will need to configure remote access on the
HADB hosts to enable the high-availability management client to communicate
among HADB nodes. This can be done before or after installing the
high-availability components. Refer to “Setting Up Host Communication” on
page 55 for instructions on configuring OpenSSH/SSH or RSH.
Accessing the Documentation
30 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Accessing the Documentation
The Sun ONE Application Server documentation is provided in a number of ways:
• Manuals—The Sun ONE Application Server 7, Enterprise Edition manuals and
release notes, in HTML and in printable PDF, are available on the
documentation CD-ROM that comes with the product.
• Online help—Click the Help button in the graphical interface to launch a
context-sensitive help window.
• Man pages—To view man pages at the command line, you must first add
install_dir/man to your MANPATH environment variable (Solaris) and add the
HADB /bin directory to PATH. After setting the variable, you can access man
pages for the Sun ONE Application Server commands by typing man
command_name on the command line. For example:
man asadmin
man hadbm
31
Chapter 2
Installing Enterprise Edition Software
This chapter provides instructions for installing the Sun ONE Application Server 7,
Enterprise Edition product. You can install this version of the product interactively
or you can use silent mode to replicate an installation scenario on multiple
machines. Refer to “Installation Roadmap” on page 16 to see the full sequence of
events for implementing the Sun ONE Application Server 7, Enterprise Edition
product.
The following topics are addressed here:
• About Installation
• Installing Application Server Software
• Installing the Load Balancer Plug-in
• Installing in Silent Mode (Non-Interactive)
You should be familiar with the information in “Preparing to Install” on page 15
before beginning the tasks in this chapter.
For any late-breaking updates to these instructions, check the Sun ONE Application
Server Release Notes. For more information about configuring your application
server after installation, refer to the Sun ONE Application Server Administrator’s
Guide.
The following location contains product downloads in addition to other useful
information:
http://www.sun.com/software/products/appsrvr/home_appsrvr.html
About Installation
32 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
About Installation
Only one Sun ONE Application Server 7 installation can reside on a single
machine. If an installation of Application Server 7 already exists on your system,
the installation program will not overwrite it, but the pre-existing installation will
be detected and you will not be allowed to proceed with the installation until you
have removed the existing Application Server 7 software using the uninstallation
program.
The following topics are addressed in this section:
• Installation Components
• Installation Options
Installation Components
In general, you are installing the basic components that provide the functionality of
the Sun ONE Application Server Version 7, Enterprise Edition product. You can
choose not to install some of the components. Later, if you want to add a
component that you initially chose not to install, you can do an incremental
installation of that component, providing dependencies are met.
Since only one installation of the same component package on the same system is
allowed, the installation program tries to detect components that are already
installed. If a component is already installed, installation of that component is
disabled.
NOTE Solaris 9 bundled installations and non-package-based evaluation
installations do not affect the Enterprise Edition installation
program, so they do not need to be removed from your system.
NOTE Using either of the interactive methods, you can do a partial
installation which can be followed by any number of incremental
(subsequent) installations. For silent mode, you can do a partial
initial installation, but any subsequent installations must be done
using an interactive method.
About Installation
Chapter 2 Installing Enterprise Edition Software 33
The installation program enforces component dependencies as specified for each
component. Once component dependencies are satisfied, component life cycles are
independent. A particular component can be installed or uninstalled dynamically
through incremental installation and partial uninstallation mechanisms without
corrupting other components.
The following installation components are included with the Sun ONE Application
Server 7, Enterprise Edition product:
• Sun ONE Application Server—all of Sun ONE Appserver 7, including its
graphical and command-line administrative tools, the asadmin command, and
Sun ONE Message Queue 3.0.1
• Sun ONE Application Server Administration Client—only the asadmin
command
• Java 2 Software Development Kit (J2SE), Standard Edition 1.4.0_03
• Sample applications (Optional)
• High-Availability Database (HADB)—all of HADB, including the hadbm
command
• HADB Management Client—only the hadbm command
• Load balancer plug-in for web servers
NOTE If you want to install the Application Server and an HADB server
node on the same system, select both components. Otherwise select
only one of them.
The clsetup command must be run from a machine where the
asadmin and the hadbm utilities are available. Instructions for using
the clsetup command to create a basic cluster can be found in
“Using the clsetup Command” on page 65.
NOTE You can choose to install the administration client command-line
version separately on a machine where the Application Server is not
installed. Do this by selecting only the Administration Client
component during incremental installation.
About Installation
34 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
The load balancer plug-in is usually installed in a separate process on a
separate machine. Refer to “Installing the Load Balancer Plug-in” on page 44
for guidelines.
See “Installation Components” on page 17 for further description of the Sun ONE
Application Server components.
Installation Options
There are three ways you can perform the installation:
• Graphical method (interactive)—The installation program prompts you using
a sequence of graphical screens. This is the default method.
• Command-line method (interactive)—The installation program prompts you
using a sequence of command-line prompts and messages.
• Silent mode—The installation program reads installation parameters from a
supplied configuration file and logs all output into a log file.
The setup command allows you to specify the method you want to use for
installation, and allows you to create a configuration file for silent installation.
Use the following syntax when running the setup command:
setup [-console] [-silent config_file] [-savestate]
Table 2-1 describes the setup command options.
NOTE The default installation mode is the graphical method, so if you
don’t specify an option when you run setup, the installation
program presents the graphical screens.
Table 2-1 Options for the setup Command
Option Description
-console Runs the installation using the command-line method.
About Installation
Chapter 2 Installing Enterprise Edition Software 35
Installation Syntax
• To run the installation using the graphical interface, type the following at the
command prompt (no options; this is the default method):
./setup
• To run the installation using the command-line interface, type:
./setup -console
• To run the installation using the graphical interface and create an installation
configuration file for silent mode installation:
./setup -savestate
The file called statefile will be created in install_dir.
• To run the installation using the command-line interface and create an
installation configuration file for silent mode installation:
./setup -console -savestate
The file called statefile will be created in install_dir.
• To run a silent mode installation based on an existing installation configuration
file:
./setup -silent config_file
Refer to “Installing in Silent Mode (Non-Interactive)” on page 48 for further
specifics on silent mode installation and the installation configuration file.
-silent config_file Runs the installation in silent mode. Installation parameters are read from
an existing installation configuration file. This option is mutually exclusive
with the savestate option.
The installation configuration file path must be explicitly provided; there
is no default file path. Refer to “Installing in Silent Mode
(Non-Interactive)” on page 48 for further specifics on silent mode
installation and the installation configuration file.
-savestate Runs the installation using either the graphical or command-line method
and creates an installation configuration file based on this installation. This
option is mutually exclusive with the silent option. If you do not specify
this option, no installation configuration file will be created.
The file will be called statefile and located in install_dir.
Table 2-1 Options for the setup Command (Continued)
Option Description
Installing Application Server Software
36 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• To display the available command-line arguments for the setup command:
./setup -help
or
./setup -h
Installing Application Server Software
This section provides instructions for installing the Sun ONE Application Server
software using either the graphical-interface or command-line interface. The steps
are identical for both methods. However, for the command-line interface,
text-based screens are displayed instead of graphical screens.
After you have planned the topology, run the installation program on each system,
selecting and installing the appropriate components based on your topology.
1. Uninstall any previous versions of the Sun ONE Application Server 7 software
on the machines where you are going to install the Enterprise Edition of the
Application Server.
2. Verify that all requirements that apply to your installation have been met. See
“Installation Requirements” on page 23 for information on requirements.
3. Log in as root and create a temporary directory for the product distribution
file.
TIP If you are familiar with high availability concepts and installation of
enterprise-level products, you may want to use the summary
checklists in Appendix A, “Installation Cheatsheet.”
NOTE If the previously-installed packages are bundled in the Solaris
operating environment, they need not be removed. However, port
conflicts must be resolved.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 37
4. Start all the processes on your system that use ports and are expected to run at
the same time as the Application Server software. This allows the installation
program to detect what ports are in use and avoid assigning them for other
purposes.
5. For a download, unzip the .gz file as follows:
gunzip sun-appserver7-sol.tar.gz
6. For a download, untar the unzipped file as follows:
tar -xvf sun-appserver7-sol.tar
This process may take a little time. When the files are unpacked, you will see
the sun-appserver7 directory, which contains the setup file and the pkg
directory.
7. Navigate to the sun-appserver7 directory.
8. Select your installation method.
Refer to “Installation Options” on page 34 for guidelines on selecting the
correct options to use with the setup command.
When the installation starts, the Welcome page of the installation program is
displayed.
9. Read the Welcome page and click Next.
The License Agreement page is displayed.
10. Read the License Agreement and click Yes to agree to the terms of the license
(or type Yes at the command line), then click Next.
NOTE If you are installing the load balancer plug-in, your web server
must already be installed on the machines where you are going
to install the load balancer plug-in before you start the
installation process. Refer to “High-Availability Requirements”
on page 28.
NOTE Click the Help button to display context-sensitive information for a
page.
Installing Application Server Software
38 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
After you accept the License Agreement, the Select Installation Directory page
is displayed.
11. Specify the path to your Sun ONE Application Server installation directory
(default is /opt/SUNWappserver7).
If you are installing only the HADB Server component, you can choose /opt as
the installation directory. This will install the HADB packages into their
default location, which is /opt/SUNWhadb.
? Click Browse to browse for a directory (or press Enter at the command line
to accept the default installation directory).
? If you enter a directory name that does not already exist, the Create New
Directory? dialog is displayed.
• Click Create Directory (or type 1 at the command line) to create a new
directory.
• Click Choose New (or type 2 at the command line) to return to the
Select Installation Directory page.
The Component Selection page displays the available components.
12. Choose from the components listed on the Component Selection page (or type
Yes or press Enter to accept a component from the command line).
NOTE You must accept the license agreement to continue with the
installation.
NOTE You must select identical installation directories on all systems
hosting HADB Server nodes.
NOTE When installing the Sun ONE Application Server together with
HADB, if you do not want to use the default installation folder,
you can create alternate directories, then create symlinks (ln -s)
to these directories from the /var/opt and /etc/opt directories.
The standards for packaging Solaris packages require that the
licenses and configuration files are located in the /var/opt and
/etc/opt directories.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 39
? Sun ONE Application Server, with graphical and command-line
interfaces (J2SE and Sun ONE Message Queue are installed along with this
component)
? (Optional) Sample Applications
? Sun ONE Application Server Administration Client (select only this
component to install standalone command-line)
? High-Availability Database
? High-Availability Database Administration Client
? Load Balancer Plug-in
Refer to “Installing the Load Balancer Plug-in” on page 44 for instructions
on installing this component separately.
13. Sun ONE Message Queue—If the installation program detects a version of the
Sun ONE Message Queue preinstalled in your system, you are presented with
one of the following actions:
? If the correct version of the package-based Sun ONE Message Queue is
installed, it will be reused. You can choose to exit at this point. If you don’t
exit, the installation program will use the installed version and proceed to
the next step.
NOTE If some components are disabled on the Component Selection
page (or if a command-line mode installation did not offer them
for installation), this means the disabled component has been
detected as already installed on your system.
NOTE If you want to install Sun ONE Application Server and an
HADB server node on the same system, select them both.
Otherwise, select only one of them.
NOTE If you do not already have your web server installed on the machine
where you are installing the load balancer plug-in, you cannot
continue to install the load-balancer plug-in.
Installing Application Server Software
40 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? If there is no package-based Sun ONE Message Queue installed, you can
choose to let the installation program install the Sun ONE Message Queue
packages automatically.
? If an incorrect version of the package-based Sun ONE Message Queue is
found, a message is displayed, asking if you want to upgrade your current
version or cancel. Choose one of the following:
• To have the installation program update your current Sun ONE
Message Queue, click Upgrade (or type 1 at the command line).
• To exit the installation program, click Cancel (or type 2 at the
command line).
14. For J2SE—The installation program looks in the /usr/j2se default location to
detect if you have the correct version of the J2SE preinstalled in your machine.
You are presented with one of the following actions (if you have any problems
in this step, refer to “J2SE Installation/Upgrade Issues” on page 94):
? If the correct version of the package-based J2SE is installed, it will be
reused or you can enter the path to another correct version. The installation
program proceeds to the next step.
? If there is no package-based J2SE installed, you can choose to let the
installation program install the J2SE package automatically or reuse an
existing J2SE installation.
? If an incorrect version of the package-based J2SE is found, a message is
displayed asking if you want to upgrade your current version or cancel.
Choose one of the following options:
• To have the installation program update your current J2SE version,
click Upgrade (or type 1 at the command line).
• To exit the installation program, click Cancel (or type 2 at the
command line).
Before continuing with the installation, you must uninstall the J2SE
currently located in /usr/j2se or upgrade it to J2SE 1.4.1_03. Then
restart the Application Server installation.
NOTE Because other applications might be running and using this J2SE
installation, upgrading J2SE is a potentially disruptive process. You
may prefer to cancel the current installation and take care of all
dependencies (such as gracefully shutting down processes).
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 41
15. Specify your product configuration directory.
Accept the default (/etc/opt/SUNWappserver7) or enter the path to your Sun
ONE Application Server product configuration directory.
? Click the ellipsis (...) to browse for a directory (or press Enter at the
command line to accept the default installation directory).
? If the directory does not already exist, the Create New Directory? dialog is
displayed.
? Click Create Directory (or type 1 at the command line). You can also click
Choose New (or type 2 at the command line) to select an existing directory.
16. Specify your server configuration directory.
Accept the default (/var/opt/SUNWappserver7) or enter the path to your Sun
ONE Application Server Version 7, Enterprise Edition domains installation
directory.
? Click the ellipsis (...) to browse for a directory (or press Enter at the
command line to accept the default installation directory).
? If the directory does not already exist, the Create New Directory? dialog is
displayed.
? Click Create Directory (or type 1 at the command line). You can also click
Choose New (or type 2 at the command line) to select an existing directory.
If you selected Application Server for installation, the Server Configuration
Information page is displayed. Skip to Step 18.
If you selected the load balancer plug-in, the Web Server Directory page is
displayed. Proceed to Step 17
17. If you selected the load balancer plug-in, identify your web server as follows:
NOTE If your J2SE requires an upgrade, you will need to reboot your
machine after completing the Application Server installation.
NOTE This Sun ONE Application Server 7 software is certified to work
with J2SE 1.4.1_03 from Sun Microsystems. Third-party J2SE
development kits, even with appropriate version number, are not
supported.
Installing Application Server Software
42 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? Choose which web server you are going to install (Sun ONE Web Server or
Apache Web Server).
? Enter the web server instance path.
Default values will be offered based on web server type. The installation
program checks to see if appropriate configuration files can be found at the
provided location.
Refer to “Installing the Load Balancer Plug-in” on page 44 if you are installing
the load balancer plug-in separately.
18. If you selected the Sun ONE Application Server component, enter the
following:
? Admin User—Name of the user who administers the server (for example,
admin).
? Admin User’s Password—Password to access the Admin Server.
Minimum number of characters is 8. For example, adminadmin. Re-enter
the password to confirm your choice.
? Admin Server Port—Port number to access the Admin Server.
A default port number is displayed (for example 4848, if that port is not in
use on your machine). Change the default number if necessary. The
installation program will check port numbers for validity and availability
when you click Next.
? HTTP Server Port—Port number to access the default server instance.
A default port number is displayed (for example 80, if that port is not in use on
your machine). Change the default number if necessary. The installation
program will check port numbers for validity and availability when you click
Next.
NOTE The installation program automatically detects ports in use and
suggests currently unused ports for the default settings. By default,
the initial default ports are 80 for the HTTP server and 4848 for the
Admin Server.
If these initial default ports are being actively used on your system,
the installation program will suggest alternative port numbers.
Installing Application Server Software
Chapter 2 Installing Enterprise Edition Software 43
19. Click Next.
The installation program proceeds to verify that you have enough disk space
based on the components you selected. The Checking Disk Space progress
indicator bar is displayed.
? If you do not have enough disk space, an error message is displayed.
In this case, you need to exit the installation program, create enough space,
and restart the installation. Information on space requirements is
contained in “Platform Requirements” on page 24.
? If you have enough disk space, the Ready to Install page is displayed.
20. On the Ready to Install page, you have the following choices:
? Click Back if you want to return to the previous page. Disk space is
rechecked if you do this.
? Click Install Now (or type 1 at the command line) to start the installation
process.
? Click Cancel to exit the installation program.
An Installation progress indicator bar is displayed.
When installation finishes, the Installation Summary page is displayed.
21. Check the installation outcome on the Installation Summary page. If
installation failure has occurred, review the following log file:
? /var/sadm/install/logs/Sun_ONE_Application_Server_install.log
Refer to “About Logs and Messages” on page 93 for additional information.
22. Click Finish (or type Finish at the command line) to complete the installation.
The installation components are now installed on your systems.
23. Start the server.
You can start the Sun ONE Application Server software by using the
instructions on “Starting and Stopping the Server” on page 81.
When the Admin Console has been started, the initial page of the Application
Server graphical interface is displayed.
Installing the Load Balancer Plug-in
44 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
24. If you have not already done so, add the HADB bin directory to the PATH
environment variable as described in “Setting Up the User Environment” on
page 63.
25. If you selected the HADB components, verify that you have successfully
installed the HADB software by doing the following on each host:
hadbm --help
The result of this command should be a list of all commands available using
the hadbm command-line utility.
You are now ready to configure your system for high availability. Proceed to
“Preparing for HADB Setup” on page 53 to begin this process.
Installing the Load Balancer Plug-in
This section provides instructions for installing the load balancer plug-in
component separately.
To install the load balancer plug-in component, perform these steps:
1. Check the system that will be hosting the web server and load balancer plug-in
to see if a previously-installed load balancer plug-in or reverse proxy plug-in is
present. If it is, you will need to remove it using the unistallation program.
As root, run the following command:
pkginfo SUNWaspx
On a clean system, the following message will be displayed:
ERROR: information for "SUNWaspx" was not found.
2. Verify that the correct web server is present on the machines were you are
going to install the load balancer plug-in. Currently supported versions
include the following:
? Sun ONE Web Server 6.0 SP6
NOTE The installation program creates an initial domain called domain1
with a single instance called server1. Refer to “Creating Domains
and Instances” on page 84 for instructions on creating additional
domains and instances.
Installing the Load Balancer Plug-in
Chapter 2 Installing Enterprise Edition Software 45
? Apache Web Server 1.3.27
3. Log in as root and create a temporary directory for the product distribution
file.
4. For a download, unzip the .gz file as follows:
gunzip sun-appserver7-sol.tar.gz
5. For a download, untar the unzipped file as follows:
tar -xvf sun-appserver7-sol.tar
This process may take a little time. When the files are unpacked, you will see
the sun-appserver7 directory, which contains the setup file and the pkg
directory.
6. Navigate to the sun-appserver7 installation directory.
7. Select your installation method.
Refer to “Installation Options” on page 34 for guidelines on selecting the
correct options to use with the setup command.
When the installation starts, the Welcome page of the installation program is
displayed.
8. Read the License Agreement and click Yes to agree to the terms of the license
(or type Yes at the command line), then click Next.
After you accept the License Agreement, the Select Installation Directory page
is displayed.
9. Specify the path to your Sun ONE Application Server installation directory
(default is /opt/SUNWappsrver7).
? Click Browse to browse for a directory (or press Enter at the command line
to accept the default installation directory).
NOTE Make a note of the web server installation directory. This
information will be needed during installation.
NOTE You must accept the license agreement to continue with the
installation.
Installing the Load Balancer Plug-in
46 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
? If you enter a directory name that does not already exist, the Create New
Directory? dialog is displayed.
• Click Create Directory (or type 1 at the command line) to create a new
directory.
• Click Choose New (or type 2 at the command line) to return to the
Select Installation Directory page.
The Component Selection page displays the available components.
10. Choose the load balancer plug-in component on the Component Selection page
(or type Yes or press Enter to accept the component from the command line).
If you selected the load balancer plug-in component, the Web Server Directory
page displays.
11. Identify your web server.
? Choose the web server you have installed (Sun ONE Web Server or
Apache Web Server).
? Enter the web server instance path.
Default values will be offered based on server type. The installation
program checks to see if appropriate configuration files can be found at the
specified location.
12. Click Next.
The installation program proceeds to verify that you have enough disk space
based on the components you selected. The Checking Disk Space progress
indicator bar is displayed.
? If you do not have enough disk space, an error message is displayed.
In this case, you need to exit the installation program, create enough space,
and restart the installation. Information on space requirements is
contained in “Platform Requirements” on page 24.
? If you have enough disk space, the Ready to Install page is displayed.
NOTE If some components are disabled on the Component Selection page
(or if a command-line mode installation did not offer them for
installation), this means that disabled component has been detected
as already installed on your system.
Installing the Load Balancer Plug-in
Chapter 2 Installing Enterprise Edition Software 47
13. On the Ready to Install page, you have the following choices:
? Click Back if you want to return to the previous page. Disk space is
rechecked if you do this.
? Click Install Now (or type 1 at the command line) to start the installation
process.
? Click Cancel to exit the installation program.
An Installation progress indicator bar is displayed.
When installation finishes, the Installation Summary page is displayed.
14. Check installation outcome on the Installation Summary page. If installation
failure has occurred, review the following log file:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
Refer to “About Logs and Messages” on page 93 for additional information.
15. Click Finish (or type Finish at the command line) to complete the installation.
16. Edit the supplied loadbalancer.xml.example file to include references to
actual application server instances. This file is located in the following location:
For Sun ONE Web Server:
webserver_instance_dir/config/loadbalancer.xml.example
For Apache Web Server:
webserver_instance_dir/conf/loadbalancer.xml.example
17. After you have made your modifications, save the
loadbalancer.xml.example file as loadbalancer.xml in the same directory.
NOTE If you want to configure more than one web server instance, or want
to add additional instances at a later time, you will need to manually
configure them. Instructions for doing this are contained in the
Configuring Load Balancer Plug-in section in the Sun ONE
Application Server Administrator’s Guide.
Refer to the Apache documentation for information on the Apache
Web Server.
Installing in Silent Mode (Non-Interactive)
48 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Installing in Silent Mode (Non-Interactive)
If you choose to install the Sun ONE Application Server Version 7, Enterprise
Edition software in silent mode, the installation program runs without any user
input. This is made possible when the installation program accesses a text file that
provides the installation program with the configuration information it needs.
The following topics are discussed in this section:
• Creating the Installation Configuration File
• Installing in Silent Mode
Creating the Installation Configuration File
The installation configuration file is created when you use the savestate option
with the setup command to start a interactive installation. During the interactive
installation, your input is collected and stored in the configuration file you
specified. This forms the template for silent installation, which you can use later to
install the product on one or more machines.
If needed, you can modify the installation configuration file.
The following topics are addressed in this section:
• Syntax for Creating the Installation Configuration File
• Example Installation Configuration File
• Modifying the Installation Configuration File
Syntax for Creating the Installation Configuration File
The syntax for creating an installation configuration file is as follows:
For graphical method:
./setup -savestate
For command-line method:
./setup -console -savestate
Refer to “Installation Options” on page 34 for more detailed information.
Example Installation Configuration File
An installation configuration file looks similar to the following:
Installing in Silent Mode (Non-Interactive)
Chapter 2 Installing Enterprise Edition Software 49
# Wizard Statefile created: Mon Jan 27 16:25:26 PST 2003
# Wizard path: /tmp/herc/sun-appserver7/./appserv.class
# Install Wizard Statefile section for Sun ONE Application Server
#
[STATE_BEGIN Sun ONE Application Server 108a4222b3a6a8ed98832d45238c7e8bb16c67a5]
defaultInstallDirectory = /opt/SUNWappserver7
currentInstallDirectory = /opt/SUNWappserver7
SELECTED_COMPONENTS = Java 2 SDK, Standard Edition 1.4.1_03#Application
Server#Sun ONE Message Queue 3.0.1#Sample Applications#Load Balancing
Plugin#Uninstall#Startup
USE_BUNDLED_JDK = FALSE
JDK_LOCATION = /usr/j2se
JDK_INSTALLTYPE = PREINSTALLED
AS_INSTALL_DEFAULT_CONFIG_DIR = /etc/opt/SUNWappserver7
AS_INSTALL_CONFIG_DIR = /etc/opt/SUNWappserver7
AS_INSTALL_DEFAULT_VAR_DIR = /var/opt/SUNWappserver7
AS_INSTALL_VAR_DIR = /var/opt/SUNWappserver7
DOMAINS_DIR = /var/opt/SUNWappserver7/domains
WEBSERVER_INSTALL_DEFAULT_DIR = /usr/iplanet/servers
WEBSERVER_INSTALL_DIR = /opt/iplanet/servers/https-tesla.red.iplanet.com
INST_ASADMIN_USERNAME = admin
INST_ASADMIN_PASSWORD = adminadmin
INST_ASADMIN_PORT = 4848
INST_ASWEB_PORT = 81
INSTALL_STATUS = SUCCESS
[STATE_DONE Sun ONE Application Server 108a4222b3a6a8ed98832d45238c7e8bb16c67a5]
Modifying the Installation Configuration File
You can modify the installation configuration file by editing the variables and
values described in Table 2-2.
Table 2-2 Installation Configuration File Variables
Variable Name Valid values (if
applicable)
Content Comments
defaultInstallDirectory Default installation
directory path
Value not actively used by
installation program.
currentInstallDirectory Selected installation
directory path
Installing in Silent Mode (Non-Interactive)
50 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
SELECTED_COMPONEN
TS
List of product
components
selected for
installation
Pound (#') character is used as
list delimiter.
USE_BUNDLED_JDK TRUE
FALSE
Whether to install
J2SE bundled with
the product
JDK_LOCATION J2SE path Preinstalled J2SE path if
USE_BUNDLED_J2SE is set to
false; otherwise installation
location for bundled J2SE.
JDK_INSTALLTYPE PREINSTALLED
CANNOTUPGRADE
UPGRADABLE
CLEANINSTALL
How to handle
existing J2SE
installation
Only PREINSTALLED and
CLEANINSTALL are valid
values for silent installation
configuration file.
AS_INSTALL_DEFAULT_
CONFIG_DIR
Default
configuration files
directory path
Value not actively used by
installation program.
AS_INSTALL_CONFIG_DI
R
Selected
configuration file
directory path
AS_INSTALL_DEFAULT_
VAR_DIR
Default domains
configuration files
directory path
Value not actively used by
installation program.
AS_INSTALL_VAR_DIR Selected domains
configuration file
directory path
DOMAINS_DIR Selected domains
configuration file
directory path, plus
domains
subdirectory
AS_INSTALL_VAR_DIR and
DOMAINS_DIR are generally
redundant. However, both
entries are needed by legacy
installation program code.
WEBSERVER_INSTALL_D
EFAULT_DIR
Default web server
instance directory
path
Value not actively used by
installation program.
WEBSERVER_INSTALL_D
IR
Selected web server
instance directory
path
Table 2-2 Installation Configuration File Variables (Continued)
Variable Name Valid values (if
applicable)
Content Comments
Installing in Silent Mode (Non-Interactive)
Chapter 2 Installing Enterprise Edition Software 51
Installing in Silent Mode
To install the Sun ONE Application Server software in non-interactive silent mode,
perform these steps:
1. With a text editor, examine the current installation configuration file and verify
that it contains what you want to use for your silent installation.
2. Save your config_file with any name. For example:
cp statefile my_silent_config
3. Copy your installation configuration file to each machine where you plan to
install the Sun ONE Application Server Version 7, Enterprise Edition software.
4. Copy the Sun ONE Application Server installation files to each machine where
you plan to install the Application Server software.
INST_ASADMIN_USERN
AME
Administrator
username for initial
domain
INST_ASADMIN_PASSW
ORD
Administrator
password for initial
domain
INST_ASADMIN_PORT 0 - 65535 Administration
server port number
for initial domain
INST_ASWEB_PORT 0 - 65535 Server port number
for initial server
instance
INSTALL_STATUS SUCCESS
FAILURE
Installation
outcome
Mandated by installer
implementation. Value not
actively used by installation
program.
NOTE For silent mode, you can do a partial initial installation, but any
incremental (subsequent) installations must be done using an
interactive method.
Table 2-2 Installation Configuration File Variables (Continued)
Variable Name Valid values (if
applicable)
Content Comments
Installing in Silent Mode (Non-Interactive)
52 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
5. If you are not in the directory already, navigate to the directory where you
copied the installation files and your installation configuration file.
6. As superuser, start silent installation at the command line using the following
command format:
./setup -silent config_file
The installation program reads the specified config_file, checks for adequate disk
space, then installs the product based on the data in config_file.
When the prompt is returned, the silent installation is complete and the
installation components are now installed on your systems.
7. You can start the Application Server software by using the instructions on
“Starting and Stopping the Server” on page 81.
When the Admin Console has been started, the initial page of the Application
Server graphical interface is displayed.
You are now ready to configure your system for high availability. Proceed to
“Preparing for HADB Setup” on page 53 to begin this process.
53
Chapter 3
Preparing for HADB Setup
After the high-availability components have been installed on the servers that will
be part of an cluster, perform the tasks in this chapter to prepare for setting up high
availability. Refer to “Installation Roadmap” on page 16 to see the full sequence of
events for implementing the Sun ONE Application Server 7, Enterprise Edition
product.
The following topics are addressed here:
• Configuring Shared Memory and Semaphores
• Setting Up Host Communication
• Setting Up the User Environment
• Setting Up Administration for Non-Root
• Using the clsetup Command
After you have done the tasks here, proceed to the Sun ONE Application Server
Administrator’s Guide for comprehensive instructions on configuring and
managing the cluster, the load balancer plug-in, and the high-availability database
(HADB).
Information on high-availability topologies is available in the Sun ONE Application
Server System Deployment Guide.
Configuring Shared Memory and Semaphores
You will need to configure shared memory for the HADB host machines before
beginning to work with the HADB.
1. Log in as root.
Configuring Shared Memory and Semaphores
54 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Add lines similar to the following to the /etc/system file if they do not
already exist:
For shared memory:
set shmsys:shminfo_shmmax=0x80000000
set shmsys:shminfo_shmseg=20
This example sets maximum shared memory shmmax to 2GB (hexadecimal
0x80000000) which is sufficient for most configurations.
The shmsys:shminfo_shmmax setting is calculated as 10,000,0000 per 256 MB
and should set to be identical to the memory size for the host. To determine
your host’s memory, run this command:
prtconf | grep Memory
Then plug the value into the following formula:
((<host> MB / 256 MB) * 10,000,000)
For semaphores:
Your /etc/system file may already contain semmni, semmns, and semmnu
entries. For example:
set semsys:seminfo_semmni=10
set semsys:seminfo_semmns=60
set semsys:seminfo_semmnu=30
NOTE If necessary, make sure permissions are set correctly to
administer the HADB as non-root user. See “Setting up RSH for
HADB Administration” on page 56, Step 5.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 55
If the entries are present, increment the values by adding 16, 128, and 1000
respectively, as follows:
set semsys:seminfo_semmni=26
set semsys:seminfo_semmns=188
set semsys:seminfo_semmnu=1030
If your /etc/system file does not contain the above mentioned entries, add the
following entries at the end of the file:
set semsys:seminfo_semmni=16
set semsys:seminfo_semmns=128
set semsys:seminfo_semmnu=1000
This is sufficient to run up to 16 HADB nodes on the computer.
3. Reboot the machine for changes to take effect.
For an explanation of HADB nodes, see Configuring the High Availability
Database in the Sun ONE Application Server Administrator’s Guide.
Setting Up Host Communication
To implement remote access for HADB administration, all machines that will be
used for running HADB servers and the HADB management client must be
configured for Remote Shell (RSH) or Secured Shell (OpenSSH/SSH).
RSH is a simple remote shell command and does not have any security features.
The SSH communication channel provides a level of security by encrypting the
data that passes between the HADB nodes.
NOTE Your original /etc/system file may or may not contain all of these
entries.
Setting Up Host Communication
56 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
This section contains instructions for the following tasks:
• Setting up RSH for HADB Administration
• Setting Up SSH for HADB Administration
Setting up RSH for HADB Administration
If you want to use RSH instead of SSH, you must explicitly specify RSH using the
set managementProtocol option. Refer to Table 3-3 on page 72 for guidelines on
setting this parameter in the clresource.conf file.
To implement RSH, perform these steps:
1. Log in as root.
2. Edit the /etc/hosts file to contain entries for all the selected HADB hosts,
including the host name of the local host. Use localhost format. For example:
computer1.xbay.company.com
computer99.zmtn.company.com
3. Append this file to the /etc/hosts file of all selected installation hosts.
NOTE For Solaris 9, it is recommended that you use the default installation
of SSH. However, you can use RSH if preferred by following the
instructions in “Setting up RSH for HADB Administration” on
page 56 and then editing the clresource.conf file to specific RSH as
described in “Running the clsetup Command” on page 77.
On Solaris 8, by default SSH is not installed. Follow the instructions
in “Installing SSH for Solaris 8” on page 59 if SSH is not on your
Solaris 8 system.
If you want to use SSH, but it is not configured or not available, you
will not be able to use the hadbm command. Refer to “SSH
Requirements and Limitations” on page 58 to verify that SSH is
recognized.
NOTE SSH is the strongly recommended default for the hadbm create
command because SSH is more secure than RSH.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 57
4. Create a .rhosts file in the $HOME directory of the HADB user, if one does
not already exist.
vi .rhosts
5. Verify that permissions are set to Read Only for group and other. For example:
rw-r--r--
6. Add the host name of each HADB host, including the name of your local host,
followed by the name of your database user. For example, if the database user
is Jon:
computer1.xbay.company.com Jon
computer99.zmtn.company.com Jon
mine456.red.mycompany.com Jon
7. Append this file to the .rhosts file of each HADB host.
8. Check host communication for each host. For example:
rsh computer99.zmtn.company.com uname -a
If all is well, the identity will be returned from the other host.
Setting Up SSH for HADB Administration
SSH is strongly recommended for using the hadbm create command because SSH
is more secure than RSH.
This section contains the following sections:
• SSH Requirements and Limitations
• Installing SSH for Solaris 8
• Configuring SSH
NOTE From a security perspective, the DSA-based version 2 protocol is
recommended instead of the RSA-based version 1 protocol. The
version you select depends on the SSH client software in use at your
site.
Setting Up Host Communication
58 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
SSH Requirements and Limitations
You may need to take action on any or all of the following requirements during
your SSH setup:
• Location of the SSH binaries—The high-availability management client expects
to find the ssh and scp binaries in the following location on each HADB host:
/usr/bin
? If the binaries are on your system but this location is not correct, you will
need to make a symbolic link from /usr/bin to the correct location.
? If you are on a Solaris 8 system, the SSH binaries are not installed by
default and so may not be present. If this is the case, follow the instructions
in “Installing SSH for Solaris 8” on page 59.
• Support—The only tested support is for SunSSH and OpenSSH. If you are
using another version of SSH, it is best to refer to the setup instructions in that
product’s documentation to ensure that your SSH communications work
correctly.
• OpenSSH clients and daemons—If you are running in an environment with
OpenSSH clients and daemons, you should name the key file as follows:
~/.ssh/authorized_keys2 or ~/.ssh/authorized_keys.
• Running as root—If you are running the HADB admin clients as root, make
sure that the sshd configuration (/etc/ssh/sshd_config) on all machines has
the PermitRootLogin parameter set to yes.
NOTE Although SSH is installed by default on Solaris 9 systems, on Solaris
8, by default SSH is not installed. Instructions for installing SSH for
Solaris 8 are contained in “Installing SSH for Solaris 8” on page 59.
NOTE By default, Sun SSH does not permit root login; it is set to no. If the
sshd configuration is changed, sshd must be restarted. Type the
following to restart the service:
/etc/init.d/sshd stop/start
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 59
• No SSH protocol version 2 support—If your SSH clients and daemons do not
support SSH protocol version 2, you will need to run ssh-keygen without
options. The key file will then be named identity.pub instead of id_dsa.pub.
This file must be appended to ~/.ssh/authorized_keys.
• Mixed SSH environment—If you are operating in a mixed SSH environment,
you will need to create both files ~/.ssh/authorized_keys2 and
~/.ssh/authorized_keys; the latter may contain both version 1 and version 2
keys.
• Co-location—If the Sun ONE Application Server and the HADB are co-located
on the same machine, you will need to create a known_hosts file under the
.ssh directory by running one of the following commands:
ssh localhost
or
ssh hostname
Installing SSH for Solaris 8
The ssh and scp binaries are not installed by default on Solaris 8 systems. If the
binaries are not on your Solaris 8 system, perform these steps:
1. Go to the following site:
http://www.sunfreeware.com/openssh8.html
On this site, you may receive a message similar to the following:
===PLEASE NOTE!!!............ make a note of some of the mirror
sites so that if the servers are down, you can still download
from a mirror site.
If you receive such a message, try one of the many mirror sites listed in the
FTP/Mirror Sites link. For example:
http://sunfreeware.secsup.org/
2. On this site, follow the instructions in the Installation Steps to download and
install all the necessary OpenSSH packages and patches.
3. After you have installed OpenSSH, proceed to the next section on Configuring
SSH.
Configuring SSH
To set up SSH on a system where the ssh and scp binaries are already installed,
perform the steps in one of the following sections:
Setting Up Host Communication
60 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• SSH for Non-Mounted Home Directories
• SSH for Mounted Home Directories
SSH for Non-Mounted Home Directories
To implement SSH in systems with home directories that are not mounted,
perform these steps:
1. Verify that SSH requirements have been understood and met as specified in
“SSH Requirements and Limitations” on page 58.
2. Log in to the host as the HADB user.
3. Generate your keys by running the following:
ssh-keygen -t dsa
For SSH1 and OpenSSH/1, you normally do not need to give any parameters
to the ssh-keygen command.
4. For the next three prompts, accept the default options by pressing Enter.
5. Repeat steps 1, 2, and 3 for all machines in your cluster.
A file called identity.pub or id_dsa.pub (depends on whether you are using
SSH version 1 or version 2) located in your ~/.ssh directory holds the public key.
To connect to a machine without being asked for a password, the content of this file
must be appended to a file called authorized_keys on all the machines.
6. To set up login identity, go to your user directory:
~/.ssh.
For SSH1, OpenSSH/1:
a. Copy the identity.pub file and name it authorized_keys.
b. For each of the other machines in the cluster, copy the content of the
identity.pub file and append it to the local authorized_keys file.
OpenSSH/2:
a. Copy the id_dsa.pub file and name it authorized_keys2.
b. For each of the other machines in the cluster, copy the content of the
id_dsa.pub file and append it to the local authorized_keys2 file.
7. Copy the authorized_keys file to the ~/.ssh directory on all the HADB
machines.
Setting Up Host Communication
Chapter 3 Preparing for HADB Setup 61
8. Verify that the .ssh directory, HADB user’s home directory, and the
.ssh/authorized_keys file do not have write permissions for group and
other.
If needed, disable these group/other write permissions as follows:
chmod og-w ~/.ssh
chmod og-w ~/.ssh/authorized_keys
chmod og-w $HOME
Replace $HOME with the home directory of the HADB user. For example:
chmod og-w ~/johnsmith
9. To enable login without any user input, at initial SSH usage (after the SSH
environment is set up) you need to add the node machine name to the
known_hosts file under the /.ssh directory as follows:
a. Type the following:
ssh machine_name
You will be prompted with a Yes/No question whether to add
machine_name to the known_hosts file.
b. Answer Yes.
You will now be able to log in without any input.
10. To verify that SSH is set up correctly, SSH to each host in the cluster before
trying to run the management tool for HADB.
You are automatically logged in without a password requirement.
SSH for Mounted Home Directories
To implement SSH in systems with mounted home directories, perform these
steps:
1. Verify that SSH requirements have been understood and met as specified in
“SSH Requirements and Limitations” on page 58
NOTE If the files under the ~/.ssh directory have even read permission
given to group/other, you cannot set up an automatic SSH login
identity. In this case, if you try ssh machine_name, the system
complains about the incorrect permissions and asks for a password.
In other words, it is best not to give any permissions at all for
group/other if you want to enable automatic login.
Setting Up Host Communication
62 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Log in to host as the HADB user.
3. Generate your keys by running the following:
ssh-keygen -t dsa
For SSH1 and OpenSSH/1, you normally do not need to give any parameters
to the ssh-keygen command.
4. For the next three prompts, accept the default options by pressing Enter.
A file called identity.pub or id_dsa.pub (depends on whether you are using
SSH version 1 or version 2) located in your ~/.ssh directory holds the public
key. To connect to a machine without being asked for a password, the content
of this file must be appended to a file called authorized_keys2 on all the
machines. This can be done as follows:
5. To set up login identity, go to your user directory:
~/.ssh.
For SSH1, OpenSSH/1—Copy the identity.pub file and name it
authorized_keys.
For OpenSSH/2—Copy the id_dsa.pub file and name it authorized_keys.
6. Verify that the .ssh directory and the .ssh/authorized_keys file do not have
write permissions for group and other.
If necessary, disable these group/other write permissions as follows:
chmod og-w ~/.ssh
chmod og-w ~/.ssh/authorized_keys
chmod og-w /$HOME
Replace HOME with the home directory of the HADB user. For example:
chmod og-w ~/johnsmith.
NOTE If the files under the ~/.ssh directory have even read permission
given to group/other, you cannot set up an automatic SSH login
identity. In this case, if you try to run ssh machine_name, the system
complains about =incorrect permissions and asks for a password. In
other words, it is best not to give any permissions for group/other if
you want to enable automatic login.
Setting Up the User Environment
Chapter 3 Preparing for HADB Setup 63
7. To enable login without any user input, at initial SSH usage (after the SSH
environment is set up) you need to add the node machine name to the
known_hosts file under the /.ssh directory
a. Type the following:
ssh machine_name
You will be queried about whether or not to add machine_name to the
known_hosts file.
b. Answer Yes.
You will now be able to log in without any input.
8. To verify that SSH is set up correctly, SSH to each host in the cluster before
trying to run the management tool for HADB.
You are automatically logged in without a password requirement.
Setting Up the User Environment
After you have set up host communication, you can run the hadbm command from
the install_dir/SUNWhadb/4/bin directory location as follows:
./hadbm
However, it is much more convenient to set up your local environment to use the
high-availability management client commands from anywhere. To set this up,
perform the following steps.
1. Set the PATH variable as follows.
setenv PATH ${PATH}:install_dir/bin:install_dir/SUNWhadb/4/bin
2. Verify that the PATH settings are correct by running the following commands:
which asadmin
which hadbm
NOTE The examples in this section apply to using csh. If you are using
another shell, refer to the man page for your shell for instructions on
setting variables.
Setting Up Administration for Non-Root
64 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
3. If multiple Java versions are installed on your system, you must ensure that the
JAVA_HOME environment variable points to the correct Java version (1.4.1_03
for Enterprise Edition).
setenv JAVA_HOME java_install_dir
setenv PATH ${PATH}:${JAVA_HOME}/bin
Setting Up Administration for Non-Root
By default, during the initial installation or setup of the Sun ONE Application
Server, write permissions of the files and paths created for Sun ONE Application
Server are given to root only. For a user other than root to create or manage the Sun
ONE Application Server, write permissions on the associated files must be given to
that specific user, or to a group to which the user belongs. The files that are affected
are the following (with their default locations):
• Sun ONE Application Server configuration files—install_config_dir/cl*.conf
• Sun ONE Application Servers setup and administration scripts—
install_dir/bin/cl*
• HADB binaries—install_dir/SUNWhadb
• HADB configuration—/etc/opt/SUNWhadb
You can create a user group for managing the Sun ONE Application Server as
described in the following procedure. (An alternate approach is to set permissions
and ownership for the specific user.)
To create a Sun ONE Application Server user group and set permissions on the
installation root directory, repeat the following process for each affected file:
1. Log in as root.
2. From the command prompt, create the Sun ONE Application Server user
group. For example:
# groupadd s1asuser
You can type groupadd at the command line to see appropriate usage.
3. Change the group ownership for each affected file to the newly-created group.
For example:
chgrp -R s1asuser install_config_dir/cl*.conf
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 65
4. Set the write permission for the newly-created group:
chmod -R g+rw install_config_dir/cl*.conf
5. Repeat steps 3 and 4 for each affected file.
6. Make the clsetup and cladmin commands executable by the newly-created
group. For example:
chmod -R g+x install_dir/bin/cl*
7. Delete and recreate the default domain, domain1, using the --sysuser option.
The sysuser must also belong to the newly-created group. For example:
asadmin delete-domain domain1
asadmin create-domain --sysuser bleonard --adminport 4848
--adminuser admin --adminpassword password domain1
Using the clsetup Command
The purpose of the clsetup command is to automate the process of setting up a
basic cluster in a typical configuration. The clsetup command is located in
install_dir/bin, where install_dir is the directory where the Sun ONE Application
Server software is installed.
The clsetup command is bundled with the Sun ONE Application Server software
along with the cladmin command.
The following topics are addressed in this section:
• How the clsetup Command Works
• clsetup Requirements and Limitations
• Editing the clsetup Input Files
NOTE The cladmin command is used to streamline the process of
configuring and administering the cluster after all installation and
configuration tasks are complete, and is not documented here.
When you have completed the tasks in this Installation Guide, refer
to the Sun ONE Application Server Administrator’s Guide for
instructions on creating the HADB and on using on the cladmin
command.
Using the clsetup Command
66 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• Running the clsetup Command
• Cleanup Procedures for the clsetup Command
How the clsetup Command Works
The clsetup command is a set of Sun ONE Application Server commands that are
gathered together in a script that allows a cluster to be configured automatically,
based on prepopulated input files. As part of cluster setup, an HADB is created,
but you will still need to set up your working cluster using the hadbm commands
as described in the Sun ONE Application Server Administrator’s Guide.
The following topics are addressed in this section:
• How the Input Files Work
• What the clsetup Command Accomplishes
• Commands Used by the clsetup Command
How the Input Files Work
Three input files are used by the clsetup command to configure the cluster:
• clinstance.conf—This file is pre-populated with information about
application server instances server1 and server2. Refer to “The clinstance.conf
File” on page 70 for information on the contents of this file.
• clpassword.conf—This file is pre-populated with the Admin Server
password for domain1, which you provided when you installed the Sun ONE
Application Server 7, Enterprise Edition software. Refer to “The
clpassword.conf File” on page 71 for information on the contents of this file.
• clresource.conf—This file is pre-populated with information about the
cluster resources: HADB, JDBC connection pool, JDBC resource, and session
store and persistence. Refer to “The clresource.conf File” on page 72 for
information on the contents of this file.
NOTE The clsetup command interface is unstable. An unstable interface
may be experimental or transitional, and may therefore change
incompatibly, be removed, or be replaced by a more stable interface
in the next release.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 67
You can use the clsetup configuration parameters as they are preconfigured to set
up a typical cluster configuration. To support a different configuration, you can
make edits to any or all of the configuration files.
What the clsetup Command Accomplishes
Using the pre-populated values in the clsetup input files, the clsetup command
accomplishes the following:
• Creates a new server instance named server2 in the default domain named
domain1. The HTTP port number for server2 is the next sequential number
after the HTTP port number specified for server1 during installation (for
example, if port number 80 is provided for server1 during installation, the port
number for server2 is 81).
• Creates the HADB named hadb with two nodes on the local machine. The port
base is 15200, and the database password is password.
• Creates the HADB tables required to store session information in the HADB.
• Creates a connection pool named appservCPL in all the instances listed in the
clinstance.conf file (server1, server2).
• Creates a JDBC resource named jdbc/hastore in all the instances listed in the
clinstance.conf file (server1, server2).
• Configures the session persistence information in all the instances listed in the
clinstance.conf file (server1, server2).
• Enables high availability in all the instances listed in the clinstance.conf file
(server1, server2).
NOTE The configuration parameters required to set up the cluster are
always read from the input files, and cannot be supplied through
the command line.
NOTE Because the clresource.conf and clpassword.conf input files
store passwords, they are access-protected with 0600 permissions.
Using the clsetup Command
68 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Commands Used by the clsetup Command
The clsetup command uses a number of hadbm and asadmin commands to
perform the steps for setting up the cluster. In Table 3-1, the clsetup task is
described in the left column and the command used to accomplish the task is listed
in the right column.
clsetup Requirements and Limitations
The following requirements and limitations apply to the clsetup command:
• The install paths, device paths, configuration paths, and so on must be the
same on all machines that are of the cluster.
• Before you can use the clsetup command, the asadmin and hadbm commands
must be available on the local machine. Therefore, this command can only be
run on a machine where the following are installed:
? The Sun ONE Application Server component or the Sun ONE Application
Server Administration Client component
? The HADB component or the HADB Management Client component
• Before you can use the clsetup command, you must have configured shared
memory as described in “Configuring Shared Memory and Semaphores” on
page 53. The clsetup command does not set any shared memory values.
Table 3-1 hadbm and asadmin Commands Used by the clsetup Command
Task Performed by clsetup Command
Checks to see if database exists. hadbm status
Creates and starts the HADB. hadbm create
Gets the JDBC URL. hadbm get jdbcURL
Creates the session store. asadmin create-session-store
Checks the instance status. asadmin show-instance-status
Creates the instance. asadmin create-instance
Creates the JDBC connection pool. asadmin create-jdbc-connection-pool
Registers the data source. asadmin create-jdbc-resource
Configures the persistence type asadmin configure-session-persistence
Reconfigures the instance. asadmin reconfig -u admin
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 69
• Before you can use the clsetup command, you must have set up the HADB
cluster host communication for SSH or RSH as described in “Setting Up Host
Communication” on page 55.
• If you are using RSH (which is not the default), you will need to uncomment
the following line in the clresource.conf file (remove the # sign):
#set managementProtocol
• If you are co-locating the Application Server and the HADB on the same
machine using SSH, a known_hosts file must exist under the .ssh directory. If
it does not, run either the ssh localhostor the ssh hostname command before using
the clsetup command.
• Before running the clsetup command, you must start the Admin Servers of all
the Sun ONE Application Server instances that are part of the cluster.
• The administrator password must be the same for all domains that are part of
the cluster.
• If the entities to be handled (HADB nodes and Application Server instances)
already exist, the clsetup command does not delete or reconfigure them, and
the respective configuration steps are skipped.
• The values specified in the input files will be the same for all the instances in a
cluster. The clsetup command is not designed to set up instances with
different values. For example, this command cannot create a JDBC connection
pool with different settings for each instance.
• The clsetup command does not perform any inetd configuration; the HADB
is created with no inetd settings. Instructions for performing inetd
configuration are contained in the Sun ONE Application Server Administrator’s
Guide.
• Host names in the shell initialization files—If prompts are included with host
names in your .cshrc or .login files, the clsetup command may appear to
hang. You will need to remove any prompts and excess output in any remote
command invocations. For example, running the hostname command on hostB
should print hostB without a prompt.
• To run the clsetup command as a user other than root, you'll need to make the
changes as described in “Setting Up Administration for Non-Root” on page 64.
Using the clsetup Command
70 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Editing the clsetup Input Files
The input files that are needed for the clsetup command are installed under the
configuration installation directory, default /etc/opt/SUNWappserver7, as part of
the installation procedure. The installation program pre-populates these files with
the values to set up a typical configuration, but you can edit any or all of them as
needed using a text editor.
This section addresses the following topics:
• The clinstance.conf File
• The clpassword.conf File
• The clresource.conf File
The clinstance.conf File
For the clsetup command to work properly, all application server instances that
are part of a cluster must be defined in the clinstance.conf file. During
installation, the installation program creates a clinstance.conf file with entries
for two instances. If you add more instances to the cluster, you must add
information about these additional instances.
The format of the clinstance.conf file is as follows:
# Comment
instancename instance_name
user user_name
host localhost
port admin_port_number
domain domain_n
instanceport instance_port_number
One set of entries is required for each instance that is part of the cluster. Any line
that starts with a hash mark (#) is treated as a comment.
NOTE The order in which these entries appear in the clinstance.conf file
is important and must not be changed from the order specified here.
If you add information about more application server instances,
entries for these instances must appear in this order.
Comments can be added anywhere in the file.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 71
Table 3-2 provides information about the entries in the clinstance.conf file. The
left column contains the parameter name, the middle column defines the
parameter, and the right column contains the default value specified by the
installation program.
Example clinstance.conf File
This clinstance.conf file contains information about two instances.
#Instance 1
instancename server1
user admin
host localhost
port 4848
domain domain1
instanceport 80
#Instance 2
instancename server2
user admin
host localhost
port 4848
domain domain1
instanceport 81
The clpassword.conf File
When the clsetup command is run, the asadmin command needs the Admin
Server password, which is specified in the clpassword.conf file during
installation.
The format of the clpassword.conf file is as follows:
Table 3-2 Entries in the clinstance.conf File
Parameter Definition Default Value
instancename Application Server instance name server1, server2
user Admin Server user name admin
host Host name localhost
port Admin Server port number 4848
domain Administrative domain name domain1
instanceport Application Server instance port 80, 81
Using the clsetup Command
72 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
AS_ADMIN_PASSWORD= password
where password is the Admin Server password.
Permissions 0600 are preset on the clpassword.conf file, which can only be
accessed by the root user.
The clresource.conf File
During installation, the installation program creates the clresource.conf file to
set up a typical configuration. The clresource.conf file contains information
about the following resources that are part of the cluster:
• HADB information
• Session store information
• JDBC connection pool information
• JDBC resource information
• Session persistence information
Permissions 0600 are preset on the clresource.conf file, which can only be
accessed by the root user.
The parameters of the clresource.conf file are described in the following tables.
The left column contains the parameter name, the middle column defines the
parameter, and the right column contains the default value specified by the
installation program.
Table 3-3 describes the HADB parameters in the clresource.conf file.
NOTE Before running the clsetup command, the values specified in the
clresource.conf file can be modified for optimization, or for
setting up a different configuration. If you edit the values, make sure
that the order and format of the file is not changed.
Any line that begins with a hash mark (#) is treated as a comment.
Table 3-3 HADB Parameters in the clresource.conf File
Parameter Definition Default Value
historypath Path for the history files. /var/tmp
devicepath Path for the data and log devices. /opt/SUNWappserver7/SUNWhadb/4
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 73
Table 3-4 describes the session store parameters in the clresource.conf file.
datadevices Number of data devices on each node. 1
portbase Port base number used for node 0.
Other nodes are then assigned port
number bases in increments of 20 from
the number specified here (a random
number in the range 10000 - 63000).
15200
spares Number of spare nodes. 0
set Comma-separated list of database
configuration attributes.
For explanations of valid database
configuration attributes, see Sun ONE
Application Server Administrator’s Guide.
For example, to specify the use of RSH
instead of SSH (the default), uncomment
the following line:
#set managementProtocol=rsh
inetd Indicates if HADB runs with the inet
daemon.
false
inetdsetupdir Directory where theinet daemon
setup files will be put.
/tmp
devicesize Size of device in MB. This size is
applicable to all devices.
512
dbpassword Password for the HADB user. password
hosts All hosts used for all data nodes. Values are populated automatically
based on the hosts specified during
installation.
NOTE The database name is specified at the end of the [HADBINFO] section in the
clresource.conf file.
Table 3-3 HADB Parameters in the clresource.conf File (Continued)
Parameter Definition Default Value
Using the clsetup Command
74 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Table 3-5 describes the JDBC connection pool parameters in the clresource.conf
file.
Table 3-4 Session Store Parameters in the clresource.conf File
Parameter Definition Default Value
storeurl URL of the HADB store REPLACEURL
NOTE: Value is replaced by actual URL at
runtime.
storeuser User who has access to the
session store
appservusr
NOTE: Must match the username property in
Table 3-5.
storepassword Password for the storeuser password
NOTE: Must match the password property in
Table 3-5.
dbsystempassword Password for the HADB system
user
password
Table 3-5 JDBC Connection Pool Parameters in the clresource.conf File
Parameter Definition Default Value
steadypoolsize Minimum and initial number of
connections maintained in the pool.
8
maxpoolsize Maximum number of connections that
can be created.
32
datasourceclass
name
Name of the vendor-supplied JDBC
datasource.
Name of the vendor-supplied JDBC
datasources capable datasource class
will implement
javax.sql.XADatasource
interface.
Non-XA or Local transactions only
datasources will implement
javax.sql.Datasource interface.
com.sun.hadb.jdbc.ds.HadbDataS
ource
isolationlevel Specifies the transaction isolation level
on the pooled database connections.
repeatable-read
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 75
Table 3-6 describes the JDBC resource parameters in the clresource.conf file.
Table 3-7 describes the session persistence parameters in the clresource.conf file.
validationmetho
d
Specifies the type of validation method. meta-data
property Property used to specify username,
password, and resource configuration.
username=appservusr:password=p
assword:cacheDataBaseMetaData=
false:eliminateRedundantEndTra
nsaction=true:serverList=REPLA
CEURL
NOTE: Make sure that the username and
password properties use the same values
as shown in the Session Store Parameters
table. REPLACEURL is replaced by the
actual URL at runtime.)
NOTE The connection pool name is specified at the end of the
[JDBC_CONNECTION_POOL] section in the clresource.conf file.
Table 3-6 JDBC Resource Parameters in the clresource.conf File
Parameter Definition Default Value
connectionpoolid Name of the connection pool appservCPL
NOTE: Connection pool name is specified in
Table 3-5.
NOTE The JDBC resource name is defined at the end of the [JDBC_RESOURCE] section
in the clresource.conf file.
Table 3-5 JDBC Connection Pool Parameters in the clresource.conf File (Continued)
Parameter Definition Default Value
Using the clsetup Command
76 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Example clresource.conf File
[HADBINFO]
historypath /var/tmp
devicepath /opt/SUNWappserver7/SUNWhadb/4
datadevices 1
portbase 15200
spares 0
#set managementProtocol=rsh
inetd false
inetdsetupdir /tmp
devicesize 512
dbpassword password
hosts machine1,machine1
hadb
[SESSION_STORE]
storeurl REPLACEURL
storeuser appservusr
storepassword password
dbsystempassword password
[JDBC_CONNECTION_POOL]
steadypoolsize 8
maxpoolsize 32
datasourceclassname com.sun.hadb.jdbc.ds.HadbDataSource
isolationlevel repeatable-read
validationmethod meta-data
property
username=appservusr:password=password:cacheDataBaseMetaData=false:e
liminateRedundantEndTransaction=true:serverList=REPLACEURL
appservCPL
Table 3-7 Session Persistence Parameters in the clresource.conf File
Parameter Definition Default Value
type Session persistence type ha
frequency Session frequency web-method
scope Session scope session
store Session store jdbc/hastore
NOTE: Store name is defined at end of the
[JDBC_RESOURCE] section.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 77
[JDBC_RESOURCE]
connectionpoolid appservCPL
jdbc/hastore
[SESSION_PERSISTENCE]
type ha
frequency web-method
scope session
store jdbc/hastore
Running the clsetup Command
The syntax for running the clsetup command is as follows:
clsetup [--help] [--instancefile instance_file_location] [--resourcefile
resource_file_location] [--passwordfile password_file_location]
If no arguments are specified, the clsetup command assumes the following
defaults:
--instancefile is install_config_dir/clinstance.conf
--resourcefile is install_config_dir/clresource.conf
--passwordfile is install_config_dir/clpassword.conf
You can override these arguments by providing custom input file locations. For
example:
./clsetup --resourcefile /tmp/myappservresource.conf
To run the clsetup command, perform the following steps:
1. Verify that the requirements have been met as described in “clsetup
Requirements and Limitations” on page 68.
NOTE When providing custom input files, follow the required format
found in the input files. For information on doing this, see “Editing
the clsetup Input Files” on page 70.
NOTE If you want to run the clsetup command as a user other than root,
follow the instructions in “Setting Up Administration for Non-Root”
on page 64 to set this up.
Using the clsetup Command
78 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Verify that the input files have the information that is required to set up the
cluster. If necessary, edit the input files following the guidelines in “Editing the
clsetup Input Files” on page 70.
3. If you are using RSH, edit the clresource.conf file to uncomment the
following line (remove the # sign):
#set managementProtocol
4. Go to the Sun ONE Application Server installation /bin directory:
cd install_dir/bin
5. Invoke the clsetup command using the appropriate syntax. For example, to
run the command using the defaults:
./clsetup
The clsetup command displays the welcome message, the prerequisites for
configuring the cluster, and the following message:
Do you want to start configuring your cluster? [Yes/No]
6. To start configuring, type Yes and press Enter.
The clsetup command runs in verbose mode. The various commands are
displayed on the screen as they run, and the output is redirected to the log file,
/var/tmp/clsetup.log.
If a vital error occurs (for example, failure to create a non-existing HADB), the
configuration stops and the error is recorded in the log file. If the log file
already exists, the output is appended to the existing log file.
7. When the clsetup command completes the configuration, you are advised
about the location of the log file. It’s a good idea to scan the log file after each
run.
8. Upon completion, the clsetup command returns the exit codes as described in
Table 3-8:
NOTE If the entities to be handled (HADB nodes and Application
Server instances) already exist, the clsetup command does not
delete or reconfigure them, and the respective configuration
steps are skipped. This type of event is recorded in the log file.
Using the clsetup Command
Chapter 3 Preparing for HADB Setup 79
You can obtain a list of the exit codes by running the following command from the
command line immediately after running the clsetup command:
‘echo $?’
Cleanup Procedures for the clsetup Command
After running the clsetup command, errors that have occurred are logged in the
log file /var/tmp/clsetup.log. Examine the log file after every run of the
clsetup command and correct any significant errors that are reported (for
example, failure to create a non-existing instance).
You can undo all or part of the configuration as follows:
• To delete an Application Server instance, use the following command:
asadmin delete-instance instance_name
Table 3-8 Exit Codes for the clsetup Command
Exit Code Description
0 Successful exit
2 Usage error
3 Instance file not found
4 Instance file cannot be read
5 Resource file not found
6 Resource file cannot be read
7 Password file not found
8 Password file cannot be read
10 Script cannot find asadmin
11 Script cannot find hadbm
12 Cannot create temporary file
13 Session store configuration failed
14 Create HADB failed
15 HADB get jdbcURL failed
16 User exits in welcome message
Using the clsetup Command
80 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
For example:
asadmin delete-instance server1
• To delete the HADB, use the following commands:
a. hadbm stop database_name
For example:
hadbm stop hadb
b. hadbm delete database_name
For example:
hadbm delete hadb
• To clear the session store, use the following command:
cladmin clear-session-store --storeurl URL_information
--storeuser storeUsername --storepassword store_user_name
For example:
cladmin clear-session-store --storeurl
jdbc:sun:hadb:localhost:10005,localhost::10025 --storeuser
appservusr --storepassword password
• To delete the JDBC connection pool, use the following command:
asadmin delete-jdbc-connection-pool connectionpool_name
For example:
asadmin delete-jdbc-connection-pool appservCPL
• To delete the JDBC resource, use the following command:
cladmin delete-jdbc-resource JDBCresource_Name
For example:
cladmin delete-jdbc-resource jdbc/hastore
After you have completed the tasks in this chapter (and the post-installation tasks
in the following chapter, if needed), proceed to the Sun ONE Application Server
Administrator’s Guide for instructions on configuring the HADB and managing the
cluster, the load balancer plug-in, and the HADB.
81
Chapter 4
Post-installation Tasks
This chapter discusses some tasks you may need to perform during or after
installing the Sun ONE Application Server 7, Enterprise Edition software.
The following topics are addressed here:
• Starting and Stopping the Server
• Creating Domains and Instances
• Web Services Client Implementation
• Stopping and Starting the HADB
Starting and Stopping the Server
Because the Sun ONE Application Server is not automatically started during
installation, you will need to start the application server environment yourself
using either of the following methods:
• Using the Command-line Interface
• Using the Administration Interface
Using the Command-line Interface
You can use the asadmin command-line interface to start and stop:
• The entire application server
• A specific administrative domain
• An individual application server instance
Starting and Stopping the Server
82 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
The subcommands of asadmin listed in Table 4-1 are relevant to start and stop
operations.
Using start-domain and stop-domain
If the Application Server is running, use the following command to stop both the
Admin Server as well as the application server instance of the initially-configured
domain:
asadmin stop-domain --domain domain1 --local
where domain1 is the default name of the administrative domain defined during
installation of the Application Server.
As the command completes, you should observe the following results:
asadmin stop-domain --domain domain1 --local
Instance domain1:server1 stopped
Domain domain1 Stopped.
Likewise, you can start the initially-configured administrative domain by running
the following command:
asadmin start-domain --domain domain1
As the command completes, you should observe the following results:
Table 4-1 Start/Stop Subcommands of asadmin
Subcommand Description
start-appserv Starts the entire Application Server.
stop-appserv Stops the Application Server.
start-domain Starts the administrative server and application server
instances of the specified administrative domain
stop-domain Stops the administrative server and the application server
instances of the specified administrative domain.
start-instance Starts the specified application server instance. Can be run in
either a local or remote mode. In local mode, running this
subcommand does not require the administrative server to be
running.
stop-instance Stops the specified application server instance. Similar in
operation to start-instance.
Starting and Stopping the Server
Chapter 4 Post-installation Tasks 83
asadmin start-domain --domain domain1
Instance domain1:admin-server started
Instance domain1:server1 started
Domain domain1 Started.
Using start-instance and stop-instance
To stop a specific application server instance without relying on the presence of an
Admin Server, you can use the following command:
asadmin stop-instance --local server1
where server1 is the default name of the application server instance. If your
environment contains more than one administrative domain, then you need to
specify the administrative domain name when invoking the stop-instance
command. For example:
asadmin stop-instance --local --domain domain1 server1
To start a specific application server instance in local mode, you can use the
following command:
asadmin start-instance --local server1
If you want to start or stop an instance on a remote system, you can specify the
target Admin Server and administrative user name and password on the
start-instance and stop-instance commands.
Getting Helpful Information
If you run either of these subcommands without parameters, usage information is
displayed. For example:
asadmin start-instance
Invalid number of operands received
Command 'start-instance' not executed successfully
USAGE: start-instance [--user admin_user] [--password
admin_password] [--host localhost] [--port 4848] [--local=false]
[--domain domain_name] [--debug=false] [--secure | -s]
instancename
Alternatively, you can issue the subcommands followed by the --help option to
obtain complete usage information.
Creating Domains and Instances
84 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Using the Administration Interface
When the Admin Server is running, you can use the web-based Administration
interface to start and stop application server instances.
To start the Administration interface (also called the Admin Console or the
graphical interface):
1. Open a browser window and specify the location of your Admin Server’s
console application.
During installation, the default port number for the Admin Server is set to
4848. If this port was already in use, or you selected another port number,
specify that port number. For example:
http://localhost:4848
2. Sign into the Administration interface using the administrative user name and
password specified during installation.
After you've been successfully authenticated, the initial screen of the
Administration interface is displayed.
3. Select the server1 node to access the start and stop functions.
The application server instance is either in a Running or Not Running state.
4. Depending on the server instance state, click either Start or Stop to start or stop
the application server instance.
Creating Domains and Instances
The installation program creates an initial domain called domain1 with a single
instance called server1. Create any additional domains and server instances using
following commands:
To create a new domain:
asadmin create-domain --adminport port_number --adminuser admin
--adminpassword password domain_name [--path domain_path][--sysuser
sys_user] [--passwordfile file_name]
To create a new instance:
Web Services Client Implementation
Chapter 4 Post-installation Tasks 85
asadmin create-instance --instanceport instanceportinstance_name
[--user admin_user] [-password admin_password] [--host localhost]
[--port 4848] [--sysuser sys_user] [--domain domain_name]
[--local=false] [--passwordfile filename][--secure|-s]
Refer to the asadmin man pages for additional information on these commands.
Web Services Client Implementation
To install and configure the web services client, refer to the Sun ONE Application
Server Developer's Guide to Clients.
Stopping and Starting the HADB
This section addresses the following topics:
• Stopping the HADB
• Starting the HADB After Stopping
Stopping the HADB
If you are uninstalling, you will need to stop the running HADB on the node where
you are working. The hadbm stop command stops all HADM processes on each
node. It also captures the role of each node and saves this information locally to the
/etc/opt/SUNWhadb/dbdef/mydb/stopstate file. The hadbm start command
references this file so it knows what role to give the nodes when it starts the
database.
To stop a running HADB, perform these steps:
1. Log in as root on the system where the HADB is running.
2. Run the hadbm stop command using the following format:
hadbm stop hadb_name
This command stops the database.
3. Type yes or y to confirm, anything else to cancel. When the HADB is stopped,
the following is displayed:
Database successfully stopped
Stopping and Starting the HADB
86 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
4. Verify the HADB is stopped by running the following command:
hadbm status --nodes hadb_name
The state for all nodes should be Stopped.
Starting the HADB After Stopping
You must issue the hadbm start command from the host where you issued the
hadbm stop command because the stopstate file is on that host and the
stopstate file is needed for the hadbm start command.
To start all active nodes of an HADB after it has been stopped using the hadbm stop
command, perform these steps:
1. Log in as root on the system where the HADB resides.
2. Run the hadbm start command from the host where you issued the hadbm
stop command using the following format:
hadbm start hadb_name
After the HADB has started, the following is displayed:
Database successfully started
NOTE If the inetd process was still running, the clu_nsup_srv process
would be running and the state for the nodes (NodeState) would be
Starting.
87
Chapter 5
Uninstalling the Enterprise Edition
Software
This chapter contains instructions for uninstalling the Sun ONE Application Server
7, Enterprise Edition software from your system.
The following topics are addressed here:
• About Uninstalling
• Uninstalling the Application Server Software
• Uninstalling in Silent Mode (non-interactive)
About Uninstalling
The installation program enforces component dependencies as specified for each
component. Once component dependencies are satisfied, component life cycles are
independent. A particular component can be installed or uninstalled dynamically
through incremental installation and partial uninstallation mechanisms without
corrupting other components.
Uninstallation failure will result in a complete rollback of the installation, requiring
you to reinstall the product.
NOTE If an uninstallation fails, you may need to clean up some leftover
files or processes before attempting a new installation. In this case,
perform the tasks in “Uninstallation Failure Cleanup” on page 98.
Uninstalling the Application Server Software
88 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Components
The following components can be uninstalled separately or as a complete package:
• Sun ONE Application Server, including its graphical and command-line
administrative tools and Sun ONE Message Queue 3.0.1
• Java 2 Software Development Kit (J2SE), Standard Edition 1.4.1_03
• Sun ONE Application Server Administration Client (command-line tool only)
• Sample applications
• High-Availability Database (HADB)
• Load balancer plug-in for web servers
Installation files, configuration files, and all log files are removed from local and
remote hosts during installation.
Uninstallation Requirements
The following must be true for uninstallation to succeed:
• All databases are stopped and disabled prior to uninstalling.
For guidelines on stopping the HADB, refer to the “Stopping the HADB” on
page 85.
• All database hosts are reachable by SSH or RSH for the root user.
For instructions on setting this up HADB communications, refer to “Setting Up
Host Communication” on page 55.
• The uninstallation program is run from the original installation host.
Uninstalling the Application Server Software
The uninstallation program detects any running Sun ONE Application Server
processes and stops them before continuing to uninstall.
Uninstalling the Application Server Software
Chapter 5 Uninstalling the Enterprise Edition Software 89
To uninstall the Application Server software, perform the following steps:
1. Verify that you have met the requirements in “Uninstallation Requirements”
on page 88.
2. Log in as root on the machine where you want to uninstall the Sun ONE
Application Server 7, Enterprise Edition software.
3. Navigate to your machine’s Sun ONE Application Server 7 installation
directory.
4. Select your installation method.
? To run uninstallation using the graphical interface, type the following at
the command prompt (no options; this is the default method):
./uninstall
? To run uninstallation using the command-line interface, type:
./uninstall -console
The Welcome page of the uninstallation program is displayed.
5. Read the Welcome page and click Next (or press Enter at the command line) to
continue.
6. You will be queried about whether you want to do an incremental
uninstallation.
? If you answer No, the Ready to Install page is displayed as shown in
Step 7.
? If you answer Yes, the component selection page is displayed showing the
components that are installed on your system.
NOTE If your J2SE is installed in a directory other than /usr/j2se, you
must use the following command:
./uninstall -javahome valid_j2se_directory
where valid_j2se_directory is the path to your J2SE 1.4.1_03
installation.
Uninstalling in Silent Mode (non-interactive)
90 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
7. Select which components you want to uninstall and click Next (or press Enter
at the command line).
The Ready to Uninstall page is displayed showing a list of the Application
Server components you have selected to uninstall.
8. Click Uninstall Now (or press Enter on the command line) to start the
uninstallation process.
The Uninstallation progress indicator bar is displayed.
When uninstallation finishes, the Uninstall Summary page is displayed.
9. Review the details by clicking Details.
A details listing displays the top portion of the log file. Complete information
on the uninstallation can be found in the uninstallation log file specified at the
end of the details listing:
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.log
10. Click Dismiss to close the Details page.
11. Click Close (or press Enter at the command line) to quit the uninstallation
program.
12. Verify that uninstallation succeeded by checking to see that the Application
Server components have been removed from the system.
Uninstalling in Silent Mode (non-interactive)
To uninstall the Sun ONE Application Server software in non-interactive silent
mode, perform these steps:
NOTE If uninstallation is interrupted, or if you have trouble installing the
Application Server software after removing a previous version or a
component, refer to “Uninstallation Failure Cleanup” on page 98.
NOTE The interactive methods allow you to select which components you
want to uninstall; silent mode does not. That is, incremental, or
partial, uninstallation is not available for silent mode.
Uninstalling in Silent Mode (non-interactive)
Chapter 5 Uninstalling the Enterprise Edition Software 91
1. Log in as root on the machine where you want to uninstall the Application
Server 7, Enterprise Edition software.
2. Start silent uninstallation at the command line as follows:
./uninstall -silent
When the prompt is returned, the silent uninstallation is completed.
3. Verify that uninstallation succeeded by checking to see that the Sun ONE
Application Server components have been removed from the system.
4. Repeat this process for each server where you want to uninstall.
Uninstalling in Silent Mode (non-interactive)
92 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
93
Chapter 6
Troubleshooting
This chapter describes how to solve common problems that might occur during
installation of the Sun ONE Application Server 7, Enterprise Edition software.
The following topics are addressed here:
• About Logs and Messages
• J2SE Installation/Upgrade Issues
• Forgotten User Name or Password
• Forgotten Admin Server Port Number
• Connection Refused for Administration Interface
• Server Won’t Start: CGI Error Occurs
• Uninstallation Failure Cleanup
About Logs and Messages
Both the installation and uninstallation programs create log files and log all
installation and uninstallation events to these files. The primary purpose of these
log files is to provide troubleshooting information.
In addition to installation program messages and log files, operating system
utilities such as pkginfo and showrev on Solaris can be used to gather system
information.
Log file entries include information about the attempted action, the outcome of the
action, and, if applicable, the cause of failure. The log files contain the following
types of message entries:
J2SE Installation/Upgrade Issues
94 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
• INFO—These messages mark normal completion of a particular installation
tasks.
• WARNING—These messages mark non-critical failures. Warning messages
generally contain information about the cause and the nature of the failure, and
also provide possible remedies.
• ERROR—These messages mark critical failures that cause installation or
uninstallation status to be reported as Failed. Error messages generally provide
detailed information about the nature and the cause of the problem that
occurred.
For a full listing of the Sun ONE Application Server error messages, refer to the
Enterprise Edition of the Sun ONE Application Server Error Messages Reference.
The following Application Server logs can be useful for troubleshooting:
• For problems you may have with the installation or uninstallation process:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.log
• In addition to these log files, low-level installation and uninstallation log files
are created at these locations:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.<timestamp>
/var/sadm/install/logs/Sun_ONE_Application_Server_uninstall.<timestamp>
• For problems with the clsetup command:
/var/tmp/clsetup.log
• For problems with the cladmin command:
/var/tmp/cladmin.log
J2SE Installation/Upgrade Issues
The installation program can only upgrade your J2SE installation when the
following requirements are met:
1. The following Solaris J2SE packages reside on the machine where you are
performing installation:
? SUNWj3rt
? SUNWj3dev
J2SE Installation/Upgrade Issues
Chapter 6 Troubleshooting 95
? SUNWj3man
? SUNWj3dmo
Verify this by running the pkginfo -i -l command on these packages.
2. The version of the Solaris J2SE packages is greater than or equal to version 1.3
and less than version 1.4.1_03.
3. The /usr/j2se (default) directory is writable by the user performing the
installation.
The following types of errors may occur if you attempt to upgrade your J2SE
during installation:
• Incompatible J2SE version---cannot upgrade.
• Failure to install J2SE reported through install log file.
Incompatible J2SE version---cannot upgrade.
If you receive this type of error, the first or second requirements above have not
been met.
Solution
Resolve your J2SE package or version issues by either fixing the Solaris packages or
completely removing the Solaris packages (only if they are not used by any other
application programs) using the pkgrm command.
If you remove the packages, you can then install the J2SE component using the
installation program by selecting the Install Java 2 SDK (1.4.1_03) option in the Java
Configuration panel.
Failure to install J2SE reported through install
log file.
If you receive this type of error, the third requirement above has not been met.
NOTE The installation program can only upgrade a package-based J2SE
installation, not a file-based J2SE installation.
Forgotten User Name or Password
96 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
Solution
Verify that your /usr/j2se directory is not read-only.
Forgotten User Name or Password
If you do not remember the administrative user name that was supplied during
installation, try these solutions in this order:
1. Enter the user name admin. This is the default user name specified in the server
configuration dialog during installation.
2. If this doesn’t work, look in the following file:
domain_config_dir/domain1/admin-server/config/admpw
This file contains the administrator's user name followed by the encrypted
form of the administrative user's password. Seeing the user name may jog your
memory.
3. If this doesn’t work, delete the administrative domain and recreate it with a
new password.
4. As a last resort, uninstall and reinstall the Sun ONE Application Server.
Forgotten Admin Server Port Number
If you do not remember the HTTP server port number of the Admin Server, you
can inspect the Admin Server's configuration file to determine the HTTP server
port number:
1. Navigate to domain_config_dir/domain1/admin-server/config/ and open the
server.xml file in a text editor.
2. Look for the following element:
http-listener id="http-listener-1" address="0.0.0.0"
port="4848"...
In this case, port 4848 is the HTTP port number in use.
Connection Refused for Administration Interface
Chapter 6 Troubleshooting 97
Connection Refused for Administration Interface
If the connection was refused when attempting to invoke the graphical
Administration interface, it is likely that the Admin Server is not running. The
Admin Server log file may be helpful in determining the reason the Admin Server
is not running.
To start the Admin Server, use the command-line instructions in “Starting and
Stopping the Server” on page 81.
Server Won’t Start: CGI Error Occurs
If the Sun ONE Application Server won’t start, you may receive the following
error:
[05/Aug/2002:01:12:12] SEVERE (21770): cgi_init reports:
HTTP4047: could not initialize CGI subsystem
(Cgistub path /export/home/sun/appserver7/appserv/lib/Cgistub),
err fork() failure [Not enough space]
The system may require additional resources. Possible solutions are described in
the following sections:
• Set Limits on File Descriptions
• On Solaris: Change Kernel Parameters
Set Limits on File Descriptions
You can use the ulimit command to determine the number of available file
descriptors or set limits on the system’s available file descriptors. The ulimit
command displays the limits for the current shell and its descendants.
For the sh shell, the ulimit -a command lists all the current resource limits. The
ulimit -n command lists the maximum file descriptors plus 1.
On Solaris: Change Kernel Parameters
On Solaris, increase the system resources by modifying the /etc/system file to
include the following entries:
Uninstallation Failure Cleanup
98 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
set rlim_fd_max=4086
set rlim_fd_cur=1024
The system will require a reboot for the new kernel parameters to take effect.
After you have set the shell resources, the Sun ONE Application Server should
start.
Uninstallation Failure Cleanup
If an uninstallation fails, you may need to clean up some leftover files or processes
before attempting a new installation.
1. Log in as root.
2. Navigate to your installation directory and check the content of the
/var/sadm/install/productregistry file for installed packages. That is,
check for files having the SUNW string.
cat /var/sadm/install/productregistry | grep SUNW
3. Run pkgrm for the SUNW packages that were found in the product registry file.
For example:
pkgrm SUNWasaco
4. Remove the following files, if they are present:
/tmp/setupSDKNative
/tmp/SolarisNativeToolkit_3.0_1
5. After the packages have been removed, manually remove the Sun ONE
Application Server-specific product registry file itself.
rm /var/sadm/install/productregistry
6. At the command line, find and kill all appservd processes that may be running
by typing the following:
ps -ef | grep appservd
kill -9 PID
7. Remove all remaining files under the Sun ONE Application Server installation
directories.
Uninstallation Failure Cleanup
Chapter 6 Troubleshooting 99
8. If present, remove the following log file:
/var/sadm/install/logs/Sun_ONE_Application_Server_install.log
This is necessary because every iteration of installation appends the log
information to this file if it exists.
Uninstallation Failure Cleanup
100 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
101
Appendix A
Installation Cheatsheet
Sun ONE Application Server 7, Enterprise Edition is a complex product to
implement. However, if you are an experienced installer and are familiar with
configuring high-availability systems, the summarized steps in this appendix may
be useful.
The installation phase of product implementation consists of the following
high-level tasks:
1. Fulfill the installation requirements.
2. Install the software components.
3. Complete the high-availability installation tasks.
4. Complete the post-installation tasks.
When you have finished the tasks listed in this appendix, the installation is
considered complete. You are now ready to proceed to the high-availability
configuration tasks as documented in the Sun ONE Application Server
Administrator’s Guide.
1. Fulfill the installation requirements.
Table A-2 lists the requirements that must be met in order to install the Sun ONE
Application Server Version 7, Enterprise Edition product.
Table A-1 Installation Requirements Tasks
Done Task Location of Full Instructions
Platform and HA configuration—Verify platform
and HA configuration have been met.
“Platform Requirements” on page 24
2. Install the software components.
102 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
2. Install the software components.
Table A-2 lists the tasks for installing the Sun ONE Application Server Version 7,
Enterprise Edition product components.
(if applicable) Solaris 8 patches—Verify the correct
patches are installed.
“Solaris 8 Patch Requirements” on page 26
(if applicable) Solaris 9 bundled, Message Queue
broker—Verify unique naming of domains and
instances.
“Shared Message Queue Broker Requirement”
on page 26
(if applicable) Hardened Solaris operating
system—Verify needed libraries are installed.
“Hardened Solaris Operating Environment
Requirement” on page 26
(if applicable) Existing installations—Remove any
existing Application Servers using uninstallation.
“General Requirements” on page 27
Available ports—Plan your port preferences. “General Requirements” on page 27
Root privileges—Verify that the installation person
has root privileges on target machine.
HA topology—Plan your high-availability topology. “Topology Planning” on page 28
Operational Deployment Guide
HA space—Evaluate your high-availability space
requirements
“Space Considerations” on page 29
“Platform Requirements” on page 24
Web server—Install the Sun ONE Web Server 6.0 SP6 “Web Server Installation” on page 29
iPlanet WebServer Installation Guide
Table A-2 Product Installation Tasks
Done Task Location of Full Instructions
Requirements—Verify that requirements are met. Table A-1 on page 101
Table A-1 Installation Requirements Tasks (Continued)
Done Task Location of Full Instructions
3. Complete the high-availability installation tasks.
Appendix A Installation Cheatsheet 103
3. Complete the high-availability installation
tasks.
Table A-3 lists the high-availability preparation tasks that are part of installing the
Sun ONE Application Server Version 7, Enterprise Edition product.
Start processes—Start processes that use ports and will run at same
time as Application Server.
Procedure starts here:
“Installing Application Server
Software” on page 36
(if applicable) Download the software bundle:
gunzip sun-appserver7-sol.tar.gz
tar -xvf sun-appserver7-sol.tar
Choose your installation method:
To invoke the graphical interface— ./setup
To invoke the command-line interface— ./setup -console
Select installation components (load balancer is usually installed
separately) and respond to all installation program prompts.
Check installation summary and logs.
Set PATH environment variable for HADB /bin.
Start the Application Server.
Verify that asadmin and hadbm commands run.
If a previous load balancer plugin is installed, remove it with the
uninstallation program
Procedure starts here:
“Installing the Load Balancer
Plug-in” on page 44
Verify that the correct web server is installed: Sun ONE 6.0 SP6 or
Apache Web Server 1.3.27
Invoke the Installation program to install the load balancer plugin
and respond to all installation program prompts.
(if applicable) Perform silent installation “Installing in Silent Mode
(Non-Interactive)” on page 48
Table A-3 High-Availability Installation Tasks
Done Task Location of Full Instructions
Configure shared memory for the HADB
hosts.
“Configuring Shared Memory and Semaphores” on
page 53
Table A-2 Product Installation Tasks (Continued)
Done Task Location of Full Instructions
4. Complete the post-installation tasks.
104 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
4. Complete the post-installation tasks.
Table A-4 lists the final tasks required for installing the Sun ONE Application
Server Version 7, Enterprise Edition product.
Set up communication for the HADB
hosts, either using RSH or SSH. (SSH is
Solaris 9 default)
“Setting up RSH for HADB Administration” on page 56
“Setting Up SSH for HADB Administration” on page 57
If needed for Solaris 8, install OpenSSH. “Installing SSH for Solaris 8” on page 59
Set up the user environment for hadbm. “Setting Up the User Environment” on page 63
(If applicable) Set up for running the
clsetup command as non-root.
“Setting Up Administration for Non-Root” on page 64
Verify that clsetup requirements are
met.
“clsetup Requirements and Limitations” on page 68
(If applicable) Edit the clsetup input files
for your configuration.
“Editing the clsetup Input Files” on page 70
Run clsetup to configure a basic cluster. “Running the clsetup Command” on page 77
Scan clsetup logs and correct any errors. “Cleanup Procedures for the clsetup Command” on
page 79
Table A-4 Post-Installation Tasks
Done Task Location of Full Instructions
(If needed) Use asadmin commands to
start or stop the Application Server.
“Creating Domains and Instances” on page 84
(If needed) Create additional domains. “Creating Domains and Instances” on page 84
(If needed) Use hadbm commands to stop
or start the HADB.
“Stopping and Starting the HADB” on page 85
(If needed) Install and configure web
services client.
Developer’s Guide to Clients
Table A-3 High-Availability Installation Tasks (Continued)
Done Task Location of Full Instructions
105
Index
SYMBOLS
.rhosts file 57
/etc/opt/SUNWappserver7/config 23
/etc/ssh/sshd_config 58
/opt/SUNWappserver7 23
/usr/j2se 95
/var/opt/SUNWappserver7/domains 23
A
Admin Console. See Administration interface.
Admin Server, not started 97
administration client 18, 33
Administration interface 17
connection refused 97
starting/stopping 84
administration server port 27, 42
administration tools overview 17
Always-On Technology 20
Apache Web Server 29, 42, 45, 47
AS_ADMIN_PASSWORD 72
asadmin commands 68, 83
asadmin delete-instance 80
asadmin delete-jdbc-connection-pool 80
C
cheatsheet for installation 101
cladmin clear-session-store 80
cladmin command 65
cladmin delete-jdbc-resource 80
cleanup after uninstall failure 98
cleanup after uninstallation failure 98
clinstance.conf file 70, 71
clpassword.conf file 71
clresource.conf file 72–77
clsetup command 65–80
cleanup procedures 79
exit codes 78
input files 70
log 78
non-root setup 64
requirements 68
running 77
syntax 77
command-line command 81
command-line interface method 22
communications setup for HADB 55
configuration directory, specifying 41
configuration file (silent mode) 48
modifying 49
variables 49
console option 34
customer support 12
Section
106 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
D
default port numbers 42
default_config_dir 12
dependencies 32
df -k command 25
directories 12
installation 12
instance root 12
documentation 30
directory conventions 12
font conventions 11
general conventions 11
path formats 11
URL formats 11
domains
creating 84
specifying directory 41
starting and stopping 82
E
Enterprise Edition
license 23
overview 20
environment variables HADB settings 63
error messages 94
exit codes for clsetup 78
F
font conventions 11
freeware 59
G
graphical interface method 22
gunzip 37, 45
H
HADB 69
clsetup parameters 72
communications setup 55
configuring shared memory 53
non-root setup 64
refragmentation 29
setting the environment 63
setting up remote access 55
space considerations 29
starting after stopping 86
stopping 85
HADB management client 20, 29, 63
hadbm 44, 68
hadbm delete 80
hadbm deviceinfo 29
hadbm man pages 20
hadbm start 86
hadbm stopdb 85
hardened operating environment 22, 26
high availability 53–65
commands 18
host communications setup 55
overview 20
requirements 28
HTTP reverse proxy plug-in 21
HTTP server
port 27, 42
I
incremental installation 21, 32, 51
inetd 86
inetd configuration 69
install_config_dir 12
install_dir 12
installation 31–52
cheatsheet 101
incremental 32
J2SE 40
logs 93
methods 21, 35
Section
Index 107
post-install tasks 81
roadmap 16
silent mode configuration file 48
installation root directories 12
instances
creating 84
root directories 12
starting and stopping 83
J
J2SE 40
third-party 18, 41
troubleshooting upgrade 94
Java Messaging Service (JMS) overview 19
JAVA_HOME setting 64
JDBC connection pool 69
JDBC connection pool parameters 74
JMS service startup failure 26
L
licensing 23
limitations on clsetup 68
load balancer plug-in 21, 29, 37
installation 44–47
log files 93
logs
clsetup 78
troubleshooting 94
M
man pages 20, 30
Message Queue broker issue 26
methods of installation 21, 35
N
non-root setup 64, 69
O
OpenSSH 55, 58
options for the setup command 34
P
package-based model 23, 95
parameter-driven installation 48
partial installation 21, 32, 51
password, forgotten 96
patches 23, 26
PATH 44
path formats 11
PATH setting for HADB_ROOT 63
pkginfo 44, 93
platforms, supported 24
plug-ins
HTTP reverse proxy 21
load balancer 21
port number, forgotten 96
ports 42
administration server 27, 42, 96
HTTP server 27, 42
inaccessible 97
requirements 27
ports in use 37
post-installation tasks 81–86
privileges, root 27
prtconf command 25
R
refragmentation of HADB 29
Section
108 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
remote access for HADB 55
Remote Shell (RSH) setup 55
requirements
clsetup 68
high availability 28
ports 27
space 24
system 24
technical 27
roadmap for installation 16
root directories
instance 12
root privileges 27
RSH 55
S
sample applications overview 19
savestate option 35
Secured Shell (SSH) setup 55
semaphores 53
server, starting and stopping 81
session persistence parameters 75
session store parameters 73
set shmsys 54
setenv JAVA_HOME 64
setenv PATH 63
setup command usage 34
setupSDKNative 98
shared memory, configuring for HADB 53
showrev 93
silent installation 22, 48–52
silent mode 90
silent option 35
Solaris 8 56, 58
Solaris 9 26, 56
Solaris J2SE packages 94
space for the HADB 29
space requirements 24
SSH 55
ssh-keygen 62
start-appserv 82
start-domain 82
starting a domain 82
starting an instance 83
starting the server 43, 52, 81
starting/stopping the Administration interface 84
start-instance 83
statefile 35
stop-appserv 82
stop-domain 82
stop-instance 83
stopping a domain 82
stopping an instance 83
stopping the server 81
stopstate file 86
summary of installation tasks 101
Sun ONE Message Queue 27, 39
on Solaris 26
overview 19
Sun ONE Web Server 29, 42, 44
sunfreeware 59
SUNWlibC 27
supported platforms 24
syntax for the setup command 34
system requirements 24
system resources, increasing 97
T
tar 37, 45
tasks summary 101
technical requirements 27
third-party J2SE 18, 41
top command 25
topology requirements for high-availability 28
troubleshooting 93–98
J2SE upgrade 94
logs 93
Sun ONE Message Queue broker 26
Section
Index 109
U
ulimit 97
uname command 25
uninstallation
logs 93
requirements 88
troubleshooting 94
uninstallation failure cleanup 98
uninstalling 87–91
URL formats 11
user name, forgotten 96
W
warning messages 94
web server
requirements for high availability 29, 37, 44
specifying instance path 41
web services client 85
Section
110 Sun ONE Application Server Version 7, Enterprise Edition Installation Guide • September 2003
www.it-ebooks.info
Apache Solr 4
Cookbook
Over 100 recipes to make Apache Solr faster,
more reliable, and return better results
Rafal Kuc
BIRMINGHAM - MUMBAI
www.it-ebooks.info
Apache Solr 4 Cookbook
Copyright © 2013 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system,
or transmitted in any form or by any means, without the prior written permission of the
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the
information presented. However, the information contained in this book is sold without
warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers
and distributors will be held liable for any damages caused or alleged to be caused directly
or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies
and products mentioned in this book by the appropriate use of capitals. However, Packt
Publishing cannot guarantee the accuracy of this information.
First published: July 2011
Second edition: January 2013
Production Reference: 1150113
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B3 2PB, UK.
ISBN 978-1-78216-132-5
www.packtpub.com
Cover Image by J. Blaminsky (milak6@wp.pl)
www.it-ebooks.info
Credits
Author
Rafal Kuc
Reviewers
Ravindra Bharathi
Marcelo Ochoa
Vijayakumar Ramdoss
Acquisition Editor
Andrew Duckworth
Lead Technical Editor
Arun Nadar
Technical Editors
Jalasha D'costa
Charmaine Pereira
Lubna Shaikh
Project Coordinator
Anurag Banerjee
Proofreaders
Maria Gould
Aaron Nash
Indexer
Tejal Soni
Production Coordinators
Manu Joseph
Nitesh Thakur
Cover Work
Nitesh Thakur
www.it-ebooks.info
About the Author
Rafal Kuc is a born team leader and software developer. Currently working as a Consultant
and a Software Engineer at Sematext Inc, where he concentrates on open source technologies
such as Apache Lucene and Solr, ElasticSearch, and Hadoop stack. He has more than
10 years of experience in various software branches, from banking software to e-commerce
products. He is mainly focused on Java, but open to every tool and programming language
that will make the achievement of his goal easier and faster. Rafal is also one of the founders
of the solr.pl site, where he tries to share his knowledge and help people with their
problems with Solr and Lucene. He is also a speaker for various conferences around the
world such as Lucene Eurocon, Berlin Buzzwords, and ApacheCon.
Rafal began his journey with Lucene in 2002 and it wasn't love at first sight. When he
came back to Lucene later in 2003, he revised his thoughts about the framework and saw
the potential in search technologies. Then Solr came and that was it. From then on, Rafal
has concentrated on search technologies and data analysis. Right now Lucene, Solr, and
ElasticSearch are his main points of interest.
www.it-ebooks.info
Acknowledgement
This book is an update to the first cookbook for Solr that was released almost two year ago
now. What was at the beginning an update turned out to be a rewrite of almost all the recipes
in the book, because we wanted to not only bring you an update to the already existing
recipes, but also give you whole new recipes that will help you with common situations
when using Apache Solr 4.0. I hope that the book you are holding in your hands (or reading
on a computer or reader screen) will be useful to you.
Although I would go the same way if I could get back in time, the time of writing this book
was not easy for my family. Among the ones who suffered the most were my wife Agnes
and our two great kids, our son Philip and daughter Susanna. Without their patience and
understanding, the writing of this book wouldn't have been possible. I would also like to
thank my parents and Agnes' parents for their support and help.
I would like to thank all the people involved in creating, developing, and maintaining Lucene
and Solr projects for their work and passion. Without them this book wouldn't have been written.
Once again, thank you.
www.it-ebooks.info
About the Reviewers
Ravindra Bharathi has worked in the software industry for over a decade in
various domains such as education, digital media marketing/advertising, enterprise
search, and energy management systems. He has a keen interest in search-based
applications that involve data visualization, mashups, and dashboards. He blogs at
http://ravindrabharathi.blogspot.com.
Marcelo Ochoa works at the System Laboratory of Facultad de Ciencias Exactas of the
Universidad Nacional del Centro de la Provincia de Buenos Aires, and is the CTO at Scotas.
com, a company specialized in near real time search solutions using Apache Solr and Oracle.
He divides his time between University jobs and external projects related to Oracle, and big
data technologies. He has worked in several Oracle related projects such as translation of
Oracle manuals and multimedia CBTs. His background is in database, network, web, and
Java technologies. In the XML world, he is known as the developer of the DB Generator for
the Apache Cocoon project, the open source projects DBPrism and DBPrism CMS, the
Lucene-Oracle integration by using Oracle JVM Directory implementation, and the Restlet.org
project – the Oracle XDB Restlet Adapter, an alternative to writing native REST web services
inside the database resident JVM.
Since 2006, he has been a part of the Oracle ACE program. Oracle ACEs are known for
their strong credentials as Oracle community enthusiasts and advocates, with candidates
nominated by ACEs in the Oracle Technology and Applications communities.
He is the author of Chapter 17 of the book Oracle Database Programming using Java and
Web Services, Kuassi Mensah, Digital Press and Chapter 21 of the book Professional XML
Databases, Kevin Williams, Wrox Press.
www.it-ebooks.info
www.PacktPub.com
Support files, eBooks, discount offers and more
You might want to visit www.PacktPub.com for support files and downloads related to
your book.
Did you know that Packt offers eBook versions of every book published, with PDF and ePub
files available? You can upgrade to the eBook version at www.PacktPub.com and as a print
book customer, you are entitled to a discount on the eBook copy. Get in touch with us at
service@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up
for a range of free newsletters and receive exclusive discounts and offers on Packt books
and eBooks.
http://PacktLib.PacktPub.com
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book
library. Here, you can access, read and search across Packt's entire library of books.
Why Subscribe?
ff Fully searchable across every book published by Packt
ff Copy and paste, print and bookmark content
ff On demand and accessible via web browser
Free Access for Packt account holders
If you have an account with Packt at www.PacktPub.com, you can use this to access
PacktLib today and view nine entirely free books. Simply use your login credentials for
immediate access.
www.it-ebooks.info
www.it-ebooks.info
Table of Contents
Preface 1
Chapter 1: Apache Solr Configuration 5
Introduction 5
Running Solr on Jetty 6
Running Solr on Apache Tomcat 10
Installing a standalone ZooKeeper 14
Clustering your data 15
Choosing the right directory implementation 17
Configuring spellchecker to not use its own index 19
Solr cache configuration 22
How to fetch and index web pages 27
How to set up the extracting request handler 30
Changing the default similarity implementation 32
Chapter 2: Indexing Your Data 35
Introduction 35
Indexing PDF files 36
Generating unique fields automatically 38
Extracting metadata from binary files 40
How to properly configure Data Import Handler with JDBC 42
Indexing data from a database using Data Import Handler 45
How to import data using Data Import Handler and delta query 48
How to use Data Import Handler with the URL data source 50
How to modify data while importing with Data Import Handler 53
Updating a single field of your document 56
Handling multiple currencies 59
Detecting the document's language 62
Optimizing your primary key field indexing 67
www.it-ebooks.info
ii
Table of Contents
Chapter 3: Analyzing Your Text Data 69
Introduction 70
Storing additional information using payloads 70
Eliminating XML and HTML tags from text 73
Copying the contents of one field to another 75
Changing words to other words 77
Splitting text by CamelCase 80
Splitting text by whitespace only 82
Making plural words singular without stemming 84
Lowercasing the whole string 87
Storing geographical points in the index 88
Stemming your data 91
Preparing text to perform an efficient trailing wildcard search 93
Splitting text by numbers and non-whitespace characters 96
Using Hunspell as a stemmer 99
Using your own stemming dictionary 101
Protecting words from being stemmed 103
Chapter 4: Querying Solr 107
Introduction 108
Asking for a particular field value 108
Sorting results by a field value 109
How to search for a phrase, not a single word 111
Boosting phrases over words 114
Positioning some documents over others in a query 117
Positioning documents with words closer to each other first 122
Sorting results by the distance from a point 125
Getting documents with only a partial match 128
Affecting scoring with functions 130
Nesting queries 134
Modifying returned documents 136
Using parent-child relationships 139
Ignoring typos in terms of performance 142
Detecting and omitting duplicate documents 145
Using field aliases 148
Returning a value of a function in the results 151
Chapter 5: Using the Faceting Mechanism 155
Introduction 155
Getting the number of documents with the same field value 156
Getting the number of documents with the same value range 158
www.it-ebooks.info
iii
Table of Contents
Getting the number of documents matching the query and subquery 161
Removing filters from faceting results 164
Sorting faceting results in alphabetical order 168
Implementing the autosuggest feature using faceting 171
Getting the number of documents that don't have a value in the field 174
Having two different facet limits for two different fields in the same query 177
Using decision tree faceting 180
Calculating faceting for relevant documents in groups 183
Chapter 6: Improving Solr Performance 187
Introduction 187
Paging your results quickly 188
Configuring the document cache 189
Configuring the query result cache 190
Configuring the filter cache 192
Improving Solr performance right after the startup or commit operation 194
Caching whole result pages 197
Improving faceting performance for low cardinality fields 198
What to do when Solr slows down during indexing 200
Analyzing query performance 202
Avoiding filter caching 206
Controlling the order of execution of filter queries 207
Improving the performance of numerical range queries 208
Chapter 7: In the Cloud 211
Introduction 211
Creating a new SolrCloud cluster 211
Setting up two collections inside a single cluster 214
Managing your SolrCloud cluster 216
Understanding the SolrCloud cluster administration GUI 220
Distributed indexing and searching 223
Increasing the number of replicas on an already live cluster 227
Stopping automatic document distribution among shards 230
Chapter 8: Using Additional Solr Functionalities 235
Introduction 235
Getting more documents similar to those returned in the results list 236
Highlighting matched words 238
How to highlight long text fields and get good performance 241
Sorting results by a function value 243
Searching words by how they sound 246
Ignoring defined words 248
www.it-ebooks.info
iv
Table of Contents
Computing statistics for the search results 250
Checking the user's spelling mistakes 253
Using field values to group results 257
Using queries to group results 260
Using function queries to group results 262
Chapter 9: Dealing with Problems 265
Introduction 265
How to deal with too many opened files 265
How to deal with out-of-memory problems 267
How to sort non-English languages properly 268
How to make your index smaller 272
Diagnosing Solr problems 274
How to avoid swapping 280
Appendix: Real-life Situations 283
Introduction 283
How to implement a product's autocomplete functionality 284
How to implement a category's autocomplete functionality 287
How to use different query parsers in a single query 290
How to get documents right after they were sent for indexation 292
How to search your data in a near real-time manner 294
How to get the documents with all the query words to the top
of the results set 296
How to boost documents based on their publishing date 300
Index 305
www.it-ebooks.info
Preface
Welcome to the Solr Cookbook for Apache Solr 4.0. You will be taken on a tour through the
most common problems when dealing with Apache Solr. You will learn how to deal with the
problems in Solr configuration and setup, how to handle common querying problems, how
to fine-tune Solr instances, how to set up and use SolrCloud, how to use faceting and
grouping, fight common problems, and many more things. Every recipe is based on
real-life problems, and each recipe includes solutions along with detailed descriptions
of the configuration and code that was used.
What this book covers
Chapter 1, Apache Solr Configuration, covers Solr configuration recipes, different servlet
container usage with Solr, and setting up Apache ZooKeeper and Apache Nutch.
Chapter 2, Indexing Your Data, explains data indexing such as binary file indexing, using Data
Import Handler, language detection, updating a single field of document, and much more.
Chapter 3, Analyzing Your Text Data, concentrates on common problems when analyzing your
data such as stemming, geographical location indexing, or using synonyms.
Chapter 4, Querying Solr, describes querying Apache Solr such as nesting queries, affecting
scoring of documents, phrase search, or using the parent-child relationship.
Chapter 5, Using the Faceting Mechanism, is dedicated to the faceting mechanism in
which you can find the information needed to overcome some of the situations that you can
encounter during your work with Solr and faceting.
Chapter 6, Improving Solr Performance, is dedicated to improving your Apache Solr cluster
performance with information such as cache configuration, indexing speed up, and much more.
Chapter 7, In the Cloud, covers the new feature in Solr 4.0, the SolrCloud, and the setting up
of collections, replica configuration, distributed indexing and searching, and understanding
Solr administration.
www.it-ebooks.info
Preface
2
Chapter 8, Using Additional Solr Functionalities, explains documents highlighting, sorting
results on the basis of function value, checking user spelling mistakes, and using the
grouping functionality.
Chapter 9, Dealing with Problems, is a small chapter dedicated to the most common
situations such as memory problems, reducing your index size, and similar issues.
Appendix, Real Life Situations, describes how to handle real-life situations such as
implementing different autocomplete functionalities, using near real-time search,
or improving query relevance.
What you need for this book
In order to be able to run most of the examples in the book, you will need the Java Runtime
Environment 1.6 or newer, and of course the 4.0 version of the Apache Solr search server.
A few chapters in this book require additional software such as Apache ZooKeeper 3.4.3,
Apache Nutch 1.5.1, Apache Tomcat, or Jetty.
Who this book is for
This book is for users working with Apache Solr or developers that use Apache Solr to build
their own software that would like to know how to combat common problems. Knowledge of
Apache Lucene would be a bonus, but is not required.
Conventions
In this book, you will find a number of styles of text that distinguish between different kinds of
information. Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "The lib entry in the solrconfig.xml file tells
Solr to look for all the JAR files from the ../../langid directory".
A block of code is set as follows:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text_general" indexed="true" stored="true"/>
<field name="description" type="text_general" indexed="true"
stored="true" />
<field name="langId" type="string" indexed="true" stored="true" />
www.it-ebooks.info
Preface
3
When we wish to draw your attention to a particular part of a code block, the relevant lines
or items are set in bold:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
Any command-line input or output is written as follows:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","file":{"set":"New file name"}}]'
New terms and important words are shown in bold. Words that you see on the screen, in
menus or dialog boxes for example, appear in the text like this: "clicking the Next button
moves you to the next screen".
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this
book—what you liked or may have disliked. Reader feedback is important for us to develop
titles that you really get the most out of.
To send us general feedback, simply send an e-mail to feedback@packtpub.com,
and mention the book title through the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing or
contributing to a book, see our author guide on www.packtpub.com/authors.
www.it-ebooks.info
Preface
4
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you
to get the most from your purchase.
Downloading the example code
You can download the example code files for all Packt books you have purchased from
your account at http://www.packtpub.com. If you purchased this book elsewhere,
you can visit http://www.packtpub.com/support and register to have the files
e-mailed directly to you.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be
grateful if you would report this to us. By doing so, you can save other readers from frustration
and help us improve subsequent versions of this book. If you find any errata, please report them
by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata
submission form link, and entering the details of your errata. Once your errata are verified, your
submission will be accepted and the errata will be uploaded to our website, or added to any list
of existing errata, under the Errata section of that title.
Piracy
Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt,
we take the protection of our copyright and licenses very seriously. If you come across any
illegal copies of our works, in any form, on the Internet, please provide us with the location
address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Questions
You can contact us at questions@packtpub.com if you are having a problem with any aspect
of the book, and we will do our best to address it.
www.it-ebooks.info
1
Apache Solr
Configuration
In this chapter we will cover:
ff Running Solr on Jetty
ff Running Solr on Apache Tomcat
ff Installing a standalone ZooKeeper
ff Clustering your data
ff Choosing the right directory implementation
ff Configuring spellchecker to not use its own index
ff Solr cache configuration
ff How to fetch and index web pages
ff How to set up the extracting request handler
ff Changing the default similarity implementation
Introduction
Setting up an example Solr instance is not a hard task, at least when setting up the simplest
configuration. The simplest way is to run the example provided with the Solr distribution, that
shows how to use the embedded Jetty servlet container.
If you don't have any experience with Apache Solr, please refer to the Apache Solr tutorial
which can be found at: http://lucene.apache.org/solr/tutorial.html before
reading this book.
www.it-ebooks.info
Apache Solr Configuration
6
During the writing of this chapter, I used Solr version 4.0 and Jetty
version 8.1.5, and those versions are covered in the tips of the following
chapter. If another version of Solr is mandatory for a feature to run, then
it will be mentioned.
We have a simple configuration, simple index structure described by the schema.xml file,
and we can run indexing.
In this chapter you'll see how to configure and use the more advanced Solr modules; you'll
see how to run Solr in different containers and how to prepare your configuration to different
requirements. You will also learn how to set up a new SolrCloud cluster and migrate your
current configuration to the one supporting all the features of SolrCloud. Finally, you will
learn how to configure Solr cache to meet your needs and how to pre-sort your Solr indexes
to be able to use early query termination techniques efficiently.
Running Solr on Jetty
The simplest way to run Apache Solr on a Jetty servlet container is to run the provided
example configuration based on embedded Jetty. But it's not the case here. In this recipe,
I would like to show you how to configure and run Solr on a standalone Jetty container.
Getting ready
First of all you need to download the Jetty servlet container for your platform. You can get your
download package from an automatic installer (such as, apt-get), or you can download it
yourself from http://jetty.codehaus.org/jetty/.
How to do it...
The first thing is to install the Jetty servlet container, which is beyond the scope of this book,
so we will assume that you have Jetty installed in the /usr/share/jetty directory or you
copied the Jetty files to that directory.
Let's start by copying the solr.war file to the webapps directory of the Jetty installation
(so the whole path would be /usr/share/jetty/webapps). In addition to that we need
to create a temporary directory in Jetty installation, so let's create the temp directory in the
Jetty installation directory.
Next we need to copy and adjust the solr.xml file from the context directory of the Solr
example distribution to the context directory of the Jetty installation. The final file contents
should look like the following code:
www.it-ebooks.info
Chapter 1
7
<?xml version="1.0"?>
<!DOCTYPE Configure PUBLIC "-//Jetty//Configure//EN" "http://www.
eclipse.org/jetty/configure.dtd">
<Configure class="org.eclipse.jetty.webapp.WebAppContext">
<Set name="contextPath">/solr</Set>
<Set name="war"><SystemProperty name="jetty.home"/>/webapps/solr.
war</Set>
<Set name="defaultsDescriptor"><SystemProperty name="jetty.home"/>/
etc/webdefault.xml</Set>
<Set name="tempDirectory"><Property name="jetty.home" default="."/>/
temp</Set>
</Configure>
Downloading the example code
You can download the example code files for all Packt books you
have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit
http://www.packtpub.com/support and register to have the
files e-mailed directly to you.
Now we need to copy the jetty.xml, webdefault.xml, and logging.properties files
from the etc directory of the Solr distribution to the configuration directory of Jetty, so in our
case to the /usr/share/jetty/etc directory.
The next step is to copy the Solr configuration files to the appropriate directory. I'm talking
about files such as schema.xml, solrconfig.xml, solr.xml, and so on. Those files
should be in the directory specified by the solr.solr.home system variable (in my case
this was the /usr/share/solr directory). Please remember to preserve the directory
structure you'll see in the example deployment, so for example, the /usr/share/solr
directory should contain the solr.xml (and in addition zoo.cfg in case you want to
use SolrCloud) file with the contents like so:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
All the other configuration files should go to the /usr/share/solr/collection1/conf
directory (place the schema.xml and solrconfig.xml files there along with any additional
configuration files your deployment needs). Your cores may have other names than the default
collection1, so please be aware of that.
www.it-ebooks.info
Apache Solr Configuration
8
The last thing about the configuration is to update the /etc/default/jetty file and
add –Dsolr.solr.home=/usr/share/solr to the JAVA_OPTIONS variable of that
file. The whole line with that variable could look like the following:
JAVA_OPTIONS="-Xmx256m -Djava.awt.headless=true -Dsolr.solr.home=/usr/
share/solr/"
If you didn't install Jetty with apt-get or a similar software, you may not have the /etc/
default/jetty file. In that case, add the –Dsolr.solr.home=/usr/share/solr
parameter to the Jetty startup.
We can now run Jetty to see if everything is ok. To start Jetty, that was installed, for example,
using the apt-get command, use the following command:
/etc/init.d/jetty start
You can also run Jetty with a java command. Run the following command in the Jetty
installation directory:
java –Dsolr.solr.home=/usr/share/solr –jar start.jar
If there were no exceptions during the startup, we have a running Jetty with Solr deployed
and configured. To check if Solr is running, try going to the following address with your web
browser: http://localhost:8983/solr/.
You should see the Solr front page with cores, or a single core, mentioned. Congratulations!
You just successfully installed, configured, and ran the Jetty servlet container with Solr deployed.
How it works...
For the purpose of this recipe, I assumed that we needed a single core installation with only
schema.xml and solrconfig.xml configuration files. Multicore installation is very similar
– it differs only in terms of the Solr configuration files.
The first thing we did was copy the solr.war file and create the temp directory. The WAR
file is the actual Solr web application. The temp directory will be used by Jetty to unpack
the WAR file.
The solr.xml file we placed in the context directory enables Jetty to define the context
for the Solr web application. As you can see in its contents, we set the context to be /solr,
so our Solr application will be available under http://localhost:8983/solr/. We
also specified where Jetty should look for the WAR file (the war property), where the web
application descriptor file (the defaultsDescriptor property) is, and finally where the
temporary directory will be located (the tempDirectory property).
www.it-ebooks.info
Chapter 1
9
The next step is to provide configuration files for the Solr web application. Those files should
be in the directory specified by the system solr.solr.home variable. I decided to use the
/usr/share/solr directory to ensure that I'll be able to update Jetty without the need of
overriding or deleting the Solr configuration files. When copying the Solr configuration files,
you should remember to include all the files and the exact directory structure that Solr needs.
So in the directory specified by the solr.solr.home variable, the solr.xml file should be
available – the one that describes the cores of your system.
The solr.xml file is pretty simple – there should be the root element called solr. Inside it
there should be a cores tag (with the adminPath variable set to the address where Solr's
cores administration API is available and the defaultCoreName attribute that says which
is the default core). The cores tag is a parent for cores definition – each core should have
its own cores tag with name attribute specifying the core name and the instanceDir
attribute specifying the directory where the core specific files will be available (such as
the conf directory).
If you installed Jetty with the apt-get command or similar, you will need to update
the /etc/default/jetty file to include the solr.solr.home variable for Solr
to be able to see its configuration directory.
After all those steps we are ready to launch Jetty. If you installed Jetty with apt-get
or a similar software, you can run Jetty with the first command shown in the example.
Otherwise you can run Jetty with a java command from the Jetty installation directory.
After running the example query in your web browser you should see the Solr front page
as a single core. Congratulations! You just successfully configured and ran the Jetty servlet
container with Solr deployed.
There's more...
There are a few tasks you can do to counter some problems when running Solr within the Jetty
servlet container. Here are the most common ones that I encountered during my work.
I want Jetty to run on a different port
Sometimes it's necessary to run Jetty on a different port other than the default one. We have
two ways to achieve that:
ff Adding an additional startup parameter, jetty.port. The startup command would
look like the following command:
java –Djetty.port=9999 –jar start.jar
www.it-ebooks.info
Apache Solr Configuration
10
ff Changing the jetty.xml file – to do that you need to change the following line:
<Set name="port"><SystemProperty name="jetty.port"
default="8983"/></Set>
To:
<Set name="port"><SystemProperty name="jetty.port"
default="9999"/></Set>
Buffer size is too small
Buffer overflow is a common problem when our queries are getting too long and too complex,
– for example, when we use many logical operators or long phrases. When the standard head
buffer is not enough you can resize it to meet your needs. To do that, you add the following
line to the Jetty connector in thejetty.xml file. Of course the value shown in the example
can be changed to the one that you need:
<Set name="headerBufferSize">32768</Set>
After adding the value, the connector definition should look more or less like the
following snippet:
<Call name="addConnector">
<Arg>
<New class="org.mortbay.jetty.bio.SocketConnector">
<Set name="port"><SystemProperty name="jetty.port" default="8080"/></
Set>
<Set name="maxIdleTime">50000</Set>
<Set name="lowResourceMaxIdleTime">1500</Set>
<Set name="headerBufferSize">32768</Set>
</New>
</Arg>
</Call>
Running Solr on Apache Tomcat
Sometimes you need to choose a servlet container other than Jetty. Maybe because your
client has other applications running on another servlet container, maybe because you just
don't like Jetty. Whatever your requirements are that put Jetty out of the scope of your interest,
the first thing that comes to mind is a popular and powerful servlet container – Apache
Tomcat. This recipe will give you an idea of how to properly set up and run Solr
in the Apache Tomcat environment.
www.it-ebooks.info
Chapter 1
11
Getting ready
First of all we need an Apache Tomcat servlet container. It can be found at the Apache Tomcat
website – http://tomcat.apache.org. I concentrated on the Tomcat Version 7.x because
at the time of writing of this book it was mature and stable. The version that I used during the
writing of this recipe was Apache Tomcat 7.0.29, which was the newest one at the time.
How to do it...
To run Solr on Apache Tomcat we need to follow these simple steps:
1. Firstly, you need to install Apache Tomcat. The Tomcat installation is beyond the
scope of this book so we will assume that you have already installed this servlet
container in the directory specified by the $TOMCAT_HOME system variable.
2. The second step is preparing the Apache Tomcat configuration files. To do that we
need to add the following inscription to the connector definition in the server.xml
configuration file:
URIEncoding="UTF-8"
The portion of the modified server.xml file should look like the following
code snippet:
<Connector port="8080" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
3. The third step is to create a proper context file. To do that, create a solr.xml file
in the $TOMCAT_HOME/conf/Catalina/localhost directory. The contents of
the file should look like the following code:
<Context path="/solr" docBase="/usr/share/tomcat/webapps/solr.war"
debug="0" crossContext="true">
<Environment name="solr/home" type="java.lang.String" value="/
usr/share/solr/" override="true"/>
</Context>
4. The next thing is the Solr deployment. To do that we need the apache-solr-
4.0.0.war file that contains the necessary files and libraries to run Solr that
is to be copied to the Tomcat webapps directory and renamed solr.war.
5. The one last thing we need to do is add the Solr configuration files. The files that you
need to copy are files such as schema.xml, solrconfig.xml, and so on. Those
files should be placed in the directory specified by the solr/home variable (in our
case /usr/share/solr/). Please don't forget that you need to ensure the proper
directory structure. If you are not familiar with the Solr directory structure please take
a look at the example deployment that is provided with the standard Solr package.
www.it-ebooks.info
Apache Solr Configuration
12
6. Please remember to preserve the directory structure you'll see in the example
deployment, so for example, the /usr/share/solr directory should contain
the solr.xml (and in addition zoo.cfg in case you want to use SolrCloud)
file with the contents like so:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
7. All the other configuration files should go to the /usr/share/solr/collection1/
conf directory (place the schema.xml and solrconfig.xml files there along with
any additional configuration files your deployment needs). Your cores may have other
names than the default collection1, so please be aware of that.
8. Now we can start the servlet container, by running the following command:
bin/catalina.sh start
9. In the log file you should see a message like this:
Info: Server startup in 3097 ms
10. To ensure that Solr is running properly, you can run a browser and point it to an
address where Solr should be visible, like the following:
http://localhost:8080/solr/
If you see the page with links to administration pages of each of the cores defined, that
means that your Solr is up and running.
How it works...
Let's start from the second step as the installation part is beyond the scope of this book.
As you probably know, Solr uses UTF-8 file encoding. That means that we need to ensure
that Apache Tomcat will be informed that all requests and responses made should use that
encoding. To do that, we modified the server.xml file in the way shown in the example.
The Catalina context file (called solr.xml in our example) says that our Solr application
will be available under the /solr context (the path attribute). We also specified the WAR
file location (the docBase attribute). We also said that we are not using debug (the debug
attribute), and we allowed Solr to access other context manipulation methods. The last thing
is to specify the directory where Solr should look for the configuration files. We do that by
adding the solr/home environment variable with the value attribute set to the path to
the directory where we have put the configuration files.
www.it-ebooks.info
Chapter 1
13
The solr.xml file is pretty simple – there should be the root element called solr. Inside
it there should be the cores tag (with the adminPath variable set to the address where
the Solr cores administration API is available and the defaultCoreName attribute describing
which is the default core). The cores tag is a parent for cores definition – each core should
have its own core tag with a name attribute specifying the core name and the instanceDir
attribute specifying the directory where the core-specific files will be available (such as the
conf directory).
The shell command that is shown starts Apache Tomcat. There are some other options of the
catalina.sh (or catalina.bat) script; the descriptions of these options are as follows:
ff stop: This stops Apache Tomcat
ff restart: This restarts Apache Tomcat
ff debug: This start Apache Tomcat in debug mode
ff run: This runs Apache Tomcat in the current window, so you can see the output on
the console from which you run Tomcat.
After running the example address in the web browser, you should see a Solr front page with
a core (or cores if you have a multicore deployment). Congratulations! You just successfully
configured and ran the Apache Tomcat servlet container with Solr deployed.
There's more...
There are some other tasks that are common problems when running Solr on Apache Tomcat.
Changing the port on which we see Solr running on Tomcat
Sometimes it is necessary to run Apache Tomcat on a different port other than 8080, which is
the default one. To do that, you need to modify the port variable of the connector definition
in the server.xml file located in the $TOMCAT_HOME/conf directory. If you would like your
Tomcat to run on port 9999, this definition should look like the following code snippet:
<Connector port="9999" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
While the original definition looks like the following snippet:
<Connector port="8080" protocol="HTTP/1.1"
connectionTimeout="20000"
redirectPort="8443"
URIEncoding="UTF-8" />
www.it-ebooks.info
Apache Solr Configuration
14
Installing a standalone ZooKeeper
You may know that in order to run SolrCloud—the distributed Solr installation—you need to have
Apache ZooKeeper installed. Zookeeper is a centralized service for maintaining configurations,
naming, and provisioning service synchronization. SolrCloud uses ZooKeeper to synchronize
configuration and cluster states (such as elected shard leaders), and that's why it is crucial to
have a highly available and fault tolerant ZooKeeper installation. If you have a single ZooKeeper
instance and it fails then your SolrCloud cluster will crash too. So, this recipe will show you how
to install ZooKeeper so that it's not a single point of failure in your cluster configuration.
Getting ready
The installation instruction in this recipe contains information about installing ZooKeeper
Version 3.4.3, but it should be useable for any minor release changes of Apache ZooKeeper.
To download ZooKeeper please go to http://zookeeper.apache.org/releases.html.
This recipe will show you how to install ZooKeeper in a Linux-based environment. You also
need Java installed.
How to do it...
Let's assume that we decided to install ZooKeeper in the /usr/share/zookeeper
directory of our server and we want to have three servers (with IP addresses 192.168.1.1,
192.168.1.2, and 192.168.1.3) hosting the distributed ZooKeeper installation.
1. After downloading the ZooKeeper installation, we create the necessary directory:
sudo mkdir /usr/share/zookeeper
2. Then we unpack the downloaded archive to the newly created directory. We do that
on three servers.
3. Next we need to change our ZooKeeper configuration file and specify the servers that
will form the ZooKeeper quorum, so we edit the /usr/share/zookeeper/conf/
zoo.cfg file and we add the following entries:
clientPort=2181
dataDir=/usr/share/zookeeper/data
tickTime=2000
initLimit=10
syncLimit=5
server.1=192.168.1.1:2888:3888
server.2=192.168.1.2:2888:3888
server.3=192.168.1.3:2888:3888
www.it-ebooks.info
Chapter 1
15
4. And now, we can start the ZooKeeper servers with the following command:
/usr/share/zookeeper/bin/zkServer.sh start
5. If everything went well you should see something like the following:
JMX enabled by default
Using config: /usr/share/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
And that's all. Of course you can also add the ZooKeeper service to start automatically during
your operating system startup, but that's beyond the scope of the recipe and the book itself.
How it works...
Let's skip the first part, because creating the directory and unpacking the ZooKeeper server
there is quite simple. What I would like to concentrate on are the configuration values of the
ZooKeeper server. The clientPort property specifies the port on which our SolrCloud servers
should connect to ZooKeeper. The dataDir property specifies the directory where ZooKeeper
will hold its data. So far, so good right ? So now, the more advanced properties; the tickTime
property specified in milliseconds is the basic time unit for ZooKeeper. The initLimit
property specifies how many ticks the initial synchronization phase can take. Finally, the
syncLimit property specifies how many ticks can pass between sending the request and
receiving an acknowledgement.
There are also three additional properties present, server.1, server.2, and server.3.
These three properties define the addresses of the ZooKeeper instances that will form the
quorum. However, there are three values separated by a colon character. The first part is the
IP address of the ZooKeeper server, and the second and third parts are the ports used by
ZooKeeper instances to communicate with each other.
Clustering your data
After the release of Apache Solr 4.0, many users will want to leverage SolrCloud distributed
indexing and querying capabilities. It's not hard to upgrade your current cluster to SolrCloud,
but there are some things you need to take care of. With the help of the following recipe you
will be able to easily upgrade your cluster.
Getting ready
Before continuing further it is advised to read the Installing a standalone ZooKeeper
recipe in this chapter. It shows how to set up a ZooKeeper cluster in order to be ready
for production use.
www.it-ebooks.info
Apache Solr Configuration
16
How to do it...
In order to use your old index structure with SolrCloud, you will need to add the following
field to your fields definition (add the following fragment to the schema.xml file, to its
fields section):
<field name="_version_" type="long" indexed="true" stored="true"
multiValued="false"/>
Now let's switch to the solrconfig.xml file – starting with the replication handlers. First,
you need to ensure that you have the replication handler set up. Remember that you shouldn't
add master or slave specific configurations to it. So the replication handlers' configuration
should look like the following code:
<requestHandler name="/replication" class="solr.ReplicationHandler" />
In addition to that, you will need to have the administration panel handlers present, so the
following configuration entry should be present in your solrconfig.xml file:
<requestHandler name="/admin/" class="solr.admin.AdminHandlers" />
The last request handler that should be present is the real-time get handler, which should
be defined as follows (the following should also be added to the solrconfig.xml file):
<requestHandler name="/get" class="solr.RealTimeGetHandler">
<lst name="defaults">
<str name="omitHeader">true</str>
</lst>
</requestHandler>
The next thing SolrCloud needs in order to properly operate is the transaction log
configuration. The following fragment should be added to the solrconfig.xml file:
<updateLog>
<str name="dir">${solr.data.dir:}</str>
</updateLog>
The last thing is the solr.xml file. It should be pointing to the default cores administration
address – the cores tag should have the adminPath property set to the /admin/cores
value. The example solr.xml file could look like the following code:
<solr persistent="true">
<cores adminPath="/admin/cores" defaultCoreName="collection1"
host="localhost" hostPort="8983" zkClientTimeout="15000">
<core name="collection1" instanceDir="collection1" />
</cores>
</solr>
And that's all, your Solr instances configuration files are now ready to be used with SolrCloud.
www.it-ebooks.info
Chapter 1
17
How it works...
So now let's see why all those changes are needed in order to use our old configuration files
with SolrCloud.
The _version_ field is used by Solr to enable documents versioning and optimistic locking,
which ensures that you won't have the newest version of your document overwritten by
mistake. Because of that, SolrCloud requires the _version_ field to be present in your
index structure. Adding that field is simple – you just need to place another field definition
that is stored and indexed, and based on the long type. That's all.
As for the replication handler, you should remember not to add slave or master specific
configuration, only the simple request handler definition, as shown in the previous example.
The same applies to the administration panel handlers: they need to be available under the
default URL address.
The real-time get handler is responsible for getting the updated documents right away,
even if no commit or the softCommit command is executed. This handler allows Solr
(and also you) to retrieve the latest version of the document without the need for re-opening
the searcher, and thus even if the document is not yet visible during usual search operations.
The configuration is very similar to the usual request handler configuration – you need to
add a new handler with the name property set to /get and the class property set to solr.
RealTimeGetHandler. In addition to that, we want the handler to be omitting response
headers (the omitHeader property set to true).
One of the last things that is needed by SolrCloud is the transaction log, which enables realtime
get operations to be functional. The transaction log keeps track of all the uncommitted
changes and enables a real-time get handler to retrieve those. In order to turn on transaction
log usage, one should add the updateLog tag to the solrconfig.xml file and specify the
directory where the transaction log directory should be created (by adding the dir property as
shown in the example). In the configuration previously shown, we tell Solr that we want to use
the Solr data directory as the place to store the transaction log directory.
Finally, Solr needs you to keep the default address for the core administrative interface, so
you should remember to have the adminPath property set to the value shown in the example
(in the solr.xml file). This is needed in order for Solr to be able to manipulate cores.
Choosing the right directory implementation
One of the most crucial properties of Apache Lucene, and thus Solr, is the Lucene directory
implementation. The directory interface provides an abstraction layer for Lucene on all the
I/O operations. Although choosing the right directory implementation seems simple, it can
affect the performance of your Solr setup in a drastic way. This recipe will show you how to
choose the right directory implementation.
www.it-ebooks.info
Apache Solr Configuration
18
How to do it...
In order to use the desired directory, all you need to do is choose the right directory
factory implementation and inform Solr about it. Let's assume that you would like to use
NRTCachingDirectory as your directory implementation. In order to do that, you need to
place (or replace if it is already present) the following fragment in your solrconfig.xml file:
<directoryFactory name="DirectoryFactory" class="solr.
NRTCachingDirectoryFactory" />
And that's all. The setup is quite simple, but what directory factories are available to use?
When this book was written, the following directory factories were available:
ff solr.StandardDirectoryFactory
ff solr.SimpleFSDirectoryFactory
ff solr.NIOFSDirectoryFactory
ff solr.MMapDirectoryFactory
ff solr.NRTCachingDirectoryFactory
ff solr.RAMDirectoryFactory
So now let's see what each of those factories provide.
How it works...
Before we get into the details of each of the presented directory factories, I would like to
comment on the directory factory configuration parameter. All you need to remember is that
the name attribute of the directoryFactory tag should be set to DirectoryFactory
and the class attribute should be set to the directory factory implementation of your choice.
If you want Solr to make the decision for you, you should use solr.
StandardDirectoryFactory. This is a filesystem-based directory factory that tries
to choose the best implementation based on your current operating system and Java
virtual machine used. If you are implementing a small application, which won't use many
threads, you can use solr.SimpleFSDirectoryFactory which stores the index file
on your local filesystem, but it doesn't scale well with a high number of threads. solr.
NIOFSDirectoryFactory scales well with many threads, but it doesn't work well on
Microsoft Windows platforms (it's much slower), because of the JVM bug, so you should
remember that.
solr.MMapDirectoryFactory was the default directory factory for Solr for the 64-bit Linux
systems from Solr 3.1 till 4.0. This directory implementation uses virtual memory and a kernel
feature called mmap to access index files stored on disk. This allows Lucene (and thus Solr) to
directly access the I/O cache. This is desirable and you should stick to that directory if near
real-time searching is not needed.
www.it-ebooks.info
Chapter 1
19
If you need near real-time indexing and searching, you should use solr.
NRTCachingDirectoryFactory. It is designed to store some parts of the index
in memory (small chunks) and thus speed up some near real-time operations greatly.
The last directory factory, solr.RAMDirectoryFactory, is the only one that is not
persistent. The whole index is stored in the RAM memory and thus you'll lose your index after
restart or server crash. Also you should remember that replication won't work when using
solr.RAMDirectoryFactory. One would ask, why should I use that factory? Imagine a
volatile index for an autocomplete functionality or for unit tests of your queries' relevancy.
Just anything you can think of, when you don't need to have persistent and replicated data.
However, please remember that this directory is not designed to hold large amounts of data.
Configuring spellchecker to not use its own
index
If you are used to the way spellchecker worked in the previous Solr versions, you may
remember that it required its own index to give you spelling corrections. That approach
had some disadvantages, such as the need for rebuilding the index, and replication between
master and slave servers. With the Solr Version 4.0, a new spellchecker implementation was
introduced – solr.DirectSolrSpellchecker. It allowed you to use your main index to
provide spelling suggestions and didn't need to be rebuilt after every commit. So now, let's
see how to use that new spellchecker implementation in Solr.
How to do it...
First of all, let's assume we have a field in the index called title, in which we hold titles
of our documents. What's more, we don't want the spellchecker to have its own index and
we would like to use that title field to provide spelling suggestions. In addition to that, we
would like to decide when we want a spelling suggestion. In order to do that, we need to do
two things:
1. First, we need to edit our solrconfig.xml file and add the spellchecking
component, whose definition may look like the following code:
<searchComponent name="spellcheck" class="solr.
SpellCheckComponent">
<str name="queryAnalyzerFieldType">title</str>
<lst name="spellchecker">
<str name="name">direct</str>
<str name="field">title</str>
<str name="classname">solr.DirectSolrSpellChecker</str>
<str name="distanceMeasure">internal</str>
<float name="accuracy">0.8</float>
<int name="maxEdits">1</int>
www.it-ebooks.info
Apache Solr Configuration
20
<int name="minPrefix">1</int>
<int name="maxInspections">5</int>
<int name="minQueryLength">3</int>
<float name="maxQueryFrequency">0.01</float>
</lst>
</searchComponent>
2. Now we need to add a proper request handler configuration that will use the
previously mentioned search component. To do that, we need to add the following
section to the solrconfig.xml file:
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">title</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck">on</str>
<str name="spellcheck.extendedResults">true</str>
<str name="spellcheck.count">5</str>
<str name="spellcheck.collate">true</str>
<str name="spellcheck.collateExtendedResults">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
3. And that's all. In order to get spelling suggestions, we need to run the following query:
/spell?q=disa
4. In response we will get something like the following code:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">5</int>
</lst>
<result name="response" numFound="0" start="0">
</result>
<lst name="spellcheck">
<lst name="suggestions">
<lst name="disa">
<int name="numFound">1</int>
<int name="startOffset">0</int>
<int name="endOffset">4</int>
<int name="origFreq">0</int>
<arr name="suggestion">
<lst>
<str name="word">data</str>
<int name="freq">1</int>
www.it-ebooks.info
Chapter 1
21
</lst>
</arr>
</lst>
<bool name="correctlySpelled">false</bool>
<lst name="collation">
<str name="collationQuery">data</str>
<int name="hits">1</int>
<lst name="misspellingsAndCorrections">
<str name="disa">data</str>
</lst>
</lst>
</lst>
</lst>
</response>
If you check your data folder you will see that there is not a single directory responsible
for holding the spellchecker index. So, now let's see how that works.
How it works...
Now let's get into some specifics about how the previous configuration works, starting
from the search component configuration. The queryAnalyzerFieldType property
tells Solr which field configuration should be used to analyze the query passed to the
spellchecker. The name property sets the name of the spellchecker which will be used in
the handler configuration later. The field property specifies which field should be used
as the source for the data used to build spelling suggestions. As you probably figured out,
the classname property specifies the implementation class, which in our case is solr.
DirectSolrSpellChecker, enabling us to omit having a separate spellchecker index.
The next parameters visible in the configuration specify how the Solr spellchecker should
behave and that is beyond the scope of this recipe (however, if you would like to read more
about them, please go to the following URL address: http://wiki.apache.org/solr/
SpellCheckComponent).
The last thing is the request handler configuration. Let's concentrate on all the properties
that start with the spellcheck prefix. First we have spellcheck.dictionary, which
in our case specifies the name of the spellchecking component we want to use (please
note that the value of the property matches the value of the name property in the search
component configuration). We tell Solr that we want the spellchecking results to be present
(the spellcheck property with the value set to on), and we also tell Solr that we want to see
the extended results format (spellcheck.extendedResults set to true). In addition to
the mentioned configuration properties, we also said that we want to have a maximum of five
suggestions (the spellcheck.count property), and we want to see the collation and its
extended results (spellcheck.collate and spellcheck.collateExtendedResults
both set to true).
www.it-ebooks.info
Apache Solr Configuration
22
There's more...
Let's see one more thing – the ability to have more than one spellchecker defined in a
request handler.
More than one spellchecker
If you would like to have more than one spellchecker handling your spelling suggestions you
can configure your handler to use multiple search components. For example, if you would like
to use search components (spellchecking ones) named word and better (you have to have
them configured), you could add multiple spellcheck.dictionary parameters to your
request handler. This is how your request handler configuration would look:
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">title</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck.dictionary">word</str>
<str name="spellcheck.dictionary">better</str>
<str name="spellcheck">on</str>
<str name="spellcheck.extendedResults">true</str>
<str name="spellcheck.count">5</str>
<str name="spellcheck.collate">true</str>
<str name="spellcheck.collateExtendedResults">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
Solr cache configuration
As you may already know, caches play a major role in a Solr deployment. And I'm not talking
about some exterior cache – I'm talking about the three Solr caches:
ff Filter cache: This is used for storing filter (query parameter fq) results and mainly
enum type facets
ff Document cache: This is used for storing Lucene documents which hold stored fields
ff Query result cache: This is used for storing results of queries
www.it-ebooks.info
Chapter 1
23
There is a fourth cache – Lucene's internal cache – which is a field cache, but you can't
control its behavior. It is managed by Lucene and created when it is first used by the
Searcher object.
With the help of these caches we can tune the behavior of the Solr searcher instance. In this
task we will focus on how to configure your Solr caches to suit most needs. There is one thing
to remember – Solr cache sizes should be tuned to the number of documents in the index,
the queries, and the number of results you usually get from Solr.
Getting ready
Before you start tuning Solr caches you should get some information about your Solr instance.
That information is as follows:
ff Number of documents in your index
ff Number of queries per second made to that index
ff Number of unique filter (the fq parameter) values in your queries
ff Maximum number of documents returned in a single query
ff Number of different queries and different sorts
All these numbers can be derived from Solr logs.
How to do it...
For the purpose of this task I assumed the following numbers:
ff Number of documents in the index: 1.000.000
ff Number of queries per second: 100
ff Number of unique filters: 200
ff Maximum number of documents returned in a single query: 100
ff Number of different queries and different sorts: 500
Let's open the solrconfig.xml file and tune our caches. All the changes should be made in
the query section of the file (the section between <query> and </query> XML tags).
1. First goes the filter cache:
<filterCache
class="solr.FastLRUCache"
size="200"
initialSize="200"
autowarmCount="100"/>
www.it-ebooks.info
Apache Solr Configuration
24
2. Second goes the query result cache:
<queryResultCache
class="solr.FastLRUCache"
size="500"
initialSize="500"
autowarmCount="250"/>
3. Third we have the document cache:
<documentCache
class="solr.FastLRUCache"
size="11000"
initialSize="11000" />
Of course the above configuration is based on the example values.
4. Further let's set our result window to match our needs – we sometimes need to
get 20–30 more results than we need during query execution. So we change the
appropriate value in the solrconfig.xml file to something like this:
<queryResultWindowSize>200</queryResultWindowSize>
And that's all!
How it works...
Let's start with a little bit of explanation. First of all we use the solr.FastLRUCache
implementation instead of solr.LRUCache. So the called FastLRUCache tends to be faster
when Solr puts less into caches and gets more. This is the opposite to LRUCache which tends
to be more efficient when there are more puts than gets operations. That's why we use it.
This colud be the first time you see cache configuration, so I'll explain what cache configuration
parameters mean:
ff class: You probably figured that out by now. Yes, this is the class implementing the
cache.
ff size: This is the maximum size that the cache can have.
ff initialSize: This is the initial size that the cache will have.
ff autowarmCount: This is the number of cache entries that will be copied to the
new instance of the same cache when Solr invalidates the Searcher object – for
example, during a commit operation.
As you can see, I tend to use the same number of entries for size and initialSize, and
half of those values for autowarmCount. The size and initialSize properties can be
set to the same size in order to avoid the underlying Java object resizing, which consumes
additional processing time.
www.it-ebooks.info
Chapter 1
25
There is one thing you should be aware of. Some of the Solr caches (documentCache
actually) operate on internal identifiers called docid. Those caches cannot be automatically
warmed. That's because docid is changing after every commit operation and thus copying
docid is useless.
Please keep in mind that the settings for the size of the caches is usually good for the
moment you set them. But during the life cycle of your application your data may change,
your queries may change, and your user's behavior may, and probably will change. That's why
you should keep track of the cache usage with the use of Solr administration pages, JMX, or
a specialized software such as Scalable Performance Monitoring from Sematext (see more
at http://sematext.com/spm/index.html), and see how the utilization of each of the
caches changes in time and makes proper changes to the configuration.
There's more...
There are a few additional things that you should know when configuring your caches.
Using a filter cache with faceting
If you use the term enumeration faceting method (parameter facet.method=enum)
Solr will use the filter cache to check each term. Remember that if you use this method,
your filter cache size should have at least the size of the number of unique facet values
in all your faceted fields. This is crucial and you may experience performance loss if this
cache is not configured the right way.
When we have no cache hits
When your Solr instance has a low cache hit ratio you should consider not using caches at all
(to see the hit ratio you can use the administration pages of Solr). Cache insertion is not free
– it costs CPU time and resources. So if you see that you have a very low cache hit ratio, you
should consider turning your caches off – it may speed up your Solr instance. Before you turn
off the caches please ensure that you have the right cache setup – a small hit ratio can be a
result of bad cache configuration.
When we have more "puts" than "gets"
When your Solr instance uses put operations more than get operations you should consider
using the solr.LRUCache implementation. It's confirmed that this implementation behaves
better when there are more insertions into the cache than lookups.
Filter cache
This cache is responsible for holding information about the filters and the documents that
match the filter. Actually this cache holds an unordered set of document IDs that match the
filter. If you don't use the faceting mechanism with a filter cache, you should at least set its
size to the number of unique filters that are present in your queries. This way it will be possible
for Solr to store all the unique filters with their matching document IDs and this will speed up
the queries that use filters.
www.it-ebooks.info
Apache Solr Configuration
26
Query result cache
The query result cache holds the ordered set of internal IDs of documents that match the given
query and the sort specified. That's why if you use caches you should add as many filters as you
can and keep your query (the q parameter) as clean as possible. For example, pass only the
search box content of your search application to the query parameter. If the same query will be
run more than once and the cache has enough capacity to hold the entry, it will be used to give
the IDs of the documents that match the query, thus a no Lucene (Solr uses Lucene to index
and query data that is indexed) query will be made saving the precious I/O operation for the
queries that are not in the cache – this will boost up your Solr instance performance.
The maximum size of this cache that I tend to set is the number of unique queries and their
sorts that are handled by my Solr in the time between the Searcher object's invalidation.
This tends to be enough in most cases.
Document cache
The document cache holds the Lucene documents that were fetched from the index. Basically,
this cache holds the stored fields of all the documents that are gathered from the Solr index.
The size of this cache should always be greater than the number of concurrent queries multiplied
by the maximum results you get from Solr. This cache can't be automatically warmed – that is
because every commit is changing the internal IDs of the documents. Remember that the cache
can be memory consuming in case you have many stored fields, so there will be times when you
just have to live with evictions.
Query result window
The last thing is the query result window. This parameter tells Solr how many documents
to fetch from the index in a single Lucene query. This is a kind of super set of documents
fetched. In our example, we tell Solr that we want the maximum of one hundred documents
as a result of a single query. Our query result window tells Solr to always gather two hundred
documents. Then when we need some more documents that follow the first hundred they
will be fetched from the cache, and therefore we will be saving our resources. The size of the
query result window is mostly dependent on the application and how it is using Solr. If you
tend to do a lot of paging, you should consider using a higher query result window value.
You should remember that the size of caches shown in this task is not
final, and you should adapt them to your application needs. The values and
the method of their calculation should only be taken as a starting point to
further observation and optimization of the process. Also, please remember
to monitor your Solr instance memory usage as using caches will affect the
memory that is used by the JVM.
www.it-ebooks.info
Chapter 1
27
See also
There is another way to warm your caches if you know the most common queries that are sent
to your Solr instance – auto-warming queries. Please refer to the Improving Solr performance
right after a startup or commit operation recipe in Chapter 6, Improving Solr Performance.
For information on how to cache whole pages of results please refer to the Caching whole
result pages recipe in Chapter 6, Improving Solr Performance.
How to fetch and index web pages
There are many ways to index web pages. We could download them, parse them, and index
them with the use of Lucene and Solr. The indexing part is not a problem, at least in most
cases. But there is another problem – how to fetch them? We could possibly create our own
software to do that, but that takes time and resources. That's why this recipe will cover how
to fetch and index web pages using Apache Nutch.
Getting ready
For the purpose of this task we will be using Version 1.5.1 of Apache Nutch. To download the
binary package of Apache Nutch, please go to the download section of http://nutch.
apache.org.
How to do it...
Let's assume that the website we want to fetch and index is http://lucene.apache.org.
1. First of all we need to install Apache Nutch. To do that we just need to extract the
downloaded archive to the directory of our choice; for example, I installed it in the
directory /usr/share/nutch. Of course this is a single server installation and it
doesn't include the Hadoop filesystem, but for the purpose of the recipe it will be
enough. This directory will be referred to as $NUTCH_HOME.
2. Then we'll open the file $NUTCH_HOME/conf/nutch-default.xml and set
the value http.agent.name to the desired name of your crawler (we've taken
SolrCookbookCrawler as a name). It should look like the following code:
<property>
<name>http.agent.name</name>
<value>SolrCookbookCrawler</value>
<description>HTTP 'User-Agent' request header.</description>
</property>
www.it-ebooks.info
Apache Solr Configuration
28
3. Now let's create empty directories called crawl and urls in the $NUTCH_HOME
directory. After that we need to create the seed.txt file inside the created urls
directory with the following contents:
http://lucene.apache.org
4. Now we need to edit the $NUTCH_HOME/conf/crawl-urlfilter.txt file.
Replace the +.at the bottom of the file with +^http://([a-z0-9]*\.)*lucene.
apache.org/. So the appropriate entry should look like the following code:
+^http://([a-z0-9]*\.)*lucene.apache.org/
One last thing before fetching the data is Solr configuration.
5. We start with copying the index structure definition file (called schema-solr4.
xml) from the $NUTCH_HOME/conf/ directory to your Solr installation configuration
directory (which in my case was /usr/share/solr/collection1/conf/).
We also rename the copied file to schema.xml.
We also create an empty stopwords_en.txt file or we use the one provided with Solr
if you want stop words removal.
Now we need to make two corrections to the schema.xml file we've copied:
ff The first one is the correction of the version attribute in the schema tag. We need
to change its value from 1.5.1 to 1.5, so the final schema tag would look like this:
<schema name="nutch" version="1.5.1">
ff Then we change the boost field type (in the same schema.xml file) from string
to float, so the boost field definition would look like this:
<field name="boost" type="float" stored="true" indexed="false"/>
Now we can start crawling and indexing by running the following command from the $NUTCH_
HOME directory:
bin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 50
Depending on your Internet connection and your machine configuration you should finally see
a message similar to the following one:
crawl finished: crawl-20120830171434
This means that the crawl is completed and the data was indexed to Solr.
www.it-ebooks.info
Chapter 1
29
How it works...
After installing Nutch and Solr, the first thing we did was set our crawler name. Nutch does
not allow empty names so we must choose one. The file nutch-default.xml defines more
properties than the mentioned ones, but at this time we only need to know about that one.
In the next step, we created two directories; one (crawl) which will hold the crawl data and
the second one (urls) to store the addresses we want to crawl. The contents of the seed.
txt file we created contains addresses we want to crawl, one address per line.
The crawl-urlfilter.txt file contains information about the filters that will be used to
check the URLs that Nutch will crawl. In the example, we told Nutch to accept every URL that
begins with http://lucene.apache.org.
The schema.xml file we copied from the Nutch configuration directory is prepared to be used
when Solr is used for indexing. But the one for Solr 4.0 is a bit buggy, at least in Nutch 1.5.1
distribution, and that's why we needed to make the changes previously mentioned.
We finally came to the point where we ran the Nutch command. We specified that we wanted
to store the crawled data in the crawl directory (first parameter), and the addresses to crawl
data from are in the urls directory (second parameter). The –solr switch lets you specify
the address of the Solr server that will be responsible for the indexing crawled data and is
mandatory if you want to get the data indexed with Solr. We decided to index the data to Solr
installed at the same server. The –depth parameter specifies how deep to go after the links
defined. In our example, we defined that we want a maximum of three links from the main
page. The –topN parameter specifies how many documents will be retrieved from each level,
which we defined as 50.
There's more...
There is one more thing worth knowing when you start a journey in the land of Apache Nutch.
Multiple thread crawling
The crawl command of the Nutch command-line utility has another option – it can
be configured to run crawling with multiple threads. To achieve that you add the
following parameter:
-threads N
So if you would like to crawl with 20 threads you should run the crawl command like sot:
bin/nutch crawl crawl/nutch/site -dir crawl -depth 3 -topN 50 –threads 20
www.it-ebooks.info
Apache Solr Configuration
30
See also
If you seek more information about Apache Nutch please refer to the http://nutch.
apache.org and go to the Wiki section.
How to set up the extracting request
handler
Sometimes indexing prepared text files (such as XML, CSV, JSON, and so on) is not enough.
There are numerous situations where you need to extract data from binary files. For example,
one of my clients wanted to index PDF files – actually their contents. To do that, we either
need to parse the data in some external application or set up Solr to use Apache Tika. This
task will guide you through the process of setting up Apache Tika with Solr.
How to do it...
In order to set up the extracting request handler, we need to follow these simple steps:
1. First let's edit our Solr instance solrconfig.xml and add the following
configuration:
<requestHandler name="/update/extract" class="solr.extraction.
ExtractingRequestHandler" >
<lst name="defaults">
<str name="fmap.content">text</str>
<str name="lowernames">true</str>
<str name="uprefix">attr_</str>
<str name="captureAttr">true</str>
</lst>
</requestHandler>
2. Next create the extract folder anywhere on your system (I created that folder in the
directory where Solr is installed), and place the apache-solr-cell-4.0.0.jar
from the dist directory (you can find it in the Solr distribution archive). After that you
have to copy all the libraries from the contrib/extraction/lib/ directory to the
extract directory you created before.
3. In addition to that, we need the following entries added to the solrconfig.xml file:
<lib dir="../../extract" regex=".*\.jar" />
www.it-ebooks.info
Chapter 1
31
And that's actually all that you need to do in terms of configuration.
To simplify the example, I decided to choose the following index structure (place it in the
fields section in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="text" type="text_general" indexed="true" stored="true"/>
<dynamicField name="attr_*" type="text_general" indexed="true"
stored="true" multiValued="true"/>
To test the indexing process, I've created a PDF file book.pdf using PDFCreator which
contained the following text only: This is a Solr cookbook. To index that file, I've
used the following command:
curl "http://localhost:8983/solr/update/extract?literal.id=1&commit=true"
-F "myfile=@book.pdf"
You should see the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">578</int>
</lst>
</response>
How it works...
Binary file parsing is implemented using the Apache Tika framework. Tika is a toolkit for
detecting and extracting metadata and structured text from various types of documents,
not only binary files but also HTML and XML files. To add a handler that uses Apache Tika,
we need to add a handler based on the solr.extraction.ExtractingRequestHandler
class to our solrconfig.xml file as shown in the example.
In addition to the handler definition, we need to specify where Solr should look for the
additional libraries we placed in the extract directory that we created. The dir attribute
of the lib tag should be pointing to the path of the created directory. The regex attribute
is the regular expression telling Solr which files to load.
www.it-ebooks.info
Apache Solr Configuration
32
Let's now discuss the default configuration parameters. The fmap.content parameter tells
Solr what field content of the parsed document should be extracted. In our case, the parsed
content will go to the field named text. The next parameter lowernames is set to true;
this tells Solr to lower all names that come from Tika and have them lowercased. The next
parameter, uprefix, is very important. It tells Solr how to handle fields that are not defined
in the schema.xml file. The name of the field returned from Tika will be added to the value of
the parameter and sent to Solr. For example, if Tika returned a field named creator, and we
don't have such a field in our index, then Solr would try to index it under a field named attrcreator
which is a dynamic field. The last parameter tells Solr to index Tika XHTML elements
into separate fields named after those elements.
Next we have a command that sends a PDF file to Solr. We are sending a file to the /update/
extract handler with two parameters. First we define a unique identifier. It's useful to
be able to do that during document sending because most of the binary document won't
have an identifier in its contents. To pass the identifier we use the literal.id parameter.
The second parameter we send to Solr is the information to perform the commit right after
document processing.
See also
To see how to index binary files please refer to the Indexing PDF files and Extracting metadata
from binary files recipes in Chapter 2, Indexing Your Data.
Changing the default similarity
implementation
Most of the time, the default way of calculating the score of your documents is what you need.
But sometimes you need more from Solr; that's just the standard behavior. Let's assume that
you would like to change the default behavior and use a different score calculation algorithm
for the description field of your index. The current version of Solr allows you to do that and
this recipe will show you how to leverage this functionality.
Getting ready
Before choosing one of the score calculation algorithms available in Solr, it's good to read
a bit about them. The description of all the algorithms is beyond the scope of the recipe and
the book, but I would suggest going to the Solr Wiki pages (or look at Javadocs) and read the
basic information about available implementations.
www.it-ebooks.info
Chapter 1
33
How to do it...
For the purpose of the recipe let's assume we have the following index structure (just add the
following entries to your schema.xml file to the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_general" indexed="true" stored="true"/>
<field name="description" type="text_general_dfr" indexed="true"
stored="true" />
The string and text_general types are available in the default schema.xml file provided
with the example Solr distribution. But we want DFRSimilarity to be used to calculate the
score for the description field. In order to do that, we introduce a new type, which is defined
as follows (just add the following entries to your schema.xml file to the types section):
<fieldType name="text_general_dfr" class="solr.TextField"
positionIncrementGap="100">
<analyzer type="index">
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="stopwords.txt" enablePositionIncrements="true" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<analyzer type="query">
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="stopwords.txt" enablePositionIncrements="true" />
<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt"
ignoreCase="true" expand="true"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<similarity class="solr.DFRSimilarityFactory">
<str name="basicModel">P</str>
<str name="afterEffect">L</str>
<str name="normalization">H2</str>
<float name="c">7</float>
</similarity>
</fieldType>
Also, to use per-field similarity we have to add the following entry to your schema.xml file:
<similarity class="solr.SchemaSimilarityFactory"/>
And that's all. Now let's have a look and see how that works.
www.it-ebooks.info
Apache Solr Configuration
34
How it works...
The index structure presented in this recipe is pretty simple as there are only three fields.
The one thing we are interested in is that the description field uses our own custom
field type called text_general_dfr.
The thing we are mostly interested in is the new field type definition called text_general_
dfr. As you can see, apart from the index and query analyzer there is an additional section
– similarity. It is responsible for specifying which similarity implementation to use to
calculate the score for a given field. You are probably used to defining field types, filters,
and other things in Solr, so you probably know that the class attribute is responsible for
specifying the class implementing the desired similarity implementation which in our case
is solr.DFRSimilarityFactory. Also, if there is a need, you can specify additional
parameters that configure the behavior of your chosen similarity class. In the previous
example, we've specified four additional parameters: basicModel, afterEffect,
normalization, and c, which all define the DFRSimilarity behavior.
solr.SchemaSimilarityFactory is required to be able to specify the similarity
for each field.
There's more...
In addition to per-field similarity definition, you can also configure the global similarity:
Changing the global similarity
Apart from specifying the similarity class on a per-field basis, you can choose any other
similarity than the default one in a global way. For example, if you would like to use
BM25Similarity as the default one, you should add the following entry to your
schema.xml file:
<similarity class="solr.BM25SimilarityFactory"/>
As well as with the per-field similarity, you need to provide the name of the factory class
that is responsible for creating the appropriate similarity class.
www.it-ebooks.info
2
Indexing Your Data
In this chapter, we will cover:
ff Indexing PDF files
ff Generating unique fields automatically
ff Extracting metadata from binary files
ff How to properly configure Data Import Handler with JDBC
ff Indexing data from a database using Data Import Handler
ff How to import data using Data Import Handler and delta query
ff How to use Data Import Handler with the URL data source
ff How to modify data while importing with Data Import Handler
ff Updating a single field of your document
ff Handling multiple currencies
ff Detecting the document language
ff Optimizing your primary key field indexing
Introduction
Indexing data is one of the most crucial things in every Lucene and Solr deployment. When
your data is not indexed properly your search results will be poor. When the search results
are poor, it's almost certain the users will not be satisfied with the application that uses Solr.
That's why we need our data to be prepared and indexed as well as possible.
www.it-ebooks.info
Indexing Your Data
36
On the other hand, preparing data is not an easy task. Nowadays we have more and more
data floating around. We need to index multiple formats of data from multiple sources.
Do we need to parse the data manually and prepare the data in XML format? The answer is
no – we can let Solr do that for us. This chapter will concentrate on the indexing process and
data preparation beginning from how to index data that is a binary PDF file, teaching how to
use the Data Import Handler to fetch data from database and index it with Apache Solr, and
finally describing how we can detect the document's language during indexing.
Indexing PDF files
Imagine that the library on the corner that we used to go to wants to expand its collection and
make it available for the wider public though the World Wide Web. It asked its book suppliers
to provide sample chapters of all the books in PDF format so they can share it with the online
users. With all the samples provided by the supplier came a problem – how to extract data for
the search box from more than 900 thousand PDF files. Solr can do it with the use of Apache
Tika. This recipe will show you how to handle such a task.
Getting ready
Before you start getting deeper into the task, please refer to the How to set up the extracting
request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through
the process of configuring Solr to use Apache Tika. We will use the same index structure
and Solr configuration presented in that recipe, and I assume you already have Solr properly
configured (according to the mentioned recipe) and ready to work.
How to do it...
To test the indexing process I've created a PDF file book.pdf using PDFCreator
(http://sourceforge.net/projects/pdfcreator/) which contained the
following text only: This is a Solr cookbook.. To index that file I've used the
following command:
curl "http://localhost:8983/solr/update/extract?literal.id=1&commit=true"
-F "myfile=@cookbook.pdf"
You should then see the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">578</int>
</lst>
</response>
www.it-ebooks.info
Chapter 2
37
To see what was indexed I've run the following within a web browser:
http://localhost:8983/solr/select/?q=text:solr
In return I've got:
<?xml version="1.0" encoding="UTF-8"?>
<response>
…
<result name="response" numFound="1" start="0">
<doc>
<arr name="attr_created"><str>Thu Oct 21 16:11:51 CEST 2010</
str></arr>
<arr name="attr_creator"><str>PDFCreator Version 1.0.1</str></arr>
<arr name="attr_producer"><str>GPL Ghostscript 8.71</str></arr>
<arr name="attr_stream_content_type"><str>application/octetstream</
str></arr>
<arr name="attr_stream_name"><str>cookbook.pdf</str></arr>
<arr name="attr_stream_size"><str>3209</str></arr>
<arr name="attr_stream_source_info"><str>myfile</str></arr>
<str name="author">Gr0</str>
<arr name="content_type"><str>application/pdf</str></arr>
<str name="id">1</str>
<str name="keywords"/>
<date name="last_modified">2010-10-21T14:11:51Z</date>
<str name="subject"/>
<arr name="title"><str>cookbook</str></arr>
</doc>
</result>
</response>
How it works...
The curl command we used sends a PDF file to Solr. We are sending a file to the /update/
extract handler along with two parameters. It's useful to be able to do that during document
sending because most of the binary documents won't have an identifier in its contents. To
pass the identifier we use the literal.id parameter. The second parameter we send asks
Solr to perform the commit operation right after document processing.
The test file I've created, for the purpose of the recipe, contained a simple sentence:
"This is a Solr cookbook".
Remember the contents of the PDF file I created? It contained the word "Solr". That's why
I asked Solr to give me documents which contain the word "Solr" in a field named text.
www.it-ebooks.info
Indexing Your Data
38
In response, I got one document which matched the given query. To simplify the example,
I removed the response header part. As you can see in the response there were a few fields
that were indexed dynamically – their names start with attr_. Those fields contained
information about the file such as the size, the application that created it, and so on.
As we can see, we have our identifier indexed as we wished, and some other fields that
were present in the schema.xmlfile that Apache Tika could parse and return to Solr.
Generating unique fields automatically
Imagine you have an application that crawls the web and index documents found during
that crawl. The problem is that for some particular reason you can't set the document
identifier during indexing, and you would like Solr to generate one for you. This recipe
will help you, if you faced a similar problem.
How to do it...
The following steps will help you to generate unique fields automatically:
1. First let's create our index structure by adding the following entries to the schema.
xmlfields section:
<field name="id" type="uuid" indexed="true" stored="true"
default="NEW" multiValued="false"/>
<field name="name" type="text_general" indexed="true"
stored="true"/>
<field name="text" type="text_general" indexed="true"
stored="true"/>
2. In addition to that, we need to define the uuid field type by adding the following entry
to the types section of our schema.xml file:
<fieldType name="uuid" class="solr.UUIDField" indexed="true" />
3. In addition to that, we must remove the unique field definition, because Solr doesn't
allow using a unique field with the default="NEW" configuration, so the following
needs to be removed:
<uniqueKey>id</uniqueKey>
4. And now, let's try to index a document without an id field, for example one like this:
<add>
<doc>
<field name="name">Test name</field>
<field name="text">Test text contents</field>
</doc>
</add>
www.it-ebooks.info
Chapter 2
39
In order to see if Solr generated an identifier for the document, let's run the following query:
http://localhost:8983/solr/select?q=*:*&indent=true
The response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="name">Test name</str>
<str name="text">Test text contents</str>
<str name="id">b6f17c35-e5ad-4a09-b799-71580ca6be8a</str>
</doc>
</result>
</response>
As you can see in the response, our document had one additional field we didn't add manually
– the id field, which is what we wanted to have.
How it works...
The idea is quite simple – we let Solr generate the id field for us. To do that, we defined
the id field to be based on the uuid field type, and to have a default value of new
(default="NEW"). By doing this we tell Solr that we want that kind of behavior. If you
look at the uuid field type, you can see that it is a simple type definition based on solr.
UUIDField. Nothing complicated.
Having your document's identifiers generated automatically is handy in some cases, but it
also comes with some restrictions from Solr and its components. One of the issues is that
you can't have the unique field defined, and because of that, the elevation component won't
work. Of course that's only an example. But if your application doesn't know the identifiers of
the documents and can't generate them, then using solr.UUIDField is one of the ways of
having document identifiers for your indexed documents.
www.it-ebooks.info
Indexing Your Data
40
Extracting metadata from binary files
Suppose that our current client has a video and music store. Not the e-commerce one,
just the regular one – just around the corner. And now he wants to expand his business
to e-commerce. He wants to sell the products online. But his IT department said that
this will be tricky – because they need to hire someone to fill up the database with the
product names and their metadata. And that is the place where you come in and tell
them that you can extract titles and authors from the MP3 files that are available as
samples.Now let's see how that can be achieved.
Getting ready
Before you start getting deeper into the task, please refer to the How to set up the extracting
request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through
the process of configuring Solr to use Apache Tika.
How to do it...
1. Let's start by defining an index structure in the file schema.xml. The field definition
section should look like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="author" type="string" indexed="true" stored="true"
multiValued="true"/>
<field name="title" type="text" indexed="true" stored="true"/>
<dynamicField name="ignored_*" type="string" indexed="false"
stored="false" multiValued="true"/>
2. Now let's get the solrconfig.xml file ready:
<requestHandler name="/update/extract" class="solr.extraction.
ExtractingRequestHandler">
<lst name="defaults">
<str name="lowernames">true</str>
<str name="uprefix">ignored_</str>
<str name="captureAttr">true</str>
</lst>
</requestHandler>
3. Now we can start sending the documents to Solr. To do that, let's run the
following command:
curl "http://localhost:8983/solr/update/extract?literal.
id=1&commit=true" -F "myfile=@sample.mp3"
www.it-ebooks.info
Chapter 2
41
4. Let's check how the document was indexed. To do that type a query like the following
to your web browser:
http://localhost:8983/solr/select/?q=title:207
As a result I've got the following document:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">title:207</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="author">Armin Van Buuren</str>
<str name="id">1</str>
<str name="title">Desiderium 207 (Feat Susana)</str>
</doc>
</result>
</response>
So it seems that everything went well.
How it works...
First we define an index structure that will suit our needs. I decided that besides the unique
ID, I need to store the title and author name. We also defined a dynamic field called ignored
to handle the data we don't want to index (not indexed and not stored).
The next step is to define a new request handler to handle our updates, as you already
know. We also added a few default parameters to define our handler behavior. In our case
the parameter uprefix tells Solr to index all unknown fields to the dynamic field whose
name begins with ignored_, thus the additional data will not be visible in the index.
The last parameter tells Solr to index Tika XHTML elements into separate fields named
after those elements.
Next we have a command that sends an MP3 file to Solr. We are sending a file to the /
update/extract handler with two parameters. First we define a unique identifier and
pass that identifier to Solr using the literal.id parameter. The second parameter we
send to Solr is information to perform a commit right after document processing.
www.it-ebooks.info
Indexing Your Data
42
The query is a simple one, so I'll skip commenting on this part.
The last listing is an XML with Solr response. As you can see, there are only fields that are
explicitly defined in schema.xml – no dynamic fields. Solr and Tika managed to extract the
name and author of the file.
See also
ff If you want to index other types of binary files please refer to the Indexing PDF files
recipe in this chapter.
How to properly configure Data Import
Handler with JDBC
One of our clients is having a problem. His database of users grew to such size that even
the simple SQL select statement is taking too much time, and he seeks how to improve the
search time. Of course he heard about Solr but he doesn't want to generate XML or any other
data format and push it to Solr; he would like the data to be fetched. What can we do about
it? Well there is one thing – we can use one of the contribute modules of Solr, Data Import
Handler. This task will show you how to configure the basic setup of Data Import Handler and
how to use it.
How to do it...
1. First of all, copy the appropriate libraries that are required to use Data Import
Handler. So, let's create the dih folder anywhere on your system (I created it in the
directory where Solr is installed), and place apache-solr-dataimporthandler-
4.0.0.jar and apache-solr-dataimporthandler-extras-4.0.0.jar from
the Solr distribution dist directory in the folder. In addition to that, we need the
following entry to be added to the solrconfig.xml file:
<lib dir="../../dih" regex=".*\.jar" />
2. Next we need to modify the solrconfig.xml file. You should add an entry like the
following code:
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</requestHandler>
www.it-ebooks.info
Chapter 2
43
3. Now we will create the db-data-config.xml file that is responsible for the Data
Import Handler configuration. It should have contents like the following example:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<document>
<entity name="user" query="SELECT user_id, user_name from
users">
<field column="user_id" name="id" />
<field column="user_name" name="name" />
<entity name="user_desc" query="select desc from users_
description where user_id=${user.user_id}">
<field column="description" name="description" />
</entity>
</entity>
</document>
</dataConfig></dataConfig>
If you want to use other database engines, please change the driver, url,
and user and password attributes.
4. Now, let's create a sample index structure. To do that we need to modify the fields
section of the schema.xml file to something like the following snippet:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="text" indexed="true" stored="true" />
<field name="user_desc" type="text" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true"
stored="true"/>
5. One more thing before the indexing – you should copy an appropriate JDBC driver
to the lib directory of your Solr installation or the dih directory we created before.
You can get the library for PostgreSQL here http://jdbc.postgresql.org/
download.html.
6. Now we can start indexing. Run the following query to Solr:
http://localhost:8983/solr/dataimport?command=full-import
As you may know, the HTTP protocol is asynchronous, and thus you won't be updated on how
the process of indexing is going. To check the status of the indexing process, you can run the
command once again.
And that's how we configure Data Import Handler.
www.it-ebooks.info
Indexing Your Data
44
How it works...
First we have a solrconfig.xml part which actually defines a new request handler, Data
Import Handler, to be used by Solr. The <str name="config"> XML tag specifies the name
of the Data Import Handler configuration file.
The second listing is the actual configuration of Data Import Handler. I used the JDBC
source connection sample to illustrate how to configure Data Import Handler. The contents
of this configuration file start with the root tag named dataConfig which is followed by
a second tag defining a data source and named dataSource. In the example, I used the
PostgreSQL database and thus the JDBC driver is org.postgresql.Driver. We also
define the database connection URL (attribute named url), and the database credentials
(attributes user and password).
Next we have a document definition – a tag named document. This is the section containing
information about the document that will be sent to Solr. The document definition is made of
database queries – the entities.
The entity is defined by a name (the name attribute) and a SQL query (the query attribute).
The entity name can be used to reference values in sub-queries – you can see an example
of such a behavior in the second entity named user_desc. As you may already have noticed,
entities can be nested to handle sub-queries. The SQL query is there to fetch the data from
the database and use it to fill the entity variables which will be indexed.
After the entity comes the mapping definition. There is a single field tag for every column
returned by a query, but that is not a must – Data Import Handler can guess what the
mapping is (for example, where the entity field name matches the column name), but I
tend to use mappings because I find it easier to maintain. But let's get back to fields. The
field tag is defined by two attributes: column which is the column name returned by a
query, and name which is the field to which the data will be written.
Next we have a Solr query to start the indexing process. There are actually five commands
that can be run:
ff /dataimport: This will return the actual status.
ff /dataimport?command=full-import: This command will start the full import
process. Remember that the default behavior is to delete the index contents at
the beginning.
ff /dataimport?command=delta-import: This command will start the
incremental indexing process.
ff /dataimport?command=reload-config: This command will force
a configuration reload.
ff /dataimport?command=abort: This command will stop the indexing process.
www.it-ebooks.info
Chapter 2
45
There's more...
If you don't want to delete the index contents at the start of the full indexing using Data
Import Handler, add the clean=false parameter to your query. An example query should
look like this:
http://localhost:8983/solr/data?command=full-import&clean=false
Indexing data from a database using Data
Import Handler
Let's assume that we want to index the Wikipedia data, and we don't want to parse the whole
Wikipedia data and make another XML file. Instead we asked our DB expert to import the
data dump information from the PostgreSQL database, so we could fetch that data. Did I say
fetch? Yes it is possible – with the use of Data Import Handler and JDBC data source. This
task will guide you through how to do it.
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter
to get to know the basics about how Data Import Handler is configured. I'll assume that
you already have Solr set up according to the instructions available in the mentioned recipe.
How to do it...
The Wikipedia data I used in this example is available under the Wikipedia downloads
page at http://download.wikimedia.org/.
1. First let's add a sample index structure. To do that we need to modify the fields
section of the schema.xml file so it looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="string" indexed="true" stored="true"/>
<field name="revision_id" type="string" indexed="true"
stored="true"/>
<field name="contents" type="text" indexed="true" stored="true"/>
www.it-ebooks.info
Indexing Your Data
46
2. The next step is to add the request handler definition to the solrconfig.xml file,
like so:
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</requestHandler>
3. Now we have to add a db-data-config.xml file to the conf directory of your Solr
instance (or core):
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/wikipedia" user="wikipedia"
password="secret" />
<document>
<entity name="page" query="SELECT page_id, page_title from
page">
<field column="page_id" name="id" />
<field column="page_title" name="name" />
<entity name="revision" query="select rev_id from revision
where rev_page=${page.page_id}">
<field column="rev_id" name="revision_id" />
<entity name="pagecontent" query="select old_text from
pagecontent where old_id=${revision.rev_id}">
<field column="old_text" name="contents" />
</entity>
</entity>
</entity>
</document>
</dataConfig>
4. Now let's start indexing. Type the following URL into your browser:
http://localhost:8983/solr/dataimport?command=full-import
5. Let's check the indexing status during import. To do that we run the following query:
http://localhost:8983/solr/dataimport
Solr will show us a response like the following reponse:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
</lst>
<lst name="initArgs">
<lst name="defaults">
www.it-ebooks.info
Chapter 2
47
<str name="config">db-data-config.xml</str>
</lst>
</lst>
<str name="status">busy</str>
<str name="importResponse">A command is still running...</str>
<lst name="statusMessages">
<str name="Time Elapsed">0:1:15.460</str>
<str name="Total Requests made to DataSource">39547</str>
<str name="Total Rows Fetched">59319</str>
<str name="Total Documents Processed">19772</str>
<str name="Total Documents Skipped">0</str>
<str name="Full Dump Started">2010-10-25 14:28:00</str>
</lst>
<str name="WARNING">This response format is experimental.
It is likely to change in the future.</str>
</response>
6. Running the same query after the importing process is done should result in a
response like the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
</lst>
<lst name="initArgs">
<lst name="defaults">
<str name="config">db-data-config.xml</str>
</lst>
</lst>
<str name="status">idle</str>
<str name="importResponse"/>
<lst name="statusMessages">
<str name="Total Requests made to DataSource">2118645</str>
<str name="Total Rows Fetched">3177966</str>
<str name="Total Documents Skipped">0</str>
<str name="Full Dump Started">2010-10-25 14:28:00</str>
<str name="">Indexing completed. Added/Updated: 1059322
documents. Deleted 0 documents.</str>
<str name="Committed">2010-10-25 14:55:20</str>
<str name="Optimized">2010-10-25 14:55:20</str>
<str name="Total Documents Processed">1059322</str>
<str name="Time taken ">0:27:20.325</str>
</lst>
<str name="WARNING">This response format is experimental.
It is likely to change in the future.</str>
</response>
www.it-ebooks.info
Indexing Your Data
48
How it works...
To illustrate how Data Import Handler works, I decided to index the Polish Wikipedia data.
I decided to store four fields: page identifier, page name, page revision number, and its
contents. The field definition part is fairly simple so I decided to skip commenting on this.
The request handler definition, the Data Import Handler configuration, and command queries
were discussed in the How to properly configure Data Import Handler with JDBC recipe in this
chapter. The portions of interest in this task are in the db-data-config.xml file.
As you can see, we have three entities defined. The first entity gathers data from the page
table and maps two of the columns to the index fields. The next entity is nested inside the
first one and gathers the revision identifier from the table revision with the appropriate
condition. The revision identifier is then mapped to the index field. The last entity is nested
inside the second and gathers data from the pagecontent table again with the appropriate
condition. And again, the returned column is mapped to the index field.
We have the response which shows us that the import is still running (the listing with <str
name="importResponse">A command is still running...</str>). As you can
see there is information about how many data rows were fetched, how many requests to
the database were made, how many Solr documents were processed, and how many were
deleted. There is also information about the start of the indexing process. One thing you
should be aware of: this response can change in the next versions of Solr and Data
Import Handler.
The last listing shows us the summary of the indexing process.
How to import data using Data Import
Handler and delta query
Do you remember the task with the users import from the recipe named How to properly
configure Data Import Handler? We imported all the users from our client database but it took
ages – about two weeks. Our client is very happy with the results. His database is now not
used for searching but only updating. And yes, that is the problem for us – how do we update
data in the index? We can't fetch the whole data every time – it took two weeks. What we can
do is an incremental import which will modify only the data that has changed since the last
import. This task will show you how to do that.
www.it-ebooks.info
Chapter 2
49
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter to
get to know the basics of the Data Import Handler configuration. I assume that Solr is set up
according to the description given in the mentioned recipe.
How to do it...
1. The first thing you should do is add an additional column to the tables you use. So
in our case let's assume that we added a column named last_modified (which
should be a timestamp-based column). Now our db-data-config.xml will look
like the following code:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<document>
<entity name="user" query="SELECT user_id, user_name FROM users"
deltaImportQuery="select user_id, user_name FROM users WHERE user_
id = '${dataimporter.delta.user_id}'"deltaQuery="select user_id
FROM users WHERE last_modified &gt; '${dataimporter.last_index_
time}'">
<field column="user_id" name="id" />
<field column="user_name" name="name" />
<entity name="user_desc" query="select description from
users_description where user_id=${user.user_id}">
<field column="description" name="description" />
</entity>
</entity>
</document>
</dataConfig></dataConfig>
2. After that we run a new kind of query to start delta import:
http://localhost:8983/solr/dataimport?command=delta-import
How it works...
First we modified our database table to include a column named last_modified. We need
to ensure that the column will be modified at the same time as the table is. Solr will not
modify the database, so you have to ensure that your application will do that.
www.it-ebooks.info
Indexing Your Data
50
When running a delta import, Data Import Handler will create a file named dataimport.
properties inside a Solr configuration directory. In that file, the last index time will be
stored as a timestamp. This timestamp will be later used to distinguish whether the data
was changed or not. It can be used in a query by using a special variable: ${dataimporter.
last_index_time}.
You may have already noticed the two differences – two additional attributes defining an
entity named user – deltaQuery and deltaImportQuery. The first one is responsible
for getting the information about which users were modified since the last index. Actually
it only gets the user's unique identifiers. It uses the last_modified field to determine
which users were modified since the last import. Then the second query is executed –
deltaImportQuery. This query gets users with the appropriate unique identifier, to get
all the data which we want to index. One thing worth noticing is the way that I used the user
identifier in deltaImportQuery. I used the delta variable with its user_id (the same
name as the table column name) variable to get it: ${dataimporter.delta.user_id}.
You may have noticed that I left the query attribute in the entity definition. It's left on
purpose; you may need to index the entire data once again, so that configuration will
be useful for full imports as well as for the partial ones.
Next we have a query that shows how to run the delta import. You may have noticed that
compared to the full import, we didn't use the full-import command – we've sent the
delta-import command.
The statuses that are returned by Solr are the same as with the full import, so please refer to
the appropriate chapters to see what information they carry.
One more thing – the delta queries are only supported for the default SqlEntityProcessor
class. This means that you can only use those queries with JDBC data sources.
How to use Data Import Handler with the
URL data source
Do you remember the first example with the Wikipedia data? We asked our fellow DB expert
to import the data dump into PostgreSQL and we fetched the data from there. But what if our
colleague is sick and can't help us, and we need to import that data? We can parse the data
and send it to Solr, but that's not an option – we don't have much time to do that. So what to
do? Yes, you guessed – we can use Data Import Handler and one of its data sources, file data
source. This task will show you how to do that.
www.it-ebooks.info
Chapter 2
51
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter
to get to know the basics of the Data Import Handler configuration. I assume that Solr is
set up according to the description given in the mentioned recipe.
How to do it...
Let's take a look at our data source. To be consistent, I chose to index the Wikipedia data,
which you should already be familiar with.
1. First of all, the index structure. Our field definition part of schema.xml should look
like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="name" type="string" indexed="true" stored="true"/>
<field name="revision_id" type="string" indexed="true"
stored="true"/>
<field name="contents" type="text" indexed="true" stored="true"/>
2. The next step is to define a Data Import Handler request handler (put that definition
in the solrconfig.xml file):
<requestHandler name="/dataimport" class="org.apache.solr.handler.
dataimport.DataImportHandler">
<lst name="defaults">
<str name="config">data-config.xml</str>
</lst>
</requestHandler>
3. And now the data-config.xml file:
<dataConfig>
<dataSource type="FileDataSource" encoding="UTF-8" />
<document>
<entity name="page" processor="XPathEntityProcessor"
stream="true" forEach="/mediawiki/page/" url="/solrcookbook/data/
enwiki-20120802-pages-articles.xml"transformer="RegexTransformer">
<field column="id" xpath="/mediawiki/page/id" />
<field column="name" xpath="/mediawiki/page/title" />
<field column="revision_id" xpath="/mediawiki/page/revision/id"
/>
<field column="contents" xpath="/mediawiki/page/revision/text"
/>
<field column="$skipDoc" regex="^#REDIRECT .*"
replaceWith="true" sourceColName="contents"/>
www.it-ebooks.info
Indexing Your Data
52
</entity>
</document>
</dataConfig>
4. Now let's start indexing by sending the following query to Solr:
http://localhost:8983/solr/dataimport?command=full-import
After the import is done, we will have the data indexed.
How it works...
The Wikipedia data I used in this example is available under the Wikipedia downloads page
at http://download.wikimedia.org/enwiki/. I've chosen the pages-articles.
xml.bz2 file (actually it was named enwiki-20120802-pages-articles.xml.bz2)
which is about 6 GB. We only want to index some of the data from the file: page identifier,
name, revision, and page contents. I also wanted to skip articles that are only linking to
other articles in Wikipedia.
The field definition part of the schema.xml file is fairly simple and contains only four fields
and there is nothing unusual within it, so I'll skip commenting on it.
The solrconfig.xml file contains the handler definition with the information about the
Data Import Handler configuration filename.
Next we have the data-config.xml file where the actual configuration is written. We
have a new data source type here named FileDataSource. This data source will read the
data from a local directory. You can use HttpDataSource if you want to read data from
an outer location. The XML tag defining the data source also specifies the file encoding
(the encoding attribute) and in our example it's UTF-8. Next we have an entity definition,
which has a name under which it will be visible, a processor which will process our data. The
processor attribute is only mandatory when not using a database source. This value must
be set to XPathEntityProcessor in our case. The stream attribute, which is set to true,
informs Data Import Handler to stream the data from the file which is a must in our case
when the data is large. Following that we have a forEach attribute which specifies an XPath
expression – this path will be iterated over. There is a location of the data file defined in the
url attribute and a transformer defined in the transformer attribute. A transformer is a
mechanism that will transform every row of data and process it before sending it to Solr.
Under the entity definition we have field mapping definitions. We have columns which are
the same as the index field names thus I skipped the name field. There is one additional
attribute named xpath in the mapping definitions. It specifies the XPath expression that
defines where the data is located in the XML file. If you are not familiar with XPath please
refer to the http://www.w3schools.com/xpath/default.asp tutorial.
www.it-ebooks.info
Chapter 2
53
We also have a special column named $skipDoc. It tells Solr which documents to skip (if
the value of the column is true then Solr will skip the document). The column is defined
by a regular expression (attribute regex), a column to which the regular expression applies
(attribute sourceColName), and the value that will replace all the occurrences of the given
regular expression (replaceWith attribute). If the regular expression matches (in this case,
if the data in the column specified by the sourceColName attribute starts with #REDIRECT),
then the $skipDoc column will be set to true and thus the document will be skipped.
The actual indexing time was more than four hours on my machine, so if you try to index the
sample Wikipedia data please take that into consideration.
How to modify data while importing with
Data Import Handler
After we indexed the users and made the indexing incremental (the How to properly configure
Data Import Handler and How to import data using Data Import Handler and delta query
recipes), we were asked if we could modify the data a bit. Actually it would be perfect if we could
split name and surname into two fields in the index while those two reside in a single column in
the database. And of course, updating the database is not an option (trust me – it almost never
is). Can we do that? Of course we can, we just need to add some more configuration details in
Data Import Handler and use a transformer. This task will show you how to do that.
Getting ready
Please refer to the How to properly configure Data Import Handler recipe in this chapter to
get to know the basics about the Data Import Handler configuration. Also, to be able to run
examples in this chapter, you need to run Solr in the servlet container run on Java 6 or later. I
assume that Solr is set up according to the description given in the mentioned recipe.
How to do it...
Let's assume that we have a database table. To select users from our table we use the
following SQL query:
SELECT user_id, user_name, description FROM users
The response may look like this:
| user_id | user_name | description |
| 1 | John Kowalski | superuser |
| 2 | Amanda Looks | user |
www.it-ebooks.info
Indexing Your Data
54
Our task is to split the name from the surname and place it in two fields: name and surname.
1. First of all change the index structure, so our field definition part of schema.xml
should look like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="firstname" type="string" indexed="true"
stored="true"/>
<field name="surname" type="string" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true"
stored="true"/>
2. Now we have to add a db-data-config.xml file:
<dataConfig>
<dataSource driver="org.postgresql.Driver"
url="jdbc:postgresql://localhost:5432/users" user="users"
password="secret" />
<script><![CDATA[
function splitName(row) {
var nameTable = row.get('user_name').split(' ');
row.put('firstname', nameTable[0]);
row.put('surname', nameTable[1]);
row.remove('name');
return row;
}
]]></script>
<document>
<entity name="user" transformer="script:splitName" query="SELECT
user_id, user_name, description from users">
<field column="user_id" name="id" />
<field column="firstname" />
<field column="surname" />
<field column="description" />
</entity>
</document>
</dataConfig>
3. And now you can follow the normal indexing procedure which was discussed in the
How to properly configure Data Import Handler recipe in this chapter.
www.it-ebooks.info
Chapter 2
55
How it works...
The first two listings are the sample SQL query and the result given by a database. Next we
have a field definition part of the schema.xml file which defines four fields. Look at the
example database rows once again. See the difference? We have four fields in our index while
our database rows have only three columns. We must split the contents of the user_name
column into two index fields: firstname and surname. To do that, we will use JavaScript
language and the script transformer functionality of Data Import Handler.
The solrconfig.xml file is the same as the one discussed in the How to properly configure
Data Import Handler recipe in this chapter, so I'll skip that as well.
Next we have the updated contents of the db-data-config.xml file which we use to define
the behavior of Data Import Handler. The first and the biggest difference is the script tag
that will be holding our scripts that parse the data. The scripts should be held in the CDATA
section. I defined a simple function called splitName that takes one parameter, database row
(remember that the functions that operate on entity data should always take one parameter).
The first thing in the function is getting the contents of the user_name column, split it with the
space character, and assign it into a JavaScript table. Then we create two additional columns
in the processed row – firstname and surname. The contents of those rows come from the
JavaScript table we created. Then we remove the user_name column because we don't want it
to be indexed. The last operation is the returning of the processed row.
To enable script processing you must add one additional attribute to the entity definition – the
transformer attribute with the contents such as script:functionName. In our example,
it looks like this: transformer:"script:splitName". It tells Data Import Handler to use
the defined function name for every row returned by the query.
And that's how it works. The rest is the usual indexing process described in the How to
properly configure Data Import Handler task in this chapter.
There's more...
If you want to use a different language other than JavaScript, then you have to specify it in the
language attribute of the <script> tag. Just remember that the scripting language that you
want to use must be supported by Java 6. The example definition would look as follows:
<script language="ECMAScript">…</script>
www.it-ebooks.info
Indexing Your Data
56
Updating a single field of your document
Imagine that you have a system where you store a document your users upload. In addition
to that, your users can add other users to have access to the files they uploaded. As you
probably know, before Solr 4.0, when you wanted to update a single field in a document
you had to re-index the whole document. Solr 4.0 allows you to update a single field if
you fulfill some basic requirements. So let's see how we can do that in Solr 4.0.
How to do it...
For the purpose of the recipe, let's assume we have the following index structure
(put the following entries to your schema.xml file's fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="file" type="text_general" indexed="true" stored="true"/>
<field name="user" type="string" indexed="true" stored="true"
multiValued="true" />
In addition to that, we need the _version_ field:
<field name="_version_" type="long" indexed="true" stored="true"/>
And that's all when it comes to the schema.xml file. In addition to that, let's assume
we have the following data indexed:
<add>
<doc>
<field name="id">1</field>
<field name="file">Sample file</field>
<field name="user">gro</field>
<field name="user">negativ</field>
</doc>
</add>
So, we have a sample file and two user names specifying which users of our system can
access that file. But what if we would like to add another user called jack. Is that possible?
Yes, with Solr 4.0 it is. To add the value to a field which has multiple values, we should send
the following command:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","user":{"add":"jack"}}]'
Let's see if it worked by sending the following query:
http://localhost:8983/solr/select?q=*:*&indent=true
www.it-ebooks.info
Chapter 2
57
The response sent by Solr was as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="file">Sample file</str>
<arr name="user">
<str>gro</str>
<str>negativ</str>
<str>jack</str>
</arr>
<long name="_version_">1411121765349851136</long></doc>
</result>
</response>
As you can see it worked without any problems. Imagine that now one of the users changed
the name of the document, and we would also like to update the file field of that document
to match that change. In order to do so, we should send the following command:
curl 'localhost:8983/solr/update?commit=true' -H 'Contenttype:
application/json' -d '[{"id":"1","file":{"set":"New file name"}}]'
And again, we send the same query as before to see if the command succeeded:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
www.it-ebooks.info
Indexing Your Data
58
<str name="id">1</str>
<str name="file">New file name</str>
<arr name="user">
<str>gro</str>
<str>negativ</str>
<str>jack</str>
</arr>
<long name="_version_">1411121902871642112</long></doc>
</result>
</response>
It worked again. So now let's see how Solr does that.
How it works...
As you can see the index structure is pretty simple; we have a document identifier, its name,
and users that can access that file. As you can see all the fields in the index are marked as
stored (stored="true"). This is required for the partial update functionality to work. This
is because, under the hood, Solr takes all the values from the fields and updates the one we
mentioned to be updated. So it is just a typical document indexing, but instead of you having
to provide all the information, it's Solr's responsibility to get it from the index.
Another thing that is required for the partial update functionality to work is the _version_
field. You don't have to set it during indexing, it is used internally by Solr. The example data
we are indexing is also very simple. It is a single document with two users defined.
[{"id":"1","user":{"add":"jack"}}]
The interesting stuff comes with the update command. As you can see, that command
is run against a standard update handler you run indexing against. The commit=true
parameter tells Solr to perform the commit operation right after update. The -H 'Contenttype:
application/json' part is responsible for setting the correct HTTP headers for the
update request. Next we have the request contents itself. It is sent as a JSON object.
We specified that we are interested in the document with the identifier "1" ("id":"1").
We want to change the user field and we want to add the jack value to that field (the
add command). So as you can see, the add command is used when we want to add a
new value to a field which can hold multiple values.
The second command shown as an example shows how to change the value of a
single-valued field. It is very similar to what we had before, but instead of using the
add command, we use the set command. And again, as you can see, it worked perfectly.
www.it-ebooks.info
Chapter 2
59
Handling multiple currencies
Imagine a situation where you run an e-commerce site and you sell your products all over the
world. One day you say that you would like to calculate the currencies by yourself and have
all the goodies that Solr gives you on all the currencies you support. You could of course add
multiple fields, one for each currency. On the other hand, you can use the new functionality
introduced in Solr 3.6 and create a field that will use the provided currency exchange rates.
How to do it...
This recipe will show you how to configure and use multiple currencies using a single field
in the index:
1. Let's start with creating a sample index structure, by modifying the fields section
in your schema.xml file so it looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_general" indexed="true"
stored="true" />
<field name="price" type="currencyField" indexed="true"
stored="true" />
2. In addition to that, we need to provide the definition for the type the price field is
based on (add the following entry to the types section in your schema.xml file):
<fieldType class="solr.CurrencyField" name="currencyField"
defaultCurrency="USD" currencyConfig="currencyExchange.xml" />
3. Another file that we need to create is the currencyExchange.xml file, which should
be placed in the conf directory of your collection and have the following contents:
<currencyConfig version="1.0">
<rates>
<rate from="USD" to="EUR" rate="0.743676" comment="European
Euro" />
<rate from="USD" to="HKD" rate="7.801922" comment="HONG KONG
Dollar" />
<rate from="USD" to="GBP" rate="0.647910" comment="UNITED
KINGDOM Pound" />
</rates>
</currencyConfig>
www.it-ebooks.info
Indexing Your Data
60
4. Now we can index some example data. For this recipe, I decided to index the
following documents:
<add>
<doc>
<field name="id">1</field>
<field name="name">Test document one</field>
<field name="price">10.10,USD</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Test document two</field>
<field name="price">12.01,USD</field>
</doc>
</add>
5. Let's now check if that works. Our second document costs 12.01 USD and we have
defined the exchange rate for European Euro as 0.743676. This gives us about 7.50
EUR for the first document and about 8.90 EUR for the second one. Let's check that
by sending the following query to Solr:
http://localhost:8983/solr/select?q=name:document&fq=price:[8.00,E
UR TO 9.00,EUR]
6. The result returned by Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price:[8.00,EUR TO 9.00,EUR]</str>
<str name="q">name:document</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Test document two</str>
<str name="price">12.01,USD</str></doc>
</result>
</response>
As you can see, we got the document we wanted.
www.it-ebooks.info
Chapter 2
61
How it works...
The idea behind the functionality is simple – we create a field based on a certain type and
we provide a file with a currency exchange rate, and that's all. After that we can query our
Solr instance with the use of all the currencies we defined exchange rates for. But now, let's
discuss all the previous configuration changes in detail.
The index structure is very simple; it contains three fields of which one is responsible for
holding the price of the document and is based on the currencyField type. This type is
based on solr.CurrencyField. Its defaultCurrency attribute specifies the default
currency for all the fields using this type. This is important, because Solr will return prices
in the defined default currency, no matter what currency is used during the query. The
currencyConfig attribute specifies the name of the file with the exchange rate definition.
Our currencyExchange.xml file provides exchange rate for three currencies:
ff EUR
ff HKD
ff GBP
The file should be structured similar to the example one previously shown. This means
that each exchange rate should have the from attribute telling Solr from which currency
the exchange will be done, the to attribute specifying to which currency the exchange will
be done, and the rate attribute specifying the actual exchange rate. In addition to that,
it can also have the comment attribute if we want to include some short comment.
During indexing, we need to specify the currency we want the data to be indexed with. In
the previous example, we indexed data with USD. This is done by specifying the price, a
colon character, and the currency code after it. So 10.10,USD will mean ten dollars
and ten cents in USD.
The last thing is the query. As you can see, you can query Solr with different currencies from
the one used during indexing. This is possible because of the provided exchange rates file.
As you can see, when we use a range query for a price field, we specify the value, the colon
character, and the currency code after it. Please remember that if you provide a currency
code unknown to Solr, it will throw an exception saying that the currency is not known.
There's more...
You can also have the exchange rates being updated automatically by specifying the
currency provider.
www.it-ebooks.info
Indexing Your Data
62
Setting up your own currency provider
Specifying the currency exchange rate file is great, but we need to update that file because
the exchange rates change constantly. Luckily for us, Solr committers thought about it and
gave us the option to provide an exchange rate provider instead of a plain file. The provider
is a class responsible for providing the exchange rate data. The default exchange rate provider
available in Solr uses exchange rates from http://openexchangerates.org, which are
updated hourly. In order to use it, we need to modify our currencyField field
type definition and introduce three new properties (and remove the currencyConfig one):
ff providerClass: This class implements the exchange rates provider,
which in our case will be the default one available in Solr – solr.
OpenExchangeRatesOrgProvider
ff refreshInterval: This determines how often to refresh the rates
(specified in minutes)
ff ratesFileLocation: This determines the location of the file with rates in open
exchange format
So the final configuration should look like the following snippet:
<fieldType name="currencyField" class="solr.CurrencyField"
providerClass="solr.OpenExchangeRatesOrgProvider"
refreshInterval="120" ratesFileLocation="http://192.168.10.10/latest.
json"/>
You can download the sample exchange file from the http://openexchangerates.org
site after creating an account there.
Detecting the document's language
Imagine a situation where you have users from different countries and you would like to give
them a choice to only see content you index that is written in their native language. Sounds quite
interesting, right? Let us see how we can identify the language of the documents during indexing
and store that information along with the documents in the index for later use.
How to do it...
For the language identification we will use one of the Solr contrib modules, but let's start from
the beginning.
1. For the purpose of the recipe, I assume that we will be using the following index
structure (add the following to the fields section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
www.it-ebooks.info
Chapter 2
63
<field name="name" type="text_general" indexed="true"
stored="true"/>
<field name="description" type="text_general" indexed="true"
stored="true" />
<field name="langId" type="string" indexed="true" stored="true" />
We will use the langId field to store the information about the identified language.
2. The next thing we need to do is create a langid directory somewhere on your
filesystem (I'll assume that the directory is created in the same directory that Solr is
installed) and copy the following libraries to that directory:
?? apache-solr-langid-4.0.0.jar (from the dist directory of Apache
Solr distribution)
?? jsonic-1.2.7.jar (from the contrib/langid/lib directory of Apache
Solr distribution)
?? langdetect-1.1.jar (from the contrib/langid/lib directory of
Apache Solr distribution)
3. Next we need to add some information to the solrconfig.xml file. First we need to
inform Solr that we want it to load the additional libraries. We do that by adding the
following entry to the config section of that file:
<lib dir="../../langid/" regex=".*\.jar" />
4. In addition to that we configure a new update processor by adding the following to the
config section of the solrconfig.xml file:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
LangDetectLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
5. Now, we need some data to be indexed. I decided to use the following test data
(stored in a data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">First</field>
<field name="description">>Water is a chemical substance with
the chemical formula H2O. A water molecule contains one oxygen
and two hydrogen atoms connected by covalent bonds. Water is a
www.it-ebooks.info
Indexing Your Data
64
liquid at ambient conditions, but it often co-exists on Earth with
its solid state, ice, and gaseous state (water vapor or steam).
Water also
exists in a liquid crystal state near hydrophilic surfaces.[1]
[2] Under nomenclature used to name chemical compounds, Dihydrogen
monoxide is the scientific name for water, though it is almost
never used.</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Zweite</field>
<field name="description">Wasser (H2O) ist eine chemische
Verbindung aus den Elementen Sauerstoff (O) und Wasserstoff
(H). Wasser ist die einzige chemische Verbindung auf der Erde,
die in der Natur in allen drei Aggregatzuständen vorkommt.
Die Bezeichnung Wasser wird dabei besonders für den flüssigen
Aggregatzustand verwendet. Im festen (gefrorenen) Zustand spricht
man von Eis, im gasförmigen Zustand von Wasserdampf.</field>
</doc>
</add>
6. And now the indexing. To index the above test file I used the following commands:
curl 'http://localhost:8983/solr/update?update.chain=langid'
--data-binary @data.xml -H 'Content-type:application/xml'
curl 'http://localhost:8983/solr/update?update.chain=langid'
--data-binary '<commit/>' -H 'Content-type:application/xml'
7. After sending the previous two commands, we can finally test if that worked. We will
just ask Solr to return all the documents by sending the q=*:* query. The following
results will be returned:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">6</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">First</str>
<str name="description">&gt;Water is a chemical substance with
the
www.it-ebooks.info
Chapter 2
65
chemical formula H2O. A water molecule contains one oxygen and two
hydrogen atoms connected by covalent bonds. Water is a liquid at
ambient conditions, but it often co-exists on Earth with its solid
state, ice, and gaseous state (water vapor or steam). Water also
exists in a liquid crystal state near hydrophilic surfaces.[1]
[2] Under nomenclature used to name chemical compounds, Dihydrogen
monoxide is the scientific name for water, though it is almost
never used.</str>
<str name="langId">en</str></doc>
<doc>
<str name="id">2</str>
<str name="name">Zweite</str>
<str name="description">Wasser (H2O) ist eine chemische
Verbindung
aus den Elementen Sauerstoff (O) und Wasserstoff (H). Wasser ist
die einzige chemische Verbindung auf der Erde, die in der Natur
in allen drei Aggregatzuständen vorkommt. Die Bezeichnung Wasser
wird dabei besonders für den flüssigen Aggregatzustand verwendet.
Im festen (gefrorenen) Zustand spricht man von Eis, im gasförmigen
Zustand von Wasserdampf.</str>
<str name="langId">de</str></doc>
</result>
</response>
As you can see, the langId field was filled with the correct language.
How it works...
The index structure we used is quite simple; it contains four fields and we are most interested
in the langId field which won't be supplied with the data, but instead of that we want Solr
to fill it.
The mentioned libraries are needed in order for the language identification to work. The lib
entry in the solrconfig.xml file tells Solr to look for all the JAR files from the ../../
langid directory. Remember to change that to reflect your setup.
Now the update request processor chain definition comes. We need
that definition to include org.apache.solr.update.processor.
LangDetectLanguageIdentifierUpdateProcessorFactory in order to detect the
document language. The langid.fl property tells the defined processor which fields
should be used to detect the language. langid.langField specifies to which field the
detected language should be written. The last property, langid.fallback, tells the
language detection library what language should be set if it fails to detect a language.
The solr.LogUpdateProcessorFactory and solr.RunUpdateProcessorFactory
processors are there to log the updates and actually run them.
www.it-ebooks.info
Indexing Your Data
66
As for data indexing, in order to use the defined update request processor chain, we need to
tell Solr that we want it to be used. In order to do that, when sending data to Solr we specify
the additional parameter called update.chain with the name of the update chain we want
to use, which in our case is langid. The --data-binary switch tells the curl command to
send that data in a binary format and the -H switch tells curl which content type should
be used. In the end we send the commit command to write the data to the Lucene index.
There's more...
If you don't want to use the previously mentioned processor to detect the document language,
you can use the one that uses the Apache Tika library:
Language identification based on Apache Tika
If LangDetectLanguageIdentifierUpdateProcessorFactory is not good
enough for you, you can try using language identification based on the Apache Tika
library. In order to do that you need to provide all the libraries from the contrib/
extraction directory in the Apache Solr distribution package instead of the ones
from contrib/langid/lib, and instead of using the org.apache.solr.update.
processor.LangDetectLanguageIdentifierUpdateProcessorFactory
processor use org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory. So the final configuration should
look like the following code:
<updateRequestProcessorChain name="langid">
<processor class="org.apache.solr.update.processor.
TikaLanguageIdentifierUpdateProcessorFactory">
<str name="langid.fl">name,description</str>
<str name="langid.langField">langId</str>
<str name="langid.fallback">en</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
However, remember to still specify the update.chain parameter during indexing or add the
defined processor to your update handler configuration.
www.it-ebooks.info
Chapter 2
67
Optimizing your primary key field indexing
Most of the data stored in Solr has some kind of primary key. Primary keys are different from
most of the fields in your data as each document has a unique value stored; because they are
primary in most cases they are unique. Because of that, a search on this primary field
is not always as fast as you would expect when you compare it to databases. So, is there
anything we can do to make it faster? With Solr 4.0 we can, and this recipe will show
you how to improve the execution time of queries run against unique fields in Solr.
How to do it...
Let's assume we have the following field defined as a unique key for our Solr collection.
So, in your schema.xml file, you would have the following:
ff In your fields section you would have the following:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
ff After your fields section the following entry could be found:
<uniqueKey>id</uniqueKey>
The following steps will help you optimize the indexing of your primary key field:
1. Now, we would like to use the Lucene flexible indexing and use PulsingCodec
to handle our id field. In order to do that we introduce the following field type (just
place it in the types section of your schema.xml file):
<fieldType name="string_pulsing" class="solr.StrField"
postingsFormat="Pulsing40"/>
2. In addition to that, we need to change the id field definition to use the new type.
So, we should change the type attribute from string to string_pulsing:
<field name="id" type="string_pulsing" indexed="true"
stored="true" required="true" />
3. In addition to that we need to put the following entry in the solrconfig.xml file:
<codecFactory class="solr.SchemaCodecFactory"/>
And that's all. Now you can start indexing your data.
www.it-ebooks.info
Indexing Your Data
68
How it works...
The changes we made use the new feature introduced in Apache Lucene 4.0 and in Solr – the
so-called flexible indexing. It allows us to modify the way data is written into an inverted index
and thus configure it to our own needs. In the previous example, we used PulsingCodec
(postingsFormat="Pulsing40") in order to store the unique values in a special way. The
idea behind that codec is that the data for low frequency terms is written in a special way to
save a single I/O seek operation when retrieving a document or documents for those terms
from the index. That's why in some cases, when you do a noticeable amount of search to your
unique field (or any high cardinality field indexed with PulsingCodec), you can see a drastic
performance increase for that fields.
The last change, the one we made to the solrconfig.xml file, is required; without
it Solr wouldn't let us use specified codes and would throw an exception during startup.
It just specifies which codec factory should be used to create codec instances.
Please keep in mind that the previously mentioned method is very case dependent and
you may not see a great performance increase with the change.
www.it-ebooks.info
3
Analyzing Your
Text Data
In this chapter, we will cover:
ff Storing additional information using payloads
ff Eliminating XML and HTML tags from text
ff Copying the contents of one field to another
ff Changing words to other words
ff Splitting text by CamelCase
ff Splitting text by white space only
ff Making plural words singular without stemming
ff Lowercasing the whole string
ff Storing geographical points in the index
ff Stemming your data
ff Preparing text to perform an efficient trailing wildcard search
ff Splitting text by numbers and non-white space characters
ff Using Hunspell as a stemmer
ff Using your own stemming dictionary
ff Protecting words from being stemmed
www.it-ebooks.info
Analyzing Your Text Data
70
Introduction
The process of data indexing can be divided into parts. One of the parts, actually one of the
last parts of that process, is data analysis. It's one of the crucial parts of data preparation.
It defines how your data will be written into an index. It defines its structure and so on. In Solr,
data behavior is defined by types. A type's behavior can be defined in the context of the indexing
process or the context of the query process, or both. Furthermore, a type definition is composed
of a tokenizer (or multiple ones–one for querying and one for indexing) and filters (both token
filters and character filters).
A tokenizer specifies how your data will be pre-processed after it is sent to the appropriate field.
Analyzer operates on the whole data that is sent to the field. Types can only have one tokenizer.
The result of the tokenizer's work is a stream of objects called tokens. Next in the analysis chain
are the filters. They operate on the tokens in the token stream. They can do anything with the
tokens – change them, remove them, or make them lowercase, for example. Types can have
multiple filters.
One additional type of filter is character filters. They do not operate on tokens from the token
stream. They operate on the data that is sent to the field, and they are invoked before the
data is sent to the analyzer.
This chapter will focus on data analysis and how to handle common day-to-day analysis
questions and problems.
Storing additional information using
payloads
Imagine you have a powerful preprocessing tool that can extract information about all the
words in the text. Your boss would like you to use it with Solr or at least store the information it
returns in Solr. So what can you do? We can use something called payload to store that data.
This recipe will show you how to do it.
How to do it...
I assume that we already have an application that takes care of recognizing the part of
speech in our text data. What we need to add is the data to the Solr index. To do that we
will use a payload – a metadata that can be stored with each occurrence of a term.
www.it-ebooks.info
Chapter 3
71
1. First of all, you need to modify the index structure. To do this, we will add the
new field type to the schema.xml file (the following entries should be added
to the types section):
<fieldtype name="partofspeech" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.DelimitedPayloadTokenFilterFactory"
encoder="integer" delimiter="|"/>
</analyzer>
</fieldtype>
2. Now we'll add the field definition part to the schema.xml file (the following entries
should be added to the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="text" type="text" indexed="true" stored="true" />
<field name="speech" type="partofspeech" indexed="true"
stored="true" multivalued="true" />
3. Now let's look at what the example data looks like (I named it ch3_payload.xml):
<add>
<doc>
<field name="id">1</field>
<field name="text">ugly human</field>
<field name="speech">ugly|3 human|6</field>
</doc>
<doc>
<field name="id">2</field>
<field name="text">big book example</field>
<field name="speech">big|3 book|6 example|1</field>
</doc>
</add>
4. The next step is to index our data. To do that, we run the following command from
the exampledocs directory (put the ch3_payload.xml file there):
java -jar post.jar ch3_payload.xml
www.it-ebooks.info
Analyzing Your Text Data
72
5. To check if the payloads were written to the index, we will use the analysis capabilities
or the Solr administration panel. We will test the test|7 term with the associated
payload. The following is what it looks like:
How it works...
What information can the payload hold? It may hold information that is compatible with the
encoder type you define for the solr.DelimitedPayloadTokenFilterFactory filter.
In our case, we don't need to write our own encoder – we will use the supplied one to store
integers. We will use it to store the boost of the term. For example, nouns will be given a token
boost value of 6, while the adjectives will be given a boost value of 3.
So first, we have the type definition. We defined a new type in the schema.xml file named
partofspeech based on the Solr text field (attribute class="solr.TextField"). Our
tokenizer splits the given text on whitespace characters. Then we have a new filter which
handles our payloads. The filter defines an encoder which in our case is an integer (attribute
encoder="integer"). Furthermore it defines a delimiter which separates the term from
the payload. In our case the separator is the pipe character (|).
www.it-ebooks.info
Chapter 3
73
Finally we have the field definitions. In our example we only define three fields:
ff Identifier
ff Text
ff Recognized speech part with payload
Now let's take a look at the example data. We have two simple fields – id and text. The one
that we are interested in is the speech field. Look at how it is defined. It contains pairs which
are made of a term, a delimiter, and a boost value. For example, book|6. In the example, I
decided to boost nouns with a boost value of 6 and adjectives with the boost value of 3. I also
decided that words that cannot be identified by my application, which is used to identify parts of
speech, will be given a boost of 1. Pairs are separated with a space character, which in our case
will be used to split those pairs – that is the task of the tokenizer which we defined earlier.
To index the documents we use simple post tools provided with the example deployment of
Solr. To use it, invoke the command shown in the example. The post tools will send the data
to the default update handler found under the address http://localhost:8983/solr/
update. The following parameter is the file that is going to be sent to Solr. You can
also post a list of files, not only a single one.
As you can see on the provided screenshot, payload is being properly encoded and
written to the index – you can see [0 0 0 7] in the payload section of the solr.
DelimitedPayloadTokenFilterFactory filter.
Eliminating XML and HTML tags from text
There are many real-life situations when you have to clean your data. Let's assume that you want
to index web pages that your client sends you. You don't know anything about the structure of
that page; one thing you know is that you must provide a search mechanism that will enable
searching through the content of the pages. Of course you could index the whole page, splitting
it by whitespaces, but then you would probably hear the clients complain about the HTML tags
being searchable and so on. So before we enable searching the contents of the page, we need
to clean the data. In this example we need to remove the HTML tags. This recipe will show you
how to do it with Solr.
How to do it...
1. Let's start with assuming that our data looks like this (the ch3_html.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="html"><html><head><title>My page</title></
head><body><p>This is a <b>my</b><i>sample</i> page</body></
html></field>
www.it-ebooks.info
Analyzing Your Text Data
74
</doc>
</add>
2. Now let's take care of the schema.xml file. First add the type definition to the
schema.xml file:
<fieldType name="html_strip" class="solr.TextField">
<analyzer>
<charFilter class="solr.HTMLStripCharFilterFactory"/>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. The next step is to add the following to the field definition part of the schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="html" type="html_strip" indexed="true" stored="false"
/>
4. We can now index our data and have the HTML tags removed, right? Let's check
that, by going to the analysis section of the Solr administration pages and passing
the <html><head><title>My page</title></head><body><p>This is a
<b>my</b><i>sample</i> page</body></html> text to analysis there:
www.it-ebooks.info
Chapter 3
75
How it works...
First of all we have the data example. In the example we see one file with two fields, the identifier
and some HTML data nested in the CDATA section. You must remember to surround HTML data
in CDATA tags if they are full pages and start from HTML tags like our example. Otherwise Solr
will have problems with parsing the data. But if you only have some tags present in the data, you
shouldn't worry.
Next we have the html_strip type definition. It is based on solr.TextField to enable
full text searching. Following that we have a character filter which handles the HTML and the
XML tag stripping. The character filters are invoked before the data is sent to the tokenizer.
This way they operate on un-tokenized data. In our case the character filter strips the HTML
and XML tags, attributes, and so on and then sends the data to the tokenizer which splits the
data by whitespace characters. The one and only filter defined in our type makes the tokens
lowercase to simplify the search.
If you want to check how your data was indexed, remember not to be mistaken when you
choose to store the field contents (attribute stored="true"). The stored value is the
original one sent to Solr, so you won't be able to see the filters in action. If you wish to check
the actual data structures, take a look at the Luke utility (a utility that lets you see the index
structure and field values, and operate on the index). Luke can be found at the following
address: http://code.google.com/p/luke. Instead of using Luke, I decided to use the
analysis capabilities of the Solr administration pages and see how the html field behaves
when we pass the example value provided in the example data file.
Copying the contents of one field to another
Imagine that you have many big XML files that hold information about the books that are
stored on library shelves. There is not much data, just a unique identifier, the name of the
book and the author. One day your boss comes to you and says: "Hey, we want to facet and
sort on the basis of book author". You can change your XML and add two fields, but why do
that, when you can use Solr to do that for you? Well, Solr won't modify your data, but can
copy the data from one field to another. This recipe will show you how to do that.
How to do it...
In order to achieve what we want, we need the contents of the author field to be present in
the fields named author, author_facet, and authorsort.
1. Let's assume that our data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook</field>
www.it-ebooks.info
Analyzing Your Text Data
76
<field name="author">John Kowalsky</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Some other book</field>
<field name="author">Jane Kowalsky</field>
</doc>
</add>
2. Now let's add the following fields' definition to the fields section of your
schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="author" type="text" indexed="true" stored="true"
multiValued="true"/>
<field name="name" type="text" indexed="true" stored="true"/>
<field name="author_facet" type="string" indexed="true"
stored="false"/>
<field name="author_sort" type="alphaOnlySort" indexed="true"
stored="false"/>
3. In order to make Solr copy data from the author field to the author_facet
and author_sort field we need to define the copy fields in the schema.xml
file (place the following entries right after the field section):
<copyField source="author" dest="author_facet"/>
<copyField source="author" dest="author_sort"/>
4. Now we can index our example data file by running the following command from the
exampledocs directory (put the data.xml file there):
java -jar post.jar data.xml
How it works...
As you can see in the example, we only have three fields defined in our sample data XML file.
There are two fields which we are not particularly interested in – id and name. The field that
interests us the most is the author field. As I have previously mentioned, we want to place
the contents of that field into three fields:
ff author (the actual field that will be holding the data)
ff author_sort
ff author_facet
www.it-ebooks.info
Chapter 3
77
To do that we use copy fields. Those instructions are defined in the schema.xml file, right
after the field definitions; that is, after the </fields> tag. To define a copy field, we need
to specify a source field (attribute source) and a destination field (attribute dest).
After the definitions, like those in the example, Solr will copy the contents of the source fields
to the destination fields during the indexing process. There is one thing that you have to be
aware of – the content is copied before the analysis process takes place. That means that
the data is copied as it is stored in the source.
There's more...
Solr also allows us to do more with copy fields than a simple copying from one field to another.
Copying contents of dynamic fields to one field
You can also copy multiple fields' content to one field. To do that you should define a copy field
like so:
<copyField source="*_author" dest="authors"/>
The definition, like the one previously mentioned, would copy all of the fields that end with
_author to one field named authors. Remember that if you copy multiple fields to one
field, the destination field should be defined as multi-valued.
Limiting the number of characters copied
There may be situations where you only need to copy a defined number of characters from
one field to another. To do that we add the maxChars attribute to the copy field definition.
It can look like the following line of code:
<copyField source="author" dest="author_facet" maxChars="200"/>
The preceding definition tells Solr to copy up to 200 characters from the author field to the
author_facet field. This attribute can be very useful when copying the content of multiple
fields to one field.
Changing words to other words
Let's assume we have an e-commerce client and we are providing a search system based on
Solr. Our index has hundreds of thousands of documents which mainly consist of books. And
everything works fine! Then one day, someone from the marketing department comes into
your office and says that he wants to be able to find books that contain the word "machine"
when he types "electronics" into the search box. The first thing that comes to mind is, "Hey, I'll
do it in the source and index that". But that is not an option this time, because there can be
many documents in the database that have those words. We don't want to change the whole
database. That's when synonyms come into play and this recipe will show you how to use them.
www.it-ebooks.info
Analyzing Your Text Data
78
How to do it...
To make the example as simple as possible, I assumed that we only have two fields in our index.
1. Let's start by defining our index structure by adding the following field definition
section to the schema.xml file (just add it to your schema.xml file in the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="description" type="text_syn" indexed="true"
stored="true" />
2. Now let's add the text_syn type definition to the schema.xml file as shown in
the following code snippet:
<fieldType name="text_syn" class="solr.TextField">
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.SynonymFilterFactory" synonyms="synonyms.
txt" ignoreCase="true" expand="false" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. As you have noticed there is a file mentioned – synonyms.txt. Let's take a look at
its contents:
machine => electronics
The synonyms.txt file should be placed in the same directory as other
configuration files, which is usually the conf directory.
4. Finally we can look at the analysis page of the Solr administration panel to see if the
synonyms are properly recognized and applied:
www.it-ebooks.info
Chapter 3
79
How it works...
First we have our field definition. There are two fields, an identifier and a description. The second
one should be of interest r to us ight now. It's based on the new type text_syn which is shown
in the second listing.
Now about the new type, text_syn – it's based on the solr.TextField class. Its definition
is divided; it behaves in one way while indexing and in a different way while querying. So the
first thing we see is the query time analyzer definition. It consists of the tokenizer that splits the
data on the basis of whitespace characters, and then the lowercase filter converts all the tokens
to lowercase. The interesting part is the index time behavior. It starts with the same tokenizer,
but then the synonyms filter comes into play. Its definition starts like all the other filters – with
a factory definition. Next we have a synonyms attribute which defines which file contains the
synonyms definition. Following that we have the ignoreCase attribute which tells Solr to ignore
the case of the tokens and the contents of the synonyms file.
www.it-ebooks.info
Analyzing Your Text Data
80
The last attribute named expand is set to false. This means that Solr won't be expanding
the synonyms – all equivalent synonyms will be reduced to the first synonym in the line.
If the attribute is set to true, all synonyms will be expanded to all equivalent forms.
The example synonyms.txt file tells Solr that when the word "machine" appears in the field
based on the text_syn type it should be replaced by "electronics". But not vice versa. Each
synonym rule should be placed in a separate line in the synonyms.txt file. Also remember
that the file should be written in the UTF-8 file encoding. This is crucial and you should always
remember it because Solr will expect the file to be encoded in UTF-8.
As you can see in the provided screenshot from the Solr administration pages, the defined
synonym was properly applied during the indexing phase.
There's more...
There is one more thing associated to using synonyms in Solr.
Equivalent synonyms setup
Let's get back to our example for a second. What if the person from the marketing
department says that he/she wants not only to be able to find books that have the word
"machine" to be found when entering the word "electronics", but also all the books that
have the word "electronics", to be found when entering the word "machine". The answer
is simple. First, we would set the expand attribute (of the filter) to true. Then we would
change our synonyms.txt file to something like this:
machine, electronics
As I said earlier Solr would expand synonyms to equivalent forms.
Splitting text by CamelCase
Let's suppose that you run an e-commerce site with an electronic assortment. The marketing
department can be a source of many great ideas. Imagine that your colleague from this
department comes to you and says that they would like your search application to be able to
find documents containing the word "PowerShot" by entering the words "power" and "shot" into
the search box. So can we do that? Of course, and this recipe will show you how.
How to do it...
1. Let's start by creating the following index structure (add this to your schema.xml file
to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
www.it-ebooks.info
Chapter 3
81
<field name="description" type="text_split" indexed="true"
stored="true" />
2. To split text in the description field, we should add the following type definition
to the schema.xml file:
<fieldType name="text_split" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.WordDelimiterFilterFactory"
generateWordParts="1" splitOnCaseChange="1"/>
<filter class="solr.LowerCaseFilterFactory" />
</analyzer>
</fieldType>
3. Now let's index the following XML file:
<add>
<doc>
<field name="id">1</field>
<field name="description">TextTest</field>
</doc>
</add>
4. Finally, let's run the following query in the web browser:
http://localhost:8983/solr/select?q=description:test
You should get the indexed document as the response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:test</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="description">TextTest</str></doc>
</result>
</response>
www.it-ebooks.info
Analyzing Your Text Data
82
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml file.
This is pretty straightforward. We have two fields defined–one that is responsible for holding
information about the identifier (the id field) and the second one that is responsible for the
product description (the description field).
Next we see the interesting part. We name our type text_split and have it based on
a text type, solr.TextField. We also told Solr that we want our text to be tokenized
by whitespaces by adding the whitespace tokenizer (the tokenizer tag). To do what we
want to do–split by case change–we need more than this. Actually we need a filter named
WordDelimiterFilter which is created by the solr.WordDelimiterFilterFactory
class and a filter tag. We also need to define the appropriate behavior of the filter, so we
add two attributes – generateWordParts and splitOnCaseChange. The values of those
two parameters are set to 1 which means that they are turned on. The first attribute tells Solr
to generate word parts, which means that the filter will split the data on non-letter characters.
We also add the second attribute which tells Solr to split the tokens by case change.
What will that configuration do with our sample data? As you can see we have one document
sent to Solr. The data in the description field will be split into two words: text and test.
Please remember that we won't see the analyzed text in the Solr response, we only see the
stored fields and the original content of those, not the analyzed one.
Splitting text by whitespace only
One of the most common problems that you probably came across is having to split text
with whitespaces in order to segregate words from each other, to be able to process it further.
This recipe will show you how to do it.
How to do it...
1. Let's start with the assumption that we have the following index structure (add this
to your schema.xml file in the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="description_string" type="string" indexed="true"
stored="true" />
<field name="description_split" type="text_split" indexed="true"
stored="true" />
2. To split the text in the description field, we should add the following type definition:
<fieldType name="text_split" class="solr.TextField">
<analyzer>
www.it-ebooks.info
Chapter 3
83
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
</analyzer>
</fieldType>
3. To test our type, I've indexed the following XML file:
<add>
<doc>
<field name="id">1</field>
<field name="description_string">test text</field>
<field name="description_text">test text</field>
</doc>
</add>
4. Finally, let's run the following query in the web browser:
http://localhost:8983/solr/select?q=description_split:text
In the response to the preceding query, we got the indexed document:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description_split:text</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="description_string">test text</str>
<str name="description_split">test text</str></doc>
</result>
</response>
5. On the other hand, we won't get the indexed document in the response after
running the following query:
http://localhost:8983/solr/select?q=description_string:text
The response to the preceding query:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
www.it-ebooks.info
Analyzing Your Text Data
84
<str name="q">description_string:text</str>
</lst>
</lst>
<result name="response" numFound="0" start="0">
</result>
</response>
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml
file. This is pretty straightforward. We have three fields defined – one for the identifier of the
document (the id field), and one named description_string which is based on a string
field and thus not analyzed. The third one is the descriptionsplit
field which is based
on our text_split type and will be tokenized on the basis of whitespace characters.
Next we see the interesting part. We named our type text_split and had it based on a text
type – solr.TextField. We told Solr that we want our text to be tokenized by whitespaces
by adding a whitespace tokenizer (the tokenizer tag). Because there are no filters defined,
the text will only be tokenized by whitespace characters and nothing more.
That's why our sample data in the field description_text will be split into two words,
test and text. On the other hand, the text in the description_string field won't be
split. That's why the first example query will result in one document in the response, while
the second example won't find the example document. Please remember that we won't see
the analyzed text in the Solr response, we only see stored fields and we see the original
content of those, not the analyzed one.
Making plural words singular without
stemming
Nowadays it's nice to have stemming algorithms (algorithms that will reduce words to their
stems or root form) in your application, which will allow you to find the words such as cat
and cats by typing cat. But let's imagine you have a search engine that searches through
the contents of books in the library. One of the requirements is changing the plural forms
of the words from plural to singular – nothing less, nothing more. Can Solr do that? Yes,
the newest version can and this recipe will show you how to do that.
How to do it...
1. First of all let's start with a simple two field index (add this to your schema.xml
file to the field definition section):
www.it-ebooks.info
Chapter 3
85
<field name="id" type="string" indexed="true" stored="true"
required="true"/>
<field name="description" type="text_light_stem" indexed="true"
stored="true" />
2. Now let's define the text_light_stem type which should look like this (add this
to your schema.xml file):
<fieldType name="text_light_stem" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.EnglishMinimalStemFilterFactory" />
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. Now let's check the analysis tool of the Solr administration pages. You should
see that words such as ways and, keys are changed to their singular forms. Let's
check the for that words using the analysis page of the Solr administration pages:
www.it-ebooks.info
Analyzing Your Text Data
86
How it works...
First of all we need to define the fields in the schema.xml file. To do that we add the
contents from the first example into that file. It tells Solr that our index will consist of two
fields – the id field which will be responsible for holding information about the unique
identifier of the document, and the description file which will be responsible for holding
the document description.
The description field is actually where the magic is being done. We defined a new field type
for that field and we called it text_light_stem. The field definition consists of a tokenizer
and two filters. If you want to know how this tokenizer behaves please refer to the Splitting text
by whitespace only recipe in this chapter. The first filter is a new one. This is the light stemming
filter that we will use to perform minimal stemming. The class that enables Solr to use that
filter is solr.EnglishMinimalStemFilterFactory. This filter takes care of the process of
light stemming. You can see that using the analysis tools of the Solr administration panel. The
second filter defined is the lowercase filter – you can see how it works by referring to the How
to lowercase the whole string recipe in this chapter.
After adding this to your schema.xml file you should be able to use the light stemming
algorithm.
There's more...
Light stemming supports a number of different languages. To use the light stemmers for
your respective language, add the following filters to your type:
Language Filter
Russian solr.RussianLightStemFilterFactory
Portuguese solr.PortugueseLightStemFilterFactory
French solr.FrenchLightStemFilterFactory
German solr.GermanLightStemFilterFactory
Italian solr.ItalianLightStemFilterFactory
Spanish solr.SpanishLightStemFilterFactory
Hungarian solr.HungarianLightStemFilterFactory
Swedish solr.SwedishLightStemFilterFactory
Finish solr.FinnishLightStemFilterFactory
Indonesian solr.IndonesianStemFilterFactory
(with stemDerivational="false" attribute)
Norwegian solr.NorwegianLightStemFilterFactory
In the case of solr.IndonesianStemFilterFactory, you need to add the
stemDerivational="false" attribute in order to have it working as a light stemmer.
www.it-ebooks.info
Chapter 3
87
Lowercasing the whole string
Imagine you have a system where you only want to have perfect matches for names of
the documents. No matter what the cause of such a decision is, you would want such a
functionality. However there is one thing you would like to have – you would like your search
to be case independent, so it doesn't matter if the document or query is lower cased or
uppercased. Can we do something with that in Solr? Of course Solr can do that, and this
recipe will describe how to do it.
How to do it...
1. We start by defining the following index structure (add this to your schema.xml file
in the field definition section):
<field name="id " type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="string_lowercase" indexed="true"
stored="true" />
<field name="description" type="text" indexed="true" stored="true"
/>
2. To make our strings lowercase, we should add the following type definition to the
schema.xml file:
<fieldType name="string_lowercase" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. In order to test if everything is working as it should we need to index the following
XML file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook</field>
<field name="description">Simple description</field>
</doc>
</add>
4. Then we will run the following query in the web browser:
http://localhost:8983/solr/select?q=name:"solr cookbook"
You should get the indexed document in response. You should also be able to get the
indexed document in response to the following query:
http://localhost:8983/solr/select?q=name:"solr Cookbook"
www.it-ebooks.info
Analyzing Your Text Data
88
How it works...
Let's see how things work. First of all we have the field definition part of the schema.xml file.
This is pretty straightforward. We have three fields defined. First, the field named id which is
responsible for holding our unique identifier. The second one is the name field which is actually
our lowercased string field. The third field will hold the description of our documents and is
based on the standard text type defined in the example Solr deployment.
Now let's get back to our name field. It's based on the string_lowercase type. The string_
lowercase type consists of an analyzer which is defined as a tokenizer and one filter. The
solr.KeywordTokenizerFactory filter tells Solr that the data in that field should not be
tokenized in any way. It just should be passed as a single token to the token stream. Next we
have our filter, which changes all the characters to their lowercased equivalents. And that's how
this field analysis is performed.
The example queries show how the field behaves. It doesn't matter if you type lowercase or
uppercase characters, the document will be found anyway. What matters is that you must type
the whole string as it is because we used the keyword tokenizer which, as I already said, is not
tokenizing but just passing the whole data through the token stream as a single token.
Storing geographical points in the index
Imagine that up till now your application stores information about companies – not much
information, just unique identification and the company name. But now, your client wants
to store the location of the companies. In addition to that, your users would like to sort by
distance and filter by distance from a given point. Is this doable with Solr? Of course it is
and this recipe will show you how to do it.
How to do it...
1. For the purpose of this recipe, let's create a sample index structure. To do this, describe
the companies that we store in the index with three fields which are defined as follows
(add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="location" type="location" indexed="true"
stored="true" />
www.it-ebooks.info
Chapter 3
89
2. Next we will also add one dynamic field (add this to your schema.xml file in the field
definition section):
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
3. The next step is to define the location type which should look like the following code:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
4. In addition to that, we will need the tdouble field type, which should look like the
following code:
<fieldType name="tdouble" class="solr.TrieDoubleField"
precisionStep="4" positionIncrementGap="0"/>
5. The next step is to create the example data looking like the following code (I named
the data file task9.xml):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr.pl company</field>
<field name="location">54.02,23.10</field>
</doc>
</add>
6. And now let's index our data. To do that, we run the following command from the
exampledocs directory (put the task9.xml file there):
java -jar post.jar task9.xml
7. After indexing we should be able to use the query, such as the following one, to get
our data:
http://localhost:8983/solr/select?q=*:*&fq={!geofilt
sfield=location}&pt=54.00,23.00&d=10
The response should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="pt">54.00,23.00</str>
www.it-ebooks.info
Analyzing Your Text Data
90
<str name="d">10</str>
<str name="fq">{!geofiltsfield=location}</str>
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr.pl company</str>
<str name="location">54.02,23.10</str>
</doc>
</result>
</response>
How it works...
First of all we have three fields and one dynamic field defined in our schema.xml file. The first
field is the one responsible for holding the unique identifier. The second one holds the name of
the company. The third one named location is responsible for holding geographical points
and is based on the location type. The dynamic field – *_coordinate will be used internally
by our location type. It uses the tdouble field which was taken from the schema.xml file
distributed with Solr.
Next we have our location type definition. It's based on the solr.LatLonType class which
is specially designed for spatial search and is defined by a single attribute – subFieldSuffix.
That attribute specifies which fields (in our case it's the dynamic *_coordinate field) will be
used internally for holding the actual values of latitude and longitude.
So how does this type of field actually work? When defining a two-dimensional field, like we
did, there are actually three fields created in the index. The first field is named like the field we
added in the schema.xml file, so in our case it is location. This field will be responsible for
holding the stored value of the field. And one more thing – this field will only be created when
we set the field attribute store to true.
The next two fields are based on the defined dynamic field. Their names will be location
_0_coordinate and location_1_coordinate in our case. First we have the field
name, the _ character, then the index of the value, and finally the suffix defined by the
subFieldSuffix attribute of the type.
We can now look at the way the data is indexed. Please take a look at the example data file.
You can see that the values in each pair are separated by the comma character, and that's
how you can add the data to the index:
http://localhost:8983/solr/select?q=*:*&fq={!geofilt sfield=location}
&pt=54.00,23.00&d=10
www.it-ebooks.info
Chapter 3
91
Querying is a bit different. We send a query to retrieve all the documents from the index
(q=*:*). In addition to that, we want to filter the results by distance (the geofilt filter) with
the use of the location field (sfield=location). fq={!geofiltsfield=location}
uses the Solr local params syntax to send a distance filter. It can look strange comparing it to
a standard query, but it works. In addition to that, we've specified the point we will calculate
the distance from (the pt parameter) as 54.00,23.00. This is a pair of latitude and longitude
values separated by a comma character. The last parameter is d, which specifies the maximum
distance that documents can be, from the given point, to be considered as a match. We
specified it as 10 kilometers (d=10). As you can see, even though our document had its point
defined as 54.02,23.10 we found it with our query because of the distance we specified.
Stemming your data
One of the most common requirements I meet is stemming – the process of reducing the
word to their root form (or stems). Let's imagine the book e-commerce store, where you store
the books' names and descriptions. We want to be able to find words such as shown or showed
when you type the word show and vice versa. To achieve that we can use stemming algorithms.
This recipe will show you how to add stemming to your data analysis.
How to do it...
1. We need to start with the index structure. Let's assume that our index consists
of three fields (add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="description" type="text_stem" indexed="true"
stored="true" />
2. Now let's define our text_stem type which should look like the following code:
<fieldType name="text_stem" class="solr.TextField">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.SnowballPorterFilterFactory" />
</analyzer>
</fieldType>
3. Now we can index our data – to do that we need to create an example data file,
for example, the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
www.it-ebooks.info
Analyzing Your Text Data
92
<field name="description">This is a book that I'll show</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr cookbook 2</field>
<field name="description">This is a book I showed</field>
</doc>
</add>
4. After indexing, we can test how our data was analyzed. To do that, let's run the
following query:
http://localhost:8983/solr/select?q=description:show
The result we get from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:show</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<arr name="description">
<str>This is a book that I'll show</str>
</arr>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr cookbook 2</str>
<arr name="description">
<str>This is a book I showed</str>
</arr>
</doc>
</result>
</response>
www.it-ebooks.info
Chapter 3
93
That's right, Solr found two documents matching the query which means that our fields and
types are working as intended.
How it works...
Our index consists of three fields; one holding the unique identifier of the document, the
second one holding the name of the document, and the third one holding the document
description. The last field is the field that will be stemmed.
The stemmed field is based on a Solr text field and has an analyzer that is used at query
and indexing time. It is tokenized on the basis of the whitespace characters, and then the
stemming filter is used. What does the filter do? It tries to bring the words to its root form,
which means that words such as shows, showing, and show will all be changed to show
– or at least they should be changed to that form.
Please note that in order to properly use stemming algorithms they should be used at query
and indexing time. This is a must because of the stemming results.
As you can see, our test data consists of two documents. Take a look at the description.
One of the documents contains the word showed and the other has the word show in
their description fields. After indexing and running the sample query, Solr would return
two documents in the results which means that the stemming did its job.
There's more...
There are too many languages that have stemming support integrated into Solr to mention
them all. If you are using a language other than English, please refer to the http://wiki.
apache.org/solr/LanguageAnalysis page of the Solr Wiki to find the appropriate filter.
Preparing text to perform an efficient
trailing wildcard search
Many users coming from traditional RDBMS systems are used to wildcard searches. The most
common of them are the ones using * characters which means zero or more characters. You
have probably seen searches like the one as follows:
AND name LIKE 'ABC12%'
So how to do that with Solr and not kill our Solr server? This task will show you how to prepare
your data and make efficient searches.
www.it-ebooks.info
Analyzing Your Text Data
94
How to do it...
1. The first step is to create a proper index structure. Let's assume we have the following
one (add this to your schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="string_wildcard" indexed="true"
stored="true" />
2. Now, let's define our string_wildcard type (add this to the schema.xml file):
<fieldType name="string_wildcard" class="solr.TextField">
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.EdgeNGramFilterFactory" minGramSize="1"
maxGramSize="25" side="front"/>
</analyzer>
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
</analyzer>
</fieldType>
3. The third step is to create the example data which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">XYZ1234ABC12POI</field>
</doc>
</add>
4. Now send the following query to Solr:
http://localhost:8983/solr/select?q=name:XYZ1
The Solr response for the previous query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:XYZ1</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
www.it-ebooks.info
Chapter 3
95
<str name="id">1</str>
<str name="name">XYZ1234ABC12POI</str>
</doc>
</result>
</response>
As you can see, the document has been found, so our setup is working as intended.
How it works...
First of all let's look at our index structure defined in the schema.xml file. We have two fields
– one holding the unique identifier of the document (the id field) and the second one holding
the name of the document (the name field) which is actually the field we are interested in.
The name field is based on the new type we defined – string_wildcard. This type is
responsible for enabling trailing wildcards, the ones that will enable the LIKE 'WORD%' SQL
queries. As you can see the field type is divided into two analyzers, one for the data analysis
during indexing and the other for query processing. The querying analyzer is straight; it just
tokenizes the data on the basis of whitespace characters. Nothing more, nothing less.
Now the indexing time analysis (of course we are talking about the name field). Similar to
the query time, during indexing the data is tokenized on the basis of whitespace characters,
but there is also an additional filter defined. The solr.EdgeNGramFilterFactory class
is responsible for generating the filter called n-grams. In our setup, we tell Solr that the
minimum length of an n-gram is 1 (the minGramSize attribute) and the maximum length is
25 (the maxGramSize attribute). We also defined that the analysis should be started from the
beginning of the text (the side attribute set to front). So what would Solr do with our example
data? It will create the following tokens from the example text: X, XY, XYZ, XYZ1, XYZ12, and
so on. It will create tokens by adding the next character from the string to the previous token,
up to the maximum length of the n-gram filter that is given in the filter configuration.
So by typing the example query, we can be sure that the example document will be found
because of the n-gram filter defined in the configuration of the field. We also didn't define
the n-gram filter in the querying stage of analysis because we didn't want our query to be
analyzed in such a way that it could lead to false positive hits.
This functionality, as described, can also be used successfully to provide autocomplete
features to your application (if you are not familiar with the autocomplete feature please
take a look at http://en.wikipedia.org/wiki/Autocomplete).
Please remember that using n-grams will make your index a bit larger. Because of that you
should avoid having n-grams on all the fields in the index. You should carefully decide which
fields should use n-grams and which should not.
www.it-ebooks.info
Analyzing Your Text Data
96
There's more...
If you would like your field to be able to simulate SQL LIKE '%ABC' queries, you should
change the side attribute of the solr.EdgeNGramFilterFactory class to the back
value. The configuration should look like the following code snippet:
<filter class="solr.EdgeNGramFilterFactory" minGramSize="1"
maxGramSize="25" side="back"/>
It would change the end from which Solr starts to analyze the data. In our case it would start
from the end, and thus would produce n-grams as follows: I, OI, POI,2POI, 12POI, and so
on.
See also
ff If you want to propose another solution for that kind of search, please refer to the
recipe Splitting text by numbers and non-whitespace characters in this chapter
Splitting text by numbers and
non-whitespace characters
Analyzing the text data is not only about stemming, removing diacritics (if you are not familiar
with the word, please take a look at http://en.wikipedia.org/wiki/Diacritic), and
choosing the right format for the data. Let's assume that our client wants to be able to search
by words and numbers that construct product identifiers. For example, he would like to be able
to find the product identifier ABC1234XYZ by using ABC, 1234, or XYZ.
How to do it...
1. Let's start with the index that consists of three fields (add this to your schema.xml
file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="description" type="text_split" indexed="true"
stored="true" />
2. The second step is to define our text_split type which should look like the
following code (add this to your schema.xml file):
<fieldType name="text_split" class="solr.TextField">
<analyzer>
www.it-ebooks.info
Chapter 3
97
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.WordDelimiterFilterFactory"
generateWordParts="1" generateNumberParts="1" splitOnNumerics="1"
/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
3. Now you can index your data. To do that let's create an example data file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Test document</field>
<field name="description">ABC1234DEF BL-123_456
adding-documents</field>
</doc>
</add>
4. After indexing we can test how our data was analyzed. To do that let's run the
following query:
http://localhost:8983/solr/select?q=description:1234
Solr found our document which means that our field is working as intended.
The response from Solr will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">description:1234</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Test document</str>
<str name="description">ABC1234DEF BL-123_456 addingdocuments</
str></doc>
</result>
</response>
www.it-ebooks.info
Analyzing Your Text Data
98
How it works...
We have our index defined as three fields in the schema.xml file. We have a unique identifier
(an id field) indexed as a string value. We have a document name (the name field) indexed as
text (type which is provided with the example deployment of Solr), and a document description
(a description field) which is based on the text_split field which we defined ourselves.
Our type is defined to make the same text analysis, both on query time and on index time.
It consists of the whitespace tokenizer and two filters. The first filter is where the magic is
done. The solr.WordDelimiterFilterFactory behavior, in our case, is defined by
the following parameters:
ff generateWordParts: If this parameter is set to 1, it tells the filter to generate
parts of the word that are connected by non-alphanumeric characters such as
the dash character. For example, token ABC-EFG would be split into ABC and EFG.
ff generateNumberParts: If this parameter is set to 1, it tells the filter to generate
words from numbers connected by non-numeric characters, such as the dash
character. For example, token 123-456 would be split into 123 and 456.
ff splitOnNumerics: If this parameter is set to 1, it tells the filter to split letters
and numbers from each other. This means that token ABC123 would be split in
to ABC and 123.
The second filter is responsible for changing the words that lowercased the equivalents and
is discussed in the recipe How to lowercase the whole string in this chapter.
Therefore, after sending our test data to Solr we can run the example query to see if we
defined our filter properly. In addition, you probably know the result; yes, the result will contain
one document – the one that we send to Solr. That is because the word ABC1234DEF is split
into ABC, 1234, and DEF tokens, and thus can be found by the example query.
There's more...
In case you would like to preserve the original token that is passed to solr.
WordDelimiterFilterFactory, add the following attribute to the filter definition:
preserveOriginal="1"
See also
ff If you would like to know more about solr.WordDelimiterFilterFactory,
please refer to the recipe Splitting text by CamelCase in this chapter
www.it-ebooks.info
Chapter 3
99
Using Hunspell as a stemmer
Solr supports numerous stemmers for various languages. You can use various stemmers for
English, and there are ones available for French, German, and most of the European languages.
But sometimes they provide stemming results that are not of great quality. Alternatively, maybe
you are wondering if there is a stemmer out there that supports your language, which is not
included in Solr. No matter what the reason, if you are looking for a different stemmer you
should look at the Hunspell filter if it suits your needs, and this recipe will show you how to
use it in Solr.
Getting ready
Before starting, please check the http://wiki.openoffice.org/wiki/Dictionaries
page to see if Hunspell supports your language.
How to do it...
1. We should start by creating an index structure (just add the following entries to the
fields section of your schema.xml file) which looks like the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text_english" indexed="true"
stored="true"/>
<field name="description" type="text_english" indexed="true"
stored="true" />
2. Now we should define the text_english type as follows (if you don't have it in your
schema.xml file, please add it to the types section of the file):
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
3. Let's assume that we are not satisfied with the quality of solr.
PorterStemFilterFactory and we would like to have that improved
by using Hunspell. In order to do that, we need to change the solr.
PorterStemFilterFactory definition to the following one:
<filter class="solr.HunspellStemFilterFactory" dictionary="en_
GB.dic" affix="en_GB.aff" ignoreCase="true" />
www.it-ebooks.info
Analyzing Your Text Data
100
So the final text_english type configuration would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.HunspellStemFilterFactory" dictionary=
"en_GB.dic" affix="en_GB.aff" ignoreCase="true" />
</analyzer>
</fieldType>
4. The last thing we need to do is place the en_GB.dic and en_GB.aff files in the
Solr conf directory (the one where you have all your configuration files stored). Those
files can be found at the http://wiki.openoffice.org/wiki/Dictionaries
page. They are the dictionaries for English used in Great Britain. And that's all;
nothing more needs to be done.
How it works...
Our index structure is very simple – it contains three fields of which two (name and
description) are used for full text searching, and we want those fields to use the text_
english field type and thus use solr.HunspellStemFilterFactory for stemming.
The configuration of the solr.HunspellStemFilterFactory filter factory is not difficult.
Of course, there are a few attributes of the filter tag that need to be specified:
ff class: This specifies the class implementing the filter factory we want to use,
which in our case is solr.HunspellStemFilterFactory.
ff dictionary: This specifies the name of the .dic file of the dictionary we want
to use.
ff affix: This specifies the name of the .aff file of the dictionary we want to use.
ff ignoreCase: This is used to ignore cases when matching words against the
dictionary. In our case, we want to ignore cases.
The last thing we need to do is provide Solr with the dictionary files so that the Hunspell filter
can do its work. Although this is simple, this part is crucial. The dictionaries define how well
Hunspell will work. Before using a new dictionary, you should always properly conduct A/B
testing and see if things did not get worse in your case.
One last thing about the dictionaries. If you would like to use other languages with Hunspell,
the only thing you will need to do is provide the new dictionary file and change the name
of the dictionaries, so change the dictionary and affix attributes of the solr.
HunspellStemFilterFactory definition.
www.it-ebooks.info
Chapter 3
101
Using your own stemming dictionary
Sometimes, stemmers provided with Lucene and Solr don't do what you would like them
to do. That's because most of them are based on an algorithmic approach and even the
best algorithms can come to a place where you won't like the results of their work and you
would like to make some modifications. Of course, modifications to the algorithm code can
be challenging and we don't usually do that. The good thing is that Solr supports a method
of overriding the stemmer work and this recipe will show you how to use it.
Getting ready
Before we continue please remember that the method described in this recipe may not work
with custom stemmers that are not provided with Solr.
How to do it... Let's say that we want some of the words to be stemmed in a way we want. For
example,
we want the word dogs to be stemmed as doggie (of course that's only an example).
1. What we have to do first is write the words dogs and doggie in a file (let's call it
override.txt). Words should be separated from each other by a tab character
and each line of the file should contain a single stemming overwrite. For example,
our override.txt file could look like this:
dogs doggie
2. Now we should put the override.txt file in the same directory as the schema.
xml file (usually its conf). Please remember to have that file written in UTF-8
encoding. If you have characters from the classic ASCII character set, they won't
be recognized properly if you don't use UTF-8.
3. Next we need to add the solr.StemmerOverrideFilterFactory filter to
our text types. I assume we only use text_english with the following definition
(put the following definition to your types section of the schema.xml file):
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
www.it-ebooks.info
Analyzing Your Text Data
102
In order for our list of protected words to work, we need to put solr.
StemmerOverrideFilterFactory before the stemming, which is solr.
PorterStemFilterFactory in our case. The final type definition for text_
english would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.StemmerOverrideFilterFactory"
dictionary="dict.txt" />
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
This is what the analysis page of the Solr administration pages shows:
4. That's all. Now, the fields that are based on the text_english type will not
be stemmed.
www.it-ebooks.info
Chapter 3
103
How it works...
The work of the solr.StemmerOverrideFilterFactory class is simple – it changes the
words we want it to change and then marks them as protected so that the stemmer won't do
any further processing of those words. In order for this functionality to work properly, you should
remember to put solr.StemmerOverrideFilterFactory before any stemmers in your
analysis chain.
The actual configuration of solr.StemmerOverrideFilterFactory is pretty simple and
similar to other filters. It requires two attributes; the usual class attribute, which informs Solr
which filter factory should be used in order to create the filter, and the dictionary attribute,
which specifies the name of the file containing the dictionary that we want to use
for our custom stemming.
Looking at the analysis page of the Solr administration pages, we can see that our dogs
word was protected from being stemmed with the default stemmer and changed to what
we wanted, that is, doggie.
Protecting words from being stemmed
Sometimes, the stemming filters available in Solr do more than you would like them to do.
For example, they can stem brand names or the second name of a person. Sometimes, you
would like to protect some of the words that have a special meaning in your system or you
know that some words would cause trouble to a stemmer or stemmers. This recipe will show
you how to do it.
Getting started
Before we continue, please remember that the method described in this recipe may not work
with custom stemmers that are not provided with Solr.
How to do it...
In order to have the defined words protected we need a list of them. Let's say that we don't
want the words cats and dogs to be stemmed.
1. To achieve that, we should start by writing the words we want to be protected from
stemming into a file. Let's create the file called dontstem.txt with the following
contents:
cats
dogs
www.it-ebooks.info
Analyzing Your Text Data
104
2. Now let's put the created file in the same directory as the schema.xml file (usually
it's the conf directory). Please remember to have that file written in UTF-8 encoding.
If you have characters from the classic ASCII character set they won't be recognized
properly if you don't use UTF-8.
3. Now, we need to add the solr.KeywordMarkerFilterFactory filter to our text
types. I assume we only use the text_english type with the following definition:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
In order for our list of protected words to work, we need to put solr.
KeywordMarkerFilterFactory before the stemming, which is solr.
PorterStemFilterFactory in our case. So the final type definition for the text_
english type would look like the following code:
<fieldType name="text_english" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.KeywordMarkerFilterFactory"
protected="dontstem.txt" />
<filter class="solr.PorterStemFilterFactory"/>
</analyzer>
</fieldType>
www.it-ebooks.info
Chapter 3
105
This is what the analysis page of the Solr administration pages shows:
4. That's all. Now, the fields that are based on the text_english type won't
be stemmed.
www.it-ebooks.info
Analyzing Your Text Data
106
How it works...
The whole idea is pretty simple. With the use of solr.KeywordMarkerFilterFactory,
we mark the protected words and that information is used by the stemmers available in Solr
and Lucene. In order for this functionality to work properly, you should remember to put the
solr.KeywordMarkerFilterFactory filter before any stemmers in your analysis chain.
The actual configuration of solr.KeywordMarkerFilterFactory is pretty simple and
similar to other filters. It requires two attributes; the usual class attribute, which informs
Solr which filter factory should be used in order to create the filter, and the attribute protected
which specifies the name of the file containing words that we want to protect from stemming.
Looking at the analysis page of the Solr administration pages, we can see that our
dogs word was protected from being stemmed, compared to the birds word which
was changed to bird.
www.it-ebooks.info
4
Querying Solr
In this chapter, we will cover:
ff Asking for a particular field value
ff Sorting results by a field value
ff How to search for a phrase, not a single word
ff Boosting phrases over words
ff Positioning some documents over others in a query
ff Positioning documents with words closer to each other first
ff Sorting results by a distance from a point
ff Getting documents with only a partial match
ff Affecting scoring with functions
ff Nesting queries
ff Modifying returned documents
ff Using parent-child relationships
ff Ignoring typos in terms of the performance
ff Detecting and omitting duplicate documents
ff Using field aliases
ff Returning a value of a function in the results
www.it-ebooks.info
Querying Solr
108
Introduction
Making a simple query is not a hard task, but making a complex one, with faceting, local
params, parameters dereferencing, and phrase queries can be a challenging task. On the top
of all that, you must remember to write your query with performance factors in mind. That's
why something that is simple at first sight can turn into something more challenging such as
writing a good, complex query. This chapter will try to guide you through some of the tasks you
may encounter during your everyday work with Solr.
Asking for a particular field value
There are many cases where you will want to ask for a particular field value. For example,
when searching for the author of a book in the Internet library or an e-commerce shop. Of
course Solr can do that, and this recipe will show you how to do it.
How to do it...
1. Let's start with the following index structure (just add the following to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. To ask for a value in the author field, send the following query to Solr:
http://localhost:8983/solr/select?q=author:rafal
That's all. The documents you'll get from Solr will be the ones with the requested value in the
author field. Remember that the query shown in the example uses the standard query parser,
not DisMax.
How it works...
We defined three fields in the index, but this was only for the purpose of the example. As
you can see in the previous query, to ask for a particular field value, you need to send a q
parameter syntax such as FIELD_NAME:VALUE, and that's all there is to it. Of course you can
add the logical operator to the query to make it more complex. Remember that if you omit the
field name from the query your values will be checked again in the default search field that is
defined in the schema.xml file.
www.it-ebooks.info
Chapter 4
109
There's more...
When asking for a particular field value, there are a few things that are useful to know:
Querying for a particular value using the DisMax query parser
Sometimes you may need to ask for a particular field value when using the DisMax query parser.
Unfortunately the DisMax query parser doesn't support full Lucene query syntax and thus you
can't send a query like that, but there is a solution to it. You can use the extended DisMax query
parser which is an evolved DisMax query parser. It has the same list of functionalities as DisMax
and it also supports full Lucene query syntax. The following is the query shown in this task, but
by using edismax, it would look like the following:
http://localhost:8983/solr/select?q=author:rafal&defType=edismax
Querying for multiple values in the same field
You may sometimes need to ask for multiple values in a single field. For example, let's
suppose that you want to find the solr and cookbook values in the title field. To do
that you should run the following query (notice the brackets surrounding the values):
http://localhost:8983/solr/select?q=author:(solr cookbook)
Sorting results by a field value
Imagine an e-commerce site where you can't choose the sorting order of the results, you
can only browse the search results page-by-page and nothing more. That's terrible, right?
That's why with Solr you can specify the sort fields and order in which your search results
should be sorted. This recipe will show you how to do it.
How to do it...
Let's assume that you want to sort your data by an additional field, for example, the field that
contains the name of the author of the book.
1. First we add the following to your schema.xml file's field section:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. Now, let's create a simple data file which will look like the following code:
<add>
<doc>
www.it-ebooks.info
Querying Solr
110
<field name="id">1</field>
<field name="title">Solr cookbook</field>
<field name="author">Rafal Kuc</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Solr results</field>
<field name="author">John Doe</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr perfect search</field>
<field name="author">John Doe</field>
</doc>
</add>
3. As I wrote earlier, we want to sort the result list by author name in ascending order.
Additionally, we want the books that have the same author to be sorted by relevance
in the descending order. To do that we must send the following query
to Solr:
http://localhost:8983/solr/select?q=solr&sort=author+asc,
score+desc
The results returned by Solr are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">solr</str>
<str name="sort">author asc,score desc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="title">Solr results</str>
<str name="author">John Doe</str>
</doc>
<doc>
<str name="id">3</str>
www.it-ebooks.info
Chapter 4
111
<str name="title">Solr perfect search</str>
<str name="author">John Doe</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solr cookbook</str>
<str name="author">Rafal Kuc</str>
</doc>
</result>
</response>
As you can see, our data is sorted exactly how we wanted it to be.
How it works...
As you can see I defined three fields in our index. The most interesting to us is the author
field, based on which we will perform the sorting operation. Notice one thing – the type on
which the field is based is the string type. In order to sort the values of the field from the
index, you need to prepare your data well, that is, use the appropriate number types (the ones
based on the Trie types), and to sort the text field using the string field type (or text type
using the KeywordTokenizer type and a lowercase filter).
The following what you see is the data which is very simple – it only adds three documents to
the index.
I've added one additional parameter to the query that was sent to Solr – the sort parameter.
This parameter defines the sort field with the order. Each field must be followed by the order in
which the data should be sorted; asc which tells Solr to sort the data in the ascending order,
and desc which tells Solr to sort in the descending order. Pairs of field and order should be
delimited with the comma character as shown in the example.
The result list that Solr returned tells us that we did a perfect job on defining the sort order.
How to search for a phrase, not a single
word
Imagine that you have an application that searches within millions of documents that
are generated by a law company. One of the requirements is to search the titles of the
documents as a phrase, but with stemming and lowercasing. So a string-based field is
not an option. In that case, is it possible to achieve this using Solr? Yes, and this recipe will
show you how to do that.
www.it-ebooks.info
Querying Solr
112
How to do it...
1. First let's define the following type (add this part to your schema.xml file):
<fieldType name="text" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.SnowballPorterFilterFactory"
language="English"/>
</analyzer>
</fieldType>
2. Now let's add the following fields to our schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
3. The third step is to create an example data which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">2012 report</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">2009 report</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">2012 draft report</field>
</doc>
</add>
4. Now let's try to find the documents that have the phrase 2012 report in them.
To do that, make the following query to Solr:
http://localhost:8983/solr/select?q=title:"2012 report"
The result should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Chapter 4
113
<int name="QTime">1</int>
<lst name="params">
<str name="q">title:"2012 report"</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="title">2012 report</str>
</doc>
</result>
</response>
The debug query (the debugQuery=on parameter) shows us the Lucene query that
was created:
<str name="parsedquery">PhraseQuery(title:"2012 report")</str>
As you can see we only got one document which is perfectly good. Now let's see how
that happened.
How it works...
As I said in the Introduction section, our requirement was to search for phrases over fields
that are stemmed and lowercased. If you want to know more about stemming please refer
to the Stemming your data recipe in Chapter 3, Analyzing Your Text Data. Lowercasing is
described in the Lowercasing the whole string recipe in Chapter 3.
We only need two fields because we will only search the title, and return the title and
unique identifier of the field; thus the configuration is as shown in the example.
The example data is quite simple so I'll skip commenting on it.
The query is something that we should be more interested in. The query is made to the standard
Solr query parser, thus we can specify the field name and the value we are looking for. The query
differs from the standard word searching query by the use of the " character at the start and
end of the query. It tells Solr to use the phrase query instead of the term query. Using the phrase
query means that Solr will search for the whole phrase not a single word. That's why only the
document with identifier 1 was found, because the third document did not match the phrase.
The debug query only ensured that the phrase query was made instead of the usual term
query, and Solr showed us that we created the right query.
www.it-ebooks.info
Querying Solr
114
There's more...
When using queries there is one thing that is very useful to know.
Defining the distance between words in a phrase
You may sometimes need to find documents that match a phrase, but are separated by some
other words. Let's assume that you would like to find the first and third document in
our example. This means that you want documents that could have an additional word
between the word 2010 and report. To do that, we add a so-called phrase slop to the
phrase. In our case the distance (slop) between words can be the maximum of one word, so
we add the ~1 part after the phrase definition:
http://localhost:8983/solr/select?q=title:"2012 report"~1
Boosting phrases over words
Imagine you are a search expert at a leading e-commerce shop in your region. One day
disaster strikes and your marketing department says that the search results are not good
enough. They would like you to favor documents that have the exact phrase typed by the
user over the documents that have matches for separate words. Can you do it? Of course
you can, and this recipe will show you how to achieve it.
Getting ready
Before you start reading this task I suggest you read the How to search for a phrase not a
single word recipe in this chapter. It will allow you to understand the recipe better.
How to do it...
I assume that we will be using the DisMax query parser, not the standard one. We will also
use the same schema.xml file that was used in the How to search for a phrase not a single
word recipe in this chapter.
1. Let's start with our sample data file which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">Annual 2012 report last draft</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">2011 report</field>
www.it-ebooks.info
Chapter 4
115
</doc>
<doc>
<field name="id">3</field>
<field name="title">2012 draft report</field>
</doc>
</add>
2. As I already mentioned, we would like to boost those documents that have phrase
matches over others matching the query. To do that, run the following query to your
Solr instance:
http://localhost:8983/solr/select?defType=dismax&pf=title^100&q=20
12+report&qf=title&q.op=AND
You should get the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="pf">title^100</str>
<str name="q">2012 report</str>
<str name="qf">title</str>
<str name="q.op">AND</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="title">Annual 2012 report last draft</str>
</doc>
<doc>
<str name="id">3</str>
<str name="title">2012 draft report</str>
</doc>
</result>
</response>
3. To visualize the results better, I decided to include the results returned by Solr for the
same query but without adding the pf parameter, and received the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
www.it-ebooks.info
Querying Solr
116
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="qf">title</str>
<str name=" defType">dismax</str>
<str name="q.op">AND</str>
<str name="q">2012 report</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">3</str>
<str name="title">2012 draft report</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Annual 2012 report last draft</str>
</doc>
</result>
</response>
As you can see we fulfilled our requirement.
How it works...
Some of the parameters that are present in the example query may be new to you. The first
parameter is defType that tells Solr which query parser we will be using. In this example we
will be using the DisMax query parser (if you are not familiar with the DisMax query parser
please have a look at the following address http://wiki.apache.org/solr/DisMax).
One of the features of this query parser is the ability to tell what field should be used to search
for phrases, and we do that by adding the pf parameter. The pf parameter takes a list of
fields with the boost that corresponds to them, for example, pf=title^100 which means
that the phrase found in the title field will be boosted with a value of 100. The q parameter is
the standard query parameter that you are familiar with. This time we passed the words we
are searching for and the logical operator AND. This means that we are looking for documents
which contain the words 2012 and report. You should remember that you can't pass queries
such as fieldname:value to the q parameter and use the DisMax query parser. The fields
you are searching against should be specified using the qf parameter. In our case we told Solr
that we will be searching against the title field. We also included the q.op=AND parameter
because we want AND to be our logical operator for the query.
The results show us that we found two documents. The one that matches the exact query is
returned first and that is what we intended to achieve.
www.it-ebooks.info
Chapter 4
117
There's more...
You can of course boost phrases with standard query parsers, but that's not as elegant as the
DisMax query parser method. To achieve similar results, you should run the following query to
your Solr instance:
http://localhost:8983/solr/select?q=title:(2012+AND+report)+OR+title:
"2012+report"^100
The above query tells Solr to search for the words 2010 and report in a title field, and
search for a 2012 report phrase and, if found, to boost that phrase with the value of 100.
Positioning some documents over others on
a query
Imagine a situation when your client tells you that he/she wants to promote some of his/her
products by placing them at the top of the search result list. Additionally, the client would like
the product list to be flexible, that is, he/she would like to be able to define the list for some
queries and not for others. Many thoughts come into your mind such as boosting, index time
boosting, or maybe some special field to achieve that. But don't bother, Solr can help you with
a component that is known as solr.QueryElevationComponent.
How to do it...
The following recipe will help you to place document over others based on your priorities:
1. First of all let's modify the solrconfig.xml document. We need to add
the component definition. To do that add the following section to your
solrconfig.xml file:
<searchComponent name="elevator" class="solr.
QueryElevationComponent" >
<str name="queryFieldType">string</str>
<str name="config-file">elevate.xml</str>
</searchComponent>
2. Now let's add the proper request handler that will include the elevation component.
We will name it /promotion. Add this to your solrconfig.xml file:
<requestHandler name="/promotion" class="solr.SearchHandler">
<lst name="defaults">
www.it-ebooks.info
Querying Solr
118
<str name="echoParams">explicit</str>
<int name="rows">10</int>
<str name="df">name</str>
</lst>
<arr name="last-components">
<str>elevator</str>
</arr>
</requestHandler>
3. You may notice that the query elevation component contained information about
a mysterious elevate.xml file. Let's assume that we want the documents with
identifiers 3 and 1 to be in the first two places in the results list for the solr
query. For now you need to create that file in the configuration directory of your
Solr instance and paste the following content:
<?xml version="1.0" encoding="UTF-8" ?>
<elevate>
<query text="solr">
<doc id="3" />
<doc id="1" />
</query>
</elevate>
4. Now it's time for the schema.xml file. Our field definition part of the file should
contain the following code:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
5. Now let's index the following data file:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr master pieces</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr annual report</field>
</doc>
</add>
www.it-ebooks.info
Chapter 4
119
6. Now we can run Solr and test our configuration. To do that let's run the following query:
http://localhost:8983/solr/promotion?q=solr
The previous query should return the following result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">solr</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Solr annual report</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr master pieces</str>
</doc>
</result>
</response>
The query without using the elevation component returned the following result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">solr</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
</doc>
www.it-ebooks.info
Querying Solr
120
<doc>
<str name="id">2</str>
<str name="name">Solr master pieces</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Solr annual report</str>
</doc>
</result>
</response>
As you can see the component worked. Now let's see how it works.
How it works...
The first part of the configuration defines a new search component with a name under
which the component will be visible to other components and the search handler (the
name attribute). In our case the component name is elevator and it's based on the
solr.QueryElevationComponent class (the class attribute). The following that
we have are two additional attributes that define the elevation component's behavior:
ff queryFieldType: This attribute tells Solr what type of field should be used to parse
the query text that is given to the component (for example, if you want the component
to ignore the letter case, you should set this parameter to the field type that
lowercases its contents)
ff config-file: This specifies the configuration file which will be used by the component
The next part of the solrconfig.xml configuration procedure is the search handler
definition. It simply tells Solr to create a new search handler with the name of /promotion
(to be the value of the name attribute) and using the solr.SearchHandler class (the class
attribute). In addition to that, the handler definition also tells Solr to include the component
named elevator in this search handler. This means that this search handler will use our
defined component. For your information, you can use more than one search component in a
single search handler. We've also included some standard parameters to the handler, such as
df, which specifies the default search field.
What we see next is the actual configuration of the elevator component. You can see that
there is a query defined (the query XML tag) with an attribute text="solr". This defines
the behavior of the component when a user passes solr to the q parameter. Under this tag
you can see a list of the documents' unique identifiers that will be placed at the top of the
results list for the defined query. Each document is defined by a doc tag and an id attribute
(which have to be defined on the basis of solr.StrField) which holds the unique identifier.
You can have multiple query tags in a single configuration file which means that the elevation
component can be used for a variety of queries.
www.it-ebooks.info
Chapter 4
121
The index configuration and example datafile are fairly simple. The index contains two fields
that are responsible for holding information about the document. In the example datafile,
we can see three documents present. As the explanation is not crucial, I'll skip discussing
it further.
The query you see in the example returns all the documents. The query is made to our new
handler with just a simple one word q parameter (the default search field is set to name in the
schema.xml file). Recall the elevate.xml file and the documents we defined for the query
we just passed to Solr. We told Solr that we want the document with id=3 in the first place of
the results list and we want the document with id=1 in the second place of the results list. As
you can see, the documents were positioned exactly as we wanted them so it seems that the
component did its job.
There's more...
There is one more thing I would like to say about the query elevation functionality in Solr.
Excluding documents with QueryElevationComponent
The elevate component can not only place documents on top of the results list, but it can also
exclude documents from the results list. To do that you should add the exclude="true"
attribute to the document definition in your elevate.xml file. This is what the example file
would look like:
<?xml version="1.0" encoding="UTF-8" ?>
<elevate>
<query text="solr">
<doc id="3" />
<doc id="1" exclude="true" />
<doc id="2" exclude="true" />
</query>
</elevate>
See also
If you would like to know how to mark the documents that were positioned by the solr.
QueryElevationComponent class, please read the Modifying returned documents recipe
in this chapter.
www.it-ebooks.info
Querying Solr
122
Positioning documents with words closer to
each other first
Imagine an e-commerce book shop where the users have only one way to find books, that is,
by searching. Most of the users requested that the OR operator should be the default logical
operator, so that we can have many results for most of the popular queries. Once every few
days an angry user calls the call center and says that by typing "solr cookbook" the first few
pages are not relevant to the query he/she typed in, so in other words this is not what he/
she searched for. So that's the problem, now what can be done? The answer is to boost
documents with query words closer to each other. This recipe will show you how to do it.
How to do it...
For the purpose of this task I will be using the DisMax query parser.
1. Let's start with the following index structure (just add the following to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="author" type="string" indexed="true" stored="true"/>
2. Now, let's index the following data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solr perfect search cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Solr example cookbook</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
3. In addition to that we need to define a new request handler in the solrconfig.xml
file, which looks like the following code:
<requestHandler name="/closer" class="solr.
StandardRequestHandler">
www.it-ebooks.info
Chapter 4
123
<lst name="defaults">
<str name="q">_query_:"{!dismax qf=$qfQuery mm=1 pf=$pfQuery
bq=$boostQuery v=$mainQuery}"</str>
<str name="qfQuery">title</str>
<str name="pfQuery">title^1000</str>
<str name="boostQuery">_query_:"{!dismax qf=$boostQueryQf
mm=100% v=$mainQuery}"^100</str>
<str name="boostQueryQf">title</str>
<str name="df">title</str>
</lst>
</requestHandler>
4. As I wrote earlier, we want to get the documents with the words typed by the user close
to each other first in the result list. Let's assume our user typed in the dreaded solr
cookbook query. To handle the query we use the new /closer request handler we
defined earlier and we send the query using the mainQuery parameter, not the q one
(I'll describe why this is so later). So the whole query looks as follows:
http://localhost:8983/solr/closer?mainQuery=solr+cookbook&fl=score,
id,title
The result list returned by Solr is the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
</lst>
<result name="response" numFound="3" start="0"
maxScore="0.93303263">
<doc>
<str name="id">3</str>
<str name="title">Solr cookbook</str>
<float name="score">0.93303263</float>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solr perfect search cookbook</str>
<float name="score">0.035882458</float>
</doc>
<doc>
<str name="id">2</str>
<str name="title">Solr example cookbook</str>
<float name="score">0.035882458</float>
www.it-ebooks.info
Querying Solr
124
</doc>
</result>
</response>
We received the documents in the way we wanted. Now let's look at how that happened.
How it works...
First of all we have the index structure. For the purpose of the example, I assumed that our
book description will consist of three fields and we will be searching with the use of the title
field which is based on the standard text field defined in the standard schema.xml
file provided with the Solr distribution.
As you can see in the provided data file example there are three books. The first book has
two other words between the words solr and cookbook. The second book has one word
between the given words, and the third book has the words next to each other. In a perfect
situation, we would like to have the third book from the example file as the first one in the
result list, the second book from the file in the second place, and the first book from the
example data file as the last in the results list.
Now let's take a look at our new request handler. We defined it to be available under a name
/closer (the name attribute of the requestHandler tag). We also said that it should be
based on the solr.StandardRequestHandler class (in the class attribute). Next, in
the defaults list we have a number of parameters defining the query behavior we want to
achieve. First of all we have the q parameter. This contains the query constructed with the
use of local params. The _query_:"…" part of the q parameter is a way of specifying the
new query. We tell Solr that we want to use DisMax query parser (the !dismax part) and we
want to pass the value of the qfQuery parameter as the qf DisMax parser parameter. We
also want the "minimum should match" parameter to be equal one (mm=1), we want a phrase
query to be used (the one which is defined in the pfQuery (pf=$pfQuery) parameter) and
we want the boost query to be used – the one that is defined in the boostQuery parameter
(bq=$boostQuery). Finally we specify that we will pass the actual user query not with the q
parameter, but instead with the mainQuery parameter (v=$mainQuery).
Next we have boostQuery – another query constructed using local parameters. As you can
see we use the DisMax query parser (the !dismax query part), and we specify the qf and
mm DisMax query parser parameters. The value of the qf parameter will be taken from the
boostQueryQf parameter and the value of the mm parameter is set to 100%, so we want the
boost query to return only the documents that have all the words specified by the user. The
v attribute is responsible for passing the actual query, which in our case will be stored in the
mainQuery (v=$mainQuery part) parameter. We also said that we want our boost query
to be boosted by 100 (the ^100 part of the query). The boostQueryQf parameter is used
by the boost query to specify which fields should be used for search, in the query used for
boosting. Finally, the df parameter specifies the default search field.
www.it-ebooks.info
Chapter 4
125
Now let's discuss what the query is actually doing. The first query tells Solr to return all the
documents with at least one of the words that the user entered (this is defined by setting the
mm parameter to 1). But we also say that we want to boost phrases; that's why we use a phrase
query on the title field. In addition to that we specified that we want to use our boost query,
which will increase the boost of all the documents that have all the words entered by the user
(mm=100%). Combining all those factors we will end up with results that have the top documents
occupied by those documents that have all the words entered by the user present in the title
field and where all those words are close to each other.
The query is simple. We specify that we want to get a calculated score in the results for
each document, we want the id field, and the title field. We also pass the mainQuery
parameter because we have the v attribute of both the q and boostQuery parameters set
to the $mainQuery parameter. This means that Solr will take the mainQuery parameter
value and pass it to the v parameter of those queries. Because we prepared our request
handler configuration and pasted it into solrconfig.xml, now at query time we only
need to pass a single parameter that passes the words specified by our users.
The last thing is the results list. As you can see the documents are sorted in the way we
wanted them to be. You should take a look at one thing – the score field. This field shows
how relevant the document is to the query we sent to Solr.
Sorting results by a distance from a point
Suppose we have a search application that is storing information about the companies.
Every company is described by a name and two floating point numbers that represent the
geographical location of the company. One day your boss comes to your room and says that
he/she wants the search results to be sorted by distance from the user's location. This recipe
will show you how to do it.
Getting ready
Before continuing please read the Storing geographical points in the index recipe from
Chapter 3, Analyzing Your Text Data.
How to do it...
1. Let's begin with the following index (add the following to your schema.xml file to the
fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"/>
www.it-ebooks.info
Querying Solr
126
<field name="location" type="location" indexed="true"
stored="true" />
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
2. We also have the following type defined in the schema.xml file:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
I assumed that the user location will be provided from the application that is making
a query.
3. Now let's index our example data file, which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="location">56.4,40.2</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="location">50.1,48.9</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="location">23.18,39.1</field>
</doc>
</add>
4. So our user is standing at the North Pole and is using our search application. Now
let's assume that we want to get the companies sorted in such a way that the ones
that are nearer the user are at the top of the results list. The query to find such
companies could look like the following query:
http://localhost:8983/solr/select?q=company&sort=geodist(location,
0.0,0.0)+asc
The result of that query would look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Chapter 4
127
<int name="QTime">1</int>
<lst name="params">
<str name="q">company</str>
<str name="sort">geodist(location,0.0,0.0) asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="location">23.18,39.1</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="location">56.4,40.2</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company 2</str>
<str name="location">50.1,48.9</str>
</doc>
</result>
</response>
If you would like to calculate the distance by hand, you would see that the results are
sorted as they should be.
How it works...
As you can see in the index structure and in the data, every company is described by the
following three fields:
ff id: This specifies the unique identifier
ff name: This specifies the company name
ff location: This specifies the latitude and longitude of the company location
I'll skip commenting on how the actual location of the company is stored. If you want to
read more about it, please refer to the Storing geographical points in the index recipe
from Chapter 3, Analyzing Your Text Data.
www.it-ebooks.info
Querying Solr
128
We wanted to get the companies that match the given query and are sorted in the ascending
order from the North Pole. To do that we run a standard query with a non-standard sort.
The sort parameter consists of a function name, geodist, which calculates the distance
between points. In our example the function takes three parameters:
ff The first parameter specifies the field in the index that should be used to calculate
the distance
ff The second parameter is the latitude value of the point from which the distance will
be calculated
ff The third parameter is the longitude value of the point from which the distance will
be calculated
After the function there is the order of the sort which in our case is asc (ascending order).
See also
If you would like to learn how to return the calculated distance that we used for sorting please
refer to the Returning the value of a function in results recipe in this chapter.
Getting documents with only a partial match
Imagine a situation where you have an e-commerce library and you want to make a search
algorithm that tries to bring the best search results to your customers. But you noticed that
many of your customers tend to make queries with too many words, which result in an empty
results list. So you decided to make a query that will require the maximum of two of the
words that the user entered to be matched. This recipe will show you how to do it.
Getting ready
This method can only be used with the DisMax query parser. The standard query parser doesn't
support the mm parameter.
How to do it...
1. Let's begin with creating our index that has the following structure (add this to your
schema.xml file to the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
As you can see our books are described by two fields.
www.it-ebooks.info
Chapter 4
129
2. Now let's look at the example data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solrcook book revised</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book that was revised</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Another revised book</field>
</doc>
</add>
3. The third step is to made a query that will satisfy the requirements. Such a query
could look like the following:
http://localhost:8983/solr/select?q=book+revised+another+
different+word+that+doesnt+count&defType=dismax&mm=2&q.op=AND
The preceding query will return the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q.op">AND</str>
<str name="mm">2</str>
<str name="q">book revised another different word that doesnt
count</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
</doc>
<doc>
<str name="id">2</str>
www.it-ebooks.info
Querying Solr
130
<str name="title">Some book that was revised</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
</doc>
</result>
</response>
As you can see, even though the query was made up of too many words, the result list contains
all the documents from the example file. Now let's see how that happened.
How it works...
The index structure and the data are fairly simple. Every book is described by two fields:
a unique identifier and a title.
The query is the thing that we are interested in. We have passed about eight words to Solr
(the q parameter), we defined that we want to use the DisMax query parser (the defType
parameter), and we sent the mysterious mm parameter set to the value of 2. Yes, you are right,
the mm parameter, also called minimum should match, tells the DisMax query parser how
many of the words passed into the query must be matched with the document, to ascertain
that the document is a match. In our case we told the DisMax query parser that there should
be two or more words matched to identify the document as a match. We've also included
q.op=AND, so that the default logical operator for the query would be set to AND.
You should also note one thing – the document that has three words matched is at the top
of the list. The relevance algorithm is still there, which means that the documents with more
words that matched the query will be higher in the result list than those that have fewer
words that matched the query. The documentation about the mm parameter can be found at
http://wiki.apache.org/solr/DisMaxQParserPlugin.
Affecting scoring with functions
There are many situations where you would want to have an influence on how the score of the
documents is calculated. For example, you would perhaps like to boost the documents on the
basis of the purchases of it. Like in an e-commerce boost store, you would like to show relevant
results, but you would like to influence them by adding yet another factor to their score. Is it
possible? Yes, and this recipe will show you how to do it.
www.it-ebooks.info
Chapter 4
131
How to do it...
1. Let's start with the following index structure (just add the following to the field section
in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="sold" type="int" indexed="true" stored="true" />
2. The example data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="title">Solrcook book revised</field>
<field name="sold">5</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book revised</field>
<field name="sold">200</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Another revised book</field>
<field name="sold">60</field>
</doc>
</add>
3. So we want to boost our documents on the basis of a sold field while retaining the
relevance sorting. Our user typed revised into the search box, so the query would
look like the following:
http://localhost:8983/solr/select?defType=dismax&qf=title&q=revise
d&fl=*,score
And the results would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">title</str>
www.it-ebooks.info
Querying Solr
132
<str name="fl">*,score</str>
<str name="q">revised</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="0.35615897">
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
<int name="sold">5</int>
<float name="score">0.35615897</float>
</doc>
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
<int name="sold">200</int>
<float name="score">0.35615897</float>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
<int name="sold">60</int>
<float name="score">0.35615897</float>
</doc>
</result>
</response>
4. Now let's add the sold factor by adding the following to the query:
bf=product(sold)
So our modified query would look like this:
http://localhost:8983/solr/select?defType=dismax&qf=title&q=revise
d&fl=*,score&bf=product(sold)
And the results for the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">36</int>
<lst name="params">
<str name="fl">*,score</str>
www.it-ebooks.info
Chapter 4
133
<str name="q">revised</str>
<str name="qf">title</str>
<str name="bf">product(sold)</str>
<str name="defType">dismax</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="163.1048">
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
<int name="sold">200</int>
<float name="score">163.1048</float>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised book</str>
<int name="sold">60</int>
<float name="score">49.07608</float>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Solrcook book revised</str>
<int name="sold">5</int>
<float name="score">4.279089</float>
</doc>
</result>
</response>
As you can see, adding the parameter changed the whole results list. Now let's see why
that happened.
How it works...
The schema.xml file is simple. It contains the following three fields:
ff id: This field is responsible for holding the unique identifier of the book
ff title: This specifies the book title
ff sold: This specifies the number of pieces that have been sold during the last month
In the data we have three books. Each of the books has the same number of words in the title.
That's why when typing the first query all documents got the same score. As you can see,
the first book is the one with the fewest pieces sold and that's not what we want to achieve.
www.it-ebooks.info
Querying Solr
134
For the same reason we added the bf parameter. It tells Solr what function to use to affect
the scoring computation (in this case the result of the function will be added to the score of
the document). In our case it is the product function that returns the product of the values
we provide as its arguments; in our case the one and only argument of the function will be
the value of the book's sold field.
The result list of the modified query clearly shows how the scoring was affected by the function.
In the first place of the results list we have the book that was most popular during the last week.
The next book is the one which was less popular than the first book, but more popular than the
last book. The last book in the results is the least popular book.
See also
If you would like to know more about the functions available in Solr, please go to the Solr wiki
page at the following address: http://wiki.apache.org/solr/FunctionQuery.
Nesting queries
Imagine a situation where you need a query nested inside another query. Let's imagine that
you want to run a query using the standard request handler but you need to embed a query
that is parsed by the DisMax query parser inside it. This is possible with Solr 4.0 and this
recipe will show you how to do it.
How to do it...
1. Let's start with a simple index that has the following structure (just add the following
to the field section in your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
2. Now let's look at the example data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Revised solrcook book</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">Some book revised</field>
</doc>
<doc>
www.it-ebooks.info
Chapter 4
135
<field name="id">3</field>
<field name="title">Another revised little book</field>
</doc>
</add>
3. Imagine you are using the standard query parser to support the Lucene query syntax,
but you would like to boost phrases using the DisMax query parser. At first it seems
that it is impossible, but let's assume that we want to find books that have the words
book and revised in their title field, and we want to boost the book revised
phrase by 10. Let's send a query like so:
http://localhost:8983/solr/select?q=book+revised+_query_:"{!dismax
qf=title pf=title^10 v=$qq}"&qq=book+revised&q.op=AND
The results of the preceding query should look like the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
<lst name="params">
<str name="q.op">AND</str>
<str name="qq">book revised</str>
<str name="q">book revised _query_:"{!dismax qf=title
pf=title^10 v=$qq}"</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="title">Some book revised</str>
</doc>
<doc>
<str name="id">1</str>
<str name="title">Revised solrcook book</str>
</doc>
<doc>
<str name="id">3</str>
<str name="title">Another revised little book</str>
</doc>
</result>
</response>
As you can see, the results list was sorted exactly the way we wanted. Now let's see how it works.
www.it-ebooks.info
Querying Solr
136
How it works...
As you can see our index is very simple. It consists of two fields – one holding the unique
identifier (the id field) and another one holding the title of the book (the title field).
Let's look at the query. The q parameter is built from two parts. The first one, book+revised,
is just a usual query composed from two terms. The second part of the query starts with a
strange looking expression, that is, _query_. This expression tells Solr that another query
should be made that will affect the results list. Notice that the expression is surrounded
with " characters. Then we will see the expression tells Solr to use the DisMax query parser
(the !dismax part) and the parameters that will be passed to the parser (qf and pf). The
v parameter is used to pass the value of the q parameter. The value passed to the DisMax
query parser in our case will be book+revised. This is called parameter dereferencing.
By using the $qq expression, we tell Solr to use the value of the qq parameter. Of course,
we could pass the value to the v parameter, but I wanted to show you how to use the
dereferencing mechanism. The qq parameter is set to book+revised and it is used by Solr
as a parameter for the query that was passed to the DisMax query parser. The last parameter,
q.op=AND tells Solr which logical operator should be used as the default one.
The results show that we achieved exactly what we wanted.
Modifying returned documents
Let's say we are using the elevate component that Solr provides to promote some books
when necessary. But as you may already know, the standard Solr response doesn't include
the information about document being elevated or not. What we would like to achieve is to
get that information somehow from Solr. Actually we would like it to be as simple as running
a Solr query and getting the results back. This recipe will show you how to use document
transformers with the elevation component.
How to do it...
1. First of all, let's assume we have the following index structure defined in the fields
section of our schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
2. We also need to have the elevation component defined along with the search
component (place the following entries in your solrconfig.xml file):
<requestHandler name="/select" class="solr.SearchHandler">
<lst name="defaults">
www.it-ebooks.info
Chapter 4
137
<str name="echoParams">explicit</str>
<int name="rows">10</int>
<str name="df">name</str>
</lst>
<arr name="last-components">
<str>elevator</str>
</arr>
</requestHandler>
<searchComponent name="elevator" class="solr.
QueryElevationComponent">
<str name="queryFieldType">string</str>
<str name="config-file">elevate.xml</str>
</searchComponent>
3. The contents of the elevate.xml file located in the conf directory look like the
following code:
<elevate>
<query text="book">
<doc id="3" />
</query>
</elevate>
4. Our example data that we indexed looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Promoted document</field>
</doc>
</add>
5. Now let's query Solr with the following query:
http://localhost:8983/solr/select?q=book&df=name&fl=*,[elevated]
www.it-ebooks.info
Querying Solr
138
And the response we get from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">book</str>
<str name="df">name</str>
<str name="fl">*,[elevated]</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Promoted document</str>
<bool name="[elevated]">true</bool>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<bool name="[elevated]">false</bool>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Book 2</str>
<bool name="[elevated]">false</bool>
</doc>
</result>
</response>
As you can see each document does not only have its name and identifier, but also the
information about whether it was elevated or not.
How it works...
Our index structure consists of two fields, the id field which is our unique key and the name
field used for holding the name of the document. Please remember that in order to use the
elevation component you have to have a unique key defined in your schema.xml file, and this
field has to be based on the string type.
The /select request handler configuration is quite standard, although we've added the
last-components sections that define what component should be used during a query.
We defined that we want to use the component named elevator.
www.it-ebooks.info
Chapter 4
139
The next thing we did is the elevator search component definition. It is based on the solr.
QueryElevationComponent class (the class attribute) and we set its name to elevator
(the attribute name). In addition to that, we specified two attributes needed by the query
elevation component:
ff queryFieldType: This specifies the name of the type that will be used to analyze
the incoming text. We specified the string type because we want only exact
matches to include elevated documents.
ff config-file: This specifies the name of the configuration file that stores the
elevation definitions.
The elevate.xml file we use for storing the query elevation component is simple. The root tag
is named elevate and can have multiple query tags inside it. Each query tag is responsible
for elevating documents for a query defined with the text attribute. Inside the query tag we
can have multiple doc tags with an id attribute, which should have a value of the identifier of
the document to which we want add to results or modify positions. In our case, we want the
document with an identifier value of 3 to be placed in the first position when users enter the
book query.
The query we sent was simple; we asked for documents that have book in the default field
(the df parameter) which is name in our case. In addition to that, we want all stored fields to
be returned (the * part of the fl parameter) and we also want to activate one of the document
transformers, which is responsible for marking the documents that were elevated by the query
elevation component, by adding the [elevated] part of the fl parameter. This transformer
adds the <bool name="[elevated]">true</bool> field if the document was elevated,
and <bool name="[elevated]">false</bool> if the document wasn't elevated.
Using parent-child relationships
When using Solr you are probably used to having a flat structure of documents without any
relationships. However, there are situations where decomposing relationships is a cost we
can't take. Because of that Solr 4.0 comes with a join functionality that allows us to use some
basic relationships. For example, imagine that our index consists of books and workbooks and
we would like to use that relationship. This recipe will show you how to do it.
How to do it...
1. First of all, let's assume that we have the following index structure (just place the
following in the fields section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"
multiValued="false"/>
www.it-ebooks.info
Querying Solr
140
<field name="type" type="string" indexed="true" stored="true"/>
<field name="book" type="string" indexed="true" stored="true"/>
2. Now let's look at our test data that we are going to index:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="type">book</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="type">book</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook A</field>
<field name="type">workbook</field>
<field name="book">1</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook B</field>
<field name="type">workbook</field>
<field name="book">2</field>
</doc>
</add>
3. Now, let's assume we want to get all the books from Solr that have workbooks
for them. Also we want to narrow the books we got to only those that have the
character 2 in their names. In order to do that, we run the following query:
http://localhost:8983/solr/select/?q={!join from=book to=id}
type:workbook&fq=name:2
The Solr response for the preceding query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
www.it-ebooks.info
Chapter 4
141
<lst name="params">
<str name="fq">name:2</str>
<str name="q">{!join from=book to=id}type:workbook</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Book 2</str>
<str name="type">book</str>
</doc>
</result>
</response>
As you can see, the returned document was exactly the one we expected.
How it works...
Although the example index structure is simple I would like to comment on it. The id field is
responsible for holding the unique identifier of the document, the name field is the document
name, and the type field holds the document's types. The book field is optional and specifies
the identifier of the parent document. So you can see that in our example data, we have two
parent documents (those with an id field value of 1 and 2) and two child documents (those
with an id field value of 3 and 4).
Let's pause for a bit now before looking at the query, and look at our example data. If we only
query for workbooks, we would get documents with identifier values of 3 and 4. The parent
for the document with the id field equal to 3 is 1, and the parent for the document with the
id field equal to 4 is 2. If we filter 1 and 2 with the filter fq=name:2, we should only get the
document with the id field value equal to 2 as the result. So looking at the query result it
works as intended, but how does the query actually work?
I'll begin the description from the join part, that is, q={!join from=book to=id}
type:workbook. As you can see we used local params to choose the different type of
query parser – the join query parser (the !join part of the query). We specified that child
documents should use the book field (the from parameter) and join it with the id field (the to
parameter). The type:workbook part specifies the query we run, that is, we want only those
documents that have the workbook value in the type field. The fq parameter, which narrows
the result set to only those documents that have the value 2 in the name field, is applied after
the join is executed, so we only apply it to the parent documents.
www.it-ebooks.info
Querying Solr
142
Ignoring typos in terms of performance
Sometimes there are situations where you would like to have some kind of functionality that
would allow you to give your user the search results even though he/she made a typo or even
multiple typos. In Solr, there are multiple ways to undo that: using a spellchecker component
to try and correct the user's mistake, using the fuzzy query, or for example, using the ngram
approach. This recipe will concentrate on the third approach and show you how to use ngrams
to handle user typos.
How to do it...
For the purpose of the recipe, let's assume that our index is built up of four fields:
identifier, name, description, and the description_ngram field which
will be processed with the ngram filter.
1. So let's start with the fields definition of our index which should look like the following
code (place this in your schema.xml file in the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="description" type="text" indexed="true" stored="true"
/>
<field name="description_ngram" type="text_ngram" indexed="true"
stored="false" />
2. As we want to use the ngram approach, we will include the following filter in our
text_ngram field type definition:
<filter class="solr.NGramFilterFactory" minGramSize="2"
maxGramSize="2" />
The filter will be responsible for dividing the indexed data and queries into two
bi-grams. To better illustrate what I mean, take a look at the following screenshot,
which shows how the filter worked for the word "multiple":
www.it-ebooks.info
Chapter 4
143
So the whole text_ngram type definition will look like the following code:
<fieldType name="text_ngram" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.NGramFilterFactory" minGramSize="2"
maxGramSize="2" />
</analyzer>
</fieldType>
3. We also need to add the copy field definition to our schema.xml file, to automatically
copy the value of the description field to the description_ngram field. The copy
field definition looks as follows:
<copyField source="description" dest="description_ngram" />
4. Now we can index our data. For the purpose of the recipe I used the following
data sample:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook 4.0</field>
<field name="description">Solr Cookbook 4.0 contains multiple
recipes helping you with your every day work with Solr :)</field>
</doc>
</add>
5. After indexing it, I decided to test if my query can handle a single typo in each of the
words provided to the query, so I've sent the following query to Solr, where the words
I was really interested in were "contains" and "multiple":
q=description:(kontains+multyple) description_
ngram:(kontains+multyple)&q.op=OR
The result of the query was as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="indent">true</str>
<str name="q">description:(kontains multyple) description_
ngram:(kontains multyple)</str>
www.it-ebooks.info
Querying Solr
144
<str name="q.op">OR</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<str name="description">Solr Cookbook 4.0 contains multiple
recipes helping you with your every day work with Solr :)</
str></doc>
</result>
</response>
As you can see the document we were interested in was found. So let's see how that worked.
How it works...
As you can see from the index structure, we have two fields, namely name and description,
which we defined to use the text_ngram field because we want these fields to support the
returning of the search results even when the user enters a typo of some sort. To allow this
we use the solr.NGramFilterFactory filter with two attributes defined, namely, the
minGramSize which sets the minimum size of the produced ngram, and the maxGramSize
which sets the maximum size of the produced ngram. With both of these attributes set to 2,
we configured the solr.NGramFilterFactory filter to produce tokens called 2-grams, that
are built of two characters. The third attribute of the filter tag is the class attribute that
specifies
the filter factory class we want to use.
Let's concentrate on the provided screenshot (refer to step 2 in the How to do it... section) to
discuss how the solr.NGramFilterFactory filter works in our case. As I wrote earlier, we
want the ngram filter to produce grams built up of two characters. You can see how the filter
we've chosen works. From the word multiple it created the following bi-grams (n-grams built
from 2 characters):
mu ul lt ti ip pl le
So, the idea of the algorithm is quite simple – divide the word, so that we take the first
character and the character after it, and we make a bi-gram from it. Then we take the next
character and the character after it and create the second bi-gram and so on until we can't
make any more bi-grams.
www.it-ebooks.info
Chapter 4
145
Now if you look at the query there are two words we are looking for and both of them contain
a typo. The kontains word should be contain without a typo and the multyple should be
multiple without a typo. Our query also specifies that the logical query operator we want to
use is the OR operator. We use it because we want to match all documents with even a single
match to any bi-gram. If we turn the kontains and multyple tokens into bi-grams, we would
get the following (I'll use the pipe (|) character to separate the words from each other):
ko on nt ta ai in ns | mu ul lt ty yp pl le
If we turn the contains multiple tokens into bi-grams we would get the following:
co on nt ta ai in ns | mu ul lt ti ip pl le
If you compare those bi-grams you would see that only three of those differ between the proper
words and the ones with typos. The rest of them are the same. Because of that our query
finds the document we indexed. You may wonder why we queried both the description and
description_ngram fields. We did that because we don't know if the client's query is the
one with typos or without. If it is without, we want the documents with better matches to be
higher up on the results lists, than the ones that are not perfectly matched.
Of course all of that doesn't come without any downsides. One of the major downsides of this
approach is the growth of the index size because of the number of tokens produced by the
ngram filter. The second downside is the number of results produced with such an approach;
there will be many more results than you are used to and that's why we did a query to both the
description and description_ngram fields. We wanted to increase the score value of
the perfectly matched documents (you can also boost the description field higher during a
query). You can also try having the same approach work with the edismax query parser and
the "minimum should match" (mm) parameter, but this is beyond the scope of this recipe.
Detecting and omitting duplicate documents
Imagine your data consists of duplicates because they come from different sources. For
example, you have books that come from different suppliers, but you are only interested in a
single book with the same name. Of course you could use the field collapsing feature during the
query, but that affects query performance and we would like to avoid that. This recipe will show
you how to use the Solr deduplication functionality.
How to do it...
1. We start with the simple index structure. This should be placed in the fields section
of your schema.xml file:
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
www.it-ebooks.info
Querying Solr
146
<field name="name" type="text" indexed="true" stored="true"
multiValued="false"/>
<field name="type" type="string" indexed="true" stored="true"
multiValued="false"/>
2. For the purpose of the recipe, we assume that we have the following data stored in
the data.xml file:
<add>
<doc>
<field name="name">This is a book we are indexing and we think
it will be a dupe because it's almost the same as the second document
we are going to index</field>
<field name="type">book</field>
</doc>
<doc>
<field name="name">This is the book we are indexing and we think
it will be a dupe because it's almost the same as the second
document we are going to index</field>
<field name="type">book</field>
</doc>
</add>
As you can see, the file contains two documents and they only differ by a single word;
the first document contains is the is a book phrase, while the second contains the
is the book phrase. In my opinion the second document is a dupe of the first one.
3. In order to have those two documents detected and overwritten, we need to create
a new update request processor chain called dedupe and configure org.apache.
solr.update.processor.SignatureUpdateProcessorFactory as the first
update processor. So the appropriate section of our solrconfig.xml file should
look like the following code:
<updateRequestProcessorChain name="dedupe">
<processor class="org.apache.solr.update.processor.
SignatureUpdateProcessorFactory">
<bool name="enabled">true</bool>
<bool name="overwriteDupes">true</bool>
<str name="signatureField">id</str>
<str name="fields">name</str>
<str name="signatureClass">org.apache.solr.update.processor.
TextProfileSignature</str>
</processor>
<processor class="solr.LogUpdateProcessorFactory" />
<processor class="solr.RunUpdateProcessorFactory" />
</updateRequestProcessorChain>
www.it-ebooks.info
Chapter 4
147
4. Now let's index our data by running the following command:
curl 'http://localhost:8983/solr/update?update.
chain=dedupe&commit=true' --data-binary @data.xml -H 'Contenttype:
application/xml'
5. If everything went well, we should only see the second document as the first one
should be overwritten. So we should check that by running the following query:
http://localhost:8983/solr/select?q=*:*
The response to it was the following:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="name">This is the book we are indexing and we think it
will be a dupe because it's almost the same as the second
document we are going to index</str>
<str name="type">book</str>
<str name="id">a095014df10f76513387af0450768ffb</str>
</doc>
</result>
</response>
As you can see we got only a single document, and if you look again at the example data,
you would notice that it is the second document we sent, so the first one was overwritten.
How it works...
Our index structure is simple and consists of three fields – the id field which holds the unique
identifier, the name field which is a name of the book, and the type field which holds the type
of the book.
The example data you see doesn't contain the id field, which isn't a mistake, it was prepared
this way on purpose. We want our deduping to use the id field to generate a unique identifier
for us and use it to overwrite duplicate documents. Also, you can see that the two sample
documents are almost the same, so they should be marked as dupes and we should only
see one of them in the index, probably the second one.
www.it-ebooks.info
Querying Solr
148
Next we define a new update request processor chain in the solrconfig.xml file
with the name dedupe (the name property). The first processor we need to add in order
to have the deduping functionality is org.apache.solr.update.processor.
SignatureUpdateProcessorFactory. We do so by setting the class attribute of the
processor tag to the mentioned class. The next few properties configure the org.apache.
solr.update.processor.SignatureUpdateProcessorFactory behavior. By setting
the enabled property to true, we turn on the deduping mechanism. overwriteDupes set to
true tells Solr that we want the duplicate documents to be overwritten. The signatureField
field configures the name of the field where the generated signature will be stored, which in our
case is the id field. This is crucial, because Solr will use that information to identify duplicate
documents. The fields field contains information of which fields (a list separated by the
comma character) should be used to identify the duplication. We decided to use the name
field. Finally, the signatureClass class is the class implementing the signature calculation.
We've chosen org.apache.solr.update.processor.TextProfileSignature because
it works best on longer text and we expect that. You can also choose org.apache.solr.
update.processor.MD5Signature and org.apache.solr.update.processor.
Lookup3Signature. The last two processors, solr.LogUpdateProcessorFactory and
solr.RunUpdateProcessorFactory, write information about the update to the log file and
run the update.
As you can see in the response for our "match all documents" query, only the second document
is present. This is because when the index was empty the first document was indexed. Then, the
second document came and it was identified as a dupe and thus it overwrote the first one.
Using field aliases
Imagine your products have multiple prices, and depending on your client's location you search
one of the defined fields. So you have a field for price in US dollars, in Euros, and so on. But what
you would like to do is return the field you are using for displaying the price of the document as a
"price" no matter what field you use. This recipe will show you how to do it.
How to do it...
1. Let's begin with the following index structure (put all the entries in the fields
section of your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="price_usd" type="double" indexed="true" stored="true"
/>
<field name="price_eur" type="double" indexed="true" stored="true"
/>
<field name="price_pln" type="double" indexed="true" stored="true"
/>
www.it-ebooks.info
Chapter 4
149
2. We will also use the following test data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook 4.0</field>
<field name="price_usd">40.00</field>
<field name="price_pln">120.00</field>
<field name="price_eur">30.00</field>
</doc>
</add>
3. Let's assume that we have a client from the United States of America and he/she
searches for the word solr and for products with the price in US dollars ranging from
20 to 50. The query would look like the following:
q=name:solr&fq=price_usd:[20+TO+50]&fl=id,name,price_usd
And the results of the preceding query would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price_usd:[20 TO 50]</str>
<str name="fl">id,name,price_usd</str>
<str name="q">name:solr</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<double name="price_usd">40.0</double>
</doc>
</result>
</response>
4. As you can see, we have our sample document returned but we've got the price_
usd value returned as well. We would like it to be named price. So let's modify
our fl parameter value, and instead of specifying id,name,price_usd we pass
id,name,price:price_usd. So the whole query would look as follows:
q=name:solr&fq=price_usd:[20+TO+50]&fl=id,name,price:price_usd
www.it-ebooks.info
Querying Solr
150
And the returned results would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">price_usd:[20 TO 50]</str>
<str name="fl">id,name,price:price_usd</str>
<str name="q">name:solr</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook 4.0</str>
<double name="price">40.0</double>
</doc>
</result>
</response>
As you can see in the result document we got a field called price instead of price_usd
field. Now, let's see how that works.
How it works...
The index structure is pretty simple, it only contains the field responsible for holding the
identifier, name of the document, and three prices in different currencies. All the fields
are marked as stored because we want to return them (not all at the same time though)
at query time. The sample data is also simple so I decided to skip commenting on that.
The first query is simple. We are searching for the value solr in the name field and we
want only the documents with the value of the price_usd field to be between 20 and 50.
We also want to return (the fl parameter) the following fields as a document: id, name,
and price_usd.
The interesting things come with the second query. As you can see there is a different fl
parameter that you may be used to. The first part of the fl parameter is pretty obvious;
we want to return the id and name fields. The second part is new though; we specified
the following value: price:price_usd. This means that we want the price_usd field
to be returned as price. That is how field aliasing works; you add the value ALIAS_
NAME:FIELD_NAME to the fl parameter and in the results, instead of FIELD_NAME,
Solr will return ALIAS_NAME.
www.it-ebooks.info
Chapter 4
151
Returning a value of a function in the results
Imagine you have a service where your users can search for different companies. Your users
can enter a simple keyword(s) and then return all the companies matching that keyword(s).
But a day comes when you give your users the ability to choose their location, and you would
like to show how far they are from each company returned in the results. This recipe will show
you how to do it.
Getting ready
Before reading further I advise you to read the Using field aliases recipe in the current
chapter and the Storing geographical points in the index recipe from Chapter 3, Analyzing
Your Text Data.
How to do it...
1. For the purpose of the recipe, let's assume that we have the following index structure
(put the following field's definition into your schema.xml file in the fields section):
<field name="id" type="string" indexed="true" stored="true"
required="true" multiValued="false" />
<field name="name" type="text" indexed="true" stored="true"/>
<field name="loc" type="location" indexed="true" stored="true"/>
<dynamicField name="*_coordinate" type="double" indexed="true"
stored="false" />
2. Next, we need to define the location field type. It should look like the following
code (put the following definition in to your schema.xml file in the types section):
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
3. Let's also assume that we have the following data indexed:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="loc">56.4,40.2</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="loc">50.1,48.9</field>
</doc>
www.it-ebooks.info
Querying Solr
152
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="loc">23.18,39.1</field>
</doc>
</add>
4. Now, in order to get all the documents with the word company in the name field
we would run the following query:
q=name:company&fl=*
5. We have the information that our client's location is 50.0, 28.0 and we would like to
show our client the distance from his/her location to each of the companies we return
in the results. In order to do that we add the following part to the fl parameter:
dist:geodist(loc,50.0,28.0)
So the whole query looks like the following:
q=name:company&fl=*,dist:geodist(loc,50.0,28.0)
And the response from Solr is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">name:company</str>
<str name="fl">*,dist:geodist(loc,50.0,28.0)</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="loc">56.4,40.2</str>
<double name="dist">1077.4200268973314</double>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company 2</str>
<str name="loc">50.1,48.9</str>
<double name="dist">1487.4260767512278</double>
</doc>
www.it-ebooks.info
Chapter 4
153
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="loc">23.18,39.1</str>
<double name="dist">3134.746384852772</double>
</doc>
</result>
</response>
As you can see, in addition to all the stored fields, Solr returned the additional field called
dist. Let's now see how that worked.
How it works...
The index structure is simple, it contains the identifier (the id field), name of the company
(the name field), and the geographical location of the company (the loc field). Description
of how geographical points should be stored were described in Chapter 3, Analyzing Your
Text Data, in the Storing geographical points in the index recipe, so please refer to that for
the explanation.
The initial query returning all the companies that have the word company in their name field
returns all the stored fields (the fl=* part of the query). The interesting part comes with the
dist:geodist(loc,50.0,28.0) part of the fl parameter. As you remember from the
Using field aliases recipe, we told Solr that we want to have a new field called dist returned
and we want it to be a value of the dist function query which takes three parameters: the field
in the index (in our case it is loc), the latitude, and the longitude, and returns the distance
between the point stored in the loc field, and the point described by the latitude and longitude.
The value is then returned as the dist field of each of the returned documents.
www.it-ebooks.info
www.it-ebooks.info
5
Using the Faceting
Mechanism
In this chapter we will cover:
ff Getting the number of documents with the same field value
ff Getting the number of documents with the same value range
ff Getting the number of documents matching the query and the sub query
ff Removing filters from faceting results
ff Sorting faceting results in alphabetical order
ff Implementing the autosuggest feature using faceting
ff Getting the number of documents that don't have a value in the field
ff Having two different facet limits for two different fields in the same query
ff Using decision tree faceting
ff Calculating faceting for relevant document groups
Introduction
One of the advantages of Solr is the ability to group results on the basis of some fields'
contents. The Solr classification mechanism, called faceting, provides the functionalities
which can help us in several tasks that we need to do in everyday work, from getting the
number of documents with the same values in a field (for example, the companies from the
same city) using the ability of date and range faceting, to the autocomplete features based on
the faceting mechanism. This chapter will show you how to handle some of the common tasks
when using the faceting mechanism.
www.it-ebooks.info
Using the Faceting Mechanism
156
Getting the number of documents with the
same field value
Imagine a situation where besides the search results, you have to return the number of
documents with the same field value. For example, imagine that you have an application
that allows the user to search for companies in Europe, and your client wants the number
of companies in the cities where the companies that were found by the query are located.
To do this, you could of course run several queries but Solr provides a mechanism called
faceting that can do that for you. This recipe will show you how to do it.
How to do it...
For getting the number of documents with the same field value, follow these steps:
1. To start, let's assume that we have the following index structure (just add this
to your schema.xml file in the field definition section; we will use the city field
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
2. The next step is to index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="city">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="city">New Orleans</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="city">New York</field>
</doc>
</add>
3. Let's suppose that our hypothetical user searches for the word company. The query
that will get us what we want should look like this:
http://localhost:8983/solr/select?q=name:company&facet=true&facet.
field=city
www.it-ebooks.info
Chapter 5
157
The result of the query should be like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.field">city</str>
<str name="q">name:company</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="city">New York</str>
<str name="id">1</str>
<str name="name">Company 1</str>
</doc>
<doc>
<str name="city">New Orleans</str>
<str name="id">2</str>
<str name="name">Company 2</str>
</doc>
<doc>
<str name="city">New York</str>
<str name="id">3</str>
<str name="name">Company 3</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">2</int>
<int name="New Orleans">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, besides the normal results list, we got faceting results with the numbers that
we wanted. Now let's look at how that happened.
www.it-ebooks.info
Using the Faceting Mechanism
158
How it works...
The index structure and the data are pretty simple and they make the example easier to
understand. The company is described by three fields. We are particularly interested in the
city field. This is the field that we want to use to get the number of companies that have
the same value in this field—which basically means that they are in the same city.
To do that, we run a query to Solr and inform the query parser that we want the documents
that have the word company in the title field. Additionally we say that we want to enable
the faceting mechanism, by using the facet=true parameter. The facet.field parameter
tells Solr which field to use to calculate faceting numbers. You can specify the facet.field
parameter multiple times to get faceting numbers for different fields in the same query.
As you can see in the results list, the results of all types of faceting are grouped in the list
with the name="facet_counts" attribute. The field based faceting is grouped under the
list with the name="facet_fields" attribute. Every field that you specified using the
facet.field parameter has its own list which has the attribute name, the same as the
value of the parameter in the query—in our case it is city. Then finally you can see the
results that we are interested in: the pairs of values (name attribute) and how many
documents have the value in the specified field.
There's more...
There are two more things I would like to share about field faceting:
ff How to show facets with counts greater than zero: The default behavior of Solr is to
show all the faceting results irrespective of the counts. If you want to show only the
facets with counts greater than zero than you should add the facet.mincount=1
parameter to the query (you can set this parameter to another value if you are
interested in any arbitrary value).
ff Lexicographical sorting of the faceting results: If you want to sort the faceting
results lexicographically, and not by the highest count (which is the default behavior),
then you need to add the facet.sort=index parameter.
Getting the number of documents with the
same value range
Imagine that you have an application where users can search the index to find a car for rent.
One of the requirements of the application is to show a navigation panel, where the user can
choose the price range for the cars that they are interested in. To do it in an efficient way, we
will use range faceting and this recipe will show you how to do it.
www.it-ebooks.info
Chapter 5
159
How to do it...
For getting the number of documents with the same value range, follow these steps:
1. Let's begin with the following index structure (just add this to your schema.xml
file in the field definition section; we will use the price field to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
2. The example data that we will use is like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Super Mazda</field>
<field name="price">50</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mercedes Benz</field>
<field name="price">210</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Bentley</field>
<field name="price">290</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Super Honda</field>
<field name="price">99.90</field>
</doc>
</add>
3. Now, as you recall, our requirement was to show the navigation panel with
price ranges. To do that, we need to get that data from Solr. We also know
that the minimum price for car rent is 1 dollar and the maximum is 400
dollars. To get the price ranges from Solr, we send the following query:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
range=price&facet.range.start=0&facet.range.end=400&facet.range.
gap=100
www.it-ebooks.info
Using the Faceting Mechanism
160
The query will produce the following result list:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="facet.range.start">0</str>
<str name="facet.range">price</str>
<str name="facet.range.end">400</str>
<str name="facet.range.gap">100</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="4" start="0"/>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges">
<lst name="price">
<lst name="counts">
<int name="0.0">2</int>
<int name="100.0">0</int>
<int name="200.0">2</int>
<int name="300.0">0</int>
</lst>
<float name="gap">100.0</float>
<float name="start">0.0</float>
<float name="end">400.0</float>
</lst>
</lst>
</lst>
</response>
So we got exactly what we wanted. Now let's see how it works.
www.it-ebooks.info
Chapter 5
161
How it works...
As you can see, the index structure is simple. There are three fields, one responsible for
the unique identifier, one responsible for the car name, and the last one responsible for
the price of rent.
The query is where all the magic is done. As we are not interested in the search results, we
ask for all documents in the index (q=*:* parameter) and we tell Solr not to return the search
results (rows=0 parameter). Then we tell Solr that we want the faceting mechanism to be
enabled for the query (facet=true parameter). We will not be using the standard faceting
mechanism, that is, the field based faceting. Instead we will use range faceting which is
optimized to work with ranges. So, we tell Solr which field will be used for range faceting by
adding the parameter facet.range with the price value. That means that the price field
will be used for the range faceting calculation. Then we specify the lower boundary from which
the range faceting calculation will begin. We do this by adding the facet.range.start
parameter; in our example we set it to 0. Next we have the facet.range.end parameter
which tells Solr when to stop the calculation of the range faceting. The last parameter
(facet.range.gap) informs Solr about the length of the periods that will be calculated.
Remember that when using the range faceting mechanism you must specify the
three parameters:
ff facet.range.start
ff facet.range.end
ff facet.range.gap
Otherwise, the range faceting mechanism won't work.
In the faceting results you can see the periods and the number of documents that were found
in each of them. The first period can be found under the <int name="0.0"> tag. This period
consists of prices from 0 to 100 (in mathematical notation it would be <0; 100>). It contains
two cars. The next period can be found under the <int name="100.0"> tag and consists of
prices from 100 to 200 (in mathematical notation it would be <100; 200>), and so on.
Getting the number of documents matching
the query and subquery
Imagine a situation where you have an application that has a search feature for cars. One of
the requirements is not only to show search results, but also to show the number of cars with
the price period chosen by the user. There is also another thing—those queries must be fast
because of the number of queries that will be run. Can Solr handle that? The answer is yes.
This recipe will show you how to do it.
www.it-ebooks.info
Using the Faceting Mechanism
162
How to do it...
For getting the number of documents matching the query and subquery, follow these steps:
1. Let's start with creating an index with the following structure (just add this to
your schema.xml file in the field definition section; we will use the price field
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
2. Now let's index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Car 1</field>
<field name="price">70</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Car 2</field>
<field name="price">101</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Car 3</field>
<field name="price">201</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Car 4</field>
<field name="price">99.90</field>
</doc>
</add>
Now, recall our requirement cars that match the query (let's suppose that our user
typed car), and show the counts in the chosen price periods. For the purpose of the
recipe let's assume that the user has chosen two periods of prices:
?? 10 to 80
?? 90 to 300
www.it-ebooks.info
Chapter 5
163
3. The query to achieve such a requirement should look like this:
http://localhost:8983/solr/select?q=name:car&facet=true&facet.
query=price:[10 TO 80]&facet.query=price:[90 TO 300]
The result list of the query should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<arr name="facet.query">
<str>price:[10 TO 80]</str>
<str>price:[90 TO 300]</str>
</arr>
<str name="q">name:car</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="id">1</str>
<str name="name">Car 1</str>
<float name="price">70.0</float>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Car 2</str>
<float name="price">101.0</float>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Car 3</str>
<float name="price">201.0</float>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Car 4</str>
<float name="price">99.9</float>
</doc>
</result>
www.it-ebooks.info
Using the Faceting Mechanism
164
<lst name="facet_counts">
<lst name="facet_queries">
<int name="price:[10 TO 80]">1</int>
<int name="price:[90 TO 300]">3</int>
</lst>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
</lst>
</response>
How it works...
As you can see, the index structure is simple. There are three fields, one responsible for the
unique identifier, one responsible for the car name, and the last one responsible for the price.
Next we have the query. First you can see a standard query where we tell Solr that we want to
get all the documents that have the word car in the name field (the q=name:car parameter).
Next, we say that we want to use the faceting mechanism by adding the facet=true
parameter to the query. This time we will use the query faceting type. This means that we
can pass the query to the faceting mechanism and as a result we will get the number of
documents that match the given query. In our example case, we wanted two periods like this:
ff One from the price of 10 to 80
ff Another from the price of 90 to 300
This is achieved by adding the facet.query parameter with the appropriate value. The first
period is defined as a standard range query to the price field (price:[10 TO 80]). The
second query is very similar, just with different values. The value passed to the facet.query
parameter should be a Lucene query written using the default query syntax.
As you can see in the results, the query faceting results are grouped under the <lst
name="facet_queries"> XML tag with the names exactly as in the queries sent to Solr.
You can see that Solr correctly calculated the number of cars in each of the periods, which
means that this is a perfect solution for us when we can't use the range faceting mechanism.
Removing filters from faceting results
Let's assume for the purpose of this recipe that you have an application that can search for
companies within a city and state. But the requirements say that you should show not only
the search results but also the number of companies in each city and the number of
companies in each state (in the Solr way we say that you want to exclude the filter query
from the faceting results). Can Solr do that in an efficient way? Sure it can, and this recipe
will show you how to do it.
www.it-ebooks.info
Chapter 5
165
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. We start with the following index structure (just add this to your schema.xml file in
the field definition section; we will use the city and state fields to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
<field name="state" type="string" indexed="true" stored="true />
2. The second step would be to index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company 1</field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company 2</field>
<field name="city">New Orleans</field>
<field name="state">Luiziana</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company 3</field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Company 4/field>
<field name="city">New York</field>
<field name="state">New York</field>
</doc>
</add>
www.it-ebooks.info
Using the Faceting Mechanism
166
3. Let's suppose that our hypothetical user searched for the word company, and told
our application that he needs the companies matching the word in the state of New
York. In that case, the query that will fulfill our requirement should look like this:
http://localhost:8983/solr/select?q=name:company&facet=true
&fq={!tag=stateTag}state:"New York"&facet.field={!ex=stateTag}
city&facet.field={!ex=stateTag}state
The result for the query will look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<arr name="facet.field">
<str>{!ex=stateTag}city</str>
<str>{!ex=stateTag}state</str>
</arr>
<str name="fq">{!tag=stateTag}state:"New York"</str>
<str name="q">name:company</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company 1</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Company 3</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Company 4</str>
<str name="city">New York</str>
<str name="state">New York</str>
</doc>
www.it-ebooks.info
Chapter 5
167
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">3</int>
<int name="New Orleans">1</int>
</lst>
<lst name="state">
<int name="New York">3</int>
<int name="Luiziana">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
Now let's see how it works.
How it works...
The index structure is pretty simple—it contains four fields that describe the company.
The search will be performed against the name field, while the filtering and the faceting
is done with the use of the state and city fields.
So let's get on with the query. As you can see, we have some typical elements there. First
the q parameter, which just tells Solr where and what to search for. Then the facet=true
parameter that enables the faceting mechanism. So far, so good. Following that, you have
a strange looking filter query (the fq parameter) with the value of fq={!tag=stateTag}
state:"New York". It tells Solr to only show those results that have New York in the
state field. By adding the {!tag=stateTag} part, we basically gave that filter query
a name (stateTag), which we will use further.
Now, look at the two facet.field parameters. Our requirement was to show the number
of companies in all states and in all cities. The only thing that was preventing us from getting
those numbers was the filter query we added to the query. So let's exclude it from the faceting
results. How to do it ? It's simple—just add {!ex=stateTag} to the beginning of each of the
facet.field parameters, like this: facet.field={!ex=stateTag}city. It tells Solr to
exclude the filter with the passed name.
As you can see in the results list, we got the correct numbers which means that the exclude
works as intended.
www.it-ebooks.info
Using the Faceting Mechanism
168
Sorting faceting results in alphabetical
order
Imagine a situation where you have a website, where you present some kind of
advertisements, for example, house rental advertisements. One of the requirements is to
show a list of cities in which the offer, that matched the query typed by the user, are located.
So the first thing you think is to use the faceting mechanism – and that's a good idea. But
then, your boss tells you that he is not interested in the counts and you have to sort the
results in the alphabetical order. So, is Solr able to do it? Of course it is and this recipe will
show you how to do it.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. For the purpose of the recipe let's assume that we have the following index structure
(just add this to your schema.xml file to the field definition section; we will use the
city field to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="city" type="string" indexed="true" stored="true" />
2. This index structure is responsible for holding information about companies and their
location. Now, let's index the example data matching the presented index structure:
<add>
<doc>
<field name="id">1</field>
<field name="name">House 1</field>
<field name="city">New York</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">House 2</field>
<field name="city">Washington</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">House 3</field>
www.it-ebooks.info
Chapter 5
169
<field name="city">Washington</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">House 4</field>
<field name="city">San Francisco</field>
</doc>
</add>
3. Let's assume that our hypothetical user typed house in the search box. The query
to return the search results with the faceting results sorted alphabetically should
be like this:
http://localhost:8983/solr/select?q=name:house&facet=true&facet.
field=city&facet.sort=index
The results returned by Solr for the query should look like this:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.field">city</str>
<str name="facet.sort">index</str>
<str name="q">name:house</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="city">New York</str>
<str name="id">1</str>
<str name="name">House 1</str>
</doc>
<doc>
<str name="city">Washington</str>
<str name="id">2</str>
<str name="name">House 2</str>
</doc>
<doc>
<str name="city">Washington</str>
<str name="id">3</str>
<str name="name">House 3</str>
www.it-ebooks.info
Using the Faceting Mechanism
170
</doc>
<doc>
<str name="city">San Francisco</str>
<str name="id">4</str>
<str name="name">House 4</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="city">
<int name="New York">1</int>
<int name="San Francisco">1</int>
<int name="Washington">2</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see the faceting results returned by Solr are not sorted by counts but in
alphabetical order. Now let's see how it works.
How it works...
The index structure and the example data are only here to help us make a query so I'll skip
discussing them.
The query shown in the recipe differs from the standard faceting query by only one
parameter—facet.sort. It tells Solr how to sort the faceting results. The parameter
can be assigned one of two values:
ff count – which tells Solr to sort the faceting results placing the highest counts first
ff index – which tells Solr to sort the faceting results by index order, which means that
the results will be sorted lexicographically
For the purpose of the recipe we chose the second option and as you can see in the returned
results, we got what we wanted.
www.it-ebooks.info
Chapter 5
171
Implementing the autosuggest feature using
faceting
There are plenty of web-based applications that help users choose what they want to search
for. One of the features that helps users is the autocomplete (or autosuggest) feature, like
the one that most of the most used search engines have. Let's assume that we have an
e-commerce library and we want to help the user to choose a book title—we want to enable
autosuggest on the basis of the title. This recipe will show you how to do that.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. Let's begin with the assumption of having the following index structure (just add
this to your schema.xml file in the fields definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="title_autocomplete" type="lowercase" indexed="true"
stored="true">
2. We also want to add some field copying to do some operations automatically.
To do that we need to add the following line after the fields section in your
schema.xml file:
<copyField source="title" dest="title_autocomplete" />
3. The lowercase field type should look like this (just add this to your schema.xml
file to the type definitions):
<fieldType name="lowercase" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory" />
</analyzer>
</fieldType>
www.it-ebooks.info
Using the Faceting Mechanism
172
4. Now, let's index a sample data file which could look like this:
<add>
<doc>
<field name="id">1</field>
<field name="title">Lucene or Solr ?</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">My Solr and the rest of the world</field>
</doc>
<doc>
<field name="id">3</field>
<field name="title">Solr recipes</field>
</doc>
<doc>
<field name="id">4</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
5. Let's assume that our hypothetical user typed the letters so in the search box and
we want to give him the first 10 suggestions with the highest counts. We also want
to suggest the whole titles, not just single words. To do that, we should send the
following query to Solr:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
field=title_autocomplete&facet.prefix=so
As a result for the query, Solr returned the following output:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">16</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="facet.prefix">so</str>
<str name="facet.field">title_autocomplete</str>
<str name="rows">0</str>
</lst>
</lst>
www.it-ebooks.info
Chapter 5
173
<result name="response" numFound="4" start="0"/>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="title_autocomplete">
<int name="solr cookbook">1</int>
<int name="solr recipes">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, we got what we wanted in the faceting results. Now let's see how it works.
How it works...
You can see that our index structure defined in the schema.xml file is pretty simple.
Every book is described by two fields, id and title. The additional field will be used
to provide the autosuggest feature.
The copy field section is there to automatically copy the contents of the title field to
the title_autocomplete field.
The lowercase field type is a type we will use to provide the autocomplete feature; this is
the same for lowercase words typed by the users as well as uppercase words. If we want to
show different results for uppercased and lowercased letters then the string type will be
sufficient.
Now let's take a look at the query. As you can see we are searching the whole index (the
parameter q=*:*), but we are not interested in any search results (the rows=0 parameter).
We tell Solr that we want to use the faceting mechanism (facet=true parameter) and that
it will be field-based faceting on the basis of the title_autocomplete field (the facet.
field=title_autocomplete parameter). The last parameter, facet.prefix, can be
something new. Basically it tells Solr to return only those faceting results that begin with the
prefix specified as the value of this parameter, which in our case is the value of so. The use of
this parameter enables us to show the suggestions that the user is interested in, and we can
see in the results that we achieved what we wanted.
There's more...
There is one more thing I would like to say about autosuggestion functionality.
www.it-ebooks.info
Using the Faceting Mechanism
174
Suggesting words not whole phrases
If you want to suggest words instead of a whole phrase you don't have to change much
of the previous configuration. Just change the type of title_autocomplete to the type
based on solr.TextField (for example, the text_ws field type). You should remember,
though, not to use heavily analyzed text (like stemmed text) to be sure that your word won't
be modified too much.
Getting the number of documents that don't
have a value in the field
Let's imagine we have an e-commerce library where we put some of our books on a special
promotion, for example, we give them away for free. We want to share that knowledge with our
customers and say: Hey! You searched for Solr, we found this, but we also have X books that
are free! To do that, we index the books that are free without the price defined. But how do
you make a query to Solr to retrieve the data that we want? This recipe will show you how.
Getting ready
Before you start reading this recipe, please take a look at the Getting the number of
documents matching the query and the subquery recipe in this chapter.
How to do it...
1. Let's begin with the following index structure (just add this to your schema.xml
file in the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="title" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true">
2. We will also use the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="title">Lucene or Solr ?</field>
<field name="price">11</field>
</doc>
<doc>
<field name="id">2</field>
<field name="title">My Solr and the rest of the world</field>
<field name="price">44</field>
</doc>
www.it-ebooks.info
Chapter 5
175
<doc>
<field name="id">3</field>
<field name="title">Solr recipes</field>
<field name="price">15</field>
</doc>
<doc>
<field name="id">4</field>
<field name="title">Solr cookbook</field>
</doc>
</add>
As you can see, the first three documents have a value in the price field, while
the last one doesn't. So now, for the purpose of the example, let's assume that
our hypothetical user is trying to find books that have solr in their title field.
3. Besides the search results, we want to show the number of documents that don't
have a value in the price field. To do that, we send the following query to Solr:
http://localhost:8983/solr/select?q=title:solr&facet=true&facet.
query=!price:[* TO *]\
The query should result in the following output from Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.query">!price:[* TO *]</str>
<str name="q">title:solr</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
<str name="id">3</str>
<float name="price">15.0</float>
<str name="title">Solr recipes</str>
</doc>
<doc>
<str name="id">4</str>
<str name="title">Solr cookbook</str>
</doc>
<doc>
www.it-ebooks.info
Using the Faceting Mechanism
176
<str name="id">1</str>
<float name="price">11.0</float>
<str name="title">Lucene or Solr ?</str>
</doc>
<doc>
<str name="id">2</str>
<float name="price">44.0</float>
<str name="title">My Solr and the rest of the world</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries">
<int name="!price:[* TO *]">1</int>
</lst>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see we got the proper results. Now let's see how it works.
How it works...
You can see that our index structure defined in the schema.xml file is pretty simple. Every
book is described by three fields, id, title, and price. Their names speak for the type of
information they will hold.
The query is in most parts something you should be familiar with. First, we tell Solr that we
are searching for documents that have the word solr in the title field (the q=title:solr
parameter). Then we say that we want to have the faceting mechanism enabled by adding the
facet=true parameter. Then we add a facet query parameter that tells Solr to return the
number of documents that don't have a value in the price field. We do that by adding the
facet.query=!price:[* TO *] parameter. How does that work? You should be familiar
with how the facet.query parameter works, so I'll skip that part. The price:[* TO *]
expression tells Solr to count all the documents that have a value in the price field. By
adding the ! character before the fieldname, we tell Solr to negate the condition and in fact
we get the number of documents that don't have any value in the specified field.
www.it-ebooks.info
Chapter 5
177
Having two different facet limits for two
different fields in the same query
Imagine a situation where you have a database of cars in your application. Besides the
standard search results, you want to show two faceting by field results. The first of those
two faceting results, the number of cars in each category, should be shown without any
limits, while the second faceting, the one showing the cars by their manufacturer, should
be limited to a maximum of 10 results. Can we achieve it in one query? Yes, we can, and
this recipe will show you how to do it.
Getting ready
Before you start reading this recipe please take a look at the Getting the number of
documents with the same field value recipe in this chapter.
How to do it...
1. For the purpose of the recipe, let's assume that we have the following index structure
(just add this to your schema.xml file in the field definition section; we will use the
category and manufacturer fields to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="manufacturer" type="string" indexed="true"
stored="true" />
2. We will need some sample data. For example we can use a file that has the
following content:
<add>
<doc>
<field name="id">1</field>
<field name="name">Super Mazda car</field>
<field name="category">sport</field>
<field name="manufacturer">mazda</field>
</doc>
<doc>
www.it-ebooks.info
Using the Faceting Mechanism
178
<field name="id">2</field>
<field name="name">Mercedes Benz car</field>
<field name="category">limousine</field>
<field name="manufacturer">mercedes</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Bentley car</field>
<field name="category">limousine</field>
<field name="manufacturer">bentley</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Super Honda car</field>
<field name="category">sport</field>
<field name="manufacturer">honda</field>
</doc>
</add>
3. For the purpose of the example, let's assume that our hypothetical user is trying to
search the index for the word car. To do that we should send Solr the following query:
http://localhost:8983/solr/select?q=name:car&facet=true&facet.
field=category&facet.field=manufacturer&f.category.facet.limit=-
1&f.manufacturer.facet.limit=10
The query resulted in the following response from Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="f.category.facet.limit">-1</str>
<str name="facet">true</str>
<str name="q">name:car</str>
<arr name="facet.field">
<str>category</str>
<str>manufacturer</str>
</arr>
<str name="f.manufacturer.facet.limit">10</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
<doc>
www.it-ebooks.info
Chapter 5
179
<str name="id">3</str>
<str name="name">Bentley car</str>
<str name="category">limousine</str>
<str name="manufacturer">bentley</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Super Mazda car</str>
<str name="category">sport</str>
<str name="manufacturer">mazda</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Mercedes Benz car</str>
<str name="category">limousine</str>
<str name="manufacturer">mercedes</str>
</doc>
<doc>
<str name="id">4</str>
<str name="name">Super Honda car</str>
<str name="category">sport</str>
<str name="manufacturer">honda</str>
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="category">
<int name="limousine">2</int>
<int name="sport">2</int>
</lst>
<lst name="manufacturer">
<int name="bentley">1</int>
<int name="honda">1</int>
<int name="mazda">1</int>
<int name="mercedes">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
Now let's see how it works.
www.it-ebooks.info
Using the Faceting Mechanism
180
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml
file and the example data, every document is described by four fields—id, name, category,
and manufacturer. I think that their names speak for themselves and I don't need to
discuss them.
The first parts of the query are pretty standard. We ask for documents which have the word
car in their name field. Then we tell Solr to enable faceting (the facet=true parameter) and
we tell it what field will be used to calculate faceting results (the facet.field=category
and the facet.field=manufacturer parameters). Then we specify the limits. By adding
the parameter limits in a way shown in the example (f.FIELD_NAME.facet.limit) we
tell Solr to set the limits for the faceting calculation for the particular field. In our example
query, by adding the f.category.facet.limit=-1 parameter we told Solr that we don't
want any limits on the number of faceting results for the category field. By adding the
f.manufacturer.facet.limit=10 parameter we told Solr that we want a maximum of 10
faceting results for the manufacturer field.
Following the pattern you can specify per-field values for faceting properties such as sorting
and minimum count.
Using decision tree faceting
Imagine that in our store we have products divided into categories. In addition to that, we
store information about the stock of the items. Now, we want to show our crew how many of
the products in the categories are in stock and how many we are missing. The first thing that
comes to mind is using the faceting mechanism and some additional calculation. But why
bother, when Solr 4.0 can do that calculation for us with the use of so called pivot faceting.
This recipe will show you how to use it.
How to do it...
The following steps illustrate the use of pivot faceting:
1. Let's start with the following index structure (just add this to your schema.xml
file in the field definition section; we will use the category and stock fields
to do the faceting):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="stock" type="boolean" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 5
181
2. Now let's index the following example data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook 1</field>
<field name="category">workbooks</field>
<field name="stock">false</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook 2</field>
<field name="category">workbooks</field>
<field name="stock">true</field>
</doc>
</add>
3. Let's assume we are running a query from the administration panel of our
shop and we are not interested in the documents at all; we only want to know
how many documents are in stock or out of stock for each of the categories.
The query implementing that logic should look like this:
http://localhost:8983/solr/select?q=*:*&rows=0&facet=true&facet.
pivot=category,stock
The response to the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">76</int>
<lst name="params">
<str name="facet">true</str>
www.it-ebooks.info
Using the Faceting Mechanism
182
<str name="indent">true</str>
<str name="facet.pivot">category,stock</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="4" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields"/>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
<lst name="facet_pivot">
<arr name="category,stock">
<lst>
<str name="field">category</str>
<str name="value">books</str>
<int name="count">2</int>
<arr name="pivot">
<lst>
<str name="field">stock</str>
<bool name="value">true</bool>
<int name="count">2</int>
</lst>
</arr>
</lst>
<lst>
<str name="field">category</str>
<str name="value">workbooks</str>
<int name="count">2</int>
<arr name="pivot">
<lst>
<str name="field">stock</str>
<bool name="value">false</bool>
<int name="count">1</int>
</lst>
<lst>
<str name="field">stock</str>
<bool name="value">true</bool>
<int name="count">1</int>
</lst>
</arr>
</lst>
</arr>
</lst>
</lst>
</response>
You will notice that we received what we wanted, now let's see how it works.
www.it-ebooks.info
Chapter 5
183
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml file
and the example data, every document is described by four fields—id, name, category, and
stock. I think that their names speak for themselves and I don't need to discuss them.
The interesting things start with the query. We specified that we want the query to match all
the documents (q=*:* parameter), but we don't want to see any documents in the response
(rows=0 parameter). In addition to that, we want to have faceting calculation (facet=true
parameter) and we want to use the decision tree faceting, also known as pivot faceting. We
do that by specifying which fields should be included in the tree faceting. In our case we
want the top level of the pivot facet to be calculated on the basis of the category field,
and the second level (the one nested in the category field calculation) should be based
on the values available in the stock field. Of course, if you would like to have another value
of another field nested under the stock field you can do that by adding another field to the
facet.pivot query parameter. Assuming you would like to see faceting on the price field
nested under the stock field, your facet.pivot parameter would look like this: facet.
pivot=category,stock,price.
As you can see in the response, each nested faceting calculation result is written inside
the <arr name="pivot"> XML tag. So let's look at the response structure. The first
level of your facet pivot tree is based on the category field. You can see two books (<int
name="count">2</int>) in the books category (<str name="value">books</str>),
and these books have the stock field (<str name="field">stock</str>) set to true
(<bool name="value">true</bool>). For the workbooks category, the situation is a
bit different, because you can see two different sections there—one for documents with the
stock field equal to false, and the other with the stock field set to true. But in the end,
the calculation is correct and that's what we wanted!
Calculating faceting for relevant documents
in groups
If you have ever used the field collapsing functionality of Solr you may be wondering if there is a
possibility of using that functionality and faceting. Of course there is, but the default behavior still
works so that you get the faceting calculation on the basis of documents not document groups.
In this recipe, we will learn how to query Solr so that it returns facets calculated for the most
relevant document in each group in order for your user facet counts to be more or less grouped.
Getting ready
Before reading this recipe please look at the Using field to group results, Using query to group
results, and Using function query to group results recipes in Chapter 8, Using Additional Solr
Functionalities. Also, if you are not familiar with faceting functionality, please read the first
three recipes in this chapter.
www.it-ebooks.info
Using the Faceting Mechanism
184
How to do it...
1. As a first step we need to create an index. For the purpose of the recipe let's assume
that we have the following index structure (just add this to your schema.xml file to
the field definition section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="stock" type="boolean" indexed="true" stored="true" />
2. The second step is to index the data. We will use some example data which looks
like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="category">books</field>
<field name="stock">true</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Workbook 1</field>
<field name="category">workbooks</field>
<field name="stock">false</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">Workbook 2</field>
<field name="category">Workbooks</field>
<field name="stock">true</field>
</doc>
</add>
www.it-ebooks.info
Chapter 5
185
3. So now it's time for our query. So, let's assume we want our results to be grouped
on the values of the category field, and we want the faceting to be calculated on
the stock field. And remember that we are only interested in the most relevant
document from each result group when it comes to faceting. So, the query that
would tell Solr to do what we want should look like this:
http://localhost:8983/solr/select?q=*:*&facet=true&facet.
field=stock&group=true&group.field=category&group.truncate=true
The results for the query would look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="facet">true</str>
<str name="q">*:*</str>
<str name="group.truncate">true</str>
<str name="group.field">category</str>
<str name="group">true</str>
<str name="facet.field">stock</str>
</lst>
</lst>
<lst name="grouped">
<lst name="category">
<int name="matches">4</int>
<arr name="groups">
<lst>
<str name="groupValue">books</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<str name="category">books</str>
<bool name="stock">true</bool></doc>
</result>
</lst>
<lst>
<str name="groupValue">workbooks</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">3</str>
www.it-ebooks.info
Using the Faceting Mechanism
186
<str name="name">Workbook 1</str>
<str name="category">workbooks</str>
<bool name="stock">false</bool>
</doc>
</result>
</lst>
</arr>
</lst>
</lst>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="stock">
<int name="false">1</int>
<int name="true">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see everything worked as it should. Now let's see how it works.
How it works...
Our data is very simple. As you can see in the field definition section of the schema.xml file
and the example data, every document is described by four fields—id, name, category, and
stock. I think that their names speak for themselves and I don't need to discuss them.
As it comes to the query, we fetch all the documents from the index (the q=*:* parameter).
Next, we say that we want to use faceting and we want it to be calculated on the stock field.
We want a grouping mechanism to be active and we want to group documents on the basis of
the category field (all the query parameters responsible for defining the faceting and grouping
behavior are described in the appropriate recipes in this book, so please look at those if you
are not familiar with those parameters). And finally something new—the group.truncate
parameter is set to true. If set to true, like in our case, facet counts will be calculated using
only the most relevant document in each of the calculated groups. So in our case, for the group
with the category field equal to books, we have the true value in the stock field and for the
second group we have false in the stock field. Of course we are looking at the most relevant
documents, so the first ones in our case. So, as you can easily see, we've got two facet counts
for the stock field, both with a count of 1, which is what we would expect.
There is one thing more—at the time of writing this book, the group.truncate parameter
was not supported when using distributed search, so please be aware of that.
www.it-ebooks.info
6
Improving Solr
Performance
In this chapter we will cover:
ff Paging your results quickly
ff Configuring the document cache
ff Configuring the query result cache
ff Configuring the filter cache
ff Improving Solr performance right after the start up or commit operation
ff Caching whole result pages
ff Improving faceting performance for low cardinality fields
ff What to do when Solr slows down during indexing
ff Analyzing query performance
ff Avoiding filter caching
ff Controlling the order of execution of filter queries
ff Improving the performance of numerical range queries
Introduction
Performance of the application is one of the most important factors. Of course, there are
other factors, such as usability and availability—we could recite many more—but one of the
most crucial is performance. Even if our application is perfect in terms of usability, the
users won't be able to use it if they will have to wait for minutes for the search results.
www.it-ebooks.info
Improving Solr Performance
188
The standard Solr deployment is fast enough, but sooner or later a time will come when
you will have to optimize your deployment. This chapter and its recipes will try to help you
with the optimization of Solr deployment.
If your business depends on Solr, you should keep monitoring it even after optimization.
There are numerous solutions available in the market, from the generic and open-sourced
ones such as Gangila (http://ganglia.sourceforge.net/) to search-specific ones
such as Scalable Performance Monitoring (http://www.sematext.com/spm/index.
html) from Sematext.
Paging your results quickly
Imagine a situation where you have a user constantly paging through the search results.
For example, one of the clients I was working for was struggling with the performance of his
website. His users tend to search for a word and then page through the result pages – the
statistical information gathered from the application logs showed that typical users changed
the page about four to seven times. Apart from improving the query relevance (which isn't
what we will talk about in this recipe), we decided to optimize the paging. How do we do that?
This recipe will show you.
How to do it...
So, let's get back to my client deployment. As I mentioned, typical users typed a word into
the search box and then used the paging mechanism to go through a maximum of seven
pages. My client's application was showing 20 documents on a single page. So, it can be
easily calculated that we need about 140 documents in advance, apart from the first 20
documents returned by the query.
1. So what we did was actually pretty simple. First of all, we modified the
queryResultWindowSize property in the solrconfig.xml file and changed
it to the following value:
<queryResultWindowSize>160</queryResultWindowSize>
2. We then changed the maximum number of documents that can be cached for a
single query to 160, by adding the following property to the solrconfig.xml file:
<queryResultMaxDocsCached>160</queryResultMaxDocsCached>
We also modified queryResultCache, but that's a discussion for another recipe. To learn
how to change that cache, please refer to the How to configure the query result cache recipe
in this chapter.
www.it-ebooks.info
Chapter 6
189
How it works...
So how does Solr behave with the changes proposed in the preceding section? First of all,
queryResultWindowSize tells Solr to store (in documentCache) a maximum of the 160
documents IDs with every query. Therefore, after doing the initial query, we gather more
documents than we actually need. Because of this we are sure that when a user clicks on
the next page button, which is present in our application, the results will be taken from the
cache. So there won't be a need for intensive I/O operations. You must remember that the
160 documents IDs will be stored in the cache and won't be visible in the results list, as the
result size is controlled by the rows parameter.
The queryResultMaxDocsCached property tells Solr about the maximum number of
document IDs that can be cached for a single query (please remember than in this case, the
cache stores the document identifiers and not whole documents). We told Solr that we want
a maximum of 160 document IDs for a single query, because the statistics showed us that
we don't need more, at least for a typical user.
Of course, there is another thing that should be done – setting the query result cache size,
but that is discussed in another recipe.
Configuring the document cache
Cache can play a major role in your deployment's performance. One of the caches that you can
configure when setting up Solr is the document cache. It is responsible for storing the Lucene
internal documents that have been fetched from the disk. The proper configuration of this
cache can save precious I/O calls and therefore boost the whole deployment performance.
This recipe will show you how to properly configure the document cache.
How to do it...
For the purpose of this recipe, I assumed that we are dealing with the deployment of Solr
where we have about 100, 000 documents. In our case, a single Solr instance is getting a
maximum of 10 concurrent queries and the maximum number of documents that a query
can fetch is 256.
With the preceding parameters, our document cache should look similar to the following code
snippet (add this code to the solrconfig.xml configuration file):
<documentCache
class="solr.LRUCache"
size="2560"
initialSize="2560"/>
www.it-ebooks.info
Improving Solr Performance
190
Notice that we didn't specify the autowarmCount parameter—this is because the document
cache uses Lucene's internal ID to identify documents. These identifiers can't be copied
between index changes and thus we can't automatically warm this cache.
How it works...
The document cache configuration is simple. We define it in the documentCache XML tag
and specify a few parameters that define the document cache's behavior. First of all, the
class parameter tells Solr which Java class should be used for implementation. In our
example, we use solr.LRUCache because we will be adding more information into the
cache than we will be fetching from it. When you see that you are getting more information
than you add, consider using solr.FastLRUCache. The next parameter tells Solr the
maximum size of the cache (the size parameter). As the Solr wiki says, we should always
set this value to more than the maximum number of results returned by the query multiplied
by the maximum concurrent queries than we think will be sent to the Solr instance. This will
ensure that we always have enough place in the cache, so that Solr will not have to fetch the
data from the index multiple times during a single query.
The last parameter tells Solr the initial size of the cache (the initialSize parameter). I
tend to set it to the same value as the size parameter to ensure that Solr won't be wasting
its resources on cache resizing.
The more fields marked as stored in the index structure, the higher
the memory usage of this cache will be.
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way. Remember
that having a very large cache with very low hit rate can be worse than having no cache at all.
Along with everything else, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you
have a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one
of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll
repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the query result cache
The major Solr role in a typical e-commerce website is handling user queries. Of course, users
of the site can type multiple queries in the Search box and we can't easily predict how many
unique queries there may be. But, using the logs that Solr gives us, we can check how many
different queries there were in the last day, week, month, or year. Using this information, we
can configure the query result cache to suit our needs in the most optimal way, and this recipe
will show you how to do it.
www.it-ebooks.info
Chapter 6
191
How to do it...
For the purpose of this recipe, let's assume that one Solr instance of our e-commerce website
is handling about 10 to 15 queries per second. Each query can be sorted by four different
fields (the user can choose by which field). The user can also choose the order of sort. By
analyzing the logs for the past three months, we know that there are about 2000 unique
queries that users tend to type in the search box of our application. We also noticed that
our users don't usually use the paging mechanism.
On the basis of this information, we configure our query results cache as follows (add this
code to the solrconfig.xml configuration file):
<queryResultCache
class="solr.LRUCache"
size="16000"
initialSize="16000"
autowarmCount="4000"/>
How it works...
Adding the query result cache to the solrconfig.xml file is a simple task. We define it in
the queryResultCache XML tag and specify a few parameters that define the query result's
cache behavior. First of all, the class parameter tells Solr which Java class should be used
for implementation. In our example, we use solr.LRUCache because we will be adding
more information into the cache than we will fetching from it. When you see that you are get
more information than you add, consider using solr.FastLRUCache. The next parameter
tells Solr about the maximum size of the cache (the size parameter). This cache should be
able to store the ordered identifiers of the objects that were returned by the query with its
sort parameter and the range of documents requested. This means that we should take
the number of unique queries, multiply it by the number of sort parameters and the number
of possible orders of sort. So in our example, the size should be at least the result of the
following equation:
size = 2000 * 4 * 2
In our case, it is 16,000.
I tend to set the initial size of this cache to the maximum size; so in our case, I set the
initialSize parameter to a value of 16000. This is done to avoid the resizing of the cache.
The last parameter (autowarmCount) says how many entries should be copied when Solr
invalidates caches (for example, after a commit operation). I tend to set this parameter to
a quarter of the maximum size of the cache. This is done because I don't want the caches
to be warming for too long. However, please remember that the auto-warming time depends
on your deployment and the autowarmCount parameter should be adjusted if needed.
www.it-ebooks.info
Improving Solr Performance
192
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything else, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you
have a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one
of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll
repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the filter cache
Almost every client of mine who uses Solr, tends to forget or simply doesn't know how to use
filter queries or simply filters. People tend to add another clause with a logical operator to the
main query—they forget how efficient filters can be, at least when used wisely. And that's why
whenever I can, I tell people using Solr to use filter queries. But when using filter queries, it is
nice to know how to set up a cache that is responsible for holding the filters results – the filter
cache. This recipe will show you how to properly set up the filter cache.
How to do it...
For the purpose of this recipe, let's assume that we have a single Solr slave instance to
handle all the queries coming from the application. We took the logs from the last three
months and analyzed them. From this we know, that our queries are making about 2000
different filter queries. By getting this information, we can set up the filter cache for our
instance. This configuration should look similar to the following code snippet (add this
code to the solrconfig.xml configuration file):
<filterCache
class="solr.FastLRUCache"
size="2000"
initialSize="2000"
autowarmCount="1000"/>
That's it. Now let's see what those values mean.
How it works...
As you may have noticed, adding the filter cache to the solrconfig.xml file is a simple
task; you just need to know how many unique filters your Solr instance is receiving. We define
this in the filterCache XML tag and specify a few parameters that define the query result
cache behavior. First of all, the class parameter tells Solr which Java class should be used
for implementation. In our example, we use solr.LRUCache because we will be adding more
information into the cache than we will fetching from it. When you see that you are getting
more information than you add, consider using solr.FastLRUCache.
www.it-ebooks.info
Chapter 6
193
The next parameter tells Solr the maximum size of the cache (the size parameter). In our
case, we said that we have about 2000 unique filters and we set the maximum size to that
value. This is done because each entry of the filter cache stores the unordered sets of Solr
document identifiers that match the given filter. In this way, after the first use of the filter,
Solr can use the filter cache to apply filtering and thus save the I/O operations.
The next parameter – initialSize tells Solr about the initial size of the filter cache. I tend
to set it's value to the same as that of the size parameter to avoid cache resizing. So in our
example, we set it to the value of 2000.
The last parameter (autowarmCount) says how many entries should be copied when Solr
invalidates caches (for example, after a commit operation). I tend to set this parameter to
a quarter of the maximum size of the cache. This is done because I don't want the caches
to be warming for too long. However, please remember that the auto-warming time depends
on your deployment and the autowarmCount parameter should be adjusted if needed.
Please remember that when using the values shown in this example, you must always observe
your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything, you should pay attention to your cache usage as your Solr instances
work. If you see evictions, then this may be a signal that your caches are too small. If you have
a very poor hit rate, then it's sometimes better to turn the cache off. Cache setup is one of those
things in Apache Solr that is very dependent on your data, queries, and users; so I'll repeat once
again—keep an eye on your caches and don't be afraid to react and change them. For example,
take a look at the following screenshot that shows that the filter cache is probably too small,
because the evictions are happening (this is a screenshot of the Solr administration panel):
www.it-ebooks.info
Improving Solr Performance
194
Improving Solr performance right after the
startup or commit operation
Anyone with some experience with Solr would have noticed that – right after the startup, Solr
doesn't have as much of an improved query performance as after running a while. This happens
because Solr doesn't have any information stored in caches, the I/O is not optimized, and so on.
Can we do something about it? Of course we can, and this recipe will show you how to do it.
How to do it...
The following steps will explain how we can enhance Solr performance right after the startup
or commit operation:
1. First of all, we need to identify the most common and the heaviest queries that we send
to Solr. I have two ways of doing this—first of all, I analyze the logs that Solr produces
and see how queries behave. I tend to choose those queries that are run often and
those that run slowly in my opinion. The second way of choosing the right queries is by
analyzing the application that use Solr and seeing what queries they produce, which
queries will be the most crucial, and so on. Based on my experience, the log-based
approach is usually much faster and can be done using self-written scripts.
But let's assume that we have identified the following queries as good candidates:
q=cats&fq=category:1&sort=title+desc,value+desc,score+desc
q=cars&fq=category:2&sort=title+desc
q=harry&fq=category:4&sort=score+desc
2. What we will do next is just add the so called warming queries to the solrconfig.
xml file. So the listener XML tag definition in the solrconfig.xml file should
look similar to the following code snippet:
<listener event="firstSearcher"
class="solr.QuerySenderListener">
<arr name="queries">
<lst>
<str name="q">cats</str>
<str name="fq">category:1</str>
<str name="sort">
title desc,value desc,score desc
</str>
<str name="start">0</str>
www.it-ebooks.info
Chapter 6
195
<str name="rows">20</str>
</lst>
<lst>
<str name="q">cars</str>
<str name="fq">category:2</str>
<str name="sort">title desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">harry</str>
<str name="fq">category:4</str>
<str name="sort">score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
</arr>
</listener>
Basically we added the so-called warming queries to the startup of Solr. Now let's
see how it works.
How it works...
By adding the preceding fragment of configuration to the solrconfig.xml file, we told
Solr that we want it to run those queries whenever a firstSearcher event occurs. The
firstSearcher event is fired whenever a new searcher object is prepared and there is
no searcher object available in the memory. So basically, the firstSearcher event
occurs right after Solr startup.
So what happens after Solr startup? After adding the preceding fragment, Solr runs each
of the defined queries. By doing this, the caches get populated with the entries that are
significant for the queries that we identified. This means that if we did the job right, we
have Solr configured and ready to handle the most common and heaviest queries right
after its startup.
Let's just go over what all the configuration options mean. The warm up queries are always
defined under the listener XML tag. The event parameter tells Solr what event should
trigger the queries; in our case, it is firstSearcher. The class parameter is the Java
class that implements the listener mechanism. Next, we have an array of queries that
are bound together by the array tag with the name="queries" parameter. Each of
the warming queries is defined as a list of parameters that are grouped by the lst tag.
www.it-ebooks.info
Improving Solr Performance
196
There's more...
There is one more thing that I would like to mention (in the following section).
Improving Solr performance after commit operations
If you are interested in improving the performance of your Solr instance, you should also look
at the newSearcher event. This event occurs whenever a commit operation is performed
by Solr (for example, after replication). Assuming that we identified the same queries as
before as good candidates to warm the caches, we should add the following entries to the
solrconfig.xml file:
<listener event="newSearcher" class="solr.QuerySenderListener">
<arr name="queries">
<lst>
<str name="q">cats</str>
<str name="fq">category:1</str>
<str name="sort">title desc,value desc,score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">cars</str>
<str name="fq">category:2</str>
<str name="sort">title desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
<lst>
<str name="q">harry</str>
<str name="fq">category:4</str>
<str name="sort">score desc</str>
<str name="start">0</str>
<str name="rows">20</str>
</lst>
</arr>
</listener>
Please remember that the warming queries are especially important for the caches that
can't be automatically warmed.
www.it-ebooks.info
Chapter 6
197
Caching whole result pages
Imagine a situation where you have an e-commerce library and your data changes rarely. What
can you do to take away the stress on your search servers? The first thing that comes to mind
is caching; for example, HTTP caching. And yes, that is a good point. But do we have to set up
external caches prior to Solr, or can we tell Solr to use its own caching mechanism? We can
use Solr to cache whole result pages and this recipe will show you how to do it.
Getting ready
Before you continue to read this recipe, it would be nice for you to know some basics about
the HTTP cache headers. To learn something about it, please refer to the RFC document that
can be found on the W3 site at http://www.w3.org/Protocols/rfc2616/rfc2616-
sec13.html.
How to do it...
So let's configure the HTTP cache. To do this, we need to configure the Solr request
dispatcher. Let's assume that our index changes every 60 minutes.
1. Let's start by replacing the request dispatcher definition in the solrconfig.xml
file with the following content:
<requestDispatcher handleSelect="true">
<httpCaching lastModifiedFrom="openTime"
etagSeed="Solr">
<cacheControl>max-age=3600, public</cacheControl>
</httpCaching>
</requestDispatcher>
2. Now, let's try sending a query similar to the following to see the HTTP headers:
http://localhost:8983/solr/select?q=book
We get the following HTTP headers:
HTTP/1.1 200 OK
Cache-Control: max-age=3600, public
Expires: Tue, 11 Sep 2012 16:44:56 GMT
Last-Modified: Tue, 11 Sep 2012 15:43:24 GMT
ETag: "YzAwMDAwMDAwMDAwMDAwMFNvbHI="
Content-Type: application/xml; charset=UTF-8
Transfer-Encoding: chunked
From this we can tell that cache works.
www.it-ebooks.info
Improving Solr Performance
198
How it works...
The cache definition is defined inside the requestDispatcher XML tag. The
handleSelect="true" attribute describes error handling and it should be set to
true. Then, we see the httpCaching tag (notice the lack of the <httpCaching
never304="true"> XML tag), which actually configures the HTTP caching in Solr. The
lastModifiedFrom="openTime" attribute defines that the last modified HTTP header
will be relative to when the current searcher object was opened (for example, relative to the
last replication execution date). You can also set this parameter value to dirLastMod to
be relative to when the physical index was modified. Next, we have the eTagSeed attribute,
which is responsible for generating the ETag HTTP cache header.
The next configuration tag is the cacheControl tag, which can be used to specify the
generation of the cache control HTTP headers. In our example, adding the max-age=3600
parameter tells Solr that it should generate an additional HTTP cache header, which will
confirm that the cache is valid for a maximum of one hour. The public directive means
that the response can be cached by any cache type.
As you can see from the response, the headers that we got as a part of the results returned
by Solr tell us that we got what we wanted.
Improving faceting performance for low
cardinality fields
Let's assume that our data which we use to calculate faceting can be considered to have low
distinct values. For example, we have an e-commerce shop with millions of products – clothes.
Each document in our index, apart from name and price, is also described by additional
information – target size. So, we have values such as XS, S, M, L, XL, and XXL (that is, six
distinct values), and each document can only be described with a single value. In addition to
this, we run field faceting on that information and it doesn't work fast by default. This recipe
will show you how to change that.
How to do it...
The following steps will explain how we can improve faceting performance for low
cardinality fields:
1. Let's begin with the following index structure (add the following entries to your
schema.xml fields section):
<field name="id " type="string" indexed="true"
stored="true" required="true" />
www.it-ebooks.info
Chapter 6
199
<field name="name " type="text " indexed="true"
stored="true" />
<field name="size" type="string" indexed="true"
stored="true" />
The size field is the one in which we store our XS, S, M, L, XL, and XXL values
(remember: one value per document).
2. Assuming that our user typed black skirt into the Search box, our query would
look similar to the following code snippet:
q=name:(black+skirt)&q.op=AND&facet=true&facet.field=size
Assuming that the query is matching one-fourth of our documents, we can expect
the query to be executing longer than usual. This is because the default faceting
calculation is optimized for fields that have many unique values in the index and
we have the opposite—we have many documents but few unique terms.
3. In order to speed up faceting in our case, let's add the facet.method=enum
parameter to our query, so that it looks similar to the following code snippet:
q=name:(black+skirt)&q.op=AND&facet=true&facet.field=size&facet.
method=enum
If you measure the performance before and after the change you will notice the difference;
let's discuss why.
How it works...
Let's take a look at the query—we search for the given words in the name field using the AND
logical operator (q.op parameter). As our requirements state, we also run faceting on the
size field (facet=true and facet.field=size parameters).
We know that our fields have only six distinct values, and we also assumed that our queries
can return vast amount of documents. To handle such faceting calculation faster than the
default method, we decided to use the enum method of facet calculation. The default faceting
calculation method (facet.method=fc) iterates over documents that match the query and
sums the terms that appear in the field that we are calculating faceting on. The enum method
does the other thing – it enumerates all the terms in the field that we want to calculate
faceting on, and intersects the documents that match the query with the documents that
match the enumerated terms. In this way, less time and processing is needed to calculate
field faceting for low cardinality fields, such as size in our case, and thus we see faster
query execution.
It is good to know that for field faceting on Boolean fields, Solr uses the enum faceting method
by default.
www.it-ebooks.info
Improving Solr Performance
200
There's more...
You can also use the faceting method for each field you perform faceting upon.
Specifying faceting method per field
If you have multiple fields on which you run faceting, then you may only want to change the
method for one of them (or more than one, but not all). To do that, instead of adding the
facet.method=enum parameter, you can add the facet.FIELD_NAME.method=enum
parameter for each field whose faceting calculation method you would want to change. For
example, if you would like to change the faceting method for the size field, you can add the
following parameter:
facet.size.method=enum
What to do when Solr slows down during
indexing
One of the most common problems when indexing a vast amount of data is the indexing time.
Some of the problems with indexing time are not easily resolvable, but others are. Imagine that
you need to index about 300,000 documents that are in a single XML file. You run the post.
sh bash script that is provided with Solr and you wait, wait, and wait. Something is wrong –
when you index 10,000 documents you need about a minute, but now you are waiting about
an hour and the commit operation didn't take place. Is there something we can do to speed
it up? Sure, and this recipe will tell you how to.
How to do it...
The solution to the situation is very simple – just add the commit operation every now and then.
But as you may have noticed, I mentioned that our data is written in a single XML file. So, how do
we add the commit operation to that kind of data? Send it in parallel to the indexing process?
No, we need to enable the auto commit mechanism. To do that, let's modify the solrconfig.
xml file, and change the update handler definition to the following one:
<updateHandler class="solr.DirectUpdateHandler2">
<autoCommit>
<maxTime>60000</maxTime>
<openSearcher>true</openSearcher>
</autoCommit>
</updateHandler>
www.it-ebooks.info
Chapter 6
201
If you start the indexing described in the indexing process, you will notice that a commit
command will be sent once a minute while the indexing process is takes place. Now, let's
see how it works.
How it works...
Solr tends to slow down the indexing process when indexing a vast amount of data without the
commit commands being sent once in a while. This behavior is completely understandable
and is bound to the memory and how much of it Solr can use.
We can avoid the slowing down behavior by adding the commit command after the set
amount of time or set amount of data. In this recipe, we choose the first approach.
We assumed that it would be good to send the commit command once every minute. So we add
the <autoCommit> section with the <maxTime> XML tag set to a value of 60000. This value is
specified in milliseconds. We've also specified that we want the search to be reopened after the
commit and thus the data available for search (the <openSearcher>true</openSearcher>
option). If you would only like to write the data to the index and not have it available for search,
just change the <openSearcher>true</openSearcher> option to false. That's all we
need to do. After this change, Solr will send a commit command after every minute passes
during the indexing operation, and we don't have to worry that Solr indexing speed will
decrease over time.
There's more...
There are two more things about automatic commits that should be mentioned.
Commit after a set amount of documents
Sometimes, there is a need to rely not on the time between commit operations, but on the
amount of documents that were indexed. If this is the case, we can choose to automatically
send the commit command after a set amount of documents are processed. To do this, we
add the <maxDocs> XML tag with the appropriate amount. For example, if we want to send
the commit command after every 50000 documents, the update handler configuration
should look similar to the following code snippet:
<updateHandler class="solr.DirectUpdateHandler2">
<autoCommit>
<maxDocs>50000</maxDocs>
<openSearcher>true</openSearcher>
</autoCommit>
</updateHandler>
www.it-ebooks.info
Improving Solr Performance
202
Commit within a set amount of time
There may be situations when you want some of the document to be committed faster than
the auto commit settings. In order to do that, you can add the commitWithin attribute to
the <add> tag of your data XML time. This attribute will tell Solr to commit the documents
within the specified time (specified in milliseconds). For example, if we want the portion of
documents to be indexed within 100 milliseconds, our data file would look similar to the
following code snippet:
<add commitWithin="100">
<doc>
<field name="id">1</field>
<field name="title">Book 1</field>
</doc>
</add>
Analyzing query performance
Somewhere along the experience with Apache Solr (and not only Solr), you'll end up at a point
where some of your queries are not running as you would like them to run – some of them are
just slow. Of course, such a situation is not desirable and we have to do something to make
those queries run faster. But how do we know which part of the query is the one we should
look at ? This recipe will tell you what information you can get from Solr.
How to do it...
The following steps will help you analyze query performance:
1. Let's start with the assumption that we have a query that has parts that are not as
fast as we would like it to be. The query is as follows:
http://localhost:8983/solr/select?q=metal&facet=true&facet.
field=date&facet.query=from:[10+TO+2000]
2. In order to get the information we want, we need to add the debugQuery=true
parameter to our query, so that it looks similar to the following code snippet:
http://localhost:8983/solr/select?q=metal&facet=true&facet.
field=date&facet.query=from:[10+TO+2000]&debugQuery=true
The response from Solr is as follows (I've cut some parts of the response, because
it is quite large and we are only interested in the last section):
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">427</int>
www.it-ebooks.info
Chapter 6
203
</lst>
<result name="response" numFound="61553" start="0">
<doc>
(...)
</doc>
</result>
<lst name="facet_counts">
<lst name="facet_queries">
<int name="from:[10 TO 2000]">50820</int>
</lst>
<lst name="facet_fields">
<lst name="date">
<int name="0">61553</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
<lst name="debug">
<str name="rawquerystring">metal</str>
<str name="querystring">metal</str>
<str name="parsedquery">Body:metal</str>
<str name="parsedquery_toString">Body:metal</str>
<lst name="explain">
(...)
</lst>
<str name="QParser">LuceneQParser</str>
<lst name="timing">
<double name="time">426.0</double>
<lst name="prepare">
<double name="time">15.0</double>
<lst name="org.apache.solr
.handler.component.QueryComponent">
<double name="time">14.0</double>
</lst>
<lst name="org.apache.
solr.handler.component.FacetComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.MoreLikeThisComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.HighlightComponent">
<double name="time">0.0</double>
</lst>
www.it-ebooks.info
Improving Solr Performance
204
<lst name="org.apache.solr
.handler.component.StatsComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.DebugComponent">
<double name="time">0.0</double>
</lst>
</lst>
<lst name="process">
<double name="time">411.0</double>
<lst name="org.apache.solr
.handler.component.QueryComponent">
<double name="time">43.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.FacetComponent">
<double name="time">360.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.MoreLikeThisComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.HighlightComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.StatsComponent">
<double name="time">0.0</double>
</lst>
<lst name="org.apache.solr
.handler.component.DebugComponent">
<double name="time">8.0</double>
</lst>
</lst>
</lst>
</lst>
</response>
As you can see in the preceding response, there is some information about query time.
So let's see what it means.
www.it-ebooks.info
Chapter 6
205
How it works...
Let's not concentrate on the query, because it is only an example that allows us to discuss
what we want to achieve. We've added a single parameter to the query – debugQuery=true.
This parameter turns on the debug mode in Solr, as you can see in the response.
The debug mode is divided into few categories. All these categories are nested inside the
<lst name="debug"> XML tag. The first few entries let you see how the query parser
parses your query and how it is passed to Lucene, but it's beyond the scope of this chapter
to explain this. Similar information is nested inside the <lst name="explain"> XML tag;
we will talk about it in Chapter 9, Dealing with Problems.
What we are interested in is the information nested inside the <lst name="timing">
XML tag. The first information you see under this tag is the total time of your query, which
in our case is 426 milliseconds (<double name="time">426.0</double>). We have
the following two lists:
ff <lst name="prepare"> holds information regarding the query preparation time
ff <lst name="process"> holds information regarding the query execution time
You can see that nested inside those lists are components and their time.
The prepare list tells us how much time each component spends during the query
preparation phase. For example, we can see that org.apache.solr.handler.
component.QueryComponent spent 14.0 milliseconds during preparation time.
The process list tells us how much time was spent during the query processing phase,
which is the phase that is usually the longest one, because of all the computation and
I/O operations needed to execute the query. You can see that in our case, there were three
components that were working for longer than 0 milliseconds. The last one (org.apache.
solr.handler.component.DebugComponent) is the component that we added with
the query parameter, and we can skip it as it won't be used during production queries.
The second component, which was running for 43 milliseconds, was org.apache.solr.
handler.component.QueryComponent, which is responsible for parsing the query
and running it. It still takes about 10 percent time of the whole query, which is not what
we are looking for. The component that took the most amount of the query execution time
is org.apache.solr.handler.component.FacetComponent; it was working for
360 milliseconds, so for almost 90 percent of the query execution time.
As you can see, with the use of the debugQuery parameter, we identified which part of the
query is problematic and we can start optimizing it; But it's beyond the scope of this recipe.
www.it-ebooks.info
Improving Solr Performance
206
Avoiding filter caching
Imagine that some of the filters you use in your queries are not good candidates for caching.
You may wonder why, for example, those filters have a date and time with seconds or are
spatial filters scattered all over the world. Such filters are quite unique and when added to
the cache, their entries can't be reused much. Thus they are more or less useless. Caching
such filters is a waste of memory and CPU cycles. Is there something you can do to avoid filter
queries caching? Yes, there is a way and this recipe will show you how to do it.
How to do it...
Let's assume we have the following query being used to get the information we need:
q=solr+cookbook&fq=category:books&fq=date:2012-06-12T13:22:12Z
The filter query we don't want to cache is the one filtering our documents on the basis of the
date field. Of course, we still want the filtering to be done. In order to turn off caching, we
need to add {!cache=false} to our filter with the date field, so that our query should look
similar to the following code snippet:
q=solr+cookbook&fq=category:books&fq={!cache=false}date:2012-06-
12T13:22:12Z
Now let's take a look at how this works.
How it works...
The first query is very simple; we just search for the words solr cookbook and we want the
result set to be narrowed in the books category. We also want to narrow the results further to
only those that have 2012-06-12T13:22:12Z in the date field.
As you can imagine, if we have many filters with such dates as the one in the query, the
filter cache can be filled very fast. In addition to this, if you don't reuse the same value for
that field, the entry in the field cache becomes pretty useless. That's why, by adding the
{!cache=false} part to the filter query, we tell Solr that we don't want the filter query
results to be put into the filter cache. With such an approach we won't pollute the filter cache
and thus save some CPU cycles and memory. There is one more thing – the filters that are
not cached will be executed in parallel with the query, so this may be an improvement to your
query execution time.
www.it-ebooks.info
Chapter 6
207
Controlling the order of execution of filter
queries
If you use filter queries extensively, which isn't a bad thing at all, you may be wondering if
there is something you can do to improve the execution time of some of your filter queries.
For example, if you have some filter queries that use heavy function queries, you may want
to have them executed only on the documents that passed all the other filters. Let's see how
we can do this.
Getting ready
Before continuing reading please read the Avoiding filter caching recipe in this chapter.
How to do it...
The following steps will explain how we can control the order of execution of filter queries:
1. Let's assume we have the following query being used to get the information we need:
q=solr+cookbook&fq=category:books&fq={!frange l=10 u=100}log(sum(s
qrt(popularity),100))&fq={!frange l=0 u=10}if(exists(price_a),sum(
0,price_a),sum(0,price))
2. For the purpose of this recipe, let's also assume that fq={!frange l=10 u=100}
log(sum(sqrt(popularity),100)) and fq={!frange l=0 u=10}if(exis
ts(price_a),sum(0,price_a),sum(0,price)) are the filter queries that are
heavy and we would like those filters to be executed as the previous ones. We would
also like the second filter to execute only on the documents that were narrowed by
other filters. In order to do this, we need to modify our query so that it looks similar to
the following code snippet:
q=solr+cookbook&fq=category:books&fq={!frange l=10 u=100
cache=false cost=50}log(sum(sqrt(popularity),100))&fq={!frange l=0
u=10 cache=false cost=150}if(exists(price_promotion),sum(0,price_
promotion),sum(0,price))
As you can see, we've added other two attributes: cache=false and cost having values as
50 and 150. Let's see what they mean.
www.it-ebooks.info
Improving Solr Performance
208
How it works...
As you can see, we search for the words solr cookbook in the first query and we want the
result set to be narrowed by book category. We also want the documents to be narrowed to
only those that have a value of the log(sum(sqrt(popularity),100)) function between
10 and 100. In addition to this, the last filter query specifies that we want our documents to
be filtered to only those that have a price_promotion field (price if price_promotion
isn't filled) value between 0 and 10.
Our requirements are such that the second filter query (the one with log function query)
should be executed after the fq=category:books filter query and the last filter should
be executed in the end, only on the documents narrowed by other filters. To do this, we set
those two filters to not cache and we introduced the cost parameter. The cost parameter
in filter queries specifies the order in which non-cached filter queries are executed; the higher
the cost value, the later the filter query will be executed. So our second filter (the one with
cost=50) should be executed after the fq=category:books filter query and the last filter
query (the one with cost=150) are executed. In addition to this, because the cost of the
second non-cached filter query is higher or equal to 100, this filter will be executed only on
the documents that matched the main query and all the other filters. So our requirements
have been completed.
Forgive me, but I have to say it once again—please remember that the cost attribute only
works when the filter query is not cached.
Improving the performance of numerical
range queries
Let's assume we have the Apache Solr 4.0 deployment where we use range queries. Some
of those are run against string fields, while others are run against numerical fields. Using
different techniques, we identified that our numerical range queries execute slower than we
would like. The usual question arises – is there something that we can do ? Of course, and
this recipe will show you what.
How to do it...
The following steps will explain how we can control the order of execution of numerical
range queries:
1. Let's begin with the definition of a field that we use to run our numerical
range queries:
<field name="price" type="float" indexed="true" stored="true"/>
www.it-ebooks.info
Chapter 6
209
2. The second step is to define the float field type:
<fieldType name="float" class="solr.TrieFloatField"
precisionStep="8" positionIncrementGap="0"/>
3. Now the usual query that is run against the preceding field is as follows:
q=*:*&fq=price:[10.0+TO+59.00]&facet=true&facet.field=price
4. In order to have your numerical range queries performance improved, there is just a
single thing you need to do – decrease the precisionStep attribute of the float
field type; for example, from 8 to 4. So, our field type definition would look similar to
the following code snippet:
<fieldType name="float" class="solr.TrieFloatField"
precisionStep="4" positionIncrementGap="0"/>
After the preceding change, you will have to re-index your data and your numerical
queries should be run faster. How faster, depends on your setup. Now let's take a
look at how it works.
How it works...
As you can see, in the preceding examples, we used a simple float-based field to run
numerical range queries. Before the changes, we specified precisionStep on our field type
as 8. This attribute (specified in bits) tells Lucene (which Solr is built on top of) how many
tokens should be indexed for a single value in such a field. Smaller precisionStep values
(when precisionStep > 0) will lead to more tokens being generated by a single value and
thus make range queries faster. Because of this, when we decreased the precisionStep
value from 8 to 4, we saw a performance increase.
However, please remember that decreasing the precisionStep value will lead to slightly
larger indices. Also, setting the precisionStep value to 0 turns off indexing of multiple
tokens per value, so don't use that value if you want your range queries to perform faster.
www.it-ebooks.info
www.it-ebooks.info
7
In the Cloud
In this chapter we will cover:
ff Creating a new SolrCloud cluster
ff Setting up two collections inside a single cluster
ff Managing your SolrCloud cluster
ff Understanding the SolrCloud cluster administration GUI
ff Distributed indexing and searching
ff Increasing the number of replicas on an already live cluster
ff Stopping automatic document distribution among shards
Introduction
As you know, Apache Solr 4.0 introduced the new SolrCloud feature that allows us to use
distributed indexing and searching on a full scale. We can have automatic index distribution
across multiple machines, without having to think about doing it in our application. In this
chapter, we'll learn how to manage our SolrCloud instances, how to increase the number
of replicas, and have multiple collections inside the same cluster.
Creating a new SolrCloud cluster
Imagine a situation where one day you have to set up a distributed cluster with the use of Solr.
The amount of data is just too much for a single server to handle. Of course, only you can set
up a second server or go for another master database with another set of data. But before
Solr 4.0, you would have to take care of the data distribution yourself. In addition to this, you
would also have to take care of setting up replication, thinking about data duplication, and so
on. You don't have to do this now because Solr 4.0 can do it for you. Let's see how.
www.it-ebooks.info
In the Cloud
212
Getting ready
Before continuing, I advise you to read the Installing standalone ZooKeeper recipe in Chapter
1, Apache Solr Configuration. This recipe shows how to set up a ZooKeeper cluster ready for
production use. However, if you already have ZooKeeper running, you can skip that recipe.
How to do it...
Let's assume we want to create a cluster that will have four Solr servers. We would also like
to have our data divided between four Solr servers in such a way that we would have the
original data sharded to two machines. In addition to this we would also have a copy of each
shard available, in case something happens with one of the Solr instances. I also assume that
we already have our ZooKeeper cluster setup, ready, and available at the 192.168.0.10
address on port 9983.
1. Let's start with populating our cluster configuration into the ZooKeeper cluster. In
order to do this, you need to run the following command:
java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.
configName=twoShardsTwoReplicasConf -DnumShards=2
-DzkHost=192.168.0.10:9983 -jar start.jar
2. Now that we have our configuration populated, let's start the second node with the
following command:
java -DzkHost=192.168.0.10:9983 -jar start.jar
3. We now have our two shards created and want to create replicas. This is very simple
since we have already created the configuration. We just need to start two additional
servers with the following command run on each of them:
java -DzkHost=192.168.0.10:9983 -jar start.jar
If you look at the cloud configuration of the Solr administration panel, you will see that you
have a cluster that has four nodes, where the first two nodes act as leaders for the shards
and the other two nodes act as their replicas. You can start indexing your data to one of the
servers now, and Solr will take care of data distribution and will also automatically copy the
data to the replicas. Let's see how this works.
www.it-ebooks.info
Chapter 7
213
How it works...
What we need to do first is send all the configuration files to ZooKeeper in order for the Solr
servers to be able to fetch it from there. That's why, when running the first server (only during
the first start of it), we add the -Dboostrap_confdir and -Dcollection.configName
parameters. The first parameter specifies the location of the directory with the configuration
files that we would like to put into ZooKeeper. The second parameter specifies the name of
your configuration. During the first start, we also need to specify the number of shards that
should be available in our cluster, and in this example we set it to 2 (the -DnumShards
parameter). The -DzkHost parameter is used to tell Solr about the location and the port
used by the Zookeeper cluster.
As you can see, all the other commands are similar to the ones you used while running the
Solr instances. The only difference is that we specify one additional parameter, -DzkHost,
which tells Solr where to look for the ZooKeeper server on the cluster.
When setting up the SolrCloud cluster, please remember to choose the number of shards
wisely, because you can't change that for your existing cluster, at least not right now. You can
add replicas to an already created cluster, but the number of shards will remain constant.
There's more...
There is one more thing that I would like to mention – the possibility of running a ZooKeeper
server embedded into Apache Solr 4.0.
Starting the embedded ZooKeeper server
You can also start an embedded ZooKeeper server shipped with Solr for your test
environment. In order to do this, you should pass the -DzkRun parameter instead
of -DzkHost=192.168.0.10:9983, but only in the command that sends our
configuration to the ZooKeeper cluster. So the final command should look similar
to the following code snippet:
java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName
=twoShardsTwoReplicasConf -DzkHost=192.168.0.10:9983 -DnumShards=2 -jar
start.jar
www.it-ebooks.info
In the Cloud
214
Setting up two collections inside a single
cluster
Imagine that you would like to have more than a single collection inside the same Apache
Solr 4.0 cluster. For example, you would like to store books in one collection and users in
the second one. SolrCloud allows that, and this recipe will show you how to do it.
Getting ready
Before continuing, I advise you to read the Installing standalone ZooKeeper recipe in Chapter
1, Apache Solr Configuration, because this recipe assumes that we already have ZooKeeper up
and running. We assume that ZooKeeper is running on localhost and is listening on port 2181.
How to do it...
1. Since we want to start a new SolrCloud cluster that doesn't have any collections
defined, we should start with the solr.xml file. On both instances of Solr, the
solr.xml file should look similar to the following code snippet:
<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="true">
<cores adminPath="/admin/cores"
defaultCoreName="collection1" host="${host:}"
hostPort="${jetty.port:}"
hostContext="${hostContext:}"
zkClientTimeout="${zkClientTimeout:15000}">
</cores>
</solr>
2. Let's assume that we have two SolrCloud instances that form a cluster, both running
on the same physical server, one on port 8983 and the second one on 9983. They
are started with the following commands:
java -Djetty.port=8983 -DzkHost=localhost:2181 -jar start.jar
java -Djetty.port=9983 -DzkHost=localhost:2181 -jar start.jar
3. Now, we need to add the configuration files, which we want to create collections with,
to ZooKeeper. Let's assume that we have all the configuration files stored in /usr/
share/config/books/conf for the books collection, and the configuration files for
the users collection stored in /usr/share/config/users/conf. To send these files
to ZooKeeper, we should run the following commands from our $SOLR_HOME directory:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/books/conf -confnamebookscollection
www.it-ebooks.info
Chapter 7
215
And:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/users/conf -confnameuserscollection
4. We have pushed our configurations into the ZooKeeper, so we can now create the
collections we want. In order to do this, we use the following commands:
curl 'http://localhost:8983/solr/admin/collections?action=CREATE&n
ame=bookscollection&numShards=
2&replicationFactor=0'
And:
curl 'http://localhost:8983/solr/admin/collections?action=CREATE&n
ame=userscollection&numShards=
2&replicationFactor=0'
5. Now, just to test if everything went well, we will query the newly created collections
as follows:
curl 'http://localhost:8983/solr/bookscollection/select?q=*:*'
The response to the preceding command will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">39</int>
<lst name="params">
<str name="q">*:*</str>
</lst>
</lst>
<result name="response" numFound="0" start="0"
maxScore="0.0">
</result>
</response>
As you can see, Solr responded correctly. But as we don't have any data indexed,
we got 0 documents.
How it works...
As you can see, our solr.xml file on both the instances is the same and it doesn't contain
any information about the cores. This is done on purpose, since we want to have a clean
cluster – one without any collections present.
www.it-ebooks.info
In the Cloud
216
The mentioned configuration directories should store all the files (solrconfig.xml,
schema.xml, and stopwords.txt) that are needed for your Solr instance to work, if you
use one. Please remember this before sending them to ZooKeeper or else Solr will fetch
those files from ZooKeeper and create collections using them.
Now, let's look at the most interesting aspect – the scripts used to upload the configuration
files to ZooKeeper. We used the zkcli.sh script provided with the standard Solr 4.0
distribution and placedit in the cloud-scripts directory by default. The first thing is the cmd
parameter, which specifies what we want to do – in this case upconfig means that we want
to upload the configuration files. The zkhost parameter allows us to specify the host and port
of the ZooKeeper instance we want to put the configuration to. confdir is one of the most
crucial parameters and it specifies the directory in which the Solr configuration files are stored
– the ones that should be sent to ZooKeeper (in our case, /usr/share/config/users/
conf and /usr/share/config/books/conf). Finally the last parameter, confname,
specifies the name of the collection we will use the configuration with.
The command in the fourth step lets us create the actual collection in the cluster. In order to do
this, we send a request to the /admin/collections handler, which uses the newly introduced
collections API. We tell Solr that we want to create a new collection (the action=CREATE
parameter) with the name of bookscollection (name=bookscollection). Please note that
the name specified in the name parameter is the same as the confname parameter value used
during configuration files upload. The last two parameters specify the number of shards and
replicas that the collection should be created with. The number of shards is the initial number of
cores that will be used to hold the data in the collection (numShards). The number of replicas
(replicationFactor) is the exact number of copies of the shards that can be distributed
among many servers, and may increase query throughput and reliability.
Managing your SolrCloud cluster
In addition to creating a new collection with the API exposed by SolrCloud, we are also allowed
to use two additional operations. The first is to delete our collection and the second one is
to reload the whole collection. Along with the ability to create new collections, we are able
to dynamically manage our cluster. This recipe will show you how to use the delete and
reload operations and where they can be useful.
www.it-ebooks.info
Chapter 7
217
Getting ready
The content of this recipe is based on the Setting up two collections inside a single cluster
recipe in this chapter. Please read it before continuing.
How to do it...
I assume that we already have two collection deployed on our cluster –bookscollection
and userscollection – the same ones that we configured in the Setting up two
collections inside a single cluster recipe in this chapter. So our cluster view looks
similar to the following screenshot:
1. First, let's delete one of the collections – userscollection. To do this, we send the
following command:
curl 'http://localhost:8983/solr/admin/collections?action=DELETE&n
ame=userscollection'
www.it-ebooks.info
In the Cloud
218
2. Now, let's look at our cluster view once again:
As you can see, the userscollection collection was deleted.
3. Now, let's see how the reloading of collections works. In order to test it, let's
update the spellings.txt file located at /usr/share/config/books/conf
directory. The original file looks similar to the following code snippet:
pizza
history
After the update, it will look similar to the following code snippet:
after
update
4. Now, we need to update the collection configuration in ZooKeeper. To do this we
use the following command, which is run from our Solr instance's home directory:
cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181
-confdir /usr/share/config/books/conf -confnamebookscollection
5. Now that we have the updated version of our configuration files to
bookscollection in ZooKeeper, we can send the reload command to Solr:
curl 'http://localhost:8983/solr/admin/collections?action=RELOAD&n
ame=bookscollection'
www.it-ebooks.info
Chapter 7
219
6. First, let's check if the Solr administration panel sees the changes in ZooKeeper.
To do this, we'll use the tree view of the cloud section and navigate to /configs/
bookscollection/spellings.txt. We should be able to see something similar
to the following screenshot:
7. In the final check, let's see if Solr itself sees the update. In order to do this we run the
following command:
curl 'http://localhost:8983/solr/bookscollection/admin/
file?file=spellings.txt'
The response of the preceding command would be as follows:
after
update
So it seems like everything is working as it should. Now let's see how it works.
How it works...
We begin with a cluster that contains two collections. But we want to delete one of them and
update the second one. In order to do this we use the collections API provided by Solr 4.0.
www.it-ebooks.info
In the Cloud
220
We start by sending the delete action (action=DELETE) to the /solr/admin/
collections URL, which is the default address that the collections API is available at.
In addition to this, we need to provide the name of the collection we want to delete – to do
this, we use the name parameter with the name of the collection that we want to delete. After
sending the command and refreshing the Solr administration panel, we see that the second
collection was deleted just as we wanted.
Now, let's discuss the process of updating the second collection. First of all, we've changed
the contents of the spellings.txt file in order to see how it works. However, be careful
when updating collections, because some changes may force you to re-index your data; but
let's get back to our update. So after we update the file, we use one of the scripts provided
with Solr 4.0 in order to upload all the configuration files that belong to this collection into the
ZooKeeper ensemble (if you are not familiar with that command, please see the Setting up
two collections inside a single cluster recipe, later in this chapter). Now, we needed to tell Solr
to reload our collection by sending the reload command (action=RELOAD) to the same
URL as the delete command. Of course, just like with the delete command, we needed to
provide the name of the collection we want to reload using the name parameter.
As you can see, on the previous screenshot, the collection was updated at least in the
ZooKeeper ensemble. However, we want to be sure that Solr sees those changes, so we use
the /admin/file handler to get the contents of the spellings.txt file. In order to do this,
we pass the file=spellings.txt parameter to that handler. As you can see, Solr returned
the changed contents, so the collection was updated and reloaded successfully.
Understanding the SolrCloud cluster
administration GUI
With the release of Solr 4.0, we've got the ability to use a fully-distributed Solr cluster
with fully-distributed indexing and searching. Along with this comes the reworked Solr
administration panel with parts concentrated on Cloud functionalities. This recipe will
show you how to use this part of the administration panel; for example, how to see your
cluster distribution and detailed information about shards and replicas.
Getting ready
This recipe assumes that the SolrCloud cluster is up and running. If you are not familiar with
setting up the SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe
in this chapter.
www.it-ebooks.info
Chapter 7
221
How to do it...
1. First of all, let's see how we can check how our cluster distribution looks. In order
to do this, let's open our web browser to http://localhost:8983/solr/ (or
the address of the host and port of any of the Solr instances that form the cluster)
and open the Cloud graph view. We should be able to see something similar to the
following screenshot:
2. There is also a second view of the same information that can be accessed by viewing
the Graph (Radial) section, and it should look similar to the following screenshot:
www.it-ebooks.info
In the Cloud
222
3. Looks nice, doesn't it? However, there is some additional information that can be
retrieved. So now, let's look at the Tree section of the Cloud administration panel:
As you can see, there is some very detailed information available. So now, let's look at what
it means.
How it works...
First of all, remember that the best way to get used to the new administration panel is to just
run a simple SolrCloud cluster by yourself and play with it. However, let's look at the provided
examples to see what information we have there.
As you can see, in the first two screenshots provided, our cluster consists of a single collection
named collection1. It consists of two shards (shard1 and shard2) and each shard
lives on a single node. One of each shards is the primary one (the ones at gr0-vaio:8983
and gr0-vaio:7983), and each of them has a replica (the ones at gr0-vaio:6983 and
gr0-vaio:5983). Both diagrams shown in the screenshots provide the same amount of
information and they only differ in the way they present the data.
www.it-ebooks.info
Chapter 7
223
Now, let's look and discuss the last screenshot – the Tree view of the Cloud section of the
Solr administration panel. As you can see, there is much more information available there.
The tree presented in the administration panel is what your ZooKeeper ensemble sees.
The first thing is clusterstate.json, which holds detailed information about the current
state of the cluster.
Next, you can see the collections section, which holds information about each collection
deployed in the cluster – you can see the information about each shard and its replicas, such
as leaders, and some detailed information needed by the Solr and ZooKeeper.
In addition to the preceding information, you can also see the configuration files
(the /configs section) that were sent to the ZooKeeper and are used as the
configuration files for your collection or collections.
Not visible in the screenshot is the additional information connected to ZooKeeper,
which is not needed during the usual work with Solr, so I decided to omit discussing it.
Distributed indexing and searching
Having a distributed SolrCloud cluster is very useful; you can have multiple shards and replicas,
which are automatically handled by Solr itself. This means that your data will be automatically
distributed among shards and replicated between replicas. However, if you have your data
spread among multiple shards, you probably want them to be queried while you send the
query. With earlier versions of Solr before 4.0, you had to manually specify the list of shards
that should be queried. Now you don't need to do that, and this recipe will show you how to
make your queries distributed.
Getting ready
If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter. If you are not familiar with how to modify the returned
documents using the fl parameter, please read the Modifying returned documents recipe in
Chapter 4, Querying Solr.
www.it-ebooks.info
In the Cloud
224
How to do it...
1. First of all, let's assume we have a cluster that consist of three nodes and we have
a single collection deployed on that cluster; a collection with three shards. For the
purpose of this recipe, I'm using the example configuration files provided with Solr
and the example documents stored in the XML files in the exampledocs directory
of the Solr distribution package. If we look at the Solr administration panel, this is
what the Cloud graph will show:
2. Now, the best thing about distributed indexing and searching—if you are using Solr
4.0 and its distributed searching and indexing capabilities—is that you don't need to
do anything in addition to sending the proper indexing and searching requests to one
of the shards. So, in order to have the example files indexed, I've run the following
command from the exampledocs directory of the Solr instance running on port 8983:
java -jar post.jar *.xml
3. Now, let's use the non-distributed queries to check if the documents were sent to all
the shards. In order to do this, we run three queries. The first query is run to the Solr
instance holding the first shard:
curl 'http://localhost:8983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
www.it-ebooks.info
Chapter 7
225
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="8" start="0">
</result>
</response>
4. The second query is run to the Solr instance holding the second shard:
curl 'http://localhost:7983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="10" start="0">
</result>
</response>
5. The third query is run to the Solr instance holding the last shard:
curl 'http://localhost:6983/solr/select?q=*:*&rows=0&distrib=false'
Its response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="distrib">false</str>
<str name="q">*:*</str>
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="14" start="0">
</result>
</response>
www.it-ebooks.info
In the Cloud
226
6. Everything seems to be in the perfect order now, at least by judging the number of
documents. So now, let's run the default distributed query to see if all the shards
were queried. In order to do this we run the following query:
curl 'http://localhost:8983/solr/select?q=*:*&fl=id,[shard]&ro
ws=50'
Since the response was quite big, I decided to cut it a bit and show only a single
document from each shard:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">58</int>
<lst name="params">
<str name="fl">id,[shard]</str>
<str name="q">*:*</str>
<str name="rows">50</str>
</lst>
</lst>
<result name="response" numFound="32" start="0"
maxScore="1.0">
<doc>
<str name="id">SP2514N</str>
<str name="[shard]">gr0-vaio:6983/solr/collection1/
</str>
</doc>
...
<doc>
<str name="id">GB18030TEST</str>
<str name="[shard]">gr0-vaio:7983/solr/collection1/
</str>
</doc>
...
<doc>
<str name="id">IW-02</str>
<str name="[shard]">gr0-vaio:8983/solr/collection1/
</str>
</doc>
...
</result>
</response>
As you can see, we got documents from each shard that builds our cluster, so it works as
intended. Now, let's look at exactly how it works.
www.it-ebooks.info
Chapter 7
227
How it works...
As you can see, as shown in the previous screenshot, our test cluster created for the purpose
of this recipe contains thee Solr instances, where each of them contains a single shard of the
collection deployed on the cluster. This means that the data indexed to any of the shards will
be automatically divided and distributed among the shards. In order to choose which shard
the document should go to, Solr uses a hash value of the identifier of the document.
During indexing, we sent the documents to the Solr instance that is working on port
8983. However, as our example queries show, when querying only a particular shard (the
distrib=false parameter), each of them hosts different amount of documents, which
is expected. If we had many more documents, the amount of documents on each shard
would be probably almost the same if not equal. As you must have guessed by now, the
distrib=false parameter forces the query to be run on the Solr server that it was sent
to in a non-distributed manner, and we want such behavior to see how many documents
are hosted on each of the shards.
Let's now focus on the query that was used to fetch all the documents in the cluster. It's a
query that you are probably used to – fetching all the documents (q=*:*) and returning a
maximum of 50 documents (rows=50). In addition, we specify the fl parameter in such a
way that the returned document contains the id field and the information about the shard
the document was fetched from (fl=id,[shard]). As you can see, we got documents
coming from all the shards that build the collection in the response. This is because when
using the SolrCloud deployment, Solr automatically queries all the relevant shards that are
needed to be queried in order to query the whole collection. The information about shards
(and replicas, if they exist) is fetched from ZooKeeper, so we don't need to specify it.
Increasing the number of replicas on an
already live cluster
If you used Solr before the release of the 4.0 version, you are probably familiar with replication.
The way deployments usually worked is that there was a single master server and multiple slave
servers that were pulling the index from the master server. In Solr 4.0, we don't have to worry
about replication and pulling interval – it's done automatically. We can also set up our instances
in a way to achieve a similar setup as that of multiple replicas of a single shard where data is
stored. This recipe will show you how to do it.
www.it-ebooks.info
In the Cloud
228
Getting ready
If you are not familiar with setting up a SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter.
How to do it...
For the purpose of this recipe, I'll assume that we want to have a cluster with a single shard
running just like the usual Solr deployment, and we want to add two additional replicas to that
shard. So, we have more servers to handle the queries.
1. The first step is starting a new Solr 4.0 server. We will use the configuration provided
with the example Solr server, but you can use your own if you want. We will also use
the ZooKeeper server embedded into Solr, but again, you can use the standalone one.
So finally, the command that we use for starting the first instance of Solr is as follows:
java -Dbootstrap_confdir=solr/collection1/conf -Dcollection.
configName=collection1 -DzkRun -DnumShards=1 -jar start.jar
2. Now, let's take a look at the Solr administration panel to see how our cluster
state looks:
As you can see, we have a single shard in our collection that has a single replica. This
can be a bit misleading, because the single replica is actually the initial shard we've
created. So we actually have a single shard and zero copies of it. As we said earlier,
we want to change that in order to have two additional replicas of our shard. In order
to do this, we need to run two additional Solr instances. I'll run them on the same
machine as the first one on ports 7893 and 6893. But in a real life situation, you'd
probably want to have them on different servers.
www.it-ebooks.info
Chapter 7
229
3. In order to run these two additional Solr servers, we use the following commands:
java -Djetty.port=7983 -DzkHost=localhost:9983 -jar start.jar
java -Djetty.port=6983 -DzkHost=localhost:9983 -jar start.jar
4. Now, let's see how our cluster state changes, by looking at the cluster state in
the Solr administration panel again. The cluster state information looks similar
to the following screenshot after we start the two additional instances of Solr:
As you see, we still have our initial shard. But right now, we also have two additional replicas
present that will be automatically updated and will hold the same data as the primary shard
that we created in the beginning.
www.it-ebooks.info
In the Cloud
230
How it works...
We start our single shard instance with the command that allows us to run the embedded
ZooKeeper server along with Solr. The embedded ZooKeeper server is started at the port
whose number is the Solr port + 1000, which in our case if 9983. bootstrap_confdir
specifies the directory where the Solr configuration files are stored, which will be sent to the
ZooKeeper. collection.configName specifies the name of the collection, numShards
specifies the amount of shards the collection should have, and zkRun tells Solr that we want
the embedded ZooKeeper to be run. Of course, this was only used as an example, and in a
production environment you should set up a standalone ZooKeeper server.
As shown in the previous screenshot, you can see that our collection consists of a single
shard and the only replica we have is this shard. So, we have a single primary shard with
no data replication at all. In order to create two replicas that will be automatically populated
with exactly the same data as the primary shard, we just need to start the two additional Solr
servers. For the purpose of the recipe, we started these new instances on the same machine,
but usually in a production environment you would set them up on separate machines.
As you can see in the second screenshot, after adding these two new Solr instances, our
cluster is composed of a primary shard and two replicas, which will have their contents
updated automatically. So we've got what we wanted.
Stopping automatic document distribution
among shards
In most cases, the standard distribution of documents between your SolrCloud instances
will be enough, and what's more, it will be the right way to go. However, there are situations
where controlling the documents distribution outside of Solr (that is, in your application) may
be better. For example, imagine that you'll only allow your users to search in the data they
indexed. In such situations, it would be good to have documents for a single client stored in a
single shard (if that's possible). In such cases, automatic documents distribution based on the
documents identifier may not be the best way. Solr allows us to turn off automatic document
distribution and this recipe will show you how to do that.
Getting ready
If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new
SolrCloud cluster recipe in this chapter. If you are not familiar with how to modify the returned
documents using the fl parameter, please read the Modifying the returned documents recipe
in Chapter 4, Querying Solr.
www.it-ebooks.info
Chapter 7
231
How to do it...
1. Let's assume that we have the following index structure (schema.xml) defined,
and that we already have it stored in ZooKeeper:
<fields>
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="userName" type="string" indexed="true"
stored="true" />
<field name="data" type="text" indexed="true"
stored="true" />
<field name="_version_" type="long" indexed="true"
stored="true"/>
</fields>
2. We have two files that contain user data. One is called data1.xml, and it holds the
data for user1 and looks similar to the following code snippet:
<add>
<doc>
<field name="id">1</field>
<field name="userName">user1</field>
<field name="data">Data of user1</field>
</doc>
</add>
The second one is called data2.xml, and it holds the data for user2:
<add>
<doc>
<field name="id">2</field>
<field name="userName">user2</field>
<field name="data">Data of user2</field>
</doc>
<doc>
<field name="id">3</field>
<field name="userName">user2</field>
<field name="data">Another data of user2</field>
</doc>
</add>
3. In order to be able to stop the automatic document distribution between shards,
we need the following update request processor chain to be defined in the
solrconfig.xml file:
<updateRequestProcessorChain>
<processor class="solr.LogUpdateProcessorFactory" />
www.it-ebooks.info
In the Cloud
232
<processor class="solr.RunUpdateProcessorFactory" />
<processor class="
solr.NoOpDistributingUpdateProcessorFactory" />
</updateRequestProcessorChain>
4. I assume that we already have a cluster containing at least two nodes up and
running, these nodes use the preceding configuration files, and that our collection
name is collection1. One of the nodes is running on a server with the IP address
as 192.168.1.1 and the second one is running on a server with the IP address as
192.168.1.2.
5. As we discussed earlier, we want to manually distribute the data to Solr instances.
In our case, we would like the data from the data1.xml file to be indexed on the Solr
server running at 192.168.1.1, and the data from data2.xml to be indexed on the
Solr instance running on 192.168.1.2. So, we use the following commands to index
the data:
java -Durl=http://192.168.1.1:8983/solr/collection1/update -jar
post.jar data1.xml
java -Durl=http://192.168.1.2:8983/solr/collection1/update -jar
post.jar data2.xml
6. Now, let's test if it works. In order to do this, we will use the Solr functionality that
enables us to see which shard the document is stored at. In our case, it will be the
following query:
curl http://localhost:7983/solr/select?q=*:*&fl=*,[shard]
The response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">24</int>
<lst name="params">
<str name="q">*:*</str>
<str name="fl">*,[shard]</str>
</lst>
</lst>
<result name="response" numFound="3" start="0"
maxScore="1.0">
<doc>
<str name="id">2</str>
www.it-ebooks.info
Chapter 7
233
<str name="userName">user2</str>
<str name="data">Data of user2</str>
<str name="[shard]">
192.168.1.2:8983/solr/collection1/
</str>
</doc>
<doc>
<str name="id">3</str>
<str name="userName">user2</str>
<str name="data">Another data of user2</str>
<str name="[shard]">
192.168.1.2:8983/solr/collection1/
</str>
</doc>
<doc>
<str name="id">1</str>
<str name="userName">user1</str>
<str name="data">Data of user1</str>
<str name="[shard]">
192.168.1.1:8983/solr/collection1/
</str>
</doc>
</result>
</response>
It seems that we have achieved what we wanted, so let's see how it works.
How it works...
Our schema.xml file is very simple. It contains three fields that are used by our data
files at the _version_ field used internally by Solr. The actual data is nothing new
as well, so I'll skip discussing it.
The thing we want to look at is the update request processor chain definition.
As you can see, apart from the standard solr.LogUpdateProcessorFactory
and solr.RunUpdateProcessorFactory processors, it contains a solr.
NoOpDistributingUpdateProcessorFactory processor. You can think
of this additional processor as the one that forces the update command to
be indexed on the node it was sent to.
www.it-ebooks.info
In the Cloud
234
We used the standard post.jar library distributed with Solr in order to index the data. In
order to specify which server the data should be sent to, we use the –Durl parameter. We
use two available servers to send the data to – the one running at 192.168.1.1 that should
contain one document after indexing, and the one running at 192.168.1.2 that should
contain two documents. In order to check this, we use a query that returns all the documents
(q=*:*). In addition, we specify the fl parameter in such a way that the returned document
contains not only all the stored fields, but also the shard the document was fetched from
(fl=*,[shard]).
As you can see, in the results returned by Solr, the documents that belong to user2 (the ones
with id field equal to 2 and 3) were fetched from the Solr server running at 192.168.1.2
(<str name="[shard]">192.168.1.2:8983/solr/collection1/</str>), and
the one belonging to user1 came from the Solr instance running at 192.168.1.1 (<str
name="[shard]">192.168.1.1:8983/solr/collection1/</str>). So, everything
is just as we wanted it to be.
One more thing: please remember that when turning off automatic documents distribution,
you may end up with shards being uneven. This is because of the different number of
documents being stored in each of them. So, you have to carefully plan your distribution.
www.it-ebooks.info
8
Using Additional Solr
Functionalities
In this chapter we will cover:
ff Getting more documents similar to those returned in the results list
ff Highlighting matched words
ff How to highlight long text fields and get good performance
ff Sorting results by a function value
ff Searching words by how they sound
ff Ignoring defined words
ff Computing statistics for the search results
ff Checking the user's spelling mistakes
ff Using field values to group results
ff Using queries to group results
ff Using function queries to group results
Introduction
There are many features of Solr that we don't use every day. You may not encounter
highlighting words, ignoring words, or statistics computation in everyday use, but they
can come in handy in many situations. In this chapter, I'll try to show how to overcome
some typical problems that can be fixed by using some of the Solr functionalities. In
addition to that we will see how to use the Solr grouping mechanism in order to get
documents that have some fields in common.
www.it-ebooks.info
Using Additional Solr Functionalities
236
Getting more documents similar to those
returned in the results list
Imagine a situation where you want to show similar documents to those returned by Solr. Let's
imagine a situation where you have an e-commerce library shop, and you want to show users
the books similar to the ones they found while using your application. This recipe will show you
how to do that.
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"
termVectors="true" />
2. Next, let's use the following test data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our hypothetical user wants to find books that have cookbook and
second in their names. But, we also want to show him/her similar books. To do that
we send the following query:
http://localhost:8983/solr/select?q=cookbook+second&mm=2&qf=name&d
efType=edismax&mlt=true&mlt.fl=name&mlt.mintf=1&mlt.mindf=1
www.it-ebooks.info
Chapter 8
237
The results returned by Solr for the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="mm">2</str>
<str name="mlt.mindf">1</str>
<str name="mlt.fl">name</str>
<str name="q">cookbook second</str>
<str name="mlt.mintf">1</str>
<str name="qf">name</str>
<str name="mlt">true</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Solr Cookbook second edition</str>
<long name="_version_">1415606105364496384</long>
</doc>
</result>
<lst name="moreLikeThis">
<result name="2" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr Cookbook first edition</str>
<long name="_version_">1415606105279561728</long>
</doc>
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
<long name="_version_">1415606105366593536</long>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Solr by example first edition</str>
<long name="_version_">1415606105365544960</long>
</doc>
</result>
</lst>
</response>
Now let's see how it works.
www.it-ebooks.info
Using Additional Solr Functionalities
238
How it works...
As you can see the index structure and the data are really simple. One thing to note is the
termVectors attribute set to true in the name field definition. It is a good thing to have
when using the more like this component, and should be used whenever possible in
the fields on which we plan to use this component.
Now let's take a look at the query. As you can see, we added some additional parameters
besides the standard q one (and the ones such as mm and defType which specify how our
query should be handled). The parameter mlt=true says that we want to add the more
like this component to the result processing. The mlt.fl parameter specifies which
fields we want to use with the more like this component. In our case we will use the
name field. The mlt.mintf parameter asks Solr to ignore terms from the source document
(the ones from the original result list) with the term frequency below the given value. In our
case we don't want to include the terms that will have a frequency lower than 1. The last
parameter, mlt.mindf, tells Solr that words appearing less than the value of the parameter
documents should be ignored. In our case we want to consider words that appear in at least
one document.
Last is the search results. As you can see, there is an additional section (<lst
name="moreLikeThis">) that is responsible for showing us the more like this
component results. For each document in the results there is one more like this section
added to the response. In our case, Solr added a section for the document with the unique
identifier 3 (<result name="3" numFound="3" start="0">), and there were three
similar documents found. The value of the id attribute is assigned the value of the unique
identifier of the document for which the similar documents are calculated for.
Highlighting matched words
Imagine a situation where you want to show your users which words were matched in the
document shown in the results list. For example, you want to show which words in the book
name were matched and display that to the user. Do you have to store the documents and
do the matching on the application side? The answer is no. We can force Solr to do that for
us and this recipe will show you how to do that.
How to do it...
1. We begin by creating the following index structure (just add this to your schema.xml
file, to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 8
239
2. Our test data looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our user is searching for the word book. To tell Solr that we want
to highlight the matches, we send the following query:
http://localhost:8983/solr/select?q=name:book&hl=true
The response from Solr should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="hl">true</str>
<str name="q">name:book</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
</doc>
</result>
www.it-ebooks.info
Using Additional Solr Functionalities
240
<lst name="highlighting">
<lst name="4">
<arr name="name">
<str>My &lt;em&gt;book&lt;/em&gt; second edition</str>
</arr>
</lst>
</lst>
</response>
As you can see, besides the normal results list we got the highlighting results (the highlighting
results are grouped by the <lst name="highlighting"> XML tag). The word book is
surrounded by the <em> and </em> HTML tags. So everything is working as intended. Now
let's see how it works.
How it works...
As you can see the index structure and the data are really simple, so I'll skip discussing this
part of the recipe. Please note that in order to use the highlighting mechanism, your fields
should be stored and not analysed by aggressive filters (such as stemming). Otherwise the
highlighting results can be misleading to the users. Let's think of a simple example of such
behavior – imagine the user types the word bought in the search but Solr highlighted the
word buy because of the stemming algorithm.
The query is also not complicated. We can see the standard q parameter that passes the
query to Solr. But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
As you can see in the results list, in addition to the standard results, there is a new section
<lst name="highlighting">, which contains the highlighting results. For every
document, in our case the only one found (<lst name="4"> means that the highlighting
result is presented for the document with the unique identifier value of 4), there is a list
of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
You should also remember one other thing: if you are using the standard LuceneQParser
query parser then the default field used for highlighting will be the one set in the schema.
xml file. If you are using DismaxQParser then the default fields used for highlighting are
the ones specified by the qf parameter.
There's more...
There are a few things that can be useful when using the highlighting mechanism.
www.it-ebooks.info
Chapter 8
241
Specifying the fields for highlighting
In many real life situations we want to decide what fields we would want to show the
highlighting for. To do that, you must add an additional parameter – hl.fl with the list
of fields separated by the comma character. For example, if we would like to show the
highlighting for the fields name and description, our query should look as follows:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.
fl=name,description
Changing the default HTML tags that surround the matched word
There are situations where you would like to change the default <em> and </em> HTML tags
to the ones of your choice. To do that you should add the hl.simple.pre and hl.simple.
post parameters. The first one specifies the prefix that will be added in front of the matched
word and the second one specifies the postfix that will be added after the matched word. For
example, if you would like to surround the matched word with the <b> and </b> HTML tags
the query would look like this:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.simple.
pre=<b>&hl.simple.post=</b>
How to highlight long text fields and get
good performance
In certain situations, the standard highlighting mechanism may not be performing as well as
you would like it to be. For example, you may have long text fields and you want the highlighting
mechanism to work with them. This recipe will show you how to do that.
How to do it...
1. We begin the index structure configuration which looks as follows (just add this
to your schema.xml file, to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true"
termVectors="true" termPositions="true" termOffsets="true" />
2. The next step is to index the data. We will use the test data which looks like the
following code:
<add>
<doc>
<field name="id">1</field>
www.it-ebooks.info
Using Additional Solr Functionalities
242
<field name="name">Solr Cookbook first edition</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr Cookbook second edition</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Solr by example first edition</field>
</doc>
<doc>
<field name="id">4</field>
<field name="name">My book second edition</field>
</doc>
</add>
3. Let's assume that our user is searching for the word book. To tell Solr that we
want to highlight the matches, we send the following query:
http://localhost:8983/solr/select?q=name:book&hl=true&hl.
useFastVectorHighlighter=true
The response from Solr should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">15</int>
<lst name="params">
<str name="hl">true</str>
<str name="q">name:book</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">4</str>
<str name="name">My book second edition</str>
</doc>
</result>
<lst name="highlighting">
<lst name="4">
<arr name="name">
<str>My &lt;em&gt;book&lt;/em&gt; second edition</str>
</arr>
</lst>
</lst>
</response>
As you can see everything is working as intended. Now let's see how.
www.it-ebooks.info
Chapter 8
243
How it works...
As you can see the index structure and the data are really simple, but there is a difference
between using the standard highlighter and the new FastVectorHighlighting feature.
To be able to use the new highlighting mechanism, you need to store the information about
term vectors, position, and offsets. This is done by adding the following attributes to the
field definition or to the type definition: termVectors="true" termPositions="true"
termOffsets="true".
Please note that in order to use the highlighting mechanism, your fields should be stored and
not analysed by aggressive filters (such as stemming). Otherwise the highlighting results can
be misleading to the users. An example of such a behavior is simple – imagine that the user
types the word bought in the search box but Solr highlighted the word buy because of the
stemming algorithm.
The query is also not complicated. We can see the standard q parameter that passes the
query to Solr. But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
In addition we add the parameter to tell Solr to use the FastVectorHighlighting
feature: hl.useFastVectorHighlighter=true.
As you can see in the results list, in addition to the standard results, there is a new section
called <lst name="highlighting"> that contains the highlighting results. For every
document, in our case the only one found (<lst name="4"> means that the highlighting
result is presented for the document with the unique identifier value of 4), there is a list
of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
Sorting results by a function value
Let's imagine that you have an application that allows the user to search through the
companies that are stored in the index. You would like to add an additional feature to your
application to sort the results on the basis of the distance of a certain geographical point.
Is this possible with Solr? Yes, and this recipe will show you how to do that.
Getting ready
The following recipe uses spatial search. If you are not familiar with geographical search in
Solr please read the Storing geographical points in the index recipe in Chapter 3, Analyzing
Your Text Data.
www.it-ebooks.info
Using Additional Solr Functionalities
244
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="geo" type="location" indexed="true" stored="true" />
<dynamicField name="*_coordinate" type="tdouble" indexed="true"
stored="false" />
2. Our test data that we want to index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company one</field>
<field name="geo">10.1,10.1</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Company two</field>
<field name="geo">11.1,11.1</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Company three</field>
<field name="geo">12.2,12.2</field>
</doc>
</add>
3. In addition to that we also need to define the following field type in the schema.xml
file in the types section:
<fieldType name="location" class="solr.LatLonType"
subFieldSuffix="_coordinate"/>
4. Let's assume that our hypothetical user searches for the word company and the user
is in the location with the geographical point of(13, 13). So, in order to show the
results of the query and sort them by the distance from the given point, we send the
following query to Solr:
http://localhost:8983/solr/select?q=name:company&sort=geodist(geo,
13,13)+asc
The results list returned by the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
www.it-ebooks.info
Chapter 8
245
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="q">name:company</str>
<str name="sort">geodist(geo,13,13) asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company three</str>
<str name="geo">12.2,12.2</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Company two</str>
<str name="geo">11.1,11.1</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Company one</str>
<str name="geo">10.1,10.1</str>
</doc>
</result>
</response>
As you can see, everything is working as it should be. So now let's see exactly how this works.
How it works...
Let's start from the index structure. We have four fields – one for holding the unique
identifier (the id field), one for holding the name of the company (the name field), and
one field responsible for the geographical location of the company (the geo field). The
last field, the dynamic one, is needed for the location type to work. The data is pretty
simple so let's just skip discussing that.
Besides the standard q parameter responsible for the user query, you can see the sort
parameter. But the sort is a bit different from the ones you are probably used to. It uses the
geodist function to calculate the distance from the given point, and the value returned by
the function is then used to sort the documents in the results list. The first argument of the
geodist function (the geo value) tells Solr which field to use to calculate the distance. The
next two arguments specify the point from which the distance should be calculated. Of course
as with every sort we specify the order in which we want the sort to take place. In our case we
want to sort from the nearest to the furthest company (the asc value).
As you can see in the results, the documents were sorted as they should be.
www.it-ebooks.info
Using Additional Solr Functionalities
246
Searching words by how they sound
One day your boss comes to your office and says "Hey, I want our search engine to be able to
find the same documents when I enter phone or fone into the search box". You tried to say
something, but your boss is already at the other side of the door to your office. So, you wonder
if this kind of functionality is available in Solr. I think you already know the answer – yes it is,
and this recipe will show you how to configure it and use with Solr.
How to do it...
1. We start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="phonetic" indexed="true" stored="true" />
2. Next we define the phonetic type, which looks like the following code (paste it into
the schema.xml file):
<fieldtype name="phonetic" stored="false" indexed="true"
class="solr.TextField" >
<analyzer>
<tokenizer class="solr.StandardTokenizerFactory"/>
<filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
</analyzer>
</fieldtype>
3. Now we need to index our test data, which looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Phone</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Fone</field>
</doc>
</add>
4. Now let's assume that our user wants to find documents that have the word that
sounds like fon. So, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:fon
www.it-ebooks.info
Chapter 8
247
The result list returned by the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:fon</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Phone</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Fone</str>
</doc>
</result>
</response>
So, the filter worked! We got two documents in the results list. Now let's see how it worked.
How it works...
Let's start with the index structure. As you can see we have two fields, the id field responsible
for holding the unique identifier of the product and the name field responsible for holding the
name of the product.
The name field is the one that will be used for phonetic search. For that we defined a new
field type named phonetic. Besides the standard parts (such as class among many
others) we defined a new filter: DoubleMetaphoneFilterFactory. It is responsible
for analysis and checking how the words sound. This filter uses an algorithm named double
metaphone to analyse the phonetics of the words. The additional attribute inject="false"
tells Solr to replace the existing tokens instead of inserting additional ones, which mean that
the original tokens will be replaced by the ones that the filter produces.
As you can see from the query and the data, the fon word was matched to the word phone
and also to the word fone, which means that the algorithm (and thus the filter) works quite
well. But take into consideration that this is only an algorithm, so some words that you think
should be matched will not match.
www.it-ebooks.info
Using Additional Solr Functionalities
248
See also
If you would like to know other phonetic algorithms, please take a look at the Solr Wiki page
that can be found at the following URL address: http://wiki.apache.org/solr/
AnalyzersTokenizersTokenFilters.
Ignoring defined words
Imagine a situation where you would like to filter the words that are considered vulgar from
the data we are indexing. Of course, by accident, such words can be found in your data and
you don't want them to be searchable thus you want to ignore them. Can we do that with Solr?
Of course we can, and this recipe will show you how to do that.
How to do it...
1. Let's start with the following index structure (just add this to your schema.xml file,
to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text_ignored" indexed="true"
stored="true" />
2. The second step is to define the text_ignored type, which looks like the following
code:
<fieldType name="text_ignored" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.StopFilterFactory" ignoreCase="true"
words="ignored.txt" enablePositionIncrements="true" />
</analyzer>
</fieldType>
3. Now we create the ignored.txt file, whose contents looks as follows:
vulgar
vulgar2
vulgar3
4. The next step is to index our test data, which looks as follows:
<add>
<doc>
<field name="id">1</field>
<field name="name">Company name</field>
</doc>
</add>
www.it-ebooks.info
Chapter 8
249
5. Now let's assume that our user wants to find the documents that have the words
Company and vulgar. So, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:(Company+AND+vulgar)
In the standard situation there shouldn't be any results because we don't have
a document that matches the two given words. But let's look at what Solr returned
to us as the preceding query's result:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:(Company AND vulgar)</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company name</str>
</doc>
</result>
</response>
6. Hmm… it works. To be perfectly sure, let's look at the analysis page found at the
administration interface, as shown in the following screenshot:
As you can see the word vulgar was cut and thus ignored.
www.it-ebooks.info
Using Additional Solr Functionalities
250
How it works...
Let's start with the index structure. As you can see we have two fields, the id field responsible
for holding the unique identifier of the product and the name field responsible for holding the
name of the product.
The name field is the one we will use to mention the ignoring functionalities of Solr –
StopFilterFactory. As you can see the text_ignored type definition is analysed
the same way both in the query and index time. The unusual thing is the new filter –
StopFilterFactory. The words attribute of the filter definition specifies the name of the file,
encoded in UTF-8, which consists of words (a new word at every file line) that should be ignored.
The defined file should be placed in the same directory in which we placed the schema.xml file.
The ignoreCase attribute set to true tells the filter to ignore the case of the tokens and the
words defined in the file. The last attribute, enablePositionIncrements=true, tells Solr to
increment the position of the tokens in the token stream. The enablePositionIncrements
parameter should be set to true if you want to preserve the next token after the discarded one
to increment its position in the token stream.
As you can see in the query, our hypothetical user queried Solr for two words with the logical
operator AND, which means that both words must be present in the document. But, the filter
we added cut the word vulgar and thus the results list consists of the document that has
only one of the words. The same situation occurs when you are indexing your data. The words
defined in the ignored.txt file will not be indexed.
If you look at the provided screenshot from the analysis page of the Solr administration
interface (refer to step 6 of the How to do it... section), you can see that the word vulgar
was cut during the processing of the token stream in the StopFilterFactory filter.
Computing statistics for the search results
Imagine a situation where you want to compute some basic statistics about the documents
in the results list. For example, you have an e-commerce shop where you want to show the
minimum and the maximum price of the documents that were found for a given query. Of course
you could fetch all the documents and count them by yourself, but imagine Solr doing it for you.
Yes, it can! And this recipe will show you how to use that functionality.
How to do it...
1. Let's start with the index structure (just add this to your schema.xml file, to the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="price" type="float" indexed="true" stored="true" />
www.it-ebooks.info
Chapter 8
251
2. The example data that we index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Book 1</field>
<field name="price">39.99</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Book 2</field>
<field name="price">30.11</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Book 3</field>
<field name="price">27.77</field>
</doc>
</add>
3. Let's assume that we want our statistics to be computed for the price field.
To do that, we send the following query to Solr:
http://localhost:8983/solr/select?q=name:book&stats=true&stats.
field=price
The response Solr returned should be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">name:book</str>
<str name="stats">true</str>
<str name="stats.field">price</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">1</str>
<str name="name">Book 1</str>
<float name="price">39.99</float>
</doc>
<doc>
www.it-ebooks.info
Using Additional Solr Functionalities
252
<str name="id">2</str>
<str name="name">Book 2</str>
<float name="price">30.11</float>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Book 3</str>
<float name="price">27.77</float>
</doc>
</result>
<lst name="stats">
<lst name="stats_fields">
<lst name="price">
<double name="min">27.770000457763672</double>
<double name="max">39.9900016784668</double>
<long name="count">3</long>
<long name="missing">0</long>
<double name="sum">97.87000274658203</double>
<double name="sumOfSquares">3276.9852964233432</double>
<double name="mean">32.62333424886068</double>
<double name="stddev">6.486119174232198</double>
<lst name="facets"/>
</lst>
</lst>
</response>
As you can see, in addition to the standard results list, there was an additional section
available. Now let's see how it worked.
How it works...
The index structure is pretty straightforward. It contains three fields – one for holding the
unique identifier (the id field), one for holding the name (the name field), and one for holding
the price (the price field).
The file that contains the example data is simple, so I'll skip discussing it.
The query is interesting. In addition to the q parameter we have two new parameters. The
first one, stats=true, tells Solr that we want to use StatsComponent – the component
which will calculate the statistics for us. The second parameter, stats.field=price tells
StatsComponent which field to use for the calculation. In our case, we told Solr to use the
price field.
www.it-ebooks.info
Chapter 8
253
Now let's look at the result returned by Solr. As you can see, StatsComponent, added an
additional section to the results. The section contains the statistics generated for the field
that we told Solr we wanted the statistics for. The following statistics are available:
ff min: This is the minimum value that was found in the field, for the documents that
matched the query
ff max: This is the maximum value that was found in the field, for the documents that
matched the query
ff sum: This is the sum of all values in the field, for the documents that matched
the query
ff count: This specifies how many non-null values were found in the field for the
documents that matched the query
ff missing: This specifies the number of documents that matched the query but
didn't have any value in the specified field
ff sumOfSquares: This specifies the sum of all values squared in the field, for the
documents that matched the query
ff mean: This specifies the average for the values in the field, for the documents that
matched the query
ff stddev: This specifies the standard deviation for the values in the field, for the
documents that matched the query
You should also remember that you can specify a number of the stats.field parameters
to calculate the statistics for the different fields in a single query.
Please be careful when using this component on the multivalued fields as it can be a
performance bottleneck.
Checking the user's spelling mistakes
Most modern search sites have some kind of user spelling mistakes correction mechanism.
Some of those sites have a sophisticated mechanism, while others just have a basic one. But
actually that doesn't matter. If all search engines have it then there is a high probability that
your client or boss will want one too. Is there a way to integrate such a functionality into Solr?
Yes there is, and this recipe will show you how to do it.
Getting ready
In this recipe we'll learn how to use the Solr spellchecker component. The detailed information
about setting up the spellchecker component can be found in the Configuring spellchecker to
not use its own index recipe in Chapter 1, Apache Solr Configuration.
www.it-ebooks.info
Using Additional Solr Functionalities
254
How to do it...
1. Let's begin with the index structure (just add this to your schema.xml file, to the
field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
2. The data that we are going to index looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mechanics cookbook</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Other book</field>
</doc>
</add>
3. Our spell checking mechanism will work on the basis of the name field. Now,
let's add the appropriate search component to the solrconfig.xml file:
<searchComponent name="spellcheck" class="solr.
SpellCheckComponent">
<str name="queryAnalyzerFieldType">name</str>
<lst name="spellchecker">
<str name="name">direct</str>
<str name="field">name</str>
<str name="classname">solr.DirectSolrSpellChecker</str>
<str name="buildOnCommit">true</str>
</lst>
</searchComponent>
4. In addition to that we would like to have it integrated into our search handler,
so we make the default search handler definition the same as in the following
code (add this to your solrconfig.xml file):
<requestHandler name="/spell" class="solr.SearchHandler"
startup="lazy">
<lst name="defaults">
<str name="df">name</str>
<str name="spellcheck.dictionary">direct</str>
<str name="spellcheck">on</str>
www.it-ebooks.info
Chapter 8
255
<str name="spellcheck.collate">true</str>
</lst>
<arr name="last-components">
<str>spellcheck</str>
</arr>
</requestHandler>
5. Now let's check how it works. To do that we will send a query that contains a spelling
mistake. We will send the words other boak instead of other book. The query
doing that should look like as follows:
http://localhost:8983/solr/spell?q=name:(othar boak)
The Solr response for that query looks like the following response:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">3</int>
</lst>
<result name="response" numFound="0" start="0">
</result>
<lst name="spellcheck">
<lst name="suggestions">
<lst name="other">
<int name="numFound">1</int>
<int name="startOffset">6</int>
<int name="endOffset">11</int>
<arr name="suggestion">
<str>other</str>
</arr>
</lst>
<lst name="boak">
<int name="numFound">1</int>
<int name="startOffset">12</int>
<int name="endOffset">16</int>
<arr name="suggestion">
<str>book</str>
</arr>
</lst>
<str name="collation">name:(other book)</str>
</lst>
</lst>
</response>
As you can see for the preceding response, Solr corrected the spelling mistake we made.
Now let's see how that happened.
www.it-ebooks.info
Using Additional Solr Functionalities
256
How it works...
The index structure is pretty straightforward. It contains two fields, one for holding the unique
identifier (the id field), one for holding the name (the name field). The file that contains the
example data is simple, so I'll skip discussing it.
The spellchecker component configuration is something we discussed already in the
Configuring spellchecker to not use its own index recipe in the first chapter. So again,
I'll look at only the most important fragments.
As you can see in the configuration, we've defined a spellchecker component that will use
Solr DirectSolrSpellChecker in order to not store its index on the hard disk drive. In
addition to that, we configured it to use the name field for spellchecking and also to use
that field analyzer to process queries. Our /spell handler is configured to automatically
include spellchecking results (<str name="spellcheck">on</str>), to create collation
(<str name="spellcheck.collate">true</str>), and to use direct dictionary (<str
name="spellcheck.dictionary">direct</str>). All those properties were already
discussed in the previously mentioned recipe.
Now let's look at the query. We send the boak and othar words in the query parameter (q).
The spellchecker component will be activated automatically because of the configuration of
our /spell handler, and that's actually all there is to it when it comes to the query.
Finally we come to the results returned by Solr. As you can see there were no documents
found for the word boak and the word other, that's what we actually were expecting.
But as you can see there is a spellchecker component section added to the results list
(the <lst name="spellcheck"> tag). For each word there is a suggestion returned
by Solr (the tag <lst name="boak"> is the suggestion for the word boak). As you can
see, the spellchecker component informed us about the number of suggestions found
(<int name="numFound">), about the start and end offset of the suggestion (<int
name="startOffset">and <int name="endOffset">), and about the actual
suggestions (the <arr name="suggestion"> array). The only suggestion that Solr
returned was the book word (<str>book</str> under the suggestion array). The
same goes for the second word.
There is an additional section in the spellchecker component results generated by the
spellcheck.collate=true parameter, <str name="collation">name:(other
book)</str>. This tells us what query Solr suggested to us. We can either show the query
to the user or send it automatically to Solr and show to the user the corrected results list
and this one is up to you.
www.it-ebooks.info
Chapter 8
257
Using field values to group results
Imagine a situation where your data set is divided into different categories, subcategories,
price ranges, and things like that. What if you would like to not only get information about
counts in such a group (with the use of faceting), but would also like to show the most
relevant documents in each of the groups? Is there a grouping mechanism of some kind
in Solr? Yes there is, and this recipe will show you how to use this functionality in order
to divide documents into groups on the basis of field values.
How to do it...
1. Let's start with the index structure. Let's assume that we have the following fields
in our index (just add this to the schema.xml file to the field section):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="category" type="string" indexed="true" stored="true"
/>
<field name="price" type="tfloat" indexed="true" stored="true" />
2. The example data, which we are going to index, looks like the following code:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr cookbook</field>
<field name="category">it</field>
<field name="price">39.99</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Mechanics cookbook</field>
<field name="category">mechanics</field>
<field name="price">19.99</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">ElasticSearch book</field>
<field name="category">it</field>
<field name="price">49.99</field>
</doc>
</add>
www.it-ebooks.info
Using Additional Solr Functionalities
258
3. Let's assume that we would like to get our data divided into groups on the basis of
their category. In order to do that we send the following query to Solr:
http://localhost:8983/solr/select?q=*:*&group=true&group.
field=category
The results returned by the preceding query are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="group.field">category</str>
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="category">
<int name="matches">3</int>
<arr name="groups">
<lst>
<str name="groupValue">it</str>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<str name="category">it</str>
<float name="price">39.99</float>
</doc>
</result>
</lst>
<lst>
<str name="groupValue">mechanics</str>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Mechanics cookbook</str>
<str name="category">mechanics</str>
<float name="price">19.99</float>
</doc>
</result>
www.it-ebooks.info
Chapter 8
259
</lst>
</arr>
</lst>
</lst>
</response>
As you can see the grouped results are different from the ones returned during a usual
search. But as you can see we got a single document per group which means it worked.
So now let's see how.
How it works...
Our index structure is very simple. It consist four fields – one responsible for the document
identifier (the id field), one used for holding the name of the book (the name field), its
category (the category field), and the last one used to hold the price of the book (the
price field). Our example data is also very simple, but please know that the first and second
book belongs to the same it category and the second book belongs to another category.
Let's look at our query now. We said that we want to have our documents divided on the basis
of contents of the category field. In order to do that, we've added a new parameter called
group, which is set to true. This tells Solr that we want to enable the grouping functionality.
And similar to faceting, we've added a second parameter we are not familiar with. The group.
field parameter is set to the name of the field holding books category, and
that's all we need.
If we look at the results returned by Solr, they are a bit different than the usual results. You
can see the usual response header, however, the resulting groups are returned in the <lst
name="grouped"> tag. The <lst name="category"> tag is generated for each group.
field parameter passed in the query; this time it tells us that the following results will be
for the category field. The <int name="matches">3</int> tag informs us how many
documents were found for our query. This is the same as the numFound value during our
usual query.
Next we have the groups array, which holds the information about the groups that were
created by Solr in the results. Each group is described by the it value, that is, the <str
name="groupValue">it</str> section for the first group, which means that all documents
in that group have the it value in the field used for grouping. In the result tag we can see
the documents returned for the group. By default Solr will return the most relevant document
for each group. I'll skip commenting on the result tag as it is almost identical to the results
Solr returns for a non-grouped query and we are familiar with those, right?
One last thing – you can specify multiple group.field parameters with different fields in a
single query in order to get multiple grouping.
www.it-ebooks.info
Using Additional Solr Functionalities
260
There's more...
There is one more thing about grouping on the basis of field values and I would like to share a
few thoughts about that.
More than a single document in a group
Sometimes you may need to return more than a single document in a group. In order to do
that you will need to use the group.limit parameter and set it to the maximum number of
documents you want to have. For example, if we would like to have 10 documents per group
of results, we would send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.
field=category&group.limit=10
Using queries to group results
Sometimes grouping results on the basis of field values is not enough. For example, imagine
that we would like to group documents in price brackets, that is, we would like to show the
most relevant document for documents with price range of 1.0 to 19.99, a document for
documents with price range of 20.00 to 50.0, and so on. Solr allows us to group results on
the basis of query results. This recipe will show you how to do that.
Getting ready
In this chapter we will use the same index structure and test data as we used in the Using
field values to group results recipe in this chapter. Please read it before continuing.
How to do it…
As we are reusing the data and index structure from the Using field values to group results
recipe, we can start with the query. In order to group our documents on the basis of query
results, we can send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.query=price:
[20.0+TO+50.0]&group.query=price:[1.0+TO+19.99]
The results of the preceding query look as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
www.it-ebooks.info
Chapter 8
261
<arr name="group.query">
<str>price:[20.0 TO 50.0]</str>
<str>price:[1.0 TO 19.99]</str>
</arr>
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="price:[20.0 TO 50.0]">
<int name="matches">3</int>
<result name="doclist" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr cookbook</str>
<str name="category">it</str>
<float name="price">39.99</float>
</doc>
</result>
</lst>
<lst name="price:[1.0 TO 19.99]">
<int name="matches">3</int>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Mechanics cookbook</str>
<str name="category">mechanics</str>
<float name="price">19.99</float>
</doc>
</result>
</lst>
</lst>
</response>
So now let's look at how it works.
How it works...
As you can see in the query we told Solr that we want to use the grouping functionality
by using the group=true parameter. In addition to that we specify that we want to have
two groups calculated on the basis of the queries. The first group should contain the
documents that match the following range query price=[20.0+TO+50.00] (the group.
query=price:[1.0+TO+19.99] parameter), and the second group should contain
documents that match the following range query price=[1.0+TO+19.99] (the group.
query=price:[1.0+TO+19.99] parameter).
www.it-ebooks.info
Using Additional Solr Functionalities
262
If you look at the results, they are very similar to the ones for grouping on the basis of field
values. The only difference is in the name of the groups. When using the field values for
grouping, groups were named after the used field names. However, when using queries
to group documents, groups are named as our grouping queries. So in our case, we have
two groups – one named price:[1.0+TO+19.99] (the <lst name="price:[1.0
TO 19.99]"> tag) and a second one named price:[20.0 TO 50.0] (the <lst
name="price:[20.0 TO 50.0]"> tag).
Using function queries to group results
Imagine that you would like to group results not by using queries or field contents, but instead
you would like to use a value returned by a function query. Imagine you could group documents
on the basis of their distance from a point. Sounds good, Solr allows that and in the following
recipe we will see how we can use a simple function query to group results.
Getting ready
In this chapter we will use the same index structure and test data we used in the Sorting
results by a function value recipe in this chapter. We will also use some knowledge that
we gained in the Using field values to group results recipe in this chapter. Please read
them before continuing.
How to do it...
I assume that we would like to have our documents grouped on the basis of the distance
from a given point (in real life we would probably like to have some kind of bracket calculated,
but let's skip that for now).
As we are using the same index structure and test data as we used in the Sorting results by
a function value recipe in this chapter, we'll start with the query. In order to achieve what we
want we send the following query:
http://localhost:8983/solr/select?q=*:*&group=true&group.
func=geodist(geo,0.0,0.0)
The following results were returned by Solr after running the preceding query:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="group.func">geodist(geo,0.0,0.0)</str>
www.it-ebooks.info
Chapter 8
263
<str name="group">true</str>
<str name="q">*:*</str>
</lst>
</lst>
<lst name="grouped">
<lst name="geodist(geo,0.0,0.0)">
<int name="matches">3</int>
<arr name="groups">
<lst>
<double name="groupValue">1584.126028923632</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Company one</str>
<str name="geo">10.1,10.1</str>
</doc>
</result>
</lst>
<lst>
<double name="groupValue">1740.0195023531824</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Company two</str>
<str name="geo">11.1,11.1</str>
</doc>
</result>
</lst>
<lst>
<double name="groupValue">1911.187477467305</double>
<result name="doclist" numFound="1" start="0">
<doc>
<str name="id">3</str>
<str name="name">Company three</str>
<str name="geo">12.2,12.2</str>
</doc>
</result>
</lst>
</arr>
</lst>
</lst>
</response>
Everything worked as it should have, so now let's see how it worked.
www.it-ebooks.info
Using Additional Solr Functionalities
264
How it works...
As you can see, the query is very similar to the one we used when grouping our documents on
the basis of field values. So, we again pass the group=true parameter to enable grouping,
but this time in addition to that we pass the group.func parameter with the value, that is,
our function query based on whose results Solr should group our documents.
If you look at the results, they are again very similar to the ones seen in grouping on the basis
of field values. The only difference is in the names of the groups. When using the field values
for grouping, groups were named after the used field names. However, when using function
queries to group documents, groups are named by the result of the function query. So in our
case, we have three groups because our function query returned three different results, as
illustrated in the following list:
ff The group named 1584.126028923632 (the <double name="groupVal
ue">1584.126028923632</double> tag)
ff The group named 1740.0195023531824 (the <double name="groupVal
ue">1740.0195023531824</double> tag)
ff The group named 1911.187477467305 (the <double name="groupVal
ue">1911.187477467305</double> tag)
www.it-ebooks.info
9
Dealing with Problems
In this chapter we will cover:
ff How to deal with too many opened files
ff How to deal with out-of-memory problems
ff How to sort non-English languages properly
ff How to make your index smaller
ff Diagnosing Solr problems
ff How to avoid swapping
Introduction
Every Solr deployment will, sooner or later, have some kind of problem. It doesn't matter
if the deployment is small and simple or if it's a big and complicated deployment containing
multiple servers and shards. In this chapter I'll try to help you with some of the problems you
can run into when running Solr. I hope this will help you and make your task easier.
How to deal with too many opened files
Sometimes you might encounter a strange error, something that lies on the edge between
Lucene and the operating system—the "too many files opened" exception. Is there something
we can do about it? Yes, we can, and this recipe will show you how.
www.it-ebooks.info
Dealing with Problems
266
How to do it...
The following steps show how to deal with too many opened files:
1. So, for the purpose of the recipe let's assume that the header of the exception thrown
by Solr looks like this:
java.io.FileNotFoundException: /use/share/solr/data/index/_1.fdx
(Too many open files)
2. What can you do instead of pulling your hair out? First of all, this probably occurred on
a Unix-/Linux-based operating system. So, let's start with setting the opened files' limit
higher. To do that, you need to edit the /etc/security/limits.conf file of your
operating system and set the following values (I assume Solr is running as solr user):
solr soft nofile 32000
solr hard nofile 32000
3. Now let's add the following line to the .bash_profile file in the solr user home
directory:
ulimit -n 32000
The probable cause of the "too many files opened" exception is the number of files the
index is built of. The more segments the index is built of, the more files will be used.
4. The next thing sometimes worth considering is lowering the mergeFactor
parameter. To make things simple, the lower the mergeFactor setting, the fewer
files will be used to construct the index (please read the How it works... section
that follows, about the dangers of having a very low merge factor). So, let's set
mergeFactor to 2. We modify the following line in the solrconfig.xml file and
set it with the appropriate value (2 in our case):
<mergeFactor>2</mergeFactor>
After we set that configuration value, we need to run the optimization of the index. Now let's
see what the options mean.
How it works...
We don't discuss the operating system's internal working in this book, but in this section we
will make an exception. The mentioned limits.conf file in the /etc/security directory
lets you specify the opened files limit for the users of your system. In the example shown
earlier, we set the two necessary limits to 32000 for the user solr, so if you had problems
with the number of opened files in the default setup you should see the difference after
restarting Solr. However, remember that if you are working as the user and you change
the limits then you may need to log out and log in again to see those changes.
www.it-ebooks.info
Chapter 9
267
Next, we have the mergeFactor parameter. This configuration parameter lets you determine
how often Lucene segments will be merged. The lower the value of mergeFactor, the
smaller the number of index files will be. However, you have to remember that having a small
mergeFactor value will lead to more background merges being done by Lucene, and thus
the indexing speed will be lower compared to the ones with a higher mergeFactor value
and your node's I/O system will be used more extensively. On the other hand, lower values
of mergeFactor will speed up searching.
How to deal with out-of-memory problems
As with every application written in Java, sometimes memory problems happen. When talking
about Solr, those problems are usually related to heap size. They usually happen when the
heap size is too low. This recipe will show you how to deal with those problems and what to
do to avoid them.
How to do it...
Let's consider what to do when we see an exception like this:
SEVERE: java.lang.OutOfMemoryError: Java heap space
Firstly, you can do something to make your task easier. You can add more memory that the
Java virtual machine can use if you have some free physical memory available in your system.
To do that, you need to add the Xmx and, preferably, the Xms parameter to the start-up
script of your servlet container (Apache Tomcat or Jetty). To do that, I used the default
Solr deployment and modified the parameters. This is how Solr was run with more than
the default heap size:
java –Xmx1024M –Xms512m –jar start.jar
How it works...
So what do the Xmx and Xms Java virtual machine parameters do? The Xms parameter
specifies how much heap memory should be assigned by the virtual machine at the start and
thus this is the minimal size of the heap memory that will be assigned by the virtual machine.
The Xmx parameter specifies the maximum size of the heap. The Java virtual machine will not
be able to assign more memory for the heap than the Xmx parameter.
You should remember one thing—sometimes it's good to set the Xmx and Xms parameters to
the same values. It will ensure that the virtual machine won't be resizing the heap size during
application execution and thus won't lose precious time in heap resizing.
One additional thing—be careful when setting the heap size to be too big. It is usually not
advised to give the heap size more than 60 percent of your total memory available in the
system, because your operating system's I/O cache will suffer.
www.it-ebooks.info
Dealing with Problems
268
There's more...
There are a few more things I would like to discuss when it comes to memory issues.
Monitoring heap when an out-of-memory error occurs
If the out-of-memory errors occurs even after the actions you've done, you should start
monitoring your heap. One of the easiest ways to do that is to add the appropriate Java
virtual machine parameters. Those parameters are XX:+HeapDumpOnOutOfMemory and
XX:HeapDumpPath. Those two parameters tell the virtual machine to dump the heap on the
out-of-memory error and write it to a file created in the specified directory. So the default Solr
deployment's start command would look like this:
java –jar –XX:+HeapDumpOnOutOfMemoryError –XX:HeapDumpPath=/var/log/dump/
start.jar
Reducing the amount of memory needed by Solr
However there are times (even if your system has a large amount of memory available), when
you may be forced to think about Solr memory consumption reduction. In such cases there is
no general advice, but these are a few things that you can keep in mind:
ff Look at your queries and consider how they are built
ff How you use the faceting mechanism and so on (facet.method=fc tends to use
less memory when the field has many unique terms in the index)
ff Remember that fetching too many documents at a time may cause Solr to run out of
heap memory (for example, when setting a large value for the query result window)
ff Reduce the number of calculated faceting results (facet.limit parameter)
ff Check the memory usage of your caches—this can also be one of the reasons for
the problems with memory
ff If you don't need to use the normalization factor for text fields, you can set
omitNorms="true" for such fields and save some additional memory too
ff Remember that grouping mechanisms requires memory; for big result sets and
high numbers of groups, a vast amount of memory may be needed
How to sort non-English languages properly
As you probably already know, Solr supports UTF-8 encoding and thus can handle data
in many languages. But, if you ever needed to sort some languages that have characters
specific to them you probably know that it doesn't work well on a standard Solr string
type. This recipe will show you how to deal with sorting in Solr.
www.it-ebooks.info
Chapter 9
269
How to do it...
These steps tell us how to sort non-English languages properly:
1. For the purpose of this recipe, I have assumed that we will have to sort text that
contains Polish characters. To show the good and bad sorting behaviour we need
to create the following index structure (add this to your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="name_sort_bad" type="string" indexed="true"
stored="true" />
<field name="name_sort_good" type="text_sort" indexed="true"
stored="true" />
2. Now let's define some copy fields to automatically fill the name_sort_bad and
name_sort_good fields. Here is how they are defined (add this after the fields
section in the schema.xml file):
<copyField source="name" dest="name_sort_bad" />
<copyField source="name" dest="name_sort_good" />
3. The last thing about the schema.xml file is the new type. So the text_sort
definition looks like this:
<fieldType name="text_sort" class="solr.TextField">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory" />
<filter class="solr.CollationKeyFilterFactory" language="pl"
country="PL" strength="primary" />
</analyzer>
</fieldType>
4. The test we need to index looks like this:
<add>
<doc>
<field name="id">1</field>
<field name="name">Laka</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Lalka</field>
</doc>
<doc>
<field name="id">3</field>
<field name="name">Zab</field>
</doc>
</add>
www.it-ebooks.info
Dealing with Problems
270
5. First, let's take a look at how the incorrect sorting order looks. To do this, we send the
following query to Solr:
http://localhost:8983/solr/select?q=*:*&sort=name_sort_bad+asc
And now the response that was returned for the query is as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="q">*:*</str>
<str name="sort">name_sort_bad asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lalka</str>
<str name="name_sort_bad">Lalka</str>
<str name="name_sort_good">Lalka</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Zab</str>
<str name="name_sort_bad">Zab</str>
<str name="name_sort_good">Zab</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Laka</str>
<str name="name_sort_bad">Laka</str>
<str name="name_sort_good">Laka</str>
</doc>
</result>
</response>
6. Now let's send the query that should return the documents sorted in the correct
order. The query looks like this:
http://localhost:8983/solr/select?q=*:*&sort=name_sort_good+asc
www.it-ebooks.info
Chapter 9
271
And the results returned by Solr are as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">6</int>
<lst name="params">
<str name="q">*:*</str>
<str name="sort">name_sort_good asc</str>
</lst>
</lst>
<result name="response" numFound="3" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lalka</str>
<str name="name_sort_bad">Lalka</str>
<str name="name_sort_good">Lalka</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Laka</str>
<str name="name_sort_bad">Laka</str>
<str name="name_sort_good">Laka</str>
</doc>
<doc>
<str name="id">3</str>
<str name="name">Zab</str>
<str name="name_sort_bad">Zab</str>
<str name="name_sort_good">Zab</str>
</doc>
</result>
</response>
As you can see the order is different and believe me it's correct. Now let's see how it works.
How it works...
Every document in the index is built on four fields. The id field is responsible for holding the
unique identifier of the document. The name field is responsible for holding the name of the
document. The last two fields are used for sorting.
www.it-ebooks.info
Dealing with Problems
272
The name_sort_bad field is nothing new; it's just a field based on string, which
is used to perform sorting. The name_sort_good field is based on a new type, the
text_sort field type. The field is based on the solr.TextField type and on solr.
KeywordTokenizerFactory, which basically means that our text won't be tokenized. We
used this trick because we want to sort on that field and thus we don't want the data in it to
be tokenized, but we want to use a special filter on that field. The filter that allows Solr to sort
correctly is the solr.CollationKeyFilterFactory filter. We used three attributes of
this filter. First, the language attribute, which tells Solr about the language of the field. The
second attribute is country which tells Solr about the country variant (this can be skipped
if necessary). The strength attribute informs Solr about the collation strength used. More
information about those parameters can be found in the JDK documentation. One thing that is
crucial is that you need to create an appropriate field and set the appropriate attribute's value
for every non-English language you want to sort on.
The two queries you can see in the examples differ in only one thing, the field used for sorting.
The first query uses the string-based field, name_sort_bad. When sorting on this field, the
document order will be incorrect when there are non-English characters present. However,
when sorting on the name_sort_good field everything will be in the correct order as shown
in the example.
How to make your index smaller
There may be situations where you would like to make your index smaller. The reasons may be
different—you may want to have a smaller index so that it would fit into the operating system's
I/O cache or you want to store your index in RAMDirectory. This recipe will try to help you
with the process of index slimming.
How to do it...
The following steps tell us how to make your index smaller:
1. For the purpose of this recipe, I assumed that we will have four fields that describe the
document. I created the following index structure (add this to your schema.xml file):
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="true" />
<field name="description" type="text" indexed="true" stored="true"
/>
<field name="price" type="string" indexed="true" stored="true" />
Let's assume that our application has the following requirements:
?? We need to search on name and description fields
?? We need to show two fields in the results: id and price
?? We don't use highlighting and spellchecker
www.it-ebooks.info
Chapter 9
273
2. So the first thing we should do is set the stored="false" attribute for the name
and description fields.
3. Next, we set the indexed="false" attribute for the price field.
4. Now, the last thing to do is add the term options. We add the
termVectors="false", termPositions="false", and
termOffsets="false" attributes to the name and description fields.
The modified schema looks like this:
<field name="id" type="string" indexed="true" stored="true"
required="true" />
<field name="name" type="text" indexed="true" stored="false"
termVectors="false" termPositions="false" termOffsets="false"/>
<field name="description" type="text" indexed="true"
stored="false" termVectors="false" termPositions="false"
termOffsets="false"/>
<field name="price" type="string" indexed="false" stored="true" />
Let's check the index size now. I've indexed 1,000,000 sample documents with the use
of the original schema.xml file. The index size was 329,237,331 bytes. After changing
the schema.xml file and indexing the same data the index size was 84,301,603 bytes.
So as you can see, the index size was reduced.
Now let's see why we see this reduction in the index size.
How it works...
The first schema.xml file you see is the standard index structure provided with Solr example
deployment, at least when talking about the types. We have four fields, all of them indexed
and stored, which means all of them are searchable and are shown in the result list.
Now let's look at the requirements. First of all we only need to search on the name and
description fields, which mean that the rest of the fields can be set up as not indexed
(indexed="false" attribute). We set that for the price field, while we set the id field to be
searchable, as we need that to avoid duplicates. When the indexed attribute is set to false,
the information in that field is not indexed which basically means that it isn't written into the
Lucene-inverted index and thus it is not available; this saves index space. Of course you can't
set this attribute to false if you need this field to be searchable.
The second requirement tells us what fields we are obligated to show in the search results.
Those field are the ones that need the stored attribute set to true, and the rest can have
this attribute set to false. When we set this attribute to false, we tell Solr that we don't
want to store the original value—the one before analysis—thus we don't want this field to be
included in the search results. Setting this attribute to true on many fields will increase
the index size substantially.
www.it-ebooks.info
Dealing with Problems
274
The last requirement is actually information; we don't need to worry about highlighting
functionality so we can reduce the index size in a greater way. To do that we add the
termVectors="false", termPositions="false", and termOffsets="false"
attributes to the name and description fields. By doing that we tell Solr not to store
any information about terms in the index. This basically means that we can't use the
highlighting functionalities of Solr, but we have reduced our index size substantially
and we don't need highlighting.
If we don't need index time boosting and we do not care about length normalization, we could
also turn on the omitting of that factor (the omitNorms="true" attribute) for the fields
based on the text type (for primitive types such as string, integer, and so on it's turned
on by default in Solr 4.0). This would shrink the index a bit more and in addition to that save
us some memory during queries.
Last few words. Every time you think about reducing the index size, first do the optimization,
then look at your schema.xml file and see if you need all those fields. Then check which
fields shouldn't be stored and which you can omit when indexing. The last thing should
be removing the information about terms, because there may come a time when you will
need this information and the only thing you will be able to do is a full indexation of millions
of documents.
There's more...
There is one additional thing I would like to mention.
Estimating your index size and memory usage
Sometimes it's necessary to have a rough estimate of the index size and the memory
usage of your Solr instance. Currently there is a draft of the Microsoft Excel spreadsheet
that lets you do that kind of estimation. If you are interested in it, download the following
file: http://svn.apache.org/repos/asf/lucene/dev/trunk/dev-tools/sizeestimator-
lucene-solr.xls.
Diagnosing Solr problems
There are many tools out there that can help you diagnose problems with Solr. You can
monitor your operating system by yourself by using different operating system commands
such as vmstat, dstat, and iostat. You can use different Java tools such as jconsole
and jvisualvm to look at the JMX mbeans, you can monitor your garbage collector work,
and so on. However in order to properly diagnose what's happening with your Apache Solr
cluster you'll need to see the whole view as well as the specifics. There are different tools
out there that you can use, however this recipe will show you what you can find in one of
them—Scalable Performance Monitoring.
www.it-ebooks.info
Chapter 9
275
Getting ready
This recipe assumes that you have Scalable Performance Monitoring installed and running. If
you don't, please go to http://sematext.com/spm/index.html, create a free account,
and download the client that's suitable for you. The installation is very simple and you'll be
guided by the Scalable Performance Monitoring installer from the beginning to the end.
How to do it...
1. Let's assume that we want to check our Solr instance health by looking at the GUI of
Scalable Performance Monitoring. After logging we would get the following view:
This is an overview of the system, however we would like to see some details.
www.it-ebooks.info
Dealing with Problems
276
2. Let's start with the information about indices.
3. Now let's have a look at the cache usage:
4. By now we know what our index and Solr caches' usage looks like and we know if we
need to tune them or not, so now let's look at the query rate and its latency:
www.it-ebooks.info
Chapter 9
277
5. Here we can see the warm-up queries' time and execution:
We've got all the information that is connected to queries, so now we can go and see
the other crucial information such as memory and CPU usage, Java heap usage, and
how Java garbage collector works.
6. Let's start with the memory and CPU usage:
www.it-ebooks.info
Dealing with Problems
278
7. And now we can see the JVM heap statistics:
8. And finally we can see how the garbage collector works:
That's all we need during the usual work when we want to see how different parts of Solr work.
If we would like to go in depth and see how the I/O subsystem works or the swap usage we can
use other aggregated reports available in any of the monitoring systems, or you could just use
the appropriate system commands like the ones mentioned in the introduction to the recipe.
How it works...
Let's discuss the provided statistics in a bit more dtail. On the first screenshot provided you
can see the overview of the system. This part of Scalable Performance Monitoring will be
shown to you as soon as you log in to the system. You'll usually use it to get the whole idea
about the system, but you'll want to look at the detailed reports in order to see a higher
granularity of your data.
On the second screenshot you can see the index statistics (or indices depending on the
options you've chosen). You can see the information about the number of documents in
the index, the maximum size of the index, the number of segments, and the delta, which is
calculated as the maximum number of documents minus the current number of documents.
Not shown on the screenshot are the filesystem statistics which tell you about the size of the
index on the disk. With the use of this data you can see the complete information about your
core's or collection's disk drive usage.
www.it-ebooks.info
Chapter 9
279
The third screenshot is one of the most important ones—the information about Apache Solr
caches. Although I haven't shown all the information here, you can see a single cache on the
screenshot—the query result cache (we didn't show the document cache and the filter cache).
You can see information about the size of the cache, the maximum size of the cache, the
number of evictions, and so on. Remember, if your cache is too low, its size will be equal
to the maximum size and you'll start seeing evictions, which is not good and you'll probably
want to change the cache configuration.
The query rate and latency report shown in the fourth screenshot provides information about
the number of queries and their average latency. You can see how your queries were executed
and if you need to start thinking about the optimization of your system.
In order to check how your warm-up queries were executed you can look at the fifth of
the provided screenshots. You can see the amount of time for which your warm-up queries
were executed and how long it took to auto-warm your query result cache and your filter
cache. This information can be valuable when dealing with problems such as Solr hanging
during the opening of new or first searches.
The last three screenshots provide the information that is not directly connected to Apache
Solr, but very valuable from our point of view, when we have to see what is happening with
our Solr instance. So let's discuss them.
The sixth screenshot shows information about the CPU and memory usage. For the CPU
information you can see how it works; the percent of time spent idling, working on user-run
software, working with operating system software, handling interruptions, and so on. If you
look at the memory graph you will find the total, used, free, cached, and buffered statistics.
That's basically all you need in order to see if your CPU is 100 percent utilized and how your
system memory is utilized. This is crucial when your system is not working in the way that you
would like it to.
The seventh screenshot provides information about the Java virtual machine. You can see the
heap memory statistics and the threading information (which is not shown in the screenshot).
The heap usage graph allows us to see if the amount of memory we specified for our Solr
instance is enough to handle all the operations that need to be done.
The final screenshot provides information about how your JVM garbage collector works.
In most situations you will want it to run more frequently, but for a shorter period of time
stop the world events which may cause your Solr instances to stop handling queries or
indexing for a short period of time.
To sum up, all the information can be gathered manually by using different system and Java
tools. The crucial part of every monitoring system is the ability to show you graphs that will let
you point to a certain event in time. We looked at a single monitoring solution, but there are
many more available and if you don't like Scalable Performance Monitoring you can use any
available. One more thing; please remember that we only scraped the surface in this recipe
and the book (or e-book) you are holding in your hands doesn't describe all the information
regarding monitoring and dealing with problems. However I hope that this recipe will help you
at least start with this topic.
www.it-ebooks.info
Dealing with Problems
280
If you don't want to use Scalable Performance Monitoring, you can choose some other
technology that is available like Ganglia (http://ganglia.sourceforge.net/), Mumin
(http://munin-monitoring.org/), Zabix (http://www.zabbix.com/), Cacti (http://
www.cacti.net/), or any commercial ones like New Relic (http://newrelic.com/).
How to avoid swapping
One of the crucial things when running your Solr instance in production is performance. What
you want is to give your clients relevant results in the blink of an eye. If your clients have to
wait for results for too long, some of them may choose other vendors or sites that provide
similar services. One of the things to remember when running a Java application such as
Apache Solr is to ensure that the operating system won't write the heap to disk. This ensures
that the part of the memory used by Solr won't be swapped at all. This recipe will show you
how to achieve that on a Linux operating system.
Getting ready
Please note that the following recipe is only valid when running Apache Solr on a Linux
operating system. In addition to that, please be advised that turning off swapping should
only be done when you have enough memory to handle all the necessary application in
your system and you want to be sure that there won't be any swapping.
How to do it...
1. Before turning off swapping let's look at the amount of swap memory used by
our operating system. In order to do that let's look at the main page of the Solr
administration panel:
www.it-ebooks.info
Chapter 9
281
2. As you can see some swap memory is being used. In order to demonstrate how to
turn off swap usage I've freed some memory on the virtual machine I was using for
tests and after that I've run the following commands:
sudo sysctl -w vm.swappiness=0
sudo /sbin/swapoff -a
3. After the second command is done running, refresh the main page of the Solr admin
instance and this is what it will show:
4. It seems like it is working, but in order to be sure I've run the following command:
free -m
And the response of it was:
total used free shared buffers cached
Mem: 3001 2326 675 0 3 97
-/+ buffers/cache: 2226 775
Swap: 0 0 0
And again we can see that there is no swap usage. Now let's see how this works.
www.it-ebooks.info
Dealing with Problems
282
How it works...
On the first provided screenshot you can see that there is a bit more than 183 MB of
swap memory being used. This is not good; in a production environment you want to avoid
swapping, of course, if you have the necessary amount of memory. Swapping will make the
contents of the memory to be written onto the hard disk drive, thus making your operating
system and applications execute slower. This can also affect Solr.
So, in order to turn off swapping in a Linux operating system, we've run two commands. The
first one sets the vm.swappiness operating system property to 0, which means that we want
to avoid swapping. We needed to use sudo, because in order to set that property with the
use of the sysctl command we need administration privileges. The second command (the /
sbin/swapoff -a one) disables swapping on all known devices.
As you can see on the second screenshot, the Solr administration panel didn't even include
the swapping information so we may suspect that it was turned off. However in order to be
sure, we've used another Linux command, the free command with the -m switch, in order to
see the memory usage on our system. As you can see, the Swap section shows 0, so we can
now be sure that swapping was turned off.
www.it-ebooks.info
Real-life Situations
In this chapter we will cover:
ff How to implement a product's autocomplete functionality
ff How to implement a category's autocomplete functionality
ff How to use different query parsers in a single query
ff How to get documents right after they were sent for indexation
ff How to search your data in a near real-time manner
ff How to get documents with all the query words at the top of the results set
ff How to boost documents based on their publication date
Introduction
In the previous nine chapters, we discussed about the different Apache Solr functionalities
and how to overcome some common problems and situations. However, I decided that we
will describe a few of the most common problems that arise on the Apache Solr mailing list
and during our work with our clients. This chapter is dedicated to describing how to handle
such situations, and I hope that you'll find it useful.
www.it-ebooks.info
Real-life Situations
284
How to implement a product's autocomplete
functionality
The autocomplete functionality is very popular now. You can find it in most e-commerce sites,
on Google, Bing, and so on. It enables your users or clients to find what they want and do it fast.
In most cases, the autocomplete functionality also increases the relevance of your search by
pointing to the right author, title, and so on, right away without looking at the search results.
What's more, sites that use autocomplete report higher revenue after deploying it in comparison
to the situation before implementing it. Seems like a win-win situation, both for you and your
clients. So, let's look at how we can implement a product's autocomplete functionality in Solr.
How to do it...
Let's assume that we want to show the full product name whenever our users enter a part
of the word that the product name is made up of. In addition to this, we want to show the
number of documents with the same names.
1. Let's start with an example data that is going to be indexed:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
</doc>
</add>
2. We will need two main fields in the index – one for the document identifier and one
for the name. We will need two additional fields – one for autocomplete and one for
faceting that we will use. So, our index structure will look similar to the following code
snippet (we should add it to the schema.xml fields section):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="name_autocomplete" type="text_autocomplete"
indexed="true" stored="false" />
<field name="name_show" type="string" indexed="true"
stored="false" />
www.it-ebooks.info
Appendix
285
3. In addition to this, we want Solr to automatically copy the data from the name field to
the name_autocomplete and name_show fields. So, we should add the following
copy fields section to the schema.xml file:
<copyField source="name" dest="name_autocomplete"/>
<copyField source="name" dest="name_show"/>
4. Now, the final thing about the schema.xml file — that is, the text_autocomplete
field type — it should look similar to the following code snippet (place it in the types
section of the schema.xml file):
<fieldType name="text_autocomplete"
class="solr.TextField" positionIncrementGap="100">
<analyzer type="index">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.EdgeNGramFilterFactory"
minGramSize="1" maxGramSize="25" />
</analyzer>
<analyzer type="query">
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
5. That's all. Now, if we would like to show all the products that start with the word sol
to our users, we would send the following query:
curl 'http://localhost:8983/solr/select?q=name_autocomplete:sol&q.
op=AND&rows=0&&facet=true&facet.field=name_show&facet.
mincount=1&facet.limit=5'
The response returned by Solr would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="fl">name</str>
<str name="facet.mincount">1</str>
<str name="q">name_autocomplete:sol</str>
<str name="facet.limit">5</str>
<str name="q.op">AND</str>
<str name="facet.field">name_show</str>
www.it-ebooks.info
Real-life Situations
286
<str name="rows">0</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="name_show">
<int name="First Solr 4.0 CookBook">1</int>
<int name="Second Solr 4.0 CookBook">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, the faceting results returned by Solr are exactly what we were looking
for. So now, let's see how it works.
How it works...
Our example documents are pretty simple – they are only built of an identifier and a name
that we will use to make autocomplete. The index structure is where things are getting
interesting. The first two fields are the ones that you would have expected – they are used
to hold the identifier of the document and its name. However, we have two additional fields
available; the name_autocomplete field that will be used for querying and name_show that
will be used for faceting. The name_show field is based on a string type, because we want to
have a single token per name when using faceting.
With the use of the copy field sections, we can let Solr automatically copy the values of the
fields defined by the source attribute to the field defined by the dest field. Copying is done
before any analysis.
The name_autocomplete field is based on the text_autocomplete field type, which is
defined differently for indexing and querying. During query time, we divide the entered query
on the basis of white space characters using solr.WhitespaceTokenizerFactory, and
we lowercase the tokens with the use of solr.LowerCaseFilterFactory. For query time,
this is what we want because we don't want any more processing. For index time, we not only
use the same tokenizer and filter, but also solr.NGramFilterFactory. This is because
we want to allow our users to efficiently search for prefixes, so that when someone enters the
word sol, we would like to show all the products that have a word starting with that prefix,
and solr.NGramFilterFactory allows us to do that. For the word solr, it will produce
the tokens s, so, sol, and solr.
www.it-ebooks.info
Appendix
287
We've also said that we are interested in grams starting from a single character (the
minGramsSize property) and the maximum size of grams allowed is 25 (the maxGramSize
property).
Now comes the query. As you can see, we've sent the prefix of the word that the users have
entered to the name_autocomplete field (q=name_autocomplete:sol). In addition to
this, we've also said that we want words in our query to be connected with the logical AND
operator (the q.op parameter), and that we are not interested in the search results (the
rows=0 parameter). As we said, we will use faceting for our autocomplete functionality,
because we need the information about the number of documents with the same titles, so
we've turned faceting on (the facet=true parameter). We said that we want to calculate
the faceting on our name_show field (the facet.field=name_show parameter). We are
also only interested in faceting a calculation for the values that have at least one document
in them (facet.mincount=1), and we want the top five results (facet.limit=5).
As you can see, we've got two distinct values in the faceting results; both with a single
document with the same title, which matches our sample data.
How to implement a category's
autocomplete functionality
Sometimes we are not just interested in our product's name for autocomplete. Imagine that
we want to show the category of our products in the autocomplete box along with the number
of products in each category. Let's see how we can use faceting
to do that.
How to do it...
This recipe will show how we can implement a category's autocomplete functionality.
1. Let's start with the example data, which is going to be indexed and which looks
similar to the following code snippet:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
<field name="category">Books</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
<field name="category">Books And Tutorials</field>
</doc>
</add>
www.it-ebooks.info
Real-life Situations
288
2. The fields section of the schema.xml configuration file that can handle the
preceding data should look similar to the following code snippet:
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="category" type="text_lowercase"
indexed="true" stored="true" />
3. One final thing is the text_lowercase type definition, which should be placed in
the types section of the schema.xml file. It should look similar to the following
code snippet:
<fieldType name="text_lowercase" class="solr.TextField"
positionIncrementGap="100">
<analyzer>
<tokenizer class="solr.KeywordTokenizerFactory"/>
<filter class="solr.LowerCaseFilterFactory"/>
</analyzer>
</fieldType>
4. So now, if we would like to get all the categories that start with boo, along with the
number of products in those categories, we would send the following query:
curl 'http://localhost:8983/solr/select?q=*:*&rows=0&facet=tr
ue&facet.field=category&facet.mincount=1&facet.limit=5&facet.
prefix=boo'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="facet">true</str>
<str name="facet.mincount">1</str>
<str name="indent">true</str>
<str name="q">*:* </str>
<str name="facet.limit">5</str>
<str name="facet.prefix">boo</str>
<str name="facet.field">category</str>
<str name="rows">0</str>
www.it-ebooks.info
Appendix
289
</lst>
</lst>
<result name="response" numFound="2" start="0">
</result>
<lst name="facet_counts">
<lst name="facet_queries"/>
<lst name="facet_fields">
<lst name="category">
<int name="books">1</int>
<int name="books and tutorials">1</int>
</lst>
</lst>
<lst name="facet_dates"/>
<lst name="facet_ranges"/>
</lst>
</response>
As you can see, we have two categories, each containing a single product. So this is
what matches our example data. Let's now see how it works.
How it works...
Our data is very simple. We have three fields for each of our documents – one for the
identifier fields, one for holding the name of the document, and one for its category.
We will use the category field to do the autocomplete functionality, and we will use
faceting for it.
If you look at the index structure, for the category field, we use a special type – the text_
lowercase one. What it does is that it stores the category as a single token in the index
because of solr.KeywordTokenizerFactory. We also lowercase with the appropriate
filter. This is because we want to send the lowercased queries while using faceting.
The query is quite simple – we query for all the documents (q=*:* parameter), and
we don't want any results returned (the rows=0 parameter). We will use faceting for
autocomplete, so we turn it on (facet=true) and we specify the category field to calculate
the faceting (facet.field=category). We are also only interested in faceting a calculation
for the values that have at least one document in them (facet.mincount=1), and we want
the top five results (facet.limit=5). One of of the most important parameters in the query
is facet.prefix – using it we can return on those results in faceting that start with the
prefix defined by the mentioned parameter, which can be seen in the results.
www.it-ebooks.info
Real-life Situations
290
How to use different query parsers in a
single query
Sometimes, it is good to be able to choose different query parsers in the same query.
For example, imagine that you would like to use the Extended DisMax query parser for
the main query, but in addition to this, we would like to use the field query parser for
filter queries. This recipe will show you how to do it.
How to do it...
This recipe will show how we can use different query parsers in a single query.
1. Let's start with the following index structure (this should go to the field section
in the schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="category" type="string" indexed="true"
stored="true" />
2. Now, let's index the following data:
<add>
<doc>
<field name="id">1</field>
<field name="name">First Solr 4.0 CookBook</field>
<field name="category">Books</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Second Solr 4.0 CookBook</field>
<field name="category">Books And Tutorials</field>
</doc>
</add>
3. So, if we search for all the documents using the Extended DisMax query parser and
want to narrow our results to the Books And Tutorials category, then we can send
the following query:
curl 'http://localhost:8983/solr/select?q=*:*&defType=edismax&fq={
!term f=category}Books And Tutorials'
www.it-ebooks.info
Appendix
291
The results returned by Solr would be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="fq">
{!term f=category}Books And Tutorials
</str>
<str name="q">*:*</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">2</str>
<str name="name">Second Solr 4.0 CookBook</str>
<str name="category">Books And Tutorials</str>
</doc>
</result>
</response>
As you can see, we got what we expected. So let's see how it works.
How it works...
Our index structure and example data are not that relevant for this recipe, so I'll skip
discussing them.
What we want to achieve is be sure that the data we filter will be properly processed, and
we want to avoid thinking about any kind of query parsing and Lucene special characters
escaping. In order to do this, we use the term query parser. To inform Solr that we want to
use this query parser in the filter query (the fq parameter), we use local parameter syntax
and send this filter query: {!term f=category}Books And Tutorials. The !term
part of the filter query says which query parser we want to use, and the f property specifies
the field to which we want to send the provided Books And Tutorials value.
That's all; as you can see in the provided results, everything works as intended.
www.it-ebooks.info
Real-life Situations
292
How to get documents right after they were
sent for indexation
Let's say that we would like to get our documents as soon as they were sent for indexing, but
without any commit (both hard and soft) operation occurring. Solr 4.0 comes with a special
functionality called real-time get, which uses the information of uncommitted documents
and can return them as documents. Let's see how we can use it.
How to do it...
This recipe will show how we can get documents right after they were sent for indexation.
1. Let's begin with defining the following index structure (add it to the field section
in your schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
2. In addition to this, we need the _version_ field to be present, so let's also add
the following field to our schema.xml file in its field section:
<field name="_version_" type="long" indexed="true"
stored="true"/>
3. The third step is to turn on the transaction log functionality in Solr. In order to do
this, we should add the following section to the updateHandler configuration
section (in the solrconfig.xml file):
<updateLog>
<str name="dir">${solr.data.dir:}</str>
</updateLog>
4. The last thing we need to do is add a proper request handler configuration to our
solrconfig.xml file:
<requestHandler name="/get"
class="solr.RealTimeGetHandler">
<lst name="defaults">
<str name="omitHeader">true</str>
<str name="indent">true</str>
<str name="wt">xml</str>
</lst>
</requestHandler>
www.it-ebooks.info
Appendix
293
5. Now, we can test how the handler works. In order to do this, let's index the following
document (which we've stored in the data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 4.0 CookBook</field>
</doc>
</add>
6. In order to index it, we use the following command:
curl 'http://localhost:8983/solr/update' --data-binary @data.xml
-H 'Content-type:application/xml'
7. Now, let's try two things. First, let's search for the document we've just added.
In order to do this, we run the following query:
curl 'http://localhost:8983/solr/select?q=id:1'
8. As you can imagine, we didn't get any documents returned, because we didn't
send any commit command – not even the soft commit one. So now, let's use
our defined handler:
curl 'http://localhost:8983/solr/get?id=1'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<doc name="doc">
<str name="id">1</str>
<str name="name">Solr 4.0 CookBook</str>
<long name="_version_">1418467767663722496</long>
</doc>
</response>
As you can see, our document is returned by our get handler. Let's see how it
works now.
How it works...
Our index structure is simple, and there is only one relevant piece of information there
– the _version_ field. The real-time get functionality needs that field to be present in
our documents, because the transaction log relies on it. However, as you can see in the
provided example data, we don't need to worry about this field, because its filled and
updated automatically by Solr.
www.it-ebooks.info
Real-life Situations
294
But let's backtrack a bit and discuss the changes made to the solrconfig.xml file.
There are two things there. The first one is the update log (the updateLog section),
which Solr uses to store the so-called transaction log. Solr stores recent index changes
there (until hard commit), in order to provide write durability, consistency, and the ability
to provide the real-time get functionality.
The second thing is the handler we defined under the name of /get with the use of the
solr.RealTimeGetHandler class. It uses the information in the transaction log to get
the documents we want by using their identifier. It can even retrieve the documents that
weren't committed and are only stored in the transaction log. So, if we want to get the
newest version of the document, we can use it. The other configuration parameters are
the same as with the usual request handler, so I'll skip commenting them.
The next thing we do is send the update command without adding the commit command,
so that we shouldn't be able to see the document during a standard search. If you look at the
results returned by the first query, you'll notice that we didn't get that document. However, when
using the /get handler that we previously defined, we get the document we requested. This is
because Solr uses the transaction log in order to even the uncommitted document.
How to search your data in a near real-time
manner
Sometimes, we need our data to be available as soon as possible. Imagine that we have a
SolrCloud cluster up and running, and we want to have our documents available for searching
with only a slight delay. For example, our application can be a content management system
where it would be very weird if a user adds a new document, and it would take some time for
it to be searchable. In order to achieve this, Solr exposes the soft commit functionality, and
this recipe will show you how to set it up.
How to do it...
This recipe will show how we can search for data in a near real-time manner.
1. For the purpose of this recipe, let's assume that we have the following index
structure (add it to the field section in your schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
2. In addition to this, we need to set up the hard and soft automatic commits,
for which we will need to add the following section to the updateHandler
section in the solrconfig.xml file:
www.it-ebooks.info
Appendix
295
<autoCommit>
<maxTime>60000</maxTime>
<openSearcher>false</openSearcher>
</autoCommit>
<autoSoftCommit>
<maxTime>1000</maxTime>
</autoSoftCommit>
3. Let's test if that works. In order to do this, let's index the following document
(which we've stored in the data.xml file):
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 4.0 CookBook</field>
</doc>
</add>
4. In order to index it, we use the following command:
curl 'http://localhost:8983/solr/update' --data-binary @data.xml
-H 'Content-type:application/xml'
5. We didn't send any commit command, so we shouldn't see any documents, right?
I think there will be one available – the one we've just send for indexation. But, let's
check that out by running the following simple search command:
curl 'http://localhost:8983/solr/select?q=id:1'
The following response will be returned by Solr:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">0</int>
<lst name="params">
<str name="q">id:1</str>
</lst>
</lst>
<result name="response" numFound="1" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr 4.0 CookBook</str>
</doc>
</result>
</response>
As you can see, our document was returned. So, let's see how it works.
www.it-ebooks.info
Real-life Situations
296
How it works...
As you may know, the standard commit operation is quite resource-intensive – it flushes the
changes since the last commit to the disk to the new segment. If you would like to do that every
second, we could run into a problem of a very high amount of I/O writes and thus our searches
would suffer (of course, this depends on the situation). That's why, with Lucene and Solr 4.0,
the new commit type was introduced – the soft commit, which doesn't flush the changes to
disk, but just reopens the searcher object and allows us to search the data that is stored in
the memory.
As we are usually lazy and don't want to remember when it's time to send the commit and
when to use soft commit, we'll let Solr manage that so we properly need to configure the
update handler. First, we add the standard auto commit by adding the autoCommit section
and saying that we want to commit after every 60 seconds (the maxTime property is specified
in milliseconds), and that we don't want to reopen the searcher after the standard commit
(the openSearcher property is set to false).
The next thing is to configure the soft auto commit functionality by adding the softAutoCommit
section to the update handler configuration. We've specified that we want the soft commit to be
fired every second (the maxTime property is specified in milliseconds), and thus our searcher
will be reopened every second if there are changes.
As you can see, even though we didn't specify the commit command after our update
command, we are still able to find the document we've sent for indexation.
How to get the documents with all the query
words to the top of the results set
One of the most common problems that users struggle with when using Apache Solr is how to
improve the relevancy of their results. Of course, relevancy tuning is, in most cases, connected
to your business needs, but one of the common problems is to have documents that have all the
query words in their fields at the top of the results list. You can imagine a situation where you
search for all the documents that match at least a single query word, but you would like to show
the ones with all the query words first. This recipe will show you how to achieve that.
How to do it...
This recipe will show how we can get the documents with all the query words to the top of the
results set.
1. Let's start with the following index structure (add it to the field section in your
schema.xml file):
www.it-ebooks.info
Appendix
297
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="description" type="text" indexed="true"
stored="true" />
2. The second step is to index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr and all the others</field>
<field name="description">This is about Solr</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Lucene and all the others</field>
<field name="description">
This is a book about Solr and Lucene
</field>
</doc>
</add>
3. Let's assume that our usual queries look similar to the following code snippet:
http://localhost:8983/solr/select?q=solr book&defType=edismax&mm=1
&qf=name^10000+description
Nothing complicated; however, the results of such query don't satisfy us, because
they look similar to the following code snippet:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">name^10000 description</str>
<str name="mm">1</str>
<str name="q">solr book</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
www.it-ebooks.info
Real-life Situations
298
<doc>
<str name="id">1</str>
<str name="name">Solr and all the others</str>
<str name="description">This is about Solr</str>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Lucene and all the others</str>
<str name="description">
This is a book about Solr and Lucene
</str>
</doc>
</result>
</response>
4. In order to change this, let's introduce a new handler in our solrconfig.xml file:
<requestHandler name="/better"
class="solr.StandardRequestHandler">
<lst name="defaults">
<str name="indent">true</str>
<str name="q">
_query_:"{!edismaxqf=$qfQuery mm=$mmQuerypf=
$pfQuerybq=$boostQuery v=$mainQuery}"
</str>
<str name="qfQuery">name^100000 description</str>
<str name="mmQuery">1</str>
<str name="pfQuery">name description</str>
<str name="boostQuery">
_query_:"{!edismaxqf=$boostQueryQf mm=100%
v=$mainQuery}"^100000
</str>
<str name="boostQueryQf">name description</str>
</lst>
</requestHandler>
5. So, let's send a query to our new handler:
http://localhost:8983/solr/better?mainQuery=solr book
We get the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
www.it-ebooks.info
Appendix
299
<int name="QTime">2</int>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">2</str>
<str name="name">Lucene and all the others</str>
<str name="description">
This is a book about Solr and Lucene
</str>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr and all the others</str>
<str name="description">This is about Solr</str>
</doc>
</result>
</response>
As you can see, it works. So let's discuss how.
How it works...
For the purpose of the recipe, we've used a simple index structure that consists of a
document identifier, its name, and description. Our data is very simple as well; it just
contains two documents.
During the first query, the document with the identifier 1 is placed at the top of the query
results. However, what we would like to achieve is be able to boost the name. In addition to
this, we would like to have the documents with words from the query close to each other at
the top of the results.
In order to do this, we've defined a new request handler named /better, which will
leverage the local params. The first thing is the defined q parameter, which is the standard
query. It uses the Extended DisMax parser (the {!edismax part of the query), and defines
several additional parameters:
ff qf: This defines the fields against which edismax should send the query. We tell
Solr that we will provide the fields by specifying the qfQuery parameter by using
the $qfQuery value.
ff mm: This is the "minimum should match" parameter, which tells edismax how
many words from the query should be found in a document for the document to
be considered a match. We tell Solr that we will provide the fields by specifying
the mmQuery parameter, by using the $mmQuery value.
www.it-ebooks.info
Real-life Situations
300
ff pf: This is the phrase fields definition which specifies the fields on which Solr should
generate phrase queries automatically. Similar to the previous parameters that we've
specified, we will provide the fields by specifying the pfQuery parameter, by using
the $pfQuery value.
ff bq: This is the boost query that will be used to boost the documents. Again, we use
the parameter dereferencing functionality and tell Solr that we will provide the
value in the bqQuery parameter, by using the $bqQuery value.
ff v: This is the final parameter which specifies the content of the query; in our case,
the user query will be specified in the mainQuery parameter.
Basically, the preceding queries say that we will use the edismax query parser, phrase,
and boost queries. Now let's discuss the values of the parameters.
The first thing is the qfQuery parameter, which is exactly the same as the qf parameter in
the first query we sent to Solr. Using it, we just specify the fields that we want to be searched
and their boosts. Next, we have the mmQuery parameter set to 1 that will be used as mm in
edismax, which means that a document will be considered a match when a single word
from the query will be found in it. As you will remember, the pfQuery parameter value will
be passed to the pf parameter, and thus the phrase query will be automatically made on
the fields defined in those fields.
Now, the last and probably the most important part of the query, the boostQuery parameter,
specifies the value that will be passed to the bq parameter. Our boost query is very similar to
our main query, however, we say that the query should only match the documents that have
all the words from the query (the mm=100% parameter). We also specify that the documents
that match that query should be boosted by adding the ^100000 part at the end of it.
To sum up all the parameters of our query, they will promote the documents with all the words
from the query present in the fields we want to search on. In addition to this, we will promote
the documents that have phrases matched. So finally, let's look at how the newly created
handler work. As you can see, when providing our query to it with the mainQuery parameter,
the previous document is now placed as the first one. So, we have achieved what we wanted.
How to boost documents based on their
publishing date
Imagine that you would like to place documents that are newer above the ones that are older.
For example, you have a book store and want to promote the books that have been published
recently, and place them above the books that have been present in our store for a long time.
Solr lets us do this, and this recipe will show you how.
www.it-ebooks.info
Appendix
301
How to do it...
This recipe will show how we can boost documents based on their publishing date.
1. Let's begin with the following index structure (add it to the field section in your
schema.xml file):
<field name="id" type="string" indexed="true"
stored="true" required="true" />
<field name="name" type="text" indexed="true"
stored="true" />
<field name="published" type="date" indexed="true"
stored="true" default="NOW" />
2. Now, let's index the following sample data:
<add>
<doc>
<field name="id">1</field>
<field name="name">Solr 3.1 CookBook</field>
<field name="published">2011-02-02T12:00:00Z</field>
</doc>
<doc>
<field name="id">2</field>
<field name="name">Solr 4.0 CookBook</field>
<field name="published">2012-10-01T12:00:00Z</field>
</doc>
</add>
3. Now, let's run a simple query:
curl 'http://localhost:8983/solr/select?q=solr+cookbook&qf=name&de
fType=edismax'
For the preceding query, Solr will return the following results:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">1</int>
<lst name="params">
<str name="qf">name</str>
<str name="q">solr cookbook</str>
<str name="defType">edismax</str>
</lst>
</lst>
www.it-ebooks.info
Real-life Situations
302
<result name="response" numFound="2" start="0">
<doc>
<str name="id">1</str>
<str name="name">Solr 3.1 CookBook</str>
<date name="published">2011-02-02T12:00:00Z</date>
</doc>
<doc>
<str name="id">2</str>
<str name="name">Solr 4.0 CookBook</str>
<date name="published">2012-10-01T12:00:00Z</date>
</doc>
</result>
</response>
4. As you can see, the newest document is the second one, which we want to avoid. So,
we need to change our query to the following one:
curl 'http://localhost:8983/solr/select?q=solr+cookbook&qf=name&bf
=recip(ms(NOW/HOUR,published),3.16e-11,1,1)defType=edismax'
Now, the response will be as follows:
<?xml version="1.0" encoding="UTF-8"?>
<response>
<lst name="responseHeader">
<int name="status">0</int>
<int name="QTime">2</int>
<lst name="params">
<str name="qf">name</str>
<str name="bf">
recip(ms(NOW/HOUR,published),3.16e-11,1,1)
</str>
<str name="q">solr cookbook</str>
<str name="defType">edismax</str>
</lst>
</lst>
<result name="response" numFound="2" start="0">
<doc>
<str name="id">2</str>
<str name="name">Solr 4.0 CookBook</str>
<date name="published">2012-10-01T12:00:00Z</date>
</doc>
<doc>
<str name="id">1</str>
<str name="name">Solr 3.1 CookBook</str>
www.it-ebooks.info
Appendix
303
<date name="published">2011-02-02T12:00:00Z</date>
</doc>
</result>
</response>
So, we have achieved what we wanted. Now, let's see how it works.
How it works...
Our index structure consists of three fields; one responsible for holding the identifier of the
document, one for the name of the document, and the last one; the one which we will be
most interested in, in which we hold the publishing date.
The published field has one nice feature – if we don't define it in the document and send
it for indexation, then it will get the value of the date and time when it is processed (the
default="NOW" attribute).
As you can see, the first query that we sent to Solr returned results not in a way we would
like them to be sorted. The most recent document is the second one. Of course, we could
have sorted them by date, but we don't want to do that, because we would like to have the
most recent and the most relevant documents at the top, not only the newest ones.
In order to achieve this, we use the bf (boost function) parameter. We specify the boosting
function. At first, it can look very complicated, but it's not. In order to boost our documents,
we use the recip(ms(NOW/HOUR,published),3.16e-11,1,1) function query. 3.16e10
specifies the number of milliseconds that are in a single year, so we use 3.16e-11 to invert
that, and we use the reciprocal function (recip) to calculate the scaling value, which will
return values near 1 for recent documents, 1/2 for documents from about a year, 1/3 for
documents that are about two years old, 1/4 for documents that are about three years old,
and so on.
We've also used NOW/HOUR to reduce the precision of the published field, in order for
our function query to consume less memory and because we don't need that granularity;
our results will be just fine.
As you can see, our query with the bf parameter and the time-based function query work
as intended.
There's more...
If you want to read more about function queries, please refer to the http://wiki.apache.
org/solr/FunctionQuery Solr wiki page.
www.it-ebooks.info
www.it-ebooks.info
Index
Symbols
-DnumShards parameter 213
-DzkHost parameter 213
-DzkRun parameter 213
<script> tag 55
A
add command 58
administration GUI, SolrCloud
cluster 220-223
adminPath property 17
adminPath variable 9
alphabetical order
faceting results, sorting in 168-170
analyzer 70
Apache Nutch
URL, for downloading 27
URL, for info 30
Apache Solr
URL, for tutorial 5
Apache Tika 36
Apache Tika library
used, for detecting language 66
Apache Tomcat
Solr, running on 10-13
URL 11
apt-get command 6, 8
automatic document distribution
stopping, among shards 230-234
autosuggest feature
implementing, faceting used 171-173
autowarmCount parameter 190, 193
B
binary files
metadata, extracting from 40-42
bqQuery parameter 300
buffer overflow 10
C
cache 22
caches, Solr
document 22, 26
filter 22, 25
query result 22, 26
CamelCase
used, for splitting text 80, 82
Catalina context file 12
category’s autocomplete functionality
implementing 287-289
working 289
CDATA tags 75
character filters 70
clientPort property 15
cluster
collections, setting up 214-216
replica count, increasing 227-230
collections
setting up, in cluster 214-216
commit command 295
commit operation
about 200
Solr performance, improving after 194-196
conf directory 13
config-file 120
configuration, document cache 189, 190
www.it-ebooks.info
306
configuration, filter cache 192, 193
configuration, query result cache 191, 192
configuration, Solr cache
about 23, 24
document cache 26
filter cache 25
filter cache, using with faceting 25
no cache hits 25
query result cache 26
query result window 26
configuration, spellchecker 19, 21
content
copying, of dynamic fields 77
copying, of fields 75-77
context directory 6
contrib modules 62
crawl command 29
crawl-urlfilter.txt file 29
CSV 30
curl command 37
currencyConfig attribute 61
currencyExchange.xml file 61
currency provider
setting up 62
D
data
clustering 15, 17
importing, Data Import Handler used 48-50
indexing, Data Import Handler used 45-48
modifying, in Data Import Handler 53-55
searching, in near real-time manner 294-296
stemming 91-93
data analysis 70
data behavior 70
data-config.xml file 52
dataDir property 15
Data Import Handler
about 42
configuring, with JDBC 42-44
data, modifying 53-55
used, for importing data 48-50
used, for indexing data from database 45-48
using, with URL data source 50, 51
data indexing 70
db-data-config.xml file 43
debug attribute 12
decision tree faceting
using 180-183
defaultCoreName attribute 9, 13
defaultCurrency attribute 61
default HTML tags
modifying 241
default similarity implementation
modifying 32-34
defined words
ignoring 248-250
defType parameter 116
delete operation 216
different query parsers
using, in single query 290, 291
directoryFactory tag 18
directory implementation
selecting 17-19
DirectSolrSpellChecker 256
DisMax query parser
about 116, 122
used, for querying particular value 109
distance
defining, between words in phrase 114
distributed indexing 223-226
docBase attribute 12
document
language, detecting 62-66
single field, updating 56-58
document cache
about 22, 26, 189
configuring 189, 190
document count
getting, by query match 161-164
getting, by subquery match 161-164
getting, without value in field 174-176
getting, with same field value 156-158
getting, with same value range 158-161
document language
detecting 62-66
detecting, Apache Tika library used 66
documents
boosting, based on publishing date 301-303
default HTML tags, modifying 241
excluding, with
QueryElevationComponent 121
faceting, calculating for 183-186
www.it-ebooks.info
307
getting right, after indexation 292, 293
getting, with all query words at top
results set 296-300
modifying 136-138
positioning, over others on query 117-121
positioning, with closer words 122-125
retrieving, with partial match 128-130
DoubleMetaphoneFilterFactory 247
duplicate documents
detecting 145-148
omitting 145-148
dynamic fields
content, copying of 77
E
elevate.xml file 139
embedded ZooKeeper server
starting 213
enablePositionIncrements parameter 250
entities 44
Extended DisMax query parser
parameters 299
using 290, 299
extracting request handler
setting up 30, 31
F
faceting
about 155
calculating, for relevant documents
in groups 183-186
filter cache, using with 25
used, for implementing
autosuggest feature 171-173
faceting method per field
specifying 200
faceting performance
improving, for low cardinality fields 198, 199
faceting results
filters, removing from 164-167
lexicographical sorting 158
sorting, in alphabetical order 168-170
facet limits
for different fields, in same query 177-180
FastVectorHighlighting feature 243
field
updating, of document 56-58
field aliases
using 148-150
fields
content, copying of 75-77
specifying, for highlighting 241
field value
used, for grouping results 257-259
used, for sorting results 109-111
file data source 50
filter cache
about 22, 25, 192
configuring 192, 193
using, with faceting 25
filter caching
avoiding 206
filter queries
order of execution, controlling for 207, 208
filters
removing, from faceting results 164-167
flexible indexing 68
function queries
used, for grouping results 262, 263
functions
scoring, affecting with 130-34
function value
used, for sorting results 243-245
G
Gangila
URL 188
generateNumberParts parameter 98
generateWordParts parameter 98
geodist function 245
geographical points
storing, in index 88-91
global similarity
configuring 34
H
hash value 227
highlighting
fields, specifying for 241
HTML tags
eliminating, from text 73-75
www.it-ebooks.info
308
HttpDataSource 52
Hunspell
about 99
using, as stemmer 99, 100
I
ignoreCase attribute 79
ignored.txt file 248
index
geographical points, storing in 88-91
making, smaller 272, 273
indexing 35
index size
estimating 274
information
storing, payloads used 70-73
initialSize parameter 190
initLimit property 15
installation, ZooKeeper 14, 15
instanceDir attribute 9
issues, Apache Tomcat
Apache Tomcat, running on different port 13
issues, Jetty servlet container
buffer overflow 10
Jetty, running on different port 9
J
Java 6 55
java command 8, 9
JDBC
Data Import Handler, configuring with 42-44
Jetty
Solr, running on 6-9
Jetty servlet container
URL, for downloading 6
jetty.xml file 7, 10
JSON 30
L
language attribute 55
lexicographical sorting, faceting results 158
light stemming 86
logging.properties file 7
low cardinality fields
faceting performance, improving for 198, 199
Lucene directory implementation 17
LuceneQParser query parser 240
Lucene’s internal cache 23
M
matched words
highlighing 238-240
maxChars attribute 77
mergeFactor parameter 267
metadata
extracting, from binary files 40-42
mmQuery parameter
about 299
multiple currencies
configuring 59-61
handling 59
using 59-61
multiple values
querying for 109
N
n-grams
about 95
used, for handling user typos.. 142-145
non-English languages
sorting, properly 268-271
non-whitespace characters
used, for splitting text 96-98
numbers
used, for splitting text 96-98
numerical range queries
performance, improving 208, 209
O
opened files
dealing with 265-267
order of execution
controlling, of filter queries 207, 208
OR operator 122
out-of-memory issues
dealing with 267, 268
www.it-ebooks.info
309
P
parameter dereferencing 136
parameters, Extended DisMax query parser
bq 300
mm 299
pf 300
qf 299
v 300
parent-child relationships
about 139
using 140, 141
partial match
documents, retrieving with 128-130
particular field value
asking for 108
particular value
querying, DisMax query parser used 109
path attribute 12
payload
about 70
used, for storing information 70-73
PDFCreator 36
PDF files
indexing 36-38
performance
about 187
improving, of numerical range
queries 208, 209
pfQuery parameter 300
phrase
searching for 111-113
phrases
boosting, over words 114-116
boosting, with standard query parser 117
phrase slop 114
pivot faceting 180
plural words
singular, making 84-86
PostgreSQL 50
primary key 67
primary key field indexing
optimizing 67, 68
product’s autocomplete functionality
implementing 284, 285
working 286, 287
Q
qfQuery parameter 299
queries
nesting 134-136
used, for grouping results 260-262
queryAnalyzerFieldType property 21
QueryElevationComponent
document, excluding with 121
queryFieldType attribute 120
query parser 291
query performance
analyzing 202-205
query result cache
about 22, 26, 190
configuring 191, 192
queryResultMaxDocsCached property 189
query results
paging 188, 189
query result window 26
queryResultWindowSize property 188
R
real-time get 292
reload operation 216
replicas
increasing, on live cluster 227-230
replication 227
result pages
caching 197, 198
results
grouping, field values used 257-259
grouping, function queries used 262-263
grouping, queries used 260-262
sorting, by distance from point 125-128
sorting, by field value 109-111
sorting, by function value 243-245
value of function, returning in 151-153
S
Scalable Performance Monitoring 25, 188
schema.xml file 7, 29, 38, 52, 84, 133
scoring
affecting, with functions 130-134
searching 223-226
www.it-ebooks.info
310
search results
used, for computing statistics 250-253
Sematext
about 25
URL 188
server.xml file 11
similar documents
returning 236-238
softCommit command 17
Solr
about 36, 99
indexing, issues 200-202
performance, improving after commit
operation 194-196
performance, improving after startup
operation 194-196
result pages, caching 197, 198
running, on Apache Tomcat 10-12
running, on Jetty 6-9
Solr 4.0 211
Solr cache
configuring 23, 24
SolrCloud
about 211
automatic document distribution, stopping
among shards 230-234
collections, setting up in cluster 214-216
distributed indexing 223-226
replicas, increasing on live cluster 227-230
searching 223-226
SolrCloud cluster
about 211
administration GUI 220-223
creating 212
managing 216, 217-219
working 213
solrconfig.xml file 7, 16, 19, 52, 188
solr.DFRSimilarityFactory 34
solr.DirectSolrSpellchecker 19
solr.DirectSolrSpellChecker 21
Solr issues
diagnosing 274-279
solr.MMapDirectoryFactory 18
solr.NIOFSDirectoryFactory 18
solr.NRTCachingDirectoryFactory 19
solr.QueryElevationComponent 117
solr.RAMDirectoryFactory 19
solr.RealTimeGetHandler class 294
solr.SchemaSimilarityFactory 34
solr.SimpleFSDirectoryFactory 18
solr.StandardDirectoryFactory 18
solr.UUIDField 39
solr.war file 6, 8
Solr wiki page 303
solr.xml file 6-13
sounds
used, for searching words 246, 247
spellchecker
about 19
configuring 19, 21
spellchecker component
about 253
using 254-256
spelling mistakes
checking, of user 253-256
splitOnNumerics parameter 98
standard query parser
phrases, boosting with 117
startup operation
Solr performance, improving after 194-196
statistics
computing, for search results 250-253
StatsComponent 252
stemmer
Hunspell, using as 99, 100
stemming
about 91
words, protecting from 103-106
stemming algorithms 84
stemming dictionary
using 101-103
StopFilterFactory 250
string
lowercasing 87, 88
swapping
avoiding 280-282
syncLimit property 15
synonyms attribute 79
synonyms.txt file 78
T
temp directory 6
termVectors attribute 238
www.it-ebooks.info
311
text
HTML tags, eliminating from 73-75
preparing, for wildcard search 93-95
splitting, by CamelCase 80-82
splitting, by non-whitespace characters 96-98
splitting, by numbers 96-98
splitting, by whitespace 82-84
XML tags, eliminating from 73-75
text fields
highlighting 241-243
tickTime property 15
Tika 31
tokenizer 70
tokens 70
transformer 52
types 70
typos
handling, ngrams used 142-145
ignoring, in performance wise way 142-145
U
unique fields
generating, automatically 38, 39
URL data source
Data Import Handler, using with 50-53
UTF-8 file encoding 12
V
value of function
returning, in results 151-153
vQuery parameter 300
W
webapps directory 6
webdefault.xml file 7
web pages
fetching 27-29
indexing 27-29
whitespace
used, for splitting text 82-84
wildcard search
text, preparing for 93-95
words
modifying 77-79
phrases, boosting over 114-116
protecting, from stemming 103-106
searching, by sound 246, 247
X
XML 30
XML tags
eliminating, from text 73-75
XPath expression 52
Z
ZooKeeper
about 14
installing 14, 15
URL, for downloading 14
ZooKeeper cluster 212
www.it-ebooks.info
www.it-ebooks.info
Thank you for buying
Apache Solr 4 Cookbook
About Packt Publishing
Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL
Management" in April 2004 and subsequently continued to specialize in publishing highly focused
books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and
customizing today's systems, applications, and frameworks. Our solution based books give you the
knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past. Our
unique business model allows us to bring you more focused information, giving you more of what
you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge
books for communities of developers, administrators, and newbies alike. For more
information, please visit our website: www.packtpub.com.
About Packt Open Source
In 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to
continue its focus on specialization. This book is part of the Packt Open Source brand, home
to books published on software built around Open Source licences, and offering information to
anybody from advanced developers to budding web designers. The Open Source brand also runs
Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project
about whose software a book is sold.
Writing for Packt
We welcome all inquiries from people who are interested in authoring. Book proposals should
be sent to author@packtpub.com. If your book idea is still at an early stage and you would like to
discuss it first before writing a formal book proposal, contact us; one of our commissioning editors
will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing
experience, our experienced editors can help you develop a writing career, or simply get some
additional reward for your expertise.
www.it-ebooks.info
Apache Solr 3 Enterprise
Search Server
ISBN: 978-1-84951-606-8 Paperback: 418 pages
Enhance your search with faceted navigation, result
highlighting relevancy ranked sorting, and more
1. Comprehensive information on Apache Solr
3 with examples and tips so you can focus
on the important parts
2. Integration examples with databases,
web-crawlers, XSLT, Java & embedded-Solr,
PHP & Drupal, JavaScript, Ruby frameworks
3. Advice on data modeling, deployment
considerations to include security, logging,
and monitoring, and advice on scaling Solr
and measuring performance
HBase Administration
Cookbook
ISBN: 978-1-84951-714-0 Paperback: 332 pages
Master HBase configuration and administration for
optimum database performance
1. Move large amounts of data into HBase
and learn how to manage it efficiently
2. Set up HBase on the cloud, get it ready
for production, and run it smoothly with
high performance
3. Maximize the ability of HBase with the
Hadoop eco-system including HDFS,
MapReduce, Zookeeper, and Hive
Please check www.PacktPub.com for information on our titles
www.it-ebooks.info
Hadoop Real World Solutions
Cookbook
ISBN: 978-1-84951-912-0 Paperback: 325 pages
Realistic, simple code examples to solve problems at
scale with Hadoop and related technologies
1. Solutions to common problems when working
in the Hadoop environment
2. Recipes for (un)loading data, analytics, and
troubleshooting
3. In depth code examples demonstrating various
analytic models, analytic solutions, and common
best practices
Cassandra High Performance
Cookbook
ISBN: 978-1-84951-512-2 Paperback: 310 pages
Over 150 recipes to design and optimize large-scale
Apache Cassandra deployments
1. Get the best out of Cassandra using this efficient
recipe bank
2. Configure and tune Cassandra components to
enhance performance
3. Deploy Cassandra in various environments and
monitor its performance
4. Well illustrated, step-by-step recipes to make all
tasks look easy!
Please check www.PacktPub.com for information on our titles
www.it-ebooks.info
Hellerstein, chancellor’s professor of computer science at UC Berkeley.
“If you have people in the loop, it’s not real time. Most people take a
second or two to react, and that’s plenty of time for a traditional transactional
system to handle input and output.”
That doesn’t mean that developers have abandoned the quest for speed.
Supported by a Google grant, Matei Zaharia is working on his Ph.D.
at UC Berkeley. He is an author of Spark, an open source cluster computing
system that can be programmed quickly and runs fast. Spark
relies on “resilient distributed datasets” (RDDs) and “can be used to
interactively query 1 to 2 terabytes of data in less than a second.”
In scenarios involving machine learning algorithms and other multipass
analytics algorithms, “Spark can run 10x to 100x faster than Hadoop
MapReduce,” says Zaharia. Spark is also the engine behind
Shark, a data warehousing system.
According to Zaharia, companies such as Conviva and Quantifind
have written UIs that launch Spark on the back end of analytics dashboards.
“You see the statistics on a dashboard and if you’re wondering
about some data that hasn’t been computed, you can ask a question
that goes out to a parallel computation on Spark and you get back an
answer in about half a second.”
Storm is an open source low latency processing stream processing
system designed to integrate with existing queuing and bandwidth
systems. It is used by companies such as Twitter, the Weather Channel,
Groupon and Ooyala. Nathan Marz, lead engineer at BackType (acquired
by Twitter in 2011), is the author of Storm and other opensource
projects such as Cascalog and ElephantDB.
“There are really only two paradigms for data processing: batch and
stream,” says Marz. “Batch processing is fundamentally high-latency.
So if you’re trying to look at a terabyte of data all at once, you’ll never
be able to do that computation in less than a second with batch processing.”
Stream processing looks at smaller amounts of data as they arrive. “You
can do intense computations, like parallel search, and merge queries
on the fly,” says Marz. “Normally if you want to do a search query, you
need to create search indexes, which can be a slow process on one
machine. With Storm, you can stream the process across many machines,
and get much quicker results.”
10 | Chapter 3: How Real Is Real Time?
GAYLE LAAKMANN
Founder and CEO, CareerCup.com
150 programming interview questions and solutions
Plus:
• Five proven approaches to solving tough algorithm questions
• Ten mistakes candidates make -- and how to avoid them
• Steps to prepare for behavioral and technical questions
• Interviewer war stories: a view from the interviewer’s side
FOURTH
C EDITION RACKING THE
C ODING
I NTERVIEW
CRACKING THE
CODING
INTERVIEW

CRACKING THE
CODING
INTERVIEW
150 Programming Interview
Questions and Solutions
GAYLE LAAKMANN
Founder and CEO, CareerCup.com
CareerCup, LLC
Seattle, WA
CRACKING THE CODING INTERVIEW, FOURTH EDITION
Copyright © 2008 - 2010 by Gayle Laakmann. All rights reserved.
Published by CareerCup, LLC, Seattle, WA. Version 3.21090410302210.
Visit our website at: www.careercup.com. No part of this book may be used or reproduced in any manner without written permission except in the case of brief quotations in critical articles or reviews.
For more information, contact support@careercup.com.
Printed in United States of America
978-1-450-59320-5 9781450593205 (ISBN 13)

Cracking the Coding Interview
1
Table of Contents
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Behind the Scenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
The Microsoft Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
The Amazon Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
The Google Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
The Apple Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
The Yahoo Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Interview War Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Before the Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Resume Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Behavioral Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Technical Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
The Interview and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Handling Behavioral Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Handling Technical Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Five Algorithm Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
The Offer and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Top Ten Mistakes Candidates Make . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Frequently Asked Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
Chapter 1 | Arrays and Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Chapter 2 | Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Chapter 3 | Stacks and Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51
Chapter 4 | Trees and Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Concepts and Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Chapter 5 | Bit Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Chapter 6 | Brain Teasers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2
CareerCup.com
Table of Contents
Chapter 7 | Object Oriented Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Chapter 8 | Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Chapter 9 | Sorting and Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Chapter 10 | Mathematical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Chapter 11 | Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Chapter 12 | System Design and Memory Limits . . . . . . . . . . . . . . . . . . . . . . . . 71
Knowledge Based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
Chapter 13 | C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
Chapter 14 | Java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Chapter 15 | Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Chapter 16 | Low Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Chapter 17 | Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Chapter 18 | Threads and Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Additional Review Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Chapter 19 | Moderate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Chapter 20 | Hard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Index .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
Mock Interviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
About the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304

. CareerCup com 4
Foreword
Dear Readers,
Welcome to the 4th edition of Cracking the Coding Interview. This volume updates the 3rd edition with new content and refreshed information. Be sure to check out our website, www.careercup.com, to connect with other candidates and to discover new resources.
For those of you new to technical interviews, the process can seem overwhelming. Interviewers throw questions at you, expect you to whip up brilliant algorithms on the spot, and then ask you to write beautiful code on a whiteboard. Luckily, everyone else is in the same boat, and you’re already working hard to prepare. Good job!
As you get ready for your interviews, consider these suggestions:
» »Write Code on Paper: Most interviewers won’t give you a computer and will instead expect you to write code on a whiteboard or on paper. To simulate this environment, try answering interview problems by writing code on paper first, and then typing them into a computer as-is. Whiteboard / paper coding is a special skill, which can be mastered with constant practice.
» »Know Your Resume: While technical skills are extremely important, that’s no reason to neglect your own resume. Make sure to prepare yourself to give a quick summary of any project or job you were involved with, and to discuss the hardest and most interesting problems you encountered along the day.
» »Don’t Memorize Solutions: While this book offers a representative sample of interview questions, there are still thousands of interview questions out there. Memorizing solutions is not a great use of your time. Rather, use this book to explore approaches to problems, to learn new concepts, and to practice your skills.
» »Talk Out Loud: Interviewers want to understand how you think and approach problems, so talk out loud while you’re solving problems. Let the interviewer see how you’re tackling the problem, and they just might guide you as well.
And remember -- interviews are hard! In my years of interviewing at Google, I saw some interviewers ask “easy” questions while others ask harder questions. But you know what? Getting the easy questions doesn’t make it any easier to get the offer. Receiving an offer is not about solving questions flawlessly (very few candidates do!), but rather, it is about answering questions better than other candidates. So don’t stress out when you get a tricky question - everyone else probably thought it was hard too!
I'm excited for you and for the skills you are going to develop. Thorough preparation will give you a wide range of technical and communication skills. It will be well-worth it no matter where the effort takes you!
Study hard, practice, and good luck!
Gayle Laakmann
5 Cracking the Coding Interview
Introduction
Something’s Wrong
We walked out of the hiring meeting frustrated, again. Of the ten “passable” candidates we reviewed that day, none would receive offers. Were we being too harsh, we wondered?
I, in particular, was disappointed. We had rejected one of my candidates. A former student. One who I had referred. He had a 3.73 GPA from the University of Washington, one of the best computer science schools in the world, and had done extensive work on open source projects. He was energetic. He was creative. He worked hard. He was sharp. He was a true geek, in all the best ways.
But, I had to agree with the rest of the committee: the data wasn’t there. Even if my emphatic recommendation would sway them to reconsider, he would surely get rejected in the later stages of the hiring process. There were just too many red flags.
Though the interviewers generally believed that he was quite intelligent, he had struggled to develop good algorithms. Most successful candidates could fly through the first question, which was a twist on a well known problem, but he struggled to develop his algorithm. When he came up with one, he failed to consider solutions that optimized for other scenarios. Finally, when he began coding, he flew through the code with an initial solution, but it was riddled with mistakes that he then failed to catch. Though he wasn’t the worst candidate we'd seen by any measure, he was far from meeting “the bar.” Rejected.
When he asked for feedback over the phone a couple of weeks later, I struggled with what to tell him. Be smarter? No, I knew he was brilliant. Be a better coder? No, his skills were on-par with some of the best I'd seen.
Like many motivated candidates, he had prepared extensively. He had read K&R’s classic C book and he'd reviewed CLRS' famous algorithms textbook. He could describe in detail the myriad of ways of balancing a tree, and he could do things in C that no sane programmer should ever want to do.
I had to tell him the unfortunate truth: those books aren’t enough. Academic books prepare you for fancy research, but they’re not going to help you much in an interview. Why? I'll give you a hint: your interviewers haven’t seen Red-Black Trees since they were in school either.
To crack the coding interview, you need to prepare with real interview questions. You must practice on real problems, and learn their patterns.
Cracking the Coding Interview is the result of my first-hand experience interviewing at top companies. It is the result of hundreds of conversations with candidates. It is the result of the thousands of candidate- and interviewer- contributed questions. And it’s the result of seeing so many interview questions from so many firms. Enclosed in this book are 150 of the best interview questions, selected from thousands of potential problems.
. CareerCup com 6
Introduction
My Approach
The focus of Cracking the Coding Interview is algorithm, coding and design questions. Why? Because while you can and will be asked behavioral questions, the answers will be as varied as your resume. Likewise, while many firms will ask so-called “trivia” questions (e.g., “What is a virtual function?”), the skills developed through practicing these questions are limited to very specific bits of knowledge. The book will briefly touch on some of these questions, to show you what they’re like, but I have chosen to allocate space where there’s more to learn.
My Passion
Teaching is my passion. I love helping people understand new concepts, and giving them tools so that they can excel in their passions.
My first experience “officially” teaching was in college at the University of Pennsylvania, when I became a teaching assistant for an undergraduate Computer Science course during my second year. I went on to TA for several other courses, and eventually launched my own CS course at the university focused on “hands-on” skills.
As an engineer at Google, training and mentoring “Nooglers” (yes, that’s really what they call new Google employees!) were some of the things I enjoyed most. I went on to use my “20% time” to teach two Computer Science courses at the University of Washington.
Cracking the Coding Interview and CareerCup.com reflect my passion for teaching. Even now, you can often find me “hanging out” at CareerCup.com, helping users who stop by for assistance.
Join us.
Gayle Laakmann
7 Cracking the Coding Interview
Behind the Scenes
For many candidates, interviewing is a bit of a black box. You walk in, you get pounded with questions from a variety of interviewers, and then somehow or other you return with an offer... or not.
Have you ever wondered:
»»How do decisions get made?
»»Do your interviewers talk to each other?
»»What does the company really care about?
Well, wonder no more!
CareerCup sought out interviewing experts from five top companies - Microsoft, Google, Amazon, Yahoo and Apple - to show you what really happens “behind the scenes.” These experts will walk us through a typical interview day and describe what’s taking place outside of the interviewing room, and what happens after you leave.
Our interviewing experts also told us what’s different about their interview process. From bar raisers (Amazon) to Hiring Committees (Google), each company has its own quirks. Knowing these idiosyncrasies will help you to react better to a super-tough interviewer, or to avoid being intimidated when two interviewers show up at the door (Apple!).
In addition, our specialists offered insight as to what their company stresses in their interviews. While almost all software firms care about coding and algorithms, some companies focus more than others on specific aspects of the interview. Whether this is because of the company’s technology or its history, now you'll know what and how to prepare.
So, join us as we take you behind the scenes at Microsoft, Google, Amazon, Yahoo and Apple...
. CareerCup com 8
Behind the Scenes | The Microsoft Interview
Microsoft wants smart people. Geeks. People who are passionate about technology. You probably won’t be tested on the ins and outs of C++ APIs, but you will be expected to write code on the board.
In a typical interview, you'll show up at Microsoft at some time in the morning and fill out initial paper work. You'll have a short interview with a recruiter where he or she will give you a sample question. Your recruiter is usually there to prep you, and not to grill you on technical questions. Be nice to your recruiter. Your recruiter can be your biggest advocate, even pushing to re-interview you if you stumbled on your first interview. They can fight for you to be hired - or not!
During the day, you'll do four or five interviews, often with two different teams. Unlike many companies, where you meet your interviewers in a conference room, you'll meet with your Microsoft interviewers in their office. This is a great time to look around and get a feel for the team culture.
Depending on the team, interviewers may or may not share their feedback on you with the rest of the interview loop.
When you complete your interviews with a team, you might speak with a hiring manager. If so, that’s a great sign! It likely means that you passed the interviews with a particular team. It’s now down to the hiring manager’s decision.
You might get a decision that day, or it might be a week. After one week of no word from HR, send them a friendly email asking for a status update.
Definitely Prepare:
“Why do you want to work for Microsoft?”
In this question, Microsoft wants to see that you’re passionate about technology. A great answer might be, “I’ve been using Microsoft software as long as I can remember, and I'm really impressed at how Microsoft manages to create a product that is universally excellent. For example, I’ve been using Visual Studio recently to learn game programming, and it’s APIs are excellent.” Note how this shows a passion for technology!
What’s Unique:
You'll only reach the hiring manager if you’ve done well, but if you do, that’s a great sign!
9 Cracking the Coding Interview
Behind the Scenes | The Amazon Interview
Amazon’s recruiting process usually begins with one or two phone screens in which you interview with a specific team. The engineer who interviews you will usually ask you to write simple code and read it aloud on the phone. They will ask a broad set of questions to explore what areas of technology you’re familiar with.
Next, you fly to Seattle for four or five interviews with one or two teams which have selected you based on your resume and phone interviews. You will have to code on a whiteboard, and some interviewers will stress other skills. Interviewers are each assigned a specific area to probe and may seem very different from each other. They can not see other feedback until they have submitted their own and they are discouraged from discussing it until the hiring meeting.
Amazon’s “bar raiser” interviewer is charged with keeping the interview bar high. They attend special training and will interview candidates outside their group in order to balance out the group itself. If one interview seems significantly harder and different, that’s most likely the bar raiser. This person has both significant experience with interviews and veto power in the hiring decision. You will meet with your recruiter at the end of the day.
Once your interviewers have entered their feedback, they will meet to discuss it. They will be the people making the hiring decision.
While Amazon’s recruiters are excellent at following up with candidates, occasionally there are delays. If you haven’t heard from Amazon within a week, we recommend a friendly email.
Definitely Prepare:
Amazon is a web-based company, and that means they care about scale. Make sure you prepare for questions in “Large Scale.” You don’t need a background in distributed systems to answer these questions. See our recommendations in the System Design and Memory Limits Chapter.
Additionally, Amazon tends to ask a lot of questions about object oriented design. Check out the Object Oriented Design chapter for sample questions and suggestions.
What’s Unique:
The Bar Raiser, who is brought in from a different team to keep the bar high.
. CareerCup com 10
Behind the Scenes | The Google Interview
There are many scary stories floating around about Google interviews, but it’s mostly just that: stories. The interview is not terribly different from Microsoft’s or Amazon’s. However, because Google HR can be a little disorganized, we recommend being proactive in communication.
A Google engineer performs the first phone screen, so expect tough technical questions. On your on-site interview, you'll interview with four to six people, one of whom will be a lunch interviewer. Interviewer feedback is kept confidential from the other interviewers, so you can be assured that you enter each interview with blank slate. Your lunch interviewer doesn’t submit feedback, so this is a great opportunity to ask honest questions.
Written feedback is submitted to a hiring committee of engineers to make a hire/no-hire recommendation. Feedback is typically broken down into four categories (Analytical Ability, Coding, Experience and Communication) and you are given a score from 1.0 to 4.0 overall.
The hiring committee understands that you can’t be expected to excel in every interview, but if multiple people raise the same red flag (arrogance, poor coding skills, etc), that can disqualify you. A hiring committee typically wants to see one interviewer who is an “enthusiastic endorser.” In other words, a packet with scores of 3.6, 3.1, 3.1 and 2.6 is better than all 3.1s. Your phone screen is usually not a strong factor in the final decision.
The Google hiring process can be slow. If you don’t hear back within one week, politely ask your recruiter for an update. A lack of response says nothing about your performance.
Definitely Prepare:
As a web-based company, Google cares about how to design a scalable system. So, make sure you prepare for questions from “System Design and Memory Limits” Additionally, many Google interviewers will ask questions involving Bit Manipulation, so please brush up on these questions.
What’s Different:
Your interviewers do not make the hiring decision. Rather, they enter feedback which is passed to a hiring committee. The hiring committee recommends a decision which can be—though rarely is—rejected by Google executives.
11 Cracking the Coding Interview
Behind the Scenes | The Apple Interview
Much like the company itself, Apple’s interview process has minimal beaucracy. The interviewers will be looking for excellent technical skills, but a passion for the position and company is also very important. While it’s not a prerequisite to be a Mac user, you should at least be familiar with the system.
The interview process typically begins with a recruiter phone screen to get a basic sense of your skills, followed up by a series of technical phone screens with team members.
Once you’re invited on campus, you'll typically be greeted by the recruiter who provides an overview of the process. You will then have 6-8 interviews with members of the team for which you’re interviewing, as well as key people with whom your team works.
You can expect a mix of 1-on-1 and 2-on-1 interviews. Be ready to code on a whiteboard and make sure all of your thoughts are clearly communicated. Lunch is with your potential future manager and appears more casual, but is still an interview. Each interviewer is usually focused on a different area and is discouraged from sharing feedback unless there’s something they want subsequent interviewers to drill into.
Towards the end of the day, your interviewers will compare notes and if everyone still feels you’re a viable candidate, you'll interview with the director and then VP of the organization you’re applying to. While this decision is rather informal, it’s a very good sign if you make it. This decision also happens behind the scenes and if you don’t pass, you'll simply be escorted out of the building without ever having been the wiser (until now).
If you made it to the director and VP interviews, all of your interviewers will gather in a conference room to give an official thumbs up or thumbs down. The VP typically won’t be present, but can still veto the hire if they weren’t impressed. Your recruiter will usually follow up a few days later, but feel free to ping your recruiter for updates.
Definitely Prepare:
If you know what team you’re interviewing with, make sure you read up on that product. What do you like about it? What would you improve? Offering specific recommendations can show your passion for the job.
What’s Unique:
Apple does 2-on-1 interviews often, but don’t get stressed out about them - it’s the same as a 1-on-1 interview!
Also, Apple employees are huge Apple fans. You should show this same passion in your interview.
. CareerCup com 12
Behind the Scenes | The Yahoo Interview
Resume Selection & Screening: While Yahoo tends to only recruit at the top 10 – 20 schools, other candidates can still get interviewed through Yahoo’s job board (or – better yet – if they can get an internal referral). If you’re one of the lucky ones selected, your interview process will start off with a phone screen. Your phone screen will be with a senior employee (tech lead, manager, etc).
On-Site Interview: You will typically interview with 6 – 7 people on the same team for 45 minutes each. Each interviewer will have an area of focus. For example, one interviewer might focus on databases, while another interviewer might focus on your understanding of computer architecture. Interviews will often be composed as follows:
5 minutes: General conversation. Tell me about yourself, your projects, etc.
20 minutes: Coding question. For example, implement merge sort.
20 minutes: System design. For example, design a large distributed cache. These questions will often focus on an area from your past experience or on something your interviewer is currently working on.
Decision: At the end of the day, you will likely meet with a Program Manager or someone else for a general conversation (product demos, concerns about the company, your competing offers, etc). Meanwhile, your interviewers will discuss your performance and attempt to come to a decision. The hiring manager has the ultimate say and will weigh the positive feedback against the negative.
If you have done well, you will often get a decision that day, but this is not always the case. There can be many reasons that you might not be told for several days – for example, the team may feel it needs to interview several other people.
Definitely Prepare:
Yahoo, almost as a rule, asks questions about system design, so make sure you prepare for that. They want to know that you can not only write code, but that you can design software. Don’t worry if you don’t have a background in this - you can still reason your way through it!
What’s Unique:
Your phone interview will likely be performed by someone with more influence, such as a hiring manager.
Yahoo is also unusual in that it often gives a decision (if you’re hired) on the same day. Your interviewers will discuss your performance while you meet with a final interviewer.
13 Cracking the Coding Interview
Interview War Stories
The View from the Other Side of the Front, by Peter Bailey
For the eager candidate getting ready for a big job interview, Cracking the Coding Interview is an invaluable reference, containing excellent coaching and practice material that gives you an inside edge on the interview process. However, as you go over your old data structures textbook and drill yourself with homemade discrete math flash cards, don’t make the mistake of thinking of the interview as a kind of high-pressure game show – that if you just give all the right answers to the tech questions, you too can win a shiny new career (this week, on Who Wants to be a Software Engineer?)
While the technical questions on computer science obviously are very important, the most important interview question is not covered in this guidebook. In fact, it’s often the single most important question in your interviewers' minds as they grill you in that little room. Despite the questions on polymorphism and heaps and virtual machines, the question they really want an answer to is ...
Would I have a beer with this guy?
Don’t look at me like that, I'm serious! Well, I may be embellishing a little, but hear me out. The point I'm trying to make is that interviewers, especially those that you might work with, are probably just as anxious as you are. Nonsense, you say, as a nervous young professional, checking your pants for lint while you bite your fingernails, waiting for the interview team to show up in the front lobby. After all, this is the big leagues, and these guys are just waiting for you to slip up so they can rip you apart, laugh at your shriveled corpse, and grind your career dreams to dust beneath the heels of their boots.
Right? Just like pledge week, back in freshman year? Right? Hmmm?
Nothing could be further from the truth. The team of developers and managers interviewing you have their own tasks and projects waiting for them, back at their own desks. Believe me, they’re hoping that every interview is going to be the last one. They'd rather be doing anything else. There might be a batch of upcoming projects looming on their calendar, and they need more manpower if they’re going to even have a prayer of making their deadline. But the last guy the agency sent over was a complete flake who railed about Microsoft’s evil for half an hour. And the one before that couldn’t code his way out of a wet paper bag without using copy-and-paste. Sheesh, they think, where is HR getting these guys? How hard can it be to hire one lousy person?
While they may not literally be asking themselves “Would I have a beer with this guy (or gal)”, they are looking to see how well you would fit in with the team, and how you would affect team chemistry. If they hire you, you’re all going to be spending a lot of time together for
. CareerCup com 14
Interview War Stories
the next few months or years, and they want to know that they can rely on you – and maybe even come to consider you a friend and colleague. They want to know that they can depend on you. And as tempting as it might be to them to just settle and hire the next person who comes along, they know better.
In many companies, particularly large U.S. companies, it’s harder to fire somebody than it is to hire somebody. (Welcome to the US: Land of Lawsuits!) If they hire a dud, they’re stuck with them. That person might be unproductive or, even worse, a drain on the team’s productivity. So they keep interviewing, until they find the right person. They know that it’s better to reject a good candidate than hire a bad one.
Some of those interviews are real doozies. Once you’ve interviewed long enough, you build up a repertoire of horror stories. War stories, of candidates who looked promising on paper until the interviews went terribly, terribly wrong. These war stories are not only humorous – they’re also instructive.
Names have been changed to protect the innocent – or downright ridiculous.
2.1 zyxwvutsrqponmlkjihgfedcba
2.2 ZYXWVUTSRQPONMLKJIHGFEDCBA
2.3 spw~~kjlslen
2.4 0987654321+_=-)(*&^%$#@!`~[]{};':”,./<>?
2.5 ABCDEZYXW
2.6 abcdeyxw
2.7 asdabcdezyxwasdf
2.8 ~~
15 Cracking the Coding Interview
Interview War Stories | Pop Divas
Pop Divas Need Not Apply
Leonard was a very promising C++ coder, three years out of college, with a solid work history and an impressive skill set. He proved on the phone screen that he was above-average technically, and so he was invited in for an interview. We needed a savvy C++ person to work on a piece of middleware that interfaced with our database, and Leonard seemed like a sure fit.
However, once we started talking to him, things went south in a hurry. He spent most of the interview criticizing every tool and platform that we questioned him on. We used SQL Server as our database? Puhleease. We were planning to switch to Oracle soon, right? What’s that? Our team used Tool A to do all our coding in? Unacceptable. He used Tool B, and only Tool B, and after he was hired, we'd all have to switch to Tool B. And we'd have to switch to Java, because he really wanted to work with Java, despite the fact that 75 percent of the codebase would have to be rewritten. We'd thank him later. And oh, by the way, he wouldn’t be making any meetings before ten o'clock.
Needless to say, we encouraged Leonard to seek opportunities elsewhere. It wasn’t that his ideas were bad – in fact, he was “technically” right about many things, and his (strong) opinions were all backed with solid fact and sound reason (except for the ten o'clock thing – we think he may have just been making a “power play”.) But it was obvious that, if hired, Leonard wasn’t going to play well with others – he would have been toxic kryptonite for team chemistry. He actually managed to offend two of the team members during the forty-five minutes of his interview. Leonard also made the mistake of assuming that Code Purity and Algorithm Beauty were always more important than a business deadline.
In the real world, there are always compromises to be made, and knowing how to work with the business analysts is just as important as knowing how to refactor a blob of code. If Leonard would not have gotten along with other IT people, he definitely wouldn’t have gotten along with the business folks. Maybe you can get away with hiring a Leonard if he’s one of the best ten coders in the world (he wasn’t). But he was the classic failure example for the “Would you have a beer with this guy?” test.
. CareerCup com 16
Interview War Stories | Failure to Communicate
What We Have Here is Failure to Communicate
Trisha was a mid-level Java developer with a solid history of middleware and JSP work on her resume. Since she was local, we invited her in for an interview without a phone screen. When we started asking her questions, it quickly became obvious that Trisha was a woman of few words. Her answers were short and often composed of “yes/no” responses, even to questions that were meant to start a dialog. Once she did start opening up, I still wasn’t sure she was actually talking. I saw her lips moving, and heard mumbling sounds coming out, but it wasn’t anything that sounded like English.
I'm not sure if Trisha was nervous or just shy, but either way, I had to ask her numerous times to repeat herself. Now I was the one getting nervous! I didn’t want to be the guy who “ruined” the interview, so I pulled back on my questions. The other folks in the room and I exchanged uneasy glances. We felt like we were on a Seinfeld episode. It was almost impossible to understand Trisha, and when she did speak up, her halting, uncertain, confused speech patterns made us feel more like code breakers than interviewers. I am not exaggerating to say that I did not understand a single answer she gave during the interview.
Knowing, alone, isn’t good enough. You’re going to be talking with other technical people, and you’re going to be talking to customers, and sales reps, and Betty from Marketing. You will write something eventually, whether it’s documentation, or a project plan, or a requirements document. The word processor might correct your spelling, but it won’t correct your lousy writing. The ability to communicate thoughts and ideas, in a clear, concise manner, is an absolutely invaluable skill that employers seek.
The same goes for verbal communication. I used to work with a co-worker who doubled the length of every meeting he was in, because he could not answer a question in less than ten minutes. “Hey, Dennis, what time is it?” “Well, that’s kind of interesting, because I just happened to be reading an article on cesium clocks and leap seconds and the history of the Gregorian Calendar and ... ”
I'll spare you the rest.
17 Cracking the Coding Interview
Interview War Stories | You Can (Maybe) Count On Me
You Can Count on Me, Just Not Until Early Afternoon
Ahhh, 1999. The crest of the dot-com bubble, and the tightest labor market in history. Our company was racing to expand its development team, and we would have hired a German Shepherd if it knew HTML. Instead, we wound up hiring Ian. We should’ve hired the dog.
Ian was a cheerful, friendly guy who had a gift of natural charisma. He got along fantastically with all of the interviewers, and seemed very intelligent. Skill-wise, he was adequate. He hadn’t written a single line of computer code outside of his college courses, and didn’t even have his own e-mail address. When we gave Ian the chance to ask us questions at the end of the interview, he asked about flexible work hours, and how soon he could take vacation time. Instead of showing an interest in the career opportunities, or in company’s growth prospects, he asked whether he could take the all-you-could-drink break room soda home with him. The questions grew more bizarre from there.
Ian was very interested in our Legal Assistance benefit. He wanted to know if it covered the cost of filing lawsuits, if it covered him if he got sued himself, if it applied to any lawsuits he currently was involved in, and if he could “theoretically” use it to sue the company itself. He also asked us if he could use it to help him “fix” some unpaid speeding tickets.
In any other year, that should have been it for Ian right there. But, in 1999, we were hiring anybody who was even remotely competent. Ian collected paychecks from us for eighteen months, and he was about as productive as a traffic cone. He usually sauntered into the office around ten-thirty with some sort of lame excuse (by my count, he had to wait for the cable guy sixteen times in a six-month period). He usually killed the morning by answering e-mail and playing ping-pong, before breaking for a two-hour lunch. After lunch, it was more ping-pong, and maybe an hour of writing bad code, before bolting the office sometime around three. He was the dictionary definition of unreliable.
Remember, your potential future team members need to know that they can rely on you. And they need to know that you won’t need constant supervision and hand-holding. They need to know that you’re able to figure things out on your own. One of the most important messages that you, as a candidate, can convey in your interview is hiring me will make your lives easier. In fact, this is a large part of the reason for the famously difficult interview questions at places like Amazon and Google; if you can handle that kind of unpredictable pressure in an interview, then you stand a good chance of being useful to them on real projects.
To cite a more subtle example, once I was on a four person team that was desperately trying to recruit new members to help work on an old pile of software. It was a real mess; we'd inherited a nasty ball of spaghetti, and we needed people who could jump in, figure things out, and be part of the solution.
There was one very smart fellow, Terry, who would have been a great asset for our team – but we didn’t hire him, despite his excellent technical and personal skills. It was because he
. CareerCup com 18
Interview War Stories | You Can (Maybe) Count On Me
insisted on meticulous written instructions for every step of the coding process. He wasn’t going to make a suggestion or take any initiative – or blow his nose, for that matter – without a mile-long audit trail and a dozen signatures. While he insisted that he worked that way for reasons of quality (a defensible point), we got the impression that it had more to do with butt-covering, and we simply didn’t have the time for that kind of bureaucracy. Terry would have been an excellent fit in a government or aerospace IT department, something that required ISO 9000 procedures. But he would have never fit into our team; he would have been a burden, not an asset.
19 Cracking the Coding Interview
Interview War Stories | Spider Senses
My Spider Senses are Tingling
I can think of lots of interviews that just fell into the general category of weird and uncomfortable:
»»The Java coder who apparently considered hygiene optional, and had the interview room smelling like week-old blue cheese within ten minutes (my eyes were watering).
»»The young fresh-out-of-college graduate with a tongue piercing that kept tick-tick-ticking against his teeth as he talked (after half an hour, it was like Chinese water torture).
»»The girl who wore an iPod through her interview, with the volume turned loud enough that she actually had to ask the interviewers to repeat themselves a few times.
»»The poor, hyper-nervous fellow who was sweating like a marathon runner for half an hour.
»»The girl who wore a T-shirt with an obscene political slogan to her interview.
»»The guy who asked (seriously) at the end of his interview, “So, are there any hot chicks in our department?”
Those are the interviews where we politely thank the people for their time, shake their hand (except for the sweaty guy), then turn to each other after the door closes and ask – did that really just happen?
Nobody is saying that you have to be a bland, boring robot in a Brooks Brothers suit and tie. Remember, the interview team wants you to be “the one”, but they’re also very worried about the possibility that you’re going to be more of a distraction than an asset. Don’t talk or behave in a way that will set off their early warning radar. Whether or not somebody bothers to behave professionally during an interview is often a very good indicator of what kind of teammate they’re going to be.
Rudimentary social skills are part of the answer to “Would I have a beer with this guy?”, or at least, “Will I mind working next to this guy for six months?” From the interviewer’s point of view, they’re picking a neighbor that they’re going to live and work with 200 hours per month for foreseeable future. Would you really want a neighbor that smelled like a hog rendering plant?
Before the Interview
21 Cracking the Coding Interview
Before the Interview | Resume Advice
What Resume Screeners Look For
Resume screeners look for the same things that interviewers do:
»»Are you smart?
»»Can you code?
That means that you should present your resume to show those two things. Your love of tennis, traveling, or magic cards won’t do much to show that, so it’s likely just wasting space.
Keep in mind that recruiters only spend a fixed amount of time (about 20 seconds) looking at your resume. If you limit the content to the best, most impressive, most relevant items, they’ll jump out at the recruiter. Weak items only dilute your resume and distract the recruiter from what you’d like them to see.
Employment History
Relevant Jobs: Your resume does not - and should not - include a full history of every role you’ve ever had. Your job serving ice cream, for example, will not show that you’re smart or that you can code. Include only the relevant things.
Writing Strong Bullets: For each role, try to discuss your accomplishments with the following approach: “Accomplished X by implementing Y which led to Z.” Here’s an example:
»»“Reduced object rendering time by 75% by applying Floyd’s algorithm, leading to a 10% reduction in system boot time.”
Here’s another example with an alternate wording:
»»“Increased average match accuracy from 1.2 to 1.5 by implementing a new comparison algorithm based on windiff.”
Not everything you did will fit into this approach, but the principle is the
Got some extra time to prepare?
If you have at least a couple months before an interview (or if you’re in school and not graduating yet), you may be able to improve your resume.
Go out and get project experience! Take course that have major projects. Get involved in open source. Ask a professor if there is any research you can get involved in, or ask if he/she can sponsor you on an independent study.
This will put you in a better position to have your resume selected down the road. It will also give you lots of things to talk about in an interview.
. CareerCup com 22
Before the Interview | Resume Advice
same: show what you did, how you did it, and what the results were. Ideally, you should try to make the results “measurable” somehow.
Projects
Almost every candidate has some projects, even if they’re just academic projects. List them on your resume! I recommend putting a section called “Projects” on your resume and list your 2 - 4 most significant projects. State what the project was, which languages or technologies it employed, and whether it was an individual or a team project. If your project was not for a course, that’s even better! It shows passion, initiative, and work ethic. You can state the type of project by listing course projects as “Course Project” and your independent projects as “Independent Projects” (or some other wording).
Programming Languages and Software
Software: Generally speaking, I do not recommend listing that you’re familiar with Microsoft Office. Everyone is, and it just dilutes the “real” information. Familiarity with developer-specific or highly technical software (e.g., Visual Studio, Eclipse, Linux) can be useful, but it often doesn’t make much of a difference.
Languages: Knowing which languages to list on your resume is always a tricky thing. Do you list everything you’ve ever worked with? Or only the ones that you’re more comfortable with (even though that might only be one or two languages)? I recommend the following compromise: list most languages you’ve used, but add your experience level. This approach is shown below:
»»“Languages: Java (expert), C++ (proficient), JavaScript (prior experience), C (prior experience)”
Advice for Non-Native English Speakers and Internationals
Proofreading: Some companies will throw out your resume just because of a typo. Please get at least one native English speaker to proofread your resume.
Personal Information: For US positions, do not include age, marital status, or nationality. This sort of personal information is not appreciated by companies, as it creates a legal liability for them. However, you may want to include your current work authorization / visa status, particularly when applying to smaller companies who may be unable to sponsor candidates.
23 Cracking the Coding Interview
Before the Interview | Behavioral Preparation
Why Are Behavioral Questions Asked?
Behavioral questions are asked for a variety of reasons. They can be asked either to get to know your personality, to more deeply understand your resume, or just to ease you into an interview. Either way, these questions are important and can be prepared for.
How To Prepare
Behavioral questions are usually of the form “tell me about a time when you ... ”, and may ask for an example from a specific project or position. I recommend filling in the following “preparation grid” as shown below:
Project 1
Project 2
Project 3
Project 4
Most Challenging
What You Learned
Most Interesting
Hardest Bug
Enjoyed Most
Conflicts with Teammates
Along the top, as columns, you should list all the major aspects of your resume – e.g., your projects, jobs, or activities. Along the side, as rows, you should list the common questions – e.g., what you enjoyed most, what you enjoyed least, what you considered most challenging, what you learned, what the hardest bug was, etc. In each cell, put the corresponding story.
We recommend reducing each story to just a couple keywords that you can write in each cell. This will make the grid easier to study.
In your interview, when you’re asked about a project, you’ll be able to come up with an appropriate story effortlessly. Study this grid before your interview.
NOTE: If you’re doing a phone interview, you may want to have this grid out in front of you.
Some additional advice:
1. When asked about your weaknesses, give a real weakness! Answers like “My greatest weakness is that I work too hard / am a perfectionist / etc” tell your interviewer that you’re arrogant and/or won’t admit to your faults. No one wants to work with someone like that. A better answer conveys a real, legitimate weakness but emphasizes how you work to overcome it. For example: “Sometimes, I don’t have a very good attention to detail. While that’s good because it lets me execute quickly, it also means that I sometimes make careless mistakes. Because of that, I make sure to always have someone else double check my work.”
. CareerCup com 24
Before the Interview | Behavioral Preparation
2. When asked what the most challenging part was, don’t say “I had to learn a lot of new languages and technologies.” This is the “cop out” answer (e.g., you don’t know what else to say). It tells the interviewer that nothing was really that hard.
3. Remember: you’re not just answering their questions, you’re telling them about yourself! Many people try to just answer the questions. Think more deeply about what each story communicates about you.
4. If you think you’ll be asked behavioral questions (e.g., “tell me about a challenging interaction with a team member”), you should create a Behavioral Preparation Grid. This is the same as the one above, but the left side contains things like “challenging interaction”, “failure”, “success”, and “influencing people.”
What questions should you ask the interviewer?
Most interviewers will give you a chance to ask them questions. The quality of your questions will be a factor, whether subconsciously or consciously, in their decisions.
Some questions may come to you during the interview, but you can - and should - prepare questions in advance. Doing research on the company or team may help you with preparing questions.
Questions can be divided into three different categories:
Genuine Questions: These are the questions you actually want to know. Here are a few ideas of questions that are valuable to many candidates:
1. “How much of your day do you spend coding?”
2. “How many meetings do you have every week?”
3. “What is the ratio of testers to developers to product managers? What is the interaction like? How does project planning happen on the team?”
Insightful Questions: These questions are designed to demonstrate your deep knowledge of programming or technologies.
1. “I noticed that you use technology X. How do you handle problem Y?”
2. “Why did the product choose to use the X protocol over the Y protocol? I know it has benefits like A, B, C, but many companies choose not to use it because of issue D.”
Passion Questions: These questions are designed to demonstrate your passion for technology.
1. “I’m very interested in scalability. Did you come in with a background in this, or what opportunities are there to learn about it?”
2. “I’m not familiar with technology X, but it sounds like a very interesting solution. Could you tell me a bit more about how it works?”
25 Cracking the Coding Interview
Before the Interview | Technical Preparation
How to Prepare for Technical Questions
You’ve purchased this book, so you’ve already gone a long way towards good preparation. Nice work!
That said, there are better and worse ways to prepare. Many candidates just read through problems and solutions. Don’t do that! Memorizing or trying to learn specific questions won’t help you! Rather, do this:
1. Try to solve the problem on your own. I mean, really try to solve it. Many questions are designed to be tough - that’s ok! When you’re solving a problem, make sure to think about the space and time efficiency. Ask yourself if you could improve the time efficiency by reducing the space efficiency, or vice versa.
2. Write the code for the algorithm on paper. You’ve been coding all your life on a computer, and you’ve gotten used to the many nice things about it. But, in your interview, you won’t have the luxury of syntax highlighting, code completion, or compiling. Mimic this situation by coding on paper.
3. Type your paper code as-is into a computer. You’ll probably have made a bunch of mistakes. Start a list of all the mistakes you made, so that you can keep these in mind in the real interview.
4. Do a mock interview. CareerCup offers a mock interview service, or you can grab a friend to ask you questions. Though your friend may not be an expert interviewer, he or she may still be able to walk you through a coding or algorithm question.
. CareerCup com 26
Before the Interview | Technical Preparation
What You Need To Know
Most interviewers won’t ask about specific algorithms for binary tree balancing or other complex algorithms. Frankly, they probably don’t remember these algorithms either.
You’re usually only expected to know the basics. Here’s a list of the absolute must-have knowledge:
Data Structures
Algorithms
Concepts
Linked Lists
Breadth First Search
Bit Manipulation
Binary Trees
Depth First Search
Singleton Design Pattern
Tries
Binary Search
Factory Design Pattern
Stacks
Merge Sort
Memory (Stack vs Heap)
Queues
Quick Sort
Recursion
Vectors / ArrayLists
Tree Insert / Find / etc
Big-O Time
Hash Tables
This is not, of course, an all-inclusive list. Questions may be asked on areas outside of these topics. This is merely a “must know” list.
For each of the topics, make sure you understand how to implement / use them, and (where applicable) the space and time complexity.
Practicing implementing the data structures and algorithms. You might be asked to implement them in your interview, or you might be asked to implement a modification of them. Either way, the more comfortable you are with implementations the better.
Do you need to know details of C++, Java, etc?
While I personally never liked asking these sorts of questions (e.g., “what is a vtable?”), many interviewers regretfully do ask them. For big companies like Microsoft, Google, Amazon, etc, I wouldn’t stress too much about these questions. Look up the most common questions and make sure you have answers to them, but I would focus on data structures and algorithms preparation.
At smaller companies, or non-software companies, these questions can be more important. Look up your company on CareerCup.com to decide for yourself. If your company isn’t listed, look up a similar company as a reference.

The Interview and Beyond
29 Cracking the Coding Interview
At the Interview | Handling Behavioral Questions
Why Behavioral Questions
As stated earlier, interviews usually start and end with “chit chat” or “soft skills.” This is a time to answer questions about your resume or general questions, and also an opportunity for you to ask questions. This part of the interview is targeted not only at getting to know you, but also at relaxing you.
Be Specific, Not Arrogant
Arrogance is a red flag, but you still want to make yourself sound impressive. So how do you make yourself sound good without being arrogant? By being specific!
Specificity means giving just the facts and letting the interviewer derive an interpretation. Consider an example:
»»Candidate #1: “I basically did all the hard work for the team.”
»»Candidate #2: “I implemented the file system, which was considered one of the most challenging components because …”
Candidate #2 not only sounds more impressive, but she also appears less arrogant.
Limit Details
When a candidate blabbers on about a problem, it’s hard for an interviewer who isn’t well versed in the subject or project to understand it. CareerCup recommends that you stay light on details and just state the key points. That is, consider something like this: “By examining the most common user behavior and applying the Rabin-Karp algorithm, I designed a new algorithm to reduce search from O(n) to O(log n) in 90% of cases. I can go into more details if you’d like.” This demonstrates the key points while letting your interviewer ask for more details if he wants to.
Ask Good Questions
Remember those questions you came up with while preparing? Now is a great time to use them!
Structure Answers Using S.A.R.
Structure your responses using S.A.R.: Situation, Action, Response. That is, you should start off outlining the situation, then explaining the actions you took, and lastly, describing the result.
Example: “Tell me about a challenging interaction with a teammate.”
» »Situation: On my operating systems project, I was assigned to work with three other
. CareerCup com 30
At the Interview | Handling Behavioral Questions
people. While two were great, the third team member didn’t contribute much. He stayed quiet during meetings, rarely chipped in during email discussions, and struggled to complete his components.
» »Action: One day after class, I pulled him aside to speak about the course and then moved the discussion into talking about the project. I asked him open-ended questions on how he felt it was going, and which components he was excited about tackling. He suggested all the easiest components, and yet offered to do the write-up. I realized then that he wasn’t lazy – he was actually just really confused about the project and lacked confidence. I worked with him after that to break down the components into smaller pieces, and I made sure to complement him a lot on his work to boost his confidence.
» »Result: He was still the weakest member of the team, but he got a lot better. He was able to finish all his work on time, and he contributing more in discussions. We were happy to work with him on a future project.
As you can see, the SAR model helps an interviewer clearly see what you did in a certain situation and what the result was.
31 Cracking the Coding Interview
At the Interview | Handling Technical Questions
General Advice for Technical Questions
Interviews are supposed to be difficult. If you don’t get every – or any – answer immediately, that’s ok! In fact, in my experience, maybe only 10 people out of the 120+ that I’ve interviewed have gotten the question right instantly.
So when you get a hard question, don’t panic. Just start talking aloud about how you would solve it.
And, one more thing: you’re not done until the interviewer says that you’re done! What I mean here is that when you come up with an algorithm, start thinking about the problems accompanying it. When you write code, start trying to find bugs. If you’re anything like the other 110 candidates that I’ve interviewed, you probably made some mistakes.
Five Steps to a Technical Questions
A technical interview question can be solved utilizing a five step approach:
1. Ask your interviewer questions to resolve ambiguity.
2. Design an Algorithm
3. Write pseudo-code first, but make sure to tell your interviewer that you’re writing pseudo-code! Otherwise, he/she may think that you’re never planning to write “real” code, and many interviewers will hold that against you.
4. Write your code, not too slow and not too fast.
5. Test your code and carefully fix any mistakes.
Step 1: Ask Questions
Technical problems are more ambiguous than they might appear, so make sure to ask questions to resolve anything that might be unclear or ambiguous. You may eventually wind up with a very different – or much easier – problem than you had initially thought. In fact, many interviewers (especially at Microsoft) will specifically test to see if you ask good questions.
Good questions might be things like: What are the data types? How much data is there? What assumptions do you need to solve the problem? Who is the user?
Example: “Design an algorithm to sort a list.”
»»Question: What sort of list? An array? A linked list?
»»Answer: An array.
»»Question: What does the array hold? Numbers? Characters? Strings?
»»Answer: Numbers.
. CareerCup com 32
At the Interview | Handling Technical Questions
»»Question: And are the numbers integers?
»»Answer: Yes.
»»Question: Where did the numbers come from? Are they IDs? Values of something?
»»Answer: They are the ages of customers.
»»Question: And how many customers are there?
»»Answer: About a million.
We now have a pretty different problem: sort an array containing a million integers between 0 and 130. How do we solve this? Just create an array with 130 elements and count the number of ages at each value.
Step 2: Design an Algorithm
Designing an algorithm can be tough, but our five approaches to algorithms can help you out (see pg 34). While you’re designing your algorithm, don’t forget to think about:
»»What are the space and time complexities?
»»What happens if there is a lot of data?
»»Does your design cause other issues? (i.e., if you’re creating a modified version of a binary search tree, did your design impact the time for insert / find / delete?)
»»If there are other issues, did you make the right trade-offs?
»»If they gave you specific data (e.g., mentioned that the data is ages, or in sorted order), have you leveraged that information? There’s probably a reason that you’re given it.
Step 3: Pseudo-Code
Writing pseudo-code first can help you outline your thoughts clearly and reduce the number of mistakes you commit. But, make sure to tell your interviewer that you’re writing pseudo-code first and that you’ll follow it up with “real” code. Many candidates will write pseudo-code in order to ‘escape’ writing real code, and you certainly don’t want to be confused with those candidates.
Step 4: Code
You don’t need to rush through your code; in fact, this will most likely hurt you. Just go at a nice, slow methodical pace. Also, remember this advice:
»»Use Data Structures Generously: Where relevant, use a good data structure or define your own. For example, if you’re asked a problem involving finding the minimum age for a group of people, consider defining a data structure to represent a Person. This
33 Cracking the Coding Interview
At the Interview | Handling Technical Questions
shows your interviewer that you care about good object oriented design.
»»Don’t Crowd Your Coding: This is a minor thing, but it can really help. When you’re writing code on a whiteboard, start in the upper left hand corner – not in the middle. This will give you plenty of space to write your answer.
Step 5: Test
Yes, you need to test your code! Consider testing for:
»»Extreme cases: 0, negative, null, maximums, etc
»»User error: What happens if the user passes in null or a negative value?
»»General cases: Test the normal case.
If the algorithm is complicated or highly numerical (bit shifting, arithmetic, etc), consider testing while you’re writing the code rather than just at the end.
Also, when you find mistakes (which you will), carefully think through why the bug is occuring. One of the worst things I saw while interviewing was candidates who recognized a mistake and tried making “random” changes to fix the error.
For example, imagine a candidate writes a function that returns a number. When he tests his code with the number ‘5’ he notices that it returns 0 when it should be returning 1. So, he changes the last line from “return ans” to “return ans+1,” without thinking through why this would resolve the issue. Not only does this look bad, but it also sends the candidate on an endless string of bugs and bug fixes.
When you notice problems in your code, really think deeply about why your code failed before fixing the mistake.
. CareerCup com 34
At the Interview | Five Algorithm Approaches
Five Algorithm Approaches
There’s no sure fire approach to solving a tricky algorithm problem, but the approaches below can be useful. Keep in mind that the more problems you practice, the easier it will to identify which approach to use.
Also, remember that the five approaches can be “mixed and matched.” That is, once you’ve applied “Simplify & Generalize”, you may want to implement Pattern Matching next.
APPROACH I: EXAMPLIFY
Description: Write out specific examples of the problem, and see if you can figure out a general rule.
Example: Given a time, calculate the angle between the hour and minute hands.
Approach: Start with an example like 3:27. We can draw a picture of a clock by selecting where the 3 hour hand is and where the 27 minute hand is.
By playing around with these examples, we can develop a rule:
»»Minute angle (from 12 o’clock): 360 * minutes / 60
»»Hour angle (from 12 o’clock): 360 * (hour % 12) / 12 + 360 * (minutes / 60) * (1 / 12)
»»Angle between hour and minute: (hour angle - minute angle) % 360
By simple arithmetic, this reduces to 30 * hours - 5.5 * minutes.
APPROACH II: PATTERN MATCHING
Description: Consider what problems the algorithm is similar to, and figure out if you can modify the solution to develop an algorithm for this problem.
Example: A sorted array has been rotated so that the elements might appear in the order 3 4 5 6 7 1 2. How would you find the minimum element?
Similar Problems:
»»Find the minimum element in an array.
»»Find a particular element in an array (eg, binary search).
Algorithm:
Finding the minimum element in an array isn’t a particularly interesting algorithm (you could just iterate through all the elements), nor does it use the information provided (that the array is sorted). It’s unlikely to be useful here.
However, binary search is very applicable. You know that the array is sorted, but rotated. So, it must proceed in an increasing order, then reset and increase again. The minimum element is the “reset” point.
35 Cracking the Coding Interview
At the Interview | Five Algorithm Approaches
If you compare the first and middle element (3 and 6), you know that the range is still increasing. This means that the reset point must be after the 6 (or, 3 is the minimum element and the array was never rotated). We can continue to apply the lessons from binary search to pinpoint this reset point, by looking for ranges where LEFT > RIGHT. That is, for a particular point, if LEFT < RIGHT, then the range does not contain the reset. If LEFT > RIGHT, then it does.
APPROACH III: SIMPLIFY & GENERALIZE
Description: Change a constraint (data type, size, etc) to simplify the problem. Then try to solve it. Once you have an algorithm for the “simplified” problem, generalize the problem again.
Example: A ransom note can be formed by cutting words out of a magazine to form a new sentence. How would you figure out if a ransom note (string) can be formed from a given magazine (string)?
Simplification: Instead of solving the problem with words, solve it with characters. That is, imagine we are cutting characters out of a magazine to form a ransom note.
Algorithm:
We can solve the simplified ransom note problem with characters by simply creating an array and counting the characters. Each spot in the array corresponds to one letter. First, we count the number of times each character in the ransom note appears, and then we go through the magazine to see if we have all of those characters.
When we generalize the algorithm, we do a very similar thing. This time, rather than creating an array with character counts, we create a hash table. Each word maps to the number of times the word appears.
APPROACH IV: BASE CASE AND BUILD
Description: Solve the algorithm first for a base case (e.g., just one element). Then, try to solve it for elements one and two, assuming that you have the answer for element one. Then, try to solve it for elements one, two and three, assuming that you have the answer to elements one and two.
Example: Design an algorithm to print all permutations of a string. For simplicity, assume all characters are unique.
Test String: abcdefg
Case “a” --> {a}
Case “ab” --> {ab, ba}
Case “abc” --> ?
This is the first “interesting” case. If we had the answer to P(“ab”), how could we generate P(“abc”). Well, the additional letter is “c”, so we can just stick c in at every possible point. That
. CareerCup com 36
At the Interview | Five Algorithm Approaches
is:
merge(c, ab) --> cab, acb, abc
merge(c, ba) --> cba, bca, bac
Algorithm: Use a recursive algorithm. Generate all permutations of a string by “chopping off” the last character and generating all permutations of s[1… n-1]. Then, insert s[n] into every location of the string.
NOTE: Base Case and Build Algorithms often lead to natural recursive algorithms.
APPROACH V: DATA STRUCTURE BRAINSTORM
Description: This is hacky, but it often works. Simply run through a list of data structures and try to apply each one.
Example: Numbers are randomly generated and stored into an (expanding) array. How would you keep track of the median?
Data Structure Brainstorm:
»»Linked list? Probably not – linked lists tend not to do very well with accessing and sorting numbers.
»»Array? Maybe, but you already have an array. Could you somehow keep the elements sorted? That’s probably expensive. Let’s hold off on this and return to it if it’s needed.
»»Binary tree? This is possible, since binary trees do fairly well with ordering. In fact, if the binary search tree is perfectly balanced, the top might be the median. But, be careful – if there’s an even number of elements, the median is actually the average of the middle two elements. The middle two elements can’t both be at the top. This is probably a workable algorithm, but let’s come back to it.
»»Heap? A heap is really good at basic ordering and keeping track of max and mins. This is actually interesting – if you had two heaps, you could keep track of the biggest half and the smallest half of the elements. The biggest half is kept in a min heap, such that the smallest element in the biggest half is at the root. The smallest half is kept in a max heap, such that the biggest element of the smallest half is at the root. Now, with these data structures, you have the potential median elements at the roots. If the heaps are no longer the same size, you can quickly “rebalance” the heaps by popping an element off the one heap and pushing it onto the other.
Note that the more problems you do, the better instinct you will develop about which data structure to apply.
37 Cracking the Coding Interview
At the Interview | The Offer and Beyond
Congrats! You got the offer!
If you’re lucky enough to get an offer (and you will be!), congratulations! You may now be stressing over which offer to accept and all that fun stuff, so just remember that most likely, all of your options are great and you’ll be happy at any of them.
As far as which offer to take, well, we could tell you that money isn’t that important and blah blah blah… but we’ll skip over that and let you make your own decision about the importance of money. We have some other advice for you.
Negotiating
It’s Always Negotiable! Ok, maybe not always, but usually an offer is negotiable even if a recruiter tells you otherwise. It helps if you have a competing offer. But, don’t lie – Microsoft knows what Google offers, so it just won’t be realistic if you make up numbers. Also, technology is a small world, and people talk. Be honest.
What’s the money like, really?
Think about the full offer package. Many companies will have impressive salaries, but small annual bonuses. Other companies will have huge annual bonuses, but lower salaries. Make sure you look at the full package (salary, signing bonus, health care benefits, raises, annual bonus, relocation, stock, promotions, etc). It’s very confusing, and it’s often not clear which company is offering more.
What about your career options?
Even if money is all that matters, think about the long term career. If you’re lucky enough to have several offers to pick from, consider how each one will impact your long term career. The company with the lowest salary but where you’ll learn the most may just be the best move, even financially.
I can’t give you some magical formula to compute which offer to accept, but here’s what I’d recommend thinking about (in no particular order):
»»Career Path: Make a plan for your career. What do you want to do 5, 10 and 15 years out? What skills will you need to develop? Which company or position will help you get there?
»»Promotion Opportunity: Do you prefer to move into management, or would you prefer to become an increasingly senior developer?
»»Money and Benefits: Of course, the money matters (but if you’re early in your career, it probably doesn’t matter much). As mentioned above, make sure you look at the full package.
. CareerCup com 38
At the Interview | The Offer and Beyond
»»Happiness: Did you like the people? The products? The location? It’s hard to tell, of course, before you work there. What are the options to change teams if you’re unhappy?
»»Brand Name: The company’s brand name can mean a lot for your future career. Some company names will open doors, while others will not as much.
What about company stability? Personally, I think it matters much less than most people think. There are so many software companies out there. If you get laid off and need to find a new job, will it be difficult to find a new one? Only you can answer that.
On the job, and beyond...
Before starting at a company, make a career plan. What would you like your career to look like? What will it take to get there? Make sure you check in on your career plan regularly and are on track.
It’s very easy, particularly at the big companies, to get sucked into staying for a while. They’re great companies with lots of perks, and most people are truly quite happy there. If what you want is to stay an engineer for life, then there is absolutely nothing wrong with that.
However, if you want to run a company one day, or move up into management, you should stop and check your career plan. Is another year at your job going to help you get there? Or is it time to move? You, and only you, can decide.
39 Cracking the Coding Interview
At the Interview | Top Ten Mistakes Candidates Make
#1 | Practicing on a Computer
If you were training for a serious bike race in the mountains, would you practice only by biking on the streets? I hope not. The air is different. The terrain is different. Yeah, I bet you’d practice in the mountains.
Using a compiler to practice interview questions is like this - and you’ve basically been biking on the streets your entire life. Put away the compiler and get out the old pen and paper. Use a compiler only to verify your solutions.
#2 | Not Rehearsing Behavioral Questions
Many candidates spend all their time prepping for technical questions and overlook the behavioral questions. Guess what? Your interviewer is judging those too! And, not only that - your performance on behavioral questions might bias your interviewer’s perception of your technical performance. Behavioral prep is relatively easy and well-worth your time. Looking over your projects and positions and think of the key stories. Rehearse the stories. See pg 29 for more details.
#3 | Not Doing a Mock Interview
Imagine you’re preparing for a big speech. Your whole school, or company, or whatever will be there. Your future depends on this. And all you do to prepare is read the speech to yourself. Silently. In your head. Crazy, right?
Not doing a mock interview to prepare for your real interview is just like this. If you’re an engineer, you must know other engineers. Grab a buddy and ask him/her to do a mock interview for you. You can even return the favor!
#4 | Trying to Memorize Solutions
Quality beats quantity. Try to struggle through and solve questions yourself; don’t flip directly to the solutions when you get stuck. Memorizing how to solve specific problem isn’t going to help you much in an interview. Real preparation is about learning how to approach new problems.
#5 | Talking Too Much
I can’t tell you how many times I’ve asked candidates a simple question like “what was the hardest bug on Project Pod?”, only to have them ramble on and on about things I don’t understand. Five minutes later, when they finally come up for air, I’ve learned nothing - except that they’re a poor communicator. When asked a question, break your answer into three parts (Situation / Action / Response, Issue 1 / Issue 2 / Issue 3, etc) and speak for just a couple sentences about each. If I want more details, I’ll ask!
. CareerCup com 40
At the Interview | Top Ten Mistakes Candidates Make
#6 | Talking Too Little
Psst - let me tell you a secret: I don’t know what’s going on in your head. So if you aren’t talking, I don’t know what you’re thinking. If you don’t talk for a long time, I’ll assume that you aren’t making any progress. Speak up often, and try to talk your way through a solution. This shows your interviewer that you’re tackling the problem and aren’t stuck. And it lets them guide you when you get off-track, helping you get to the answer faster. And it shows your awesome communication skills. What’s not to love?
#7 | Rushing
Coding is not a race, and neither is interviewing. Take your time in a coding problem - don’t rush! Rushing leads to mistakes, and reveals you to be careless. Go slowly and methodically, testing often and thinking through the problem thoroughly. You’ll finish the problem in less time in the end, and with fewer mistakes.
#8 | Not Debugging
Would you ever write code and not run it or test it? I would hope not! So why do that in an interview? When you finish writing code in an interview, “run” (or walk through) the code to test it. Or, on more complicated problems, test the code while writing it.
#9 | Sloppy Coding
Did you know that you can write bug-free code while still doing horribly on a coding question? It’s true! Duplicated code, messy data structures (i.e., lack of object oriented design), etc. Bad, bad, bad! When you write code, imagine you’re writing for real-world maintainability. Break code into sub-routines, and design data structures to link appropriate data.
#10 | Giving Up
Have you ever taken a computer adaptive test? These are tests that give you harder questions the better you do. Take it from me - they’re not fun. Regardless of how well you’re actually doing, you suddenly find yourself stumbling through problems. Yikes!
Interviewing is sort of like this. If you whiz through the easy problems, you’re going to get more and harder problems. Or, the questions might have just started out hard to begin with! Either way, struggling on a question does not mean that you’re doing badly. So don’t give up or get discouraged. You’re doing great!
41 Cracking the Coding Interview
At the Interview | Frequently Asked Questions
Do I have to get every question right?
No. A good interviewer will stretch your mind. They’ll want to see you struggle with a difficult problem. If a candidate is good, they’ll ask harder and tougher questions until he/she is stumped! Thus, if you have trouble on a question, all it means is that the interviewer is doing their job!
Should I tell my interviewer if I know a question?
Yes! You should definitely tell your interviewer if you’ve previously heard the question. This seems silly to some people - if you already know the question (and answer), you could ace the question, right? Not quite.
Here’s why we strongly recommend that you tell your interviewer that you’ve heard the question:
1. Big honesty points. This shows a lot of integrity. That’s huge. Remember that the interviewer is evaluating you as a potential teammate. I don’t know about you, but I personally prefer to work with honest people!
2. The question might have changed ever-so-slightly. You don’t want to risk repeating the wrong answer.
3. If you easily belt out the right answer, it’s obvious to the interviewer. They know how hard a problem is supposed to be. It’s very hard to “pretend” to struggle through a question, because you just can’t approach it the same way other candidates do.
How should I dress?
Generally, candidates should dress one small step above the average employee in their position, or as nice as the nicest dressed employees in their position. In most software firms, this means that jeans (nice jeans with no holes) or slacks with a nice shirt or sweater is fine. In a bank or another more formal institution, avoid jeans and stick with slacks.
What language should I use?
Many people will tell you “whatever language you’re most comfortable with,” but ideally you want to use a language that your interviewer is comfortable with. I’d usually recommend coding in either C, C++ or Java, as the vast majority of interviewers will be comfortable in one of these languages. My personal preference for interviews is Java (unless it’s a question requiring C / C++), because it’s quick to write and almost everyone can read and understand Java, even if they code mostly in C++. (Almost all the solutions in this book are written in Java for this reason.)
I didn’t hear back immediately after my interview. Am I rejected?
. CareerCup com 42
At the Interview | Frequently Asked Questions
Absolutely not. Responses can be held up for a variety of reasons that have nothing to do with a good or bad performance. For example, an interviewer could have gone on vacation right after your interview. A company will always tell you if you’re rejected (or at least I’ve never heard of a company which didn’t).
Can I re-apply to a company after getting rejected?
Almost always, but you typically have to wait a bit (6 months – 1 year). Your first bad interview usually won’t affect you too much when you re-interview. Lots of people got rejected from Google or Microsoft and later got an offer.
How are interview questions selected?
This depends on the company, but any number of ways:
1. Pre-Assigned List of Questions: This is unusual at bigger companies.
2. Assigned Topics: Each interviewer is assigned a specific area to probe, but decides on his/her own questions.
3. Interviewer’s Choice: Each interviewer asks whatever he / she wants. Usually, under this system, the interviewers have a way of tracking which questions were asked to a candidate to ensure a good diversity of questions.
Approach #3 is the most common. This system usually means that interviewers will each have a “stock” set of five or so questions that they ask candidates.
What about experienced candidates?
This depends a lot on the company. On average though, experienced candidates will slightly get more questions about their background, and they might face higher standards when discussing system architecture (if this is relevant to their experience). For the most part though, experienced candidates face much the same process.
Yes, for better or worse, experienced candidate should expect to go through the same coding and algorithm questions. With respect to their performance, they could face either higher standards (because they have more experience) or lower standards (because it’s likely been many years since they worked with certain data structures).
43 Cracking the Coding Interview
Interview Questions
How This Book is Organized?
We have grouped interview questions into categories, with a page preceding each category offering advice and other information. Note that many questions may fall into multiple categories.
Within each category, the questions are sorted by approximate level of difficulty. Solutions for all questions are at the back.
Special Advice for Software Design Engineers in Test (SDETs)
Not only must SDETs master testing, but they also have to be great coders. Thus, we recommend the follow preparation process:
» »Prepare the Core Testing Problems: For example, how would you test a light bulb? A pen? A cash register? Microsoft Word? The Testing Chapter will give you more background on these problems.
» »Practice the Coding Questions: The #1 thing that SDETs get rejected for is coding skills. Make sure that you prepare for all the same coding and algorithm questions that a regular developer would get.
» »Practice Testing the Coding Questions: A very popular format for SDET question is “Write code to do X,” followed up by “OK, now test it.” So, even when the question doesn’t specifically ask for this, you should ask yourself, “how would you test this?” Remember: any problem can be an SDET problem!
Full, Compilable Solutions
For your convenience, you can download the full solutions to the problems at http://www.careercup.com/careercup_book_solutions. This file provides executable code for all the Java solutions. The solutions can be opened and run with Eclipse.
Suggestions and Corrections
While we do our best to ensure that all the solutions are correct, mistakes will be made. Moreover, sometimes there is no “right” answer. If you'd like to offer a suggestion or correction, please submit it at http://xrl.us/ccbook.
Interview Questions

Part 1
Data Structures
Chapter 1 | Arrays and Strings
Cracking the Coding Interview | Data Structures
47
Hash Tables
While not all problems can be solved with hash tables, a shocking number of interview problems can be. Before your interview, make sure to practice both using and implementing hash tables.
1 public HashMap<Integer, Student> buildMap(Student[] students) {
2 HashMap<Integer, Student> map = new HashMap<Integer, Student>();
3 for (Student s : students) map.put(s.getId(), s);
4 return map;
5 }
ArrayList (Dynamically Resizing Array):
An ArrayList, or a dynamically resizing array, is an array that resizes itself as needed while still providing O(1) access. A typical implementation is that when a vector is full, the array doubles in size. Each doubling takes O(n) time, but happens so rarely that its amortized time is still O(1).
1 public ArrayList<String> merge(String[] words, String[] more) {
2 ArrayList<String> sentence = new ArrayList<String>();
3 for (String w : words) sentence.add(w);
4 for (String w : more) sentence.add(w);
5 return sentence;
6 }
StringBuffer / StringBuilder
Question: What is the running time of this code?
1 public String makeSentence(String[] words) {
2 StringBuffer sentence = new StringBuffer();
3 for (String w : words) sentence.append(w);
4 return sentence.toString();
5 }
Answer: O(n^2), where n is the number of letters in sentence. Here’s why: each time you append a string to sentence, you create a copy of sentence and run through all the letters in sentence to copy them over. If you have to iterate through up to n characters each time in the loop, and you’re looping at least n times, that gives you an O(n^2) run time. Ouch!
With StringBuffer (or StringBuilder) can help you avoid this problem.
1 public String makeSentence(String[] words) {
2 StringBuffer sentence = new StringBuffer();
3 for (String w : words) sentence.append(w);
4 return sentence.toString();
5 }
Chapter 1 | Arrays and Strings
48
CareerCup.com
1.1 Implement an algorithm to determine if a string has all unique characters. What if you can not use additional data structures?
_
_________________________________________________________________pg 95
1.2 Write code to reverse a C-Style String. (C-String means that “abcd” is represented as five characters, including the null character.)
_
_________________________________________________________________pg 96
1.3 Design an algorithm and write code to remove the duplicate characters in a string without using any additional buffer. NOTE: One or two additional variables are fine. An extra copy of the array is not.
FOLLOW UP
Write the test cases for this method.
_
_________________________________________________________________pg 97
1.4 Write a method to decide if two strings are anagrams or not.
_
_________________________________________________________________pg 99
1.5 Write a method to replace all spaces in a string with ‘%20’.
_
________________________________________________________________pg 100
1.6 Given an image represented by an NxN matrix, where each pixel in the image is 4 bytes, write a method to rotate the image by 90 degrees. Can you do this in place?
_
________________________________________________________________pg 101
1.7 Write an algorithm such that if an element in an MxN matrix is 0, its entire row and column is set to 0.
_
________________________________________________________________pg 102
1.8 Assume you have a method isSubstring which checks if one word is a substring of another. Given two strings, s1 and s2, write code to check if s2 is a rotation of s1 using only one call to isSubstring (i.e., “waterbottle” is a rotation of “erbottlewat”).
_
________________________________________________________________pg 103
Chapter 2 | Linked Lists
Cracking the Coding Interview | Data Structures
49
How to Approach:
Linked list questions are extremely common. These can range from simple (delete a node in a linked list) to much more challenging. Either way, we advise you to be extremely comfortable with the easiest questions. Being able to easily manipulate a linked list in the simplest ways will make the tougher linked list questions much less tricky. With that said, we present some “must know” code about linked list manipulation. You should be able to easily write this code yourself prior to your interview.
Creating a Linked List:
NOTE: When you’re discussing a linked list in an interview, make sure to understand whether it is a single linked list or a doubly linked list.
1 class Node {
2 Node next = null;
3 int data;
4 public Node(int d) { data = d; }
5 void appendToTail(int d) {
6 Node end = new Node(d);
7 Node n = this;
8 while (n.next != null) { n = n.next; }
9 n.next = end;
10 }
11 }
Deleting a Node from a Singly Linked List
1 Node deleteNode(Node head, int d) {
2 Node n = head;
3 if (n.data == d) {
4 return head.next; /* moved head */
5 }
6 while (n.next != null) {
7 if (n.next.data == d) {
8 n.next = n.next.next;
9 return head; /* head didn’t change */
10 }
11 n = n.next;
12 }
13 }
Chapter 2 | Linked Lists
50
CareerCup.com
2.1 Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
_
________________________________________________________________pg 105
2.2 Implement an algorithm to find the nth to last element of a singly linked list.
_
________________________________________________________________pg 106
2.3 Implement an algorithm to delete a node in the middle of a single linked list, given only access to that node.
EXAMPLE
Input: the node ‘c’ from the linked list a->b->c->d->e
Result: nothing is returned, but the new linked list looks like a->b->d->e
_
________________________________________________________________pg 107
2.4 You have two numbers represented by a linked list, where each node contains a single digit. The digits are stored in reverse order, such that the 1’s digit is at the head of the list. Write a function that adds the two numbers and returns the sum as a linked list.
EXAMPLE
Input: (3 -> 1 -> 5) + (5 -> 9 -> 2)
Output: 8 -> 0 -> 8
_
________________________________________________________________pg 108
2.5 Given a circular linked list, implement an algorithm which returns node at the beginning of the loop.
DEFINITION
Circular linked list: A (corrupt) linked list in which a node’s next pointer points to an earlier node, so as to make a loop in the linked list.
EXAMPLE
input: A -> B -> C -> D -> E -> C [the same C as earlier]
output: C
_
________________________________________________________________pg 109
Chapter 3 | Stacks and Queues
Cracking the Coding Interview | Data Structures
51
How to Approach:
Whether you are asked to implement a simple stack / queue, or you are asked to implement a modified version of one, you will have a big leg up on other candidates if you can flawlessly work with stacks and queues. Practice makes perfect! Here is some skeleton code for a Stack and Queue class.
Implementing a Stack
1 class Stack {
2 Node top;
3 Node pop() {
4 if (top != null) {
5 Object item = top.data;
6 top = top.next;
7 return item;
8 }
9 return null;
10 }
11 void push(Object item) {
12 Node t = new Node(item);
13 t.next = top;
14 top = t;
15 }
16 }
Implementing a Queue
1 class Queue {
2 Node first, last;
3 void enqueue(Object item) {
4 if (!first) {
5 back = new Node(item);
6 first = back;
7 } else {
8 back.next = new Node(item);
9 back = back.next;
10 }
11 }
12 Node dequeue(Node n) {
13 if (front != null) {
14 Object item = front.data;
15 front = front.next;
16 return item;
17 }
18 return null;
19 }
20 }
Chapter 3 | Stacks and Queues
52
CareerCup.com
3.1 Describe how you could use a single array to implement three stacks.
_
________________________________________________________________pg 111
3.2 How would you design a stack which, in addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time.
_
________________________________________________________________pg 113
3.3 Imagine a (literal) stack of plates. If the stack gets too high, it might topple. Therefore, in real life, we would likely start a new stack when the previous stack exceeds some threshold. Implement a data structure SetOfStacks that mimics this. SetOfStacks should be composed of several stacks, and should create a new stack once the previous one exceeds capacity. SetOfStacks.push() and SetOfStacks.pop() should behave identically to a single stack (that is, pop() should return the same values as it would if there were just a single stack).
FOLLOW UP
Implement a function popAt(int index) which performs a pop operation on a specific sub-stack.
_
________________________________________________________________pg 115
3.4 In the classic problem of the Towers of Hanoi, you have 3 rods and N disks of different sizes which can slide onto any tower. The puzzle starts with disks sorted in ascending order of size from top to bottom (e.g., each disk sits on top of an even larger one). You have the following constraints:
(A) Only one disk can be moved at a time.
(B) A disk is slid off the top of one rod onto the next rod.
(C) A disk can only be placed on top of a larger disk.
Write a program to move the disks from the first rod to the last using Stacks.
_
________________________________________________________________pg 118
3.5 Implement a MyQueue class which implements a queue using two stacks.
_
________________________________________________________________pg 120
3.6 Write a program to sort a stack in ascending order. You should not make any assumptions about how the stack is implemented. The following are the only functions that should be used to write this program: push | pop | peek | isEmpty.
_
________________________________________________________________pg 121
Chapter 4 | Trees and Graphs
Cracking the Coding Interview | Data Structures
53
How to Approach:
Trees and graphs questions typically come in one of two forms:
1. Implement a tree / find a node / delete a node / other well known algorithm.
2. Implement a modification of a known algorithm.
Either way, it is strongly recommended to understand the important tree algorithms prior to your interview. If you’re fluent in these, it’ll make the tougher questions that much easier! We’ll list some of the most important.
WARNING: Not all binary trees are binary search trees
When given a binary tree question, many candidates assume that the interviewer means “binary search tree”, when the interviewer might only mean “binary tree.” So, listen carefully for that word “search.” If you don’t hear it, the interviewer may just mean a binary tree with no particular ordering on the nodes. If you aren’t sure, ask.
Binary Trees—”Must Know” Algorithms
You should be able to easily implement the following algorithms prior to your interview:
»»In-Order: Traverse left node, current node, then right [usually used for binary search trees]
»»Pre-Order: Traverse current node, then left node, then right node.
»»Post-Order: Traverse left node, then right node, then current node.
»»Insert Node: On a binary search tree, we insert a value v, by comparing it to the root. If v > root, we go right, and else we go left. We do this until we hit an empty spot in the tree.
Note: balancing and deletion of binary search trees are rarely asked, but you might want to have some idea how they work. It can set you apart from other candidates.
Graph Traversal—”Must Know” Algorithms
You should be able to easily implement the following algorithms prior to your interview:
»»Depth First Search: DFS involves searching a node and all its children before proceeding to its siblings.
»»Breadth First Search: BFS involves searching a node and its siblings before going on to any children.
Chapter 4 | Trees and Graphs
54
CareerCup.com
4.1 Implement a function to check if a tree is balanced. For the purposes of this question, a balanced tree is defined to be a tree such that no two leaf nodes differ in distance from the root by more than one.
_
________________________________________________________________pg 123
4.2 Given a directed graph, design an algorithm to find out whether there is a route between two nodes.
_
________________________________________________________________pg 124
4.3 Given a sorted (increasing order) array, write an algorithm to create a binary tree with minimal height.
_
________________________________________________________________pg 125
4.4 Given a binary search tree, design an algorithm which creates a linked list of all the nodes at each depth (i.e., if you have a tree with depth D, you’ll have D linked lists).
_
________________________________________________________________pg 126
4.5 Write an algorithm to find the ‘next’ node (i.e., in-order successor) of a given node in a binary search tree where each node has a link to its parent.
_
________________________________________________________________pg 127
4.6 Design an algorithm and write code to find the first common ancestor of two nodes in a binary tree. Avoid storing additional nodes in a data structure. NOTE: This is not necessarily a binary search tree.
_
________________________________________________________________pg 128
4.7 You have two very large binary trees: T1, with millions of nodes, and T2, with hundreds of nodes. Create an algorithm to decide if T2 is a subtree of T1.
_
________________________________________________________________pg 130
4.8 You are given a binary tree in which each node contains a value. Design an algorithm to print all paths which sum up to that value. Note that it can be any path in the tree - it does not have to start at the root.
_
________________________________________________________________pg 131

Part 2
Concepts and Algorithms
Cracking the Coding Interview | Concepts and Algorithms
57
Chapter 5 | Bit Manipulation
How to Approach:
Bit manipulation can be a scary thing to many candidates, but it doesn’t need to be! If you’re shaky on bit manipulation, we recommend doing a couple of arithmetic-like problems to boost your skills. Compute the following by hand:
1010 - 0001
1010 + 0110
1100^1010
1010 << 1
1001^1001
1001 & 1100
1010 >> 1
0xFF - 1
0xAB + 0x11
If you’re still uncomfortable, examine very carefully what happens when you do subtraction, addition, etc in base 10. Can you repeat that work in base 2?
NOTE: The Windows Calculator knows how to do lots of operations in binary, including ADD, SUBTRACT, AND and OR. Go to View > Programmer to get into binary mode while you practice.
Things to Watch Out For:
»»It’s really easy to make mistakes on these problems, so be careful! When you’re writing code, stop and think about what you’re writing every couple of lines - or, better yet, test your code mid-way through! When you’re done, check through your entire code.
»»If you’re bit shifting, what happens when the digits get shifted off the end? Make sure to think about this case to ensure that you’re handling it correctly.
And (&):
0 & 0 = 0
1 & 0 = 0
0 & 1 = 0
1 & 1 = 1
Or (|):
0 | 0 = 0
1 | 0 = 1
0 | 1 = 1
1 | 1 = 1
Xor (^):
0 ^ 0 = 0
1 ^ 0 = 1
0 ^ 1 = 1
1 ^ 1 = 0
Left Shift:
x << y means x shifted y bits to the left. If you start shifting and you run out of space, the bits just “drop off”. For example:
00011001 << 2 = 01100100
00011001 << 4 = 10010000
Right Shift:
x >> y means x shifted y bits to the right. If you start shifting and you run out of space, the bits just “drop off” the end. Example:
00011001 >> 2 = 00000110
00011001 >> 4 = 00000001
Chapter 5 | Bit Manipulation
58
CareerCup.com
5.1 You are given two 32-bit numbers, N and M, and two bit positions, i and j. Write a method to set all bits between i and j in N equal to M (e.g., M becomes a substring of N located at i and starting at j).
EXAMPLE:
Input: N = 10000000000, M = 10101, i = 2, j = 6
Output: N = 10001010100
_
________________________________________________________________pg 133
5.2 Given a (decimal - e.g. 3.72) number that is passed in as a string, print the binary representation. If the number can not be represented accurately in binary, print “ERROR”
_
________________________________________________________________pg 134
5.3 Given an integer, print the next smallest and next largest number that have the same number of 1 bits in their binary representation.
_
________________________________________________________________pg 135
5.4 Explain what the following code does: ((n & (n-1)) == 0).
_
________________________________________________________________pg 138
5.5 Write a function to determine the number of bits required to convert integer A to integer B.
Input: 31, 14
Output: 2
_
________________________________________________________________pg 139
5.6 Write a program to swap odd and even bits in an integer with as few instructions as possible (e.g., bit 0 and bit 1 are swapped, bit 2 and bit 3 are swapped, etc).
_
________________________________________________________________pg 140
5.7 An array A[1... n] contains all the integers from 0 to n except for one number which is missing. In this problem, we cannot access an entire integer in A with a single operation. The elements of A are represented in binary, and the only operation we can use to access them is “fetch the jth bit of A[i]”, which takes constant time. Write code to find the missing integer. Can you do it in O(n) time?
_
________________________________________________________________pg 141
Chapter 6 | Brain Teasers
Cracking the Coding Interview | Concepts and Algorithms
59
Do companies really ask brain teasers?
While many companies, including Google and Microsoft, have policies banning brain teasers, interviewers still sometimes ask these tricky questions. This is especially true since people have different definitions of brain teasers.
Advice on Approaching Brain Teasers
Don’t panic when you get a brain teaser. Interviewers want to see how you tackle a problem; they don’t expect you to immediately know the answer. Start talking, and show the interviewer how you approach a problem.
In many cases, you will also find that the brain teasers have some connection back to fundamental laws or theories of computer science.
If you’re stuck, we recommend simplifying the problem. Solve it for a small number of items or a special case, and then see if you can generalize it.
Example
You are trying to cook an egg for exactly fifteen minutes, but instead of a timer, you are given two ropes which burn for exactly 1 hour each. The ropes, however, are of uneven densities - i.e., half the rope length-wise might take only two minutes to burn.
The Approach
1. What is important? Numbers usually have a meaning behind them. The fifteen minutes and two ropes were picked for a reason.
2. Simplify! You can easily time one hour (burn just one rope).
3. Now, can you time 30 minutes? That’s half the time it takes to burn one rope. Can you burn the rope twice as fast? Yes! (Light the rope at both ends.)
4. You’ve now learned: (1) You can time 30 minutes. (2) You can burn a rope that takes X minutes in just X/2 minutes by lighting both ends.
5. Work backwards: if you had a rope of burn-length 30 minutes, that would let you time 15 minutes. Can you remove 30 minutes of burn-time from a rope?
6. You can remove 30 minutes of burn-time from Rope #2 by lighting Rope #1 at both ends and Rope #2 at one end.
7. Now that you have Rope #2 at burn-length 30 minutes, start cooking the egg and light Rope #2 at the other end. When Rope #2 burns up, your egg is done!
Chapter 6 | Brain Teasers
60
CareerCup.com
6.1 Add arithmetic operators (plus, minus, times, divide) to make the following expression true: 3 1 3 6 = 8. You can use any parentheses you’d like.
_
________________________________________________________________pg 143
6.2 There is an 8x8 chess board in which two diagonally opposite corners have been cut off. You are given 31 dominos, and a single domino can cover exactly two squares. Can you use the 31 dominos to cover the entire board? Prove your answer (by providing an example, or showing why it’s impossible).
_
________________________________________________________________pg 144
6.3 You have a five quart jug and a three quart jug, and an unlimited supply of water (but no measuring cups). How would you come up with exactly four quarts of water?
NOTE: The jugs are oddly shaped, such that filling up exactly ‘half’ of the jug would be impossible.
_
________________________________________________________________pg 145
6.4 A bunch of men are on an island. A genie comes down and gathers everyone together and places a magical hat on some people’s heads (i.e., at least one person has a hat). The hat is magical: it can be seen by other people, but not by the wearer of the hat himself. To remove the hat, those (and only those who have a hat) must dunk themselves underwater at exactly midnight. If there are n people and c hats, how long does it take the men to remove the hats? The men cannot tell each other (in any way) that they have a hat.
FOLLOW UP
Prove that your solution is correct.
_
________________________________________________________________pg 146
6.5 There is a building of 100 floors. If an egg drops from the Nth floor or above it will break. If it’s dropped from any floor below, it will not break. You’re given 2 eggs. Find N, while minimizing the number of drops for the worst case.
_
________________________________________________________________pg 148
6.6 There are one hundred closed lockers in a hallway. A man begins by opening all one hundred lockers. Next, he closes every second locker. Then he goes to every third locker and closes it if it is open or opens it if it is closed (e.g., he toggles every third locker). After his one hundredth pass in the hallway, in which he toggles only locker number one hundred, how many lockers are open?
_
________________________________________________________________pg 149
Chapter 7 | Object Oriented Design
Cracking the Coding Interview | Concepts and Algorithms
61
How to Approach
Object oriented design questions are very important, as they demonstrate the quality of a candidate’s code. A poor performance on this type of question raises serious red flags.
Handling Ambiguity in an Interview
OOD questions are often intentionally vague to test if you’ll make assumptions, or if you’ll ask clarifying questions. How do you design a class if the constraints are vague? Ask questions to eliminate ambiguity, then design the classes to handle any remaining ambiguity.
Object Oriented Design for Software
Imagine we’re designing the objects for a deck of cards. Consider the following approach:
1. What are you trying to do with the deck of cards? Ask your interviewer. Let’s assume we want a general purpose deck of cards to implement many different types of card games.
2. What are the core objects—and what “sub types” are there? For example, the core items might be: Card, Deck, Number, Suit, PointValue
3. Have you missed anything? Think about how you’ll use that deck of cards to implement different types of games, changing the class design as necessary.
4. Now, get a little deeper: how will the methods work? If you have a method like Card Deck:.getCard(Suit s, Number n), think about how it will retrieve the card.
Object Oriented Design for Real World Object
Real world objects are handled very similarly to software object oriented design. Suppose you are designing an object oriented design for a parking lot:
1. What are your goals? For example: figure out if a parking spot is taken, figure out how many cars of each type are in the parking lot, look up handicapped spots, etc.
2. Now, think about the core objects (Car, ParkingSpot, ParkingLot, ParkingMeter, etc—Car has different subclasses, and ParkingSpot is also subclassed for handicapped spot).
3. Have we missed anything? How will we represent parking restrictions based on time or payment? Perhaps, we’ll add a class called Permission which handles different payment systems. Permission will be sub-classed into classes PaidPermission (fee to park) and FreeParking (open parking). ParkingLot will have a method called GetPermission which will return the current Permission object based on the time.
4. How will we know whether or not a car is in a spot? Think about how to represent the data so that the methods are most efficient.
Chapter 7 | Object Oriented Design
62
CareerCup.com
7.1 Design the data structures for a generic deck of cards. Explain how you would subclass it to implement particular card games.
_
________________________________________________________________pg 151
7.2 Imagine you have a call center with three levels of employees: fresher, technical lead (TL), product manager (PM). There can be multiple employees, but only one TL or PM. An incoming telephone call must be allocated to a fresher who is free. If a fresher can’t handle the call, he or she must escalate the call to technical lead. If the TL is not free or not able to handle it, then the call should be escalated to PM. Design the classes and data structures for this problem. Implement a method getCallHandler().
_
________________________________________________________________pg 152
7.3 Design a musical juke box using object oriented principles.
_
________________________________________________________________pg 154
7.4 Design a chess game using object oriented principles.
_
________________________________________________________________pg 156
7.5 Design the data structures for an online book reader system.
_
________________________________________________________________pg 157
7.6 Implement a jigsaw puzzle. Design the data structures and explain an algorithm to solve the puzzle.
_
________________________________________________________________pg 159
7.7 Explain how you would design a chat server. In particular, provide details about the various backend components, classes, and methods. What would be the hardest problems to solve?
_
________________________________________________________________pg 161
7.8 Othello is played as follows: Each Othello piece is white on one side and black on the other. When a piece is surrounded by its opponents on both the left and right sides, or both the top and bottom, it is said to be captured and its color is flipped. On your turn, you must capture at least one of your opponent’s pieces. The game ends when either user has no more valid moves, and the win is assigned to the person with the most pieces. Implement the object oriented design for Othello.
_
________________________________________________________________pg 163
7.9 Explain the data structures and algorithms that you would use to design an in-memory file system. Illustrate with an example in code where possible.
_
________________________________________________________________pg 166
7.10 Describe the data structures and algorithms that you would use to implement a garbage collector in C++.
_
________________________________________________________________pg 167
Chapter 8 | Recursion
Cracking the Coding Interview | Concepts and Algorithms
63
How to Recognize
While there is a wide variety of recursive problems, many recursive problems follow similar patterns. A good hint that problem is recursive is that it appears to be built off sub-problems.
When you hear a problem beginning with the following, it’s often (though not always) a good candidate for recursion: “Design an algorithm to compute the nth ... ”; “Write code to list the first n... ”; “Implement a method to compute all... ”; etc.
Again, practice makes perfect! The more problems you do, the easier it will be to recognize recursive problems.
How to Approach
Recursive solutions, by definition, are built off solutions to sub-problems. Many times, this will mean simply to compute f(n) by adding something, removing something, or otherwise changing the solution for f(n-1). In other cases, you might have to do something more complicated. Regardless, we recommend the following approach:
1. Think about what the sub-problem is. How many sub-problems does f(n) depend on? That is, in a recursive binary tree problem, each part will likely depend on two problems. In a linked list problem, it’ll probably be just one.
2. Solve for a “base case.” That is, if you need to compute f(n), first compute it for f(0) or f(1). This is usually just a hard-coded value.
3. Solve for f(2).
4. Understand how to solve for f(3) using f(2) (or previous solutions). That is, understand the exact process of translating the solutions for sub-problems into the real solution.
5. Generalize for f(n).
This “bottom-up recursion” is often the most straight-forward. Sometimes, though, it can be useful to approach problems “top down”, where you essentially jump directly into breaking f(n) into its sub-problems.
Things to Watch Out For
1. All problems that can be solved recursively can also be solved iteratively (though the code may be much more complicated). Before diving into a recursive code, ask yourself how hard it would be to implement this algorithm iteratively. Discuss the trade-offs with your interviewer.
2. Recursive algorithms can be very space inefficient. Each recursive call adds a new layer to the stack, which means that if your algorithm has O(n) recursive calls then it uses O(n) memory. Ouch! This is one reason why an iterative algorithm may be better.
Chapter 8 | Recursion
64
CareerCup.com
8.1 Write a method to generate the nth Fibonacci number.
_
________________________________________________________________pg 169
8.2 Imagine a robot sitting on the upper left hand corner of an NxN grid. The robot can only move in two directions: right and down. How many possible paths are there for the robot?
FOLLOW UP
Imagine certain squares are “off limits”, such that the robot can not step on them. Design an algorithm to get all possible paths for the robot.
_
________________________________________________________________pg 170
8.3 Write a method that returns all subsets of a set.
_
________________________________________________________________pg 171
8.4 Write a method to compute all permutations of a string.
_
________________________________________________________________pg 173
8.5 Implement an algorithm to print all valid (e.g., properly opened and closed) combinations of n-pairs of parentheses.
EXAMPLE:
input: 3 (e.g., 3 pairs of parentheses)
output: ()()(), ()(()), (())(), ((()))
_
________________________________________________________________pg 174
8.6 Implement the “paint fill” function that one might see on many image editing programs. That is, given a screen (represented by a 2 dimensional array of Colors), a point, and a new color, fill in the surrounding area until you hit a border of that color.’
_
________________________________________________________________pg 175
8.7 Given an infinite number of quarters (25 cents), dimes (10 cents), nickels (5 cents) and pennies (1 cent), write code to calculate the number of ways of representing n cents.
_
________________________________________________________________pg 176
8.8 Write an algorithm to print all ways of arranging eight queens on a chess board so that none of them share the same row, column or diagonal.
_
________________________________________________________________pg 177
Chapter 9 | Sorting and Searching
Cracking the Coding Interview | Concepts and Algorithms
65
How to Approach:
Understanding the common sorting algorithms is incredibly valuable, as many sorting or searching solutions require tweaks of known sorting algorithms. A good approach when you are given a question like this is to run through the different sorting algorithms and see if one applies particularly well.
Example: You have a very large array of ‘Person’ objects. Sort the people in increasing order of age.
We’re given two interesting bits of knowledge here: (1) It’s a large array, so efficiency is very important. (2) We are sorting based on ages, so we know the values are in a small range. By scanning through the various sorting algorithms, we might notice that bucket sort would be a perfect candidate for this algorithm. In fact, we can make the buckets small (just 1 year each) and get O(n) running time.
Bubble Sort:
Start at the beginning of an array and swap the first two elements if the first is bigger than the second. Go to the next pair, etc, continuously making sweeps of the array until sorted. O(n^2).
Selection Sort:
Find the smallest element using a linear scan and move it to the front. Then, find the second smallest and move it, again doing a linear scan. Continue doing this until all the elements are in place. O(n^2).
Merge Sort:
Sort each pair of elements. Then, sort every four elements by merging every two pairs. Then, sort every 8 elements, etc. O(n log n) expected and worst case.
Quick Sort:
Pick a random element and partition the array, such that all numbers that are less than it come before all elements that are greater than it. Then do that for each half, then each quarter, etc. O(n log n) expected, O(n^2) worst case.
Bucket Sort:
Partition the array into a finite number of buckets, and then sort each bucket individually. This gives a time of O(n + m), where n is the number of items and m is the number of distinct items.
Chapter 9 | Sorting and Searching
66
CareerCup.com
9.1 You are given two sorted arrays, A and B, and A has a large enough buffer at the end to hold B. Write a method to merge B into A in sorted order.
_
________________________________________________________________pg 179
9.2 Write a method to sort an array of strings so that all the anagrams are next to each other.
_
________________________________________________________________pg 180
9.3 Given a sorted array of n integers that has been rotated an unknown number of times, give an O(log n) algorithm that finds an element in the array. You may assume that the array was originally sorted in increasing order.
EXAMPLE:
Input: find 5 in array (15 16 19 20 25 1 3 4 5 7 10 14)
Output: 8 (the index of 5 in the array)
_
________________________________________________________________pg 181
9.4 If you have a 2 GB file with one string per line, which sorting algorithm would you use to sort the file and why?
_
________________________________________________________________pg 182
9.5 Given a sorted array of strings which is interspersed with empty strings, write a method to find the location of a given string.
Example: find “ball” in [“at”, “”, “”, “”, “ball”, “”, “”, “car”, “”, “”, “dad”, “”, “”] will return 4
Example: find “ballcar” in [“at”, “”, “”, “”, “”, “ball”, “car”, “”, “”, “dad”, “”, “”] will return -1
_
________________________________________________________________pg 183
9.6 Given a matrix in which each row and each column is sorted, write a method to find an element in it.
_
________________________________________________________________pg 184
9.7 A circus is designing a tower routine consisting of people standing atop one another’s shoulders. For practical and aesthetic reasons, each person must be both shorter and lighter than the person below him or her. Given the heights and weights of each person in the circus, write a method to compute the largest possible number of people in such a tower.
EXAMPLE:
Input (ht, wt): (65, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68, 110)
Output: The longest tower is length 6 and includes from top to bottom: (56, 90) (60,95) (65,100) (68,110) (70,150) (75,190)
_
________________________________________________________________pg 185
Chapter 10 | Mathematical
Cracking the Coding Interview | Concepts and Algorithms
67
How to Approach:
Many of these problems read as brain teasers at first, but can be worked through in a logical way. Just remember to rely on the rules of mathematics to develop an approach, and then to carefully translate that idea into code.
Example: Given two numbers m and n, write a method to return the first number r that is divisible by both (e.g., the least common multiple).
The Approach: What does it mean for r to be divisible by m and n? It means that all the primes in m must go into r, and all primes in n must be in r. What if m and n have primes in common? For example, if m is divisible by 3^5 and n is divisible by 3^7, what does this mean about r? It means r must be divisible by 3^7.
The Rule: For each prime p such that p^a \ m (e.g., m is divisible by p^a) and p^b \ n, r must be divisible by p^max(a, b).
The Algorithm:
Define q to be 1.
for each prime number p less than m and n:
find the largest a and b such that p^a \ m and p^b \ n
let q = q * p^max(a, b)
return q
NOTE: An alternate solution involves recognizing that gcd(a, b) * lcm(a, b) = ab. One could then compute the gcd(a, b) using the Euclidean algorithm. Of course, unless you already know this fact, it’s unlikely that this rule would occur to you during an interview.
Things to Watch Out For:
1. Be careful with the difference in precision between floats vs. doubles.
2. Don’t assume that a value (such as the slope of a line) is an int unless you’ve been told so.
Bayes’ Rule and Probability
1. If A and B are independent, then P(A and B) = P(A) * P(B).
2. Else (in general), P(A and B) = P(A given B) * P(B)
3. If A and B are mutually exclusive (e.g., if one happens, the other one can’t),
P(A or B) = P(A) + P(B).
4. Else (in general), P(A or B) = P(A) + P(B) - P(A and B).
Chapter 10 | Mathematical
68
CareerCup.com
10.1 You have a basketball hoop and someone says that you can play 1 of 2 games.
Game #1: You get one shot to make the hoop.
Game #2: You get three shots and you have to make 2 of 3 shots.
If p is the probability of making a particular shot, for which values of p should you pick one game or the other?
_
________________________________________________________________pg 187
10.2 There are three ants on different vertices of a triangle. What is the probability of collision (between any two or all of them) if they start walking on the sides of the triangle?
Similarly find the probability of collision with ‘n’ ants on an ‘n’ vertex polygon.
_
________________________________________________________________pg 188
10.3 Given two lines on a Cartesian plane, determine whether the two lines would intersect.
_
________________________________________________________________pg 189
10.4 Write a method to implement *, - , / operations. You should use only the + operator.
_
________________________________________________________________pg 190
10.5 Given two squares on a two dimensional plane, find a line that would cut these two squares in half.
_
________________________________________________________________pg 192
10.6 Given a two dimensional graph with points on it, find a line which passes the most number of points.
_
________________________________________________________________pg 193
10.7 Design an algorithm to find the kth number such that the only prime factors are 3, 5, and 7.
_
________________________________________________________________pg 195
Chapter 11 | Testing
Cracking the Coding Interview | Concepts and Algorithms
69
Testing Problems: Not Just for Testers!
Although testers are obviously asked more testing problems, developers will often be asked testing problems as well. Why? Because a good developer knows how to test their code!
Types of Testing Problems:
Testing problems generally fall into one of three categories:
1. Explain how you would test this real world object (pen, paperclip, etc).
2. Explain how you would test this computer software (e.g., a web browser).
3. Write test cases / test code to test this specific method.
We’ll discuss type #1, since it’s usually the most daunting. Remember that all three types require you to not make assumptions that the input or the user will play nice. Expect abuse and plan for it.
How to Test A Real World Object
Let’s imagine that you were asked to test a paperclip. The first thing to understand is: what is it expected to be used for and who are the expected users. Ask your interviewer—the answer may not be what you think! The answer could be “by teachers, to hold papers together” or it could be “by artists, to bend into new shapes.” These two use-cases will have very different answers. Once you understand the intended use, think about:
»»What are the specific use cases for the intended purpose? For example, holding 2 sheets of paper together, and up to 30 sheets. If it fails, does it fail gracefully? (see below)
»»What does it mean for it to fail? Answer: “Failing gracefully“ means for the paperclip to not hold paper together. If it snaps easily, that’s (probably) not failing gracefully.
»»Ask your interviewer—what are the expectations of it being used outside of the intended use case? Should we ensure that it has a minimum of usefulness for the other cases?
»»What “stress” conditions might your paperclip be used in? Answer: hot weather, cold weather, frequent re-use, etc.
Chapter 11 | Testing
70
CareerCup.com
11.1 Find the mistake(s) in the following code:
1 unsigned int i;
2 for (i = 100; i <= 0; --i)
3 printf(“%d\n”, i);
_
________________________________________________________________pg 209
11.2 You are given the source to an application which crashes when it is run. After running it ten times in a debugger, you find it never crashes in the same place. The application is single threaded, and uses only the C standard library. What programming errors could be causing this crash? How would you test each one?
_
________________________________________________________________pg 210
11.3 We have the following method used in a chess game: boolean canMoveTo(int x, int y) x and y are the coordinates of the chess board and it returns whether or not the piece can move to that position. Explain how you would test this method.
_
________________________________________________________________pg 211
11.4 How would you load test a webpage without using any test tools?
_
________________________________________________________________pg 212
11.5 How would you test a pen?
_
________________________________________________________________pg 213
11.6 How would you test an ATM in a distributed banking system?
_
________________________________________________________________pg 214
Chapter 12 | System Design and Memory Limits
Cracking the Coding Interview | Concepts and Algorithms
71
How to Approach:
Don’t be scared by these types of questions. Unless you claim to know how to design large systems, your interviewer probably won’t expect you to know this stuff automatically. They just want to see how you tackle these problems.
General Approach
The general approach is as follows: Imagine we’re designing a hypothetical system X for millions of items (users, files, megabytes, etc):
1. How would you solve it for a small number of items? Develop an algorithm for this case, which is often pretty straight-forward.
2. What happens when you try to implement that algorithm with millions of items? It’s likely that you have run out of space on the computer. So, divide up the files across many computers.
»»How do you divide up data across many machines? That is, do the first 100 items appear on the same computer? Or all items with the same hash value mod 100?
»»About how many computers will you need? To estimate this, ask how big each item is and take a guess at how much space a typical computer has.
3. Now, fix the problems that occur when you are using many computers. Make sure to answer the following questions:
»»How does one machine know which machine it should access to look up data?
»»Can data get out of sync across computers? How do you handle that?
»»How can you minimize expensive reads across computers?
Example: Design a Web Crawler
1. Forget about the fact that you’re dealing with billions of pages. How would you design this system if it were just a small number of pages? You should have an understanding of how you would solve the simple, small case in order to understand how you would solve the bigger case.
2. Now, think about the issues that occur with billions of pages. Most likely you can’t fit the data on one machine. How will you divide it up? How will you figure out which computer has a particular piece of data?
3. You now have different pieces of data on different machines. What problems might that create? Can you try to solve them?
And remember, don’t get scared! This is just an ordinary problem solving question.
Chapter 12 | System Design and Memory Limits
72
CareerCup.com
12.1 If you were integrating a feed of end of day stock price information (open, high, low, and closing price) for 5,000 companies, how would you do it? You are responsible for the development, rollout and ongoing monitoring and maintenance of the feed. Describe the different methods you considered and why you would recommend your approach. The feed is delivered once per trading day in a comma-separated format via an FTP site. The feed will be used by 1000 daily users in a web application.
_
________________________________________________________________pg 197
12.2 How would you design the data structures for a very large social network (Facebook, LinkedIn, etc)? Describe how you would design an algorithm to show the connection, or path, between two people (e.g., Me -> Bob -> Susan -> Jason -> You).
_
________________________________________________________________pg 199
12.3 Given an input file with four billion integers, provide an algorithm to generate an integer which is not contained in the file. Assume you have 1 GB of memory.
FOLLOW UP
What if you have only 10 MB of memory?
_
________________________________________________________________pg 202
12.4 You have an array with all the numbers from 1 to N, where N is at most 32,000. The array may have duplicate entries and you do not know what N is. With only 4KB of memory available, how would you print all duplicate elements in the array?
_
________________________________________________________________pg 205
12.5 If you were designing a web crawler, how would you avoid getting into infinite loops?
_
________________________________________________________________pg 206
12.6 You have a billion urls, where each is a huge page. How do you detect the duplicate documents?
_
________________________________________________________________pg 207
12.7 You have to design a database that can store terabytes of data. It should support efficient range queries. How would you do it?
_
________________________________________________________________pg 208

Part 3
Knowledge Based
Chapter 13 | C++
Cracking the Coding Interview | Knowledge Based
75
How To Approach:
A good interviewer won’t demand that you code in a language you don’t profess to know. Hopefully, if you’re asked to code in C++, it’s listed on your resume. If you don’t remember all the APIs, don’t worry—your interviewer probably doesn’t care that much. We do recommend, however, studying up on basic C++ syntax.
Pointer Syntax
1 int *p; // Defines pointer.
2 p = &q; // Sets p to address of q.
3 v = *p; // Set v to value of q.
4 Foo *f = new Foo(); // Initializes f.
5 int k = f->x; // Sets k equal to the value of f’s member variable.
C++ Class Syntax
1 class MyClass {
2 private:
3 double var;
4 public:
5 MyClass(double v) {var = v; }
6 ~MyClass() {};
7 double Update(double v);
8 };
9 double Complex::Update(double v) {
10 var = v; return v;
11 }
C++ vs Java
A very common question in an interview is “describe the differences between C++ and Java.” If you aren’t comfortable with any of these concepts, we recommend reading up on them.
1. Java runs in a virtual machine.
2. C++ natively supports unsigned arithmetic.
3. In Java, parameters are always passed by value (or, with objects, their references are passed by value). In C++, parameters can be passed by value, pointer, or by reference.
4. Java has built-in garbage collection.
5. C++ allows operator overloading.
6. C++ allows multiple inheritance of classes.
Question: Which of these might be considered strengths or weaknesses of C++ or Java? Why? In what cases might you choose one language over the other?
Chapter 13 | C++
76
CareerCup.com
13.1 Write a method to print the last K lines of an input file using C++.
_
________________________________________________________________pg 215
13.2 Compare and contrast a hash table vs. an STL map. How is a hash table implemented? If the number of inputs is small, what data structure options can be used instead of a hash table?
_
________________________________________________________________pg 216
13.3 How do virtual functions work in C++?
_
________________________________________________________________pg 217
13.4 What is the difference between deep copy and shallow copy? Explain how you would use each.
_
________________________________________________________________pg 218
13.5 What is the significance of the keyword “volatile” in C?
_
________________________________________________________________pg 219
13.6 What is name hiding in C++?
_
________________________________________________________________pg 220
13.7 Why does a destructor in base class need to be declared virtual?
_
________________________________________________________________pg 221
13.8 Write a method that takes a pointer to a Node structure as a parameter and returns a complete copy of the passed-in data structure. The Node structure contains two pointers to other Node structures.
_
________________________________________________________________pg 223
13.9 Write a smart pointer (smart_ptr) class.
_
________________________________________________________________pg 224
Chapter 14 | Java
Cracking the Coding Interview | Knowledge Based
77
How to Approach:
While Java related questions are found throughout this book, this chapter deals with questions about the language and syntax. You generally will not find too many questions like this at the larger software companies (though they are sometimes asked), but these questions are very common at other companies.
What do you do when you don’t know the answer?
If you don’t know the answer to a question about the Java language, try to figure it out by doing the following: (1) Think about what other languages do. (2) Create an example of the scenario. (3) Ask yourself how you would handle the scenario if you were designing the language.
Your interviewer may be equally—or more—impressed if you can derive the answer than if you automatically knew it. Don’t try to bluff though. Tell the interviewer, “I’m not sure I can recall the answer, but let me see if I can figure it out. Suppose we have this code…”
Classes & Interfaces (Example)
1 public static void main(String args[]) { … }
2 interface Foo {
3 void abc();
4 }
5 class Foo extends Bar implements Foo { … }
final:
»»Class: Can not be sub-classed
»»Method: Can not be overridden.
»»Variable: Can not be changed.
static:
»»Method: Class method. Called with Foo.DoIt() instead of f.DoIt()
»»Variable: Class variable. Has only one copy and is accessed through the class name.
abstract:
»»Class: Contains abstract methods. Can not be instantiated.
»»Interface: All interfaces are implicitly abstract. This modifier is optional.
»»Method: Method without a body. Class must also be abstract.
Chapter 14 | Java
78
CareerCup.com
14.1 In terms of inheritance, what is the effect of keeping a constructor private?
_
________________________________________________________________pg 225
14.2 In Java, does the finally block gets executed if we insert a return statement inside the try block of a try-catch-finally?
_
________________________________________________________________pg 226
14.3 What is the difference between final, finally, and finalize?
_
________________________________________________________________pg 227
14.4 Explain the difference between templates in C++ and generics in Java.
_
________________________________________________________________pg 228
14.5 Explain what object reflection is in Java and why it is useful.
_
________________________________________________________________pg 229
14.6 Suppose you are using a map in your program, how would you count the number of times the program calls the put() and get() functions?
_
________________________________________________________________pg 230
Chapter 15 | Databases
Cracking the Coding Interview | Knowledge Based
79
How to Approach:
You could be asked about databases in a variety of ways: write a SQL query, design a database to hold certain data, or design a large database. We’ll go through the latter two types here.
Small Database Design
Imagine you are asked to design a system to represent a large, multi-location, apartment rental company.
What are the key objects?
Property. Building. Apartment. Tenant. Manager.
How do they relate to each other?
Many-to-Many:
»»A property could have multiple managers, and a manager could manage multiple properties.
One-to-Many:
»»A building can only be part of one property.
»»An apartment can only be part of one building.
What is the relationship between Tenant and Apartment? An apartment can obviously have multiple tenants. Can a tenant rent multiple apartments? It would be very unusual to, but this could actually happen (particularly if it’s a national company). Talk to your interviewer about this. There is a trade-off between simplifying your database and designing it to be flexible. If you do assume that a Tenant can only rent one Apartment, what do you have to do if this situation occurs?
Large Database Design
When designing a large, scalable database, joins (which are required in the above examples), are generally very slow. Thus, you must denormalize your data. Think carefully about how data will be used—you’ll probably need to duplicate it in multiple tables.
Chapter 15 | Databases
80
CareerCup.com
15.1 Write a method to find the number of employees in each department.
_
________________________________________________________________pg 231
15.2 What are the different types of joins? Please explain how they differ and why certain types are better in certain situations.
_
________________________________________________________________pg 232
15.3 What is denormalization? Explain the pros and cons.
_
________________________________________________________________pg 234
15.4 Draw an entity-relationship diagram for a database with companies, people, and professionals (people who work for companies).
_
________________________________________________________________pg 235
15.5 Imagine a simple database storing information for students’ grades. Design what this database might look like, and provide a SQL query to return a list of the honor roll students (top 10%), sorted by their grade point average.
_
________________________________________________________________pg 236
Chapter 16 | Low Level
Cracking the Coding Interview | Knowledge Based
81
How to Approach:
Many candidates find low level problems to be some of the most challenging. Low level questions require a large amount of knowledge about the underlying architecture of a system. But just how much do you need to know? The answer to that depends, of course, on the company. At a typical large software company where you’d be working on desktop or web applications, you usually only need a minimum amount of knowledge. However, you should understand the concepts below very well, as many interview questions are based off this information.
Big vs Little Endian:
In big endian, the most significant byte is stored at the memory address location with the lowest address. This is akin to left-to-right reading order. Little endian is the reverse: the most significant byte is stored at the address with the highest address.
Stack (Memory)
When a function calls another function which calls another function, this memory goes onto the stack. An int (not a pointer to an int) that is created in a function is stored on the stack.
Heap (Memory)
When you allocate data with new() or malloc(), this data gets stored on the heap.
Malloc
Memory allocated using malloc is persistent—i.e., it will exist until either the programmer frees the memory or the program is terminated.
void *malloc(size_t sz)
Malloc takes as input sz bytes of memory and, if it is successful, returns a void pointer which indicates that it is a pointer to an unknown data type.
void free(void * p)
Free releases a block of memory previously allocated with malloc, calloc, or realloc.
Chapter 16 | Low Level
82
CareerCup.com
16.1 Explain the following terms: virtual memory, page fault, thrashing.
_
________________________________________________________________pg 237
16.2 What is a Branch Target buffer? Explain how it can be used in reducing bubble cycles in cases of branch misprediction.
_
________________________________________________________________pg 238
16.3 Describe direct memory access (DMA). Can a user level buffer / pointer be used by kernel or drivers?
_
________________________________________________________________pg 239
16.4 Write a step by step execution of things that happen after a user presses a key on the keyboard. Use as much detail as possible.
_
________________________________________________________________pg 237
16.5 Write a program to find whether a machine is big endian or little endian.
_
________________________________________________________________pg 241
16.6 Discuss how would you make sure that a process doesn’t access an unauthorized part of the stack.
_
________________________________________________________________pg 242
16.7 What are the best practices to prevent reverse engineering of DLLs?
_
________________________________________________________________pg 244
16.8 A device boots with an empty FIFO queue. In the first 400 ns period after startup, and in each subsequent 400 ns period, a maximum of 80 words will be written to the queue. Each write takes 4 ns. A worker thread requires 3 ns to read a word, and 2 ns to process it before reading the next word. What is the shortest depth of the FIFO such that no data is lost?
_
________________________________________________________________pg 245
16.9 Write an aligned malloc & free function that takes number of bytes and aligned byte (which is always power of 2)
EXAMPLE
align_malloc (1000,128) will return a memory address that is a multiple of 128 and that points to memory of size 1000 bytes.
aligned_free() will free memory allocated by align_malloc.
_
________________________________________________________________pg 247
16.10 Write a function called my2DAlloc which allocates a two dimensional array. Minimize the number of calls to malloc and make sure that the memory is accessible by the notation arr[i][j].
_
________________________________________________________________pg 248
Chapter 17 | Networking
Cracking the Coding Interview | Knowledge Based
83
How to Approach
While the big software houses probably won’t ask you many detailed networking questions in general, some interviewers will attempt to assess your understanding of networking as far as it relates to software and system design. Thus, you should have an understanding of http post and get requests, tcp, etc.
For a more networking based company (Qualcomm, CISCO, etc), we recommend a more thorough understanding. A good way to study is to read the material below, and delve further into it on Wikipedia. When Wikipedia discusses a concept that you are unfamiliar with, click on the concept to read more.
OSI 7 Layer Model
Networking architecture can be divided into seven layers. Each layer provides services to the layer above it and receives services from the layer below it. The seven layers, from top to bottom, are:
OSI 7 Layer Model
Level 7
Application Layer
Level 6
Presentation Layer
Level 5
Session Layer
Level 4
Transport Layer
Level 3
Network Layer
Level 2
Data Link Layer
Level 1
Physical Layer
For a networking focused interview, we suggest reviewing and understanding these concepts and their implications in detail.
Chapter 17 | Networking
84
CareerCup.com
17.1 Explain what happens, step by step, after you type a URL into a browser. Use as much detail as possible.
_
________________________________________________________________pg 249
17.2 Explain any common routing protocol in detail. For example: BGP, OSPF, RIP.
_
________________________________________________________________pg 250
17.3 Compare and contrast the IPv4 and IPv6 protocols.
_
________________________________________________________________pg 252
17.4 What is a network / subnet mask? Explain how host A sends a message / packet to host B when: (a) both are on same network and (b) both are on different networks. Explain which layer makes the routing decision and how.
_
________________________________________________________________pg 254
17.5 What are the differences between TCP and UDP? Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s / receiver’s window) and congestion control.
_
________________________________________________________________pg 255
Chapter 18 | Threads and Locks
Cracking the Coding Interview | Knowledge Based
85
How to Approach:
In a Microsoft, Google or Amazon interview, it’s not terribly common to be asked to implement an algorithm with threads (unless you’re working in a team for which this is a particularly important skill). It is, however, relatively common for interviewers at any company to assess your general understanding of threads, particularly your understanding of deadlocks
Deadlock Conditions
In order for a deadlock to occur, you must have the following four conditions met:
1. Mutual Exclusion: Only one process can use a resource at a given time.
2. Hold and Wait: Processes already holding a resource can request new ones.
3. No Preemption: One process cannot forcibly remove another process’ resource.
4. Circular Wait: Two or more processes form a circular chain where each process is waiting on another resource in the chain.
Deadlock Prevention
Deadlock prevention essentially entails removing one of the above conditions, but many of these conditions are difficult to satisfy. For instance, removing #1 is difficult because many resources can only be used by one process at a time (printers, etc). Most deadlock prevention algorithms focus on avoiding condition #4: circular wait.
If you aren’t familiar with these concepts, please read http://en.wikipedia.org/wiki/Deadlock.
A Simple Java Thread
1 class Foo implements Runnable {
2 public void run() {
3 while (true) beep();
4 }
5 }
6 Foo foo = new Foo ();
7 Thread myThread = new Thread(foo);
8 myThread.start();
Chapter 18 | Threads and Locks
86
CareerCup.com
18.1 What’s the difference between a thread and a process?
_
________________________________________________________________pg 257
18.2 How can you measure the time spent in a context switch?
_
________________________________________________________________pg 258
18.3 Implement a singleton design pattern as a template such that, for any given class Foo, you can call Singleton::instance() and get a pointer to an instance of a singleton of type Foo. Assume the existence of a class Lock which has acquire() and release() methods. How could you make your implementation thread safe and exception safe?
_
________________________________________________________________pg 259
18.4 Design a class which provides a lock only if there are no possible deadlocks.
_
________________________________________________________________pg 261
18.5 Suppose we have the following code:
class Foo {
public:
A
(.....); /* If A is called, a new thread will be created and
* the corresponding function will be executed. */
B(.....); /* same as above */
C(.....); /* same as above */
}
Foo f;
f.A(.....);
f.B(.....);
f.C(.....);
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
iii) Suppose we have the following code to use class Foo. We do not know how the threads will be scheduled in the OS.
Foo f;
f.A(.....); f.B(.....); f.C(.....);
f.A(.....); f.B(.....); f.C(.....);
Can you design a mechanism to make sure that all the methods will be executed in sequence?
_
________________________________________________________________pg 262
18.6 You are given a class with synchronized method A, and a normal method C. If you have two threads in one instance of a program, can they call A at the same time? Can they call A and C at the same time?
_
________________________________________________________________pg 264

Part 4
Additional Review Problems
Chapter 19 | Moderate
Cracking the Coding Interview | Additional Review Problems
89
19.1 Write a function to swap a number in place without temporary variables.
_
________________________________________________________________pg 265
19.2 Design an algorithm to figure out if someone has won in a game of tic-tac-toe.
_
________________________________________________________________pg 266
19.3 Write an algorithm which computes the number of trailing zeros in n factorial.
_
________________________________________________________________pg 268
19.4 Write a method which finds the maximum of two numbers. You should not use if-else or any other comparison operator.
EXAMPLE
Input: 5, 10
Output: 10
_
________________________________________________________________pg 269
19.5 The Game of Master Mind is played as follows:
The computer has four slots containing balls that are red (R), yellow (Y), green (G) or blue (B). For example, the computer might have RGGB (e.g., Slot #1 is red, Slots #2 and #3 are green, Slot #4 is blue).
You, the user, are trying to guess the solution. You might, for example, guess YRGB.
When you guess the correct color for the correct slot, you get a “hit”. If you guess a color that exists but is in the wrong slot, you get a “pseudo-hit”. For example, the guess YRGB has 2 hits and one pseudo hit.
For each guess, you are told the number of hits and pseudo-hits.
Write a method that, given a guess and a solution, returns the number of hits and pseudo hits.
_
________________________________________________________________pg 270
19.6 Given an integer between 0 and 999,999, print an English phrase that describes the integer (eg, “One Thousand, Two Hundred and Thirty Four”).
_
________________________________________________________________pg 271
19.7 You are given an array of integers (both positive and negative). Find the continuous sequence with the largest sum. Return the sum.
EXAMPLE
Input: {2, -8, 3, -2, 4, -10}
Output: 5 (i.e., {3, -2, 4} )
_
________________________________________________________________pg 273
19.8 Design a method to find the frequency of occurrences of any given word in a book.
Chapter 19 | Moderate
90
CareerCup.com
_
________________________________________________________________pg 273
19.9 Since XML is very verbose, you are given a way of encoding it where each tag gets mapped to a pre-defined integer value. The language/grammar is as follows:
Element --> Element Attr* END Element END [aka, encode the element
t
ag, then its attributes, then tack on an END character, then
e
ncode its children, then another end tag]
Attr --> Tag Value [assume all values are strings]
END --> 01
Tag --> some predefined mapping to int
Value --> string value END
Write code to print the encoded version of an xml element (passed in as string).
FOLLOW UP
Is there anything else you could do to (in many cases) compress this even further?
_
________________________________________________________________pg 275
19.10 Write a method to generate a random number between 1 and 7, given a method that generates a random number between 1 and 5 (i.e., implement rand7() using rand5()).
_
________________________________________________________________pg 277
19.11 Design an algorithm to find all pairs of integers within an array which sum to a specified value.
_
________________________________________________________________pg 278
Chapter 20 | Hard
Cracking the Coding Interview | Additional Review Problems
91
20.1 Write a function that adds two numbers. You should not use + or any arithmetic operators.
_
________________________________________________________________pg 279
20.2 Write a method to shuffle a deck of cards. It must be a perfect shuffle - in other words, each 52! permutations of the deck has to be equally likely. Assume that you are given a random number generator which is perfect.
_
________________________________________________________________pg 281
20.3 Write a method to randomly generate a set of m integers from an array of size n. Each element must have equal probability of being chosen.
_
________________________________________________________________pg 282
20.4 Write a method to count the number of 2s between 0 and n.
_
________________________________________________________________pg 283
20.5 You have a large text file containing words. Given any two words, find the shortest distance (in terms of number of words) between them in the file. Can you make the searching operation in O(1) time? What about the space complexity for your solution?
_
________________________________________________________________pg 285
20.6 Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. Assume that the computer memory can hold all one billion numbers.
_
________________________________________________________________pg 286
20.7 Write a program to find the longest word made of other words in a list of words.
EXAMPLE
Input: test, tester, testertest, testing, testingtester
Output: testingtester
_
________________________________________________________________pg 287
20.8 Given a string s and an array of smaller strings T, design a method to search s for each small string in T.
_
________________________________________________________________pg 288
20.9 Numbers are randomly generated and passed to a method. Write a program to find and maintain the median value as new values are generated.
_
________________________________________________________________pg 290
20.10 Given two words of equal length that are in a dictionary, write a method to transform one word into another word by changing only one letter at a time. The new word you get in each step must be in the dictionary.
EXAMPLE
Chapter 20 | Hard
92
CareerCup.com
Input: DAMP, LIKE
Output: DAMP -> LAMP -> LIMP -> LIME -> LIKE
_
________________________________________________________________pg 291
20.11 Imagine you have a square matrix, where each cell is filled with either black or white. Design an algorithm to find the maximum subsquare such that all four borders are filled with black pixels.
_
________________________________________________________________pg 293
20.12 Given an NxN matrix of positive and negative integers, write code to find the sub-matrix with the largest possible sum.
_
________________________________________________________________pg 295
20.13 Given a dictionary of millions of words, give an algorithm to find the largest possible rectangle of letters such that every row forms a word (reading left to right) and every column forms a word (reading top to bottom).
_
________________________________________________________________pg 298
Each problem may have many 'optimal' solutions that differ in runtime, space, clarity, extensibility, etc. We have provided one (or more) optimal solutions. If you have additional solutions you would like to contribute, please contact us at
http://www.xrl.us/ccbook or support@careercup.com.
We welcome all feedback and suggestions. Contact us at
http://www.xrl.us/ccbook or support@careercup.com.
Solutions
Solutions to Chapter 1 | Arrays and Strings
Cracking the Coding Interview | Data Structures
95
1.1 Implement an algorithm to determine if a string has all unique characters. What if you can not use additional data structures?
pg 48
SOLUTION
For simplicity, assume char set is ASCII (if not, we need to increase the storage size. The rest of the logic would be the same). NOTE: This is a great thing to point out to your interviewer!
1 public static boolean isUniqueChars2(String str) {
2 boolean[] char_set = new boolean[256];
3 for (int i = 0; i < str.length(); i++) {
4 int val = str.charAt(i);
5 if (char_set[val]) return false;
6 char_set[val] = true;
7 }
8 return true;
9 }
Time complexity is O(n), where n is the length of the string, and space complexity is O(n).
We can reduce our space usage a little bit by using a bit vector. We will assume, in the below code, that the string is only lower case ‘a’ through ‘z’. This will allow us to use just a single int
1 public static boolean isUniqueChars(String str) {
2 int checker = 0;
3 for (int i = 0; i < str.length(); ++i) {
4 int val = str.charAt(i) - ‘a’;
5 if ((checker & (1 << val)) > 0) return false;
6 checker |= (1 << val);
7 }
8 return true;
9 }
Alternatively, we could do the following:
1. Check every char of the string with every other char of the string for duplicate occurrences. This will take O(n^2) time and no space.
2. If we are allowed to destroy the input string, we could sort the string in O(n log n) time and then linearly check the string for neighboring characters that are identical. Careful, though - many sorting algorithms take up extra space.
Solutions to Chapter 1 | Arrays and Strings
96
CareerCup.com
1.2 Write code to reverse a C-Style String. (C-String means that “abcd” is represented as five characters, including the null character.)
pg 48
SOLUTION
This is a classic interview question. The only “gotcha” is to try to do it in place, and to be careful for the null character.
1 void reverse(char *str) {
2 char * end = str;
3 char tmp;
4 if (str) {
5 while (*end) {
6 ++end;
7 }
8 --end;
9 while (str < end) {
10 tmp = *str;
11 *str++ = *end;
12 *end-- = tmp;
13 }
14 }
15 }
Solutions to Chapter 1 | Arrays and Strings
97 Cracking the Coding Interview | Data Structures
1.3 Design an algorithm and write code to remove the duplicate characters in a string without using any additional buffer. NOTE: One or two additional variables are fine. An extra copy of the array is not.
FOLLOW UP
Write the test cases for this method.
pg 48
SOLUTION
First, ask yourself, what does the interviewer mean by an additional buffer? Can we use an additional array of constant size?
Algorithm—No (Large) Additional Memory:
1. For each character, check if it is a duplicate of already found characters.
2. Skip duplicate characters and update the non duplicate characters.
Time complexity is O(N2).
1 public static void removeDuplicates(char[] str) {
2 if (str == null) return;
3 int len = str.length;
4 if (len < 2) return;
5
6 int tail = 1;
7
8 for (int i = 1; i < len; ++i) {
9 int j;
10 for (j = 0; j < tail; ++j) {
11 if (str[i] == str[j]) break;
12 }
13 if (j == tail) {
14 str[tail] = str[i];
15 ++tail;
16 }
17 }
18 str[tail] = 0;
19 }
Test Cases:
1. String does not contain any duplicates, e.g.: abcd
2. String contains all duplicates, e.g.: aaaa
3. Null string
4. String with all continuous duplicates, e.g.: aaabbb
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 98
5. String with non-contiguous duplicate, e.g.: abababa
Algorithm—With Additional Memory of Constant Size
1 public static void removeDuplicatesEff(char[] str) {
2 if (str == null) return;
3 int len = str.length;
4 if (len < 2) return;
5 boolean[] hit = new boolean[256];
6 for (int i = 0; i < 256; ++i) {
7 hit[i] = false;
8 }
9 hit[str[0]] = true;
10 int tail = 1;
11 for (int i = 1; i < len; ++i) {
12 if (!hit[str[i]]) {
13 str[tail] = str[i];
14 ++tail;
15 hit[str[i]] = true;
16 }
17 }
18 str[tail] = 0;
19 }
Test Cases:
1. String does not contain any duplicates, e.g.: abcd
2. String contains all duplicates, e.g.: aaaa
3. Null string
4. Empty string
5. String with all continuous duplicates, e.g.: aaabbb
6. String with non-contiguous duplicates, e.g.: abababa
Solutions to Chapter 1 | Arrays and Strings
99 Cracking the Coding Interview | Data Structures
1.4 Write a method to decide if two strings are anagrams or not.
pg 48
SOLUTION
There are two easy ways to solve this problem:
Solution #1: Sort the strings
1 boolean anagram(String s, String t) {
2 return sort(s) == sort(t);
3 }
Solution #2: Check if the two strings have identical counts for each unique char.
1 public static boolean anagram(String s, String t) {
2 if (s.length() != t.length()) return false;
3 int[] letters = new int[256];
4 int num_unique_chars = 0;
5 int num_completed_t = 0;
6 char[] s_array = s.toCharArray();
7 for (char c : s_array) { // count number of each char in s.
8 if (letters[c] == 0) ++num_unique_chars;
9 ++letters[c];
10 }
11 for (int i = 0; i < t.length(); ++i) {
12 int c = (int) t.charAt(i);
13 if (letters[c] == 0) { // Found more of char c in t than in s.
14 return false;
15 }
16 --letters[c];
17 if (letters[c] == 0) {
18 ++num_completed_t;
19 if (num_completed_t == num_unique_chars) {
20 // it’s a match if t has been processed completely
21 return i == t.length() - 1;
22 }
23 }
24 }
25 return false;
26 }
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 100
1.5 Write a method to replace all spaces in a string with ‘%20’.
pg 48
SOLUTION
The algorithm is as follows:
1. Count the number of spaces during the first scan of the string.
2. Parse the string again from the end and for each character:
»»If a space is encountered, store “%20”.
»»Else, store the character as it is in the newly shifted location.
1 public static void ReplaceFun(char[] str, int length) {
2 int spaceCount = 0, newLength, i = 0;
3 for (i = 0; i < length; i++) {
4 if (str[i] == ‘ ‘) {
5 spaceCount++;
6 }
7 }
8 newLength = length + spaceCount * 2;
9 str[newLength] = ‘\0’;
10 for (i = length - 1; i >= 0; i--) {
11 if (str[i] == ‘ ‘) {
12 str[newLength - 1] = ‘0’;
13 str[newLength - 2] = ‘2’;
14 str[newLength - 3] = ‘%’;
15 newLength = newLength - 3;
16 } else {
17 str[newLength - 1] = str[i];
18 newLength = newLength - 1;
19 }
20 }
21 }
Solutions to Chapter 1 | Arrays and Strings
101 Cracking the Coding Interview | Data Structures
1.6 Given an image represented by an NxN matrix, where each pixel in the image is 4 bytes, write a method to rotate the image by 90 degrees. Can you do this in place?
pg 48
SOLUTION
The rotation can be performed in layers, where you perform a cyclic swap on the edges on each layer. In the first for loop, we rotate the first layer (outermost edges). We rotate the edges by doing a four-way swap first on the corners, then on the element clockwise from the edges, then on the element three steps away.
Once the exterior elements are rotated, we then rotate the interior region’s edges.
1 public static void rotate(int[][] matrix, int n) {
2 for (int layer = 0; layer < n / 2; ++layer) {
3 int first = layer;
4 int last = n - 1 - layer;
5 for(int i = first; i < last; ++i) {
6 int offset = i - first;
7 int top = matrix[first][i]; // save top
8 // left -> top
9 matrix[first][i] = matrix[last-offset][first];
10
11 // bottom -> left
12 matrix[last-offset][first] = matrix[last][last - offset];
13
14 // right -> bottom
15 matrix[last][last - offset] = matrix[i][last];
16
17 // top -> right
18 matrix[i][last] = top; // right <- saved top
19 }
20 }
21 }
Solutions to Chapter 1 | Arrays and Strings
. CareerCup com 102
1.7 Write an algorithm such that if an element in an MxN matrix is 0, its entire row and column is set to 0.
pg 48
SOLUTION
At first glance, this problem seems easy: just iterate through the matrix and every time we see a 0, set that row and column to 0. There’s one problem with that solution though: we will “recognize” those 0s later on in our iteration and then set their row and column to zero. Pretty soon, our entire matrix is 0s!
One way around this is to keep a second matrix which flags the 0 locations. We would then do a second pass through the matrix to set the zeros. This would take O(MN) space.
Do we really need O(MN) space? No. Since we’re going to set the entire row and column to zero, do we really need to track which cell in a row is zero? No. We only need to know that row 2, for example, has a zero.
The code below implement this algorithm. We keep track in two arrays all the rows with zeros and all the columns with zeros. We then make a second pass of the matrix and set a cell to zero if its row or column is zero.
1 public static void setZeros(int[][] matrix) {
2 int[] row = new int[matrix.length];
3 int[] column = new int[matrix[0].length];
4 // Store the row and column index with value 0
5 for (int i = 0; i < matrix.length; i++) {
6 for (int j = 0; j < matrix[0].length;j++) {
7 if (matrix[i][j] == 0) {
8 row[i] = 1;
9 column[j] = 1;
10 }
11 }
12 }
13
14 // Set arr[i][j] to 0 if either row i or column j has a 0
15 for (int i = 0; i < matrix.length; i++) {
16 for (int j = 0; j < matrix[0].length; j++) {
17 if ((row[i] == 1 || column[j] == 1)) {
18 matrix[i][j] = 0;
19 }
20 }
21 }
22 }
Solutions to Chapter 1 | Arrays and Strings
103 Cracking the Coding Interview | Data Structures
1.8 Assume you have a method isSubstring which checks if one word is a substring of another. Given two strings, s1 and s2, write code to check if s2 is a rotation of s1 using only one call to isSubstring (i.e., “waterbottle” is a rotation of “erbottlewat”).
pg 48
SOLUTION
Just do the following checks
1. Check if length(s1) == length(s2). If not, return false.
2. Else, concatenate s1 with itself and see whether s2 is substring of the result.
input: s1 = apple, s2 = pleap ==> apple is a substring of pleappleap
input: s1 = apple, s2 = ppale ==> apple is not a substring of ppaleppale
1 public static boolean isRotation(String s1, String s2) {
2 int len = s1.length();
3 /* check that s1 and s2 are equal length and not empty */
4 if (len == s2.length() && len > 0) {
5 /* concatenate s1 and s1 within new buffer */
6 String s1s1 = s1 + s1;
7 return isSubstring(s1s1, s2);
8 }
9 return false;
10 }

Solutions to Chapter 2 | Linked Lists
Cracking the Coding Interview | Data Structures
105
2.1 Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
pg 50
SOLUTION
If we can use a buffer, we can keep track of elements in a hashtable and remove any dups:
1 public static void deleteDups(LinkedListNode n) {
2 Hashtable table = new Hashtable();
3 LinkedListNode previous = null;
4 while (n != null) {
5 if (table.containsKey(n.data)) previous.next = n.next;
6 else {
7 table.put(n.data, true);
8 previous = n;
9 }
10 n = n.next;
11 }
12 }
Without a buffer, we can iterate with two pointers: “current” does a normal iteration, while “runner” iterates through all prior nodes to check for dups. Runner will only see one dup per node, because if there were multiple duplicates they would have been removed already.
1 public static void deleteDups2(LinkedListNode head) {
2 if (head == null) return;
3 LinkedListNode previous = head;
4 LinkedListNode current = previous.next;
5 while (current != null) {
6 LinkedListNode runner = head;
7 while (runner != current) { // Check for earlier dups
8 if (runner.data == current.data) {
9 LinkedListNode tmp = current.next; // remove current
10 previous.next = tmp;
11 current = tmp; // update current to next node
12 break; // all other dups have already been removed
13 }
14 runner = runner.next;
15 }
16 if (runner == current) { // current not updated - update now
17 previous = current;
18 current = current.next;
19 }
20 }
21 }
Solutions to Chapter 2 | Linked Lists
106
CareerCup.com
2.2 Implement an algorithm to find the nth to last element of a singly linked list.
pg 50
SOLUTION
Note: This problem screams recursion, but we’ll do it a different way because it’s trickier. In a question like this, expect follow up questions about the advantages of recursion vs iteration.
Assumption: The minimum number of nodes in list is n.
Algorithm:
1. Create two pointers, p1 and p2, that point to the beginning of the node.
2. Increment p2 by n-1 positions, to make it point to the nth node from the beginning (to make the distance of n between p1 and p2).
3. Check for p2->next == null if yes return value of p1, otherwise increment p1 and p2. If next of p2 is null it means p1 points to the nth node from the last as the distance between the two is n.
4. Repeat Step 3.
1 LinkedListNode nthToLast(LinkedListNode head, int n) {
2 if (head == null || n < 1) {
3 return null;
4 }
5 LinkedListNode p1 = head;
6 LinkedListNode p2 = head;
7 for (int j = 0; j < n - 1; ++j) { // skip n-1 steps ahead
8 if (p2 == null) {
9 return null; // not found since list size < n
10 }
11 p2 = p2.next;
12 }
13 while (p2.next != null) {
14 p1 = p1.next;
15 p2 = p2.next;
16 }
17 return p1;
18 }
Solutions to Chapter 2 | Linked Lists
107 Cracking the Coding Interview | Data Structures
2.3 Implement an algorithm to delete a node in the middle of a single linked list, given only access to that node.
EXAMPLE
Input: the node ‘c’ from the linked list a->b->c->d->e
Result: nothing is returned, but the new linked list looks like a->b->d->e
pg 50
SOLUTION
The solution to this is to simply copy the data from the next node into this node and then delete the next node.
NOTE: This problem can not be solved if the node to be deleted is the last node in the linked list. That’s ok—your interviewer wants to see you point that out. You could consider marking it as dummy in that case. This is an issue you should discuss with your interviewer.
1 public static boolean deleteNode(LinkedListNode n) {
2 if (n == null || n.next == null) {
3 return false; // Failure
4 }
5 LinkedListNode next = n.next;
6 n.data = next.data;
7 n.next = next.next;
8 return true;
9 }
Solutions to Chapter 2 | Linked Lists
. CareerCup com 108
2.4 You have two numbers represented by a linked list, where each node contains a single digit. The digits are stored in reverse order, such that the 1’s digit is at the head of the list. Write a function that adds the two numbers and returns the sum as a linked list.
EXAMPLE
Input: (3 -> 1 -> 5), (5 -> 9 -> 2)
Output: 8 -> 0 -> 8
pg 50
SOLUTION
We can implement this recursively by adding node by node, just as we would digit by digit.
1. result.data = (node1 + node2 + any earlier carry) % 10
2. if node1 + node2 > 10, then carry a 1 to the next addition.
3. add the tails of the two nodes, passing along the carry.
1 LinkedListNode addLists(LinkedListNode l1, LinkedListNode l2,
2 int carry) {
3 if (l1 == null && l2 == null) {
4 return null;
5 }
6 LinkedListNode result = new LinkedListNode(carry, null, null);
7 int value = carry;
8 if (l1 != null) {
9 value += l1.data;
10 }
11 if (l2 != null) {
12 value += l2.data;
13 }
14 result.data = value % 10;
15 LinkedListNode more = addLists(l1 == null ? null : l1.next,
16 l2 == null ? null : l2.next,
17 value > 10 ? 1 : 1);
18 result.setNext(more);
19 return result;
20 }
Solutions to Chapter 2 | Linked Lists
109 Cracking the Coding Interview | Data Structures
2.5 Given a circular linked list, implement an algorithm which returns node at the beginning of the loop.
DEFINITION
Circular linked list: A (corrupt) linked list in which a node’s next pointer points to an earlier node, so as to make a loop in the linked list.
EXAMPLE
Input: A -> B -> C -> D -> E -> C [the same C as earlier]
Output: C
pg 50
SOLUTION
If we move two pointers, one with speed 1 and another with speed 2, they will end up meeting if the linked list has a loop. Why? Think about two cars driving on a track—the faster car will always pass the slower one!
The tricky part here is finding the start of the loop. Imagine, as an analogy, two people racing around a track, one running twice as fast as the other. If they start off at the same place, when will they next meet? They will next meet at the start of the next lap.
Now, let’s suppose Fast Runner had a head start of k meters on an n step lap. When will they next meet? They will meet k meters before the start of the next lap. (Why? Fast Runner would have made k + 2(n - k) steps, including its head start, and Slow Runner would have made n - k steps. Both will be k steps before the start of the loop.)
Now, going back to the problem, when Fast Runner (n2) and Slow Runner (n1) are moving around our circular linked list, n2 will have a head start on the loop when n1 enters. Specifically, it will have a head start of k, where k is the number of nodes before the loop. Since n2 has a head start of k nodes, n1 and n2 will meet k nodes before the start of the loop.
So, we now know the following:
1. Head is k nodes from LoopStart (by definition).
2. MeetingPoint for n1 and n2 is k nodes from LoopStart (as shown above).
Thus, if we move n1 back to Head and keep n2 at MeetingPoint, and move them both at the same pace, they will meet at LoopStart.
Solutions to Chapter 2 | Linked Lists
. CareerCup com 110
1 LinkedListNode FindBeginning(LinkedListNode head) {
2 LinkedListNode n1 = head;
3 LinkedListNode n2 = head;
4
5 // Find meeting point
6 while (n2.next != null) {
7 n1 = n1.next;
8 n2 = n2.next.next;
9 if (n1 == n2) {
10 break;
11 }
12 }
13
14 // Error check - there is no meeting point, and therefore no loop
15 if (n2.next == null) {
16 return null;
17 }
18
19 /* Move n1 to Head. Keep n2 at Meeting Point. Each are k steps
20 /* from the Loop Start. If they move at the same pace, they must
21 * meet at Loop Start. */
22 n1 = head;
23 while (n1 != n2) {
24 n1 = n1.next;
25 n2 = n2.next;
26 }
27 // Now n2 points to the start of the loop.
28 return n2;
29 }
n1 and n2 will meet here, 3 nodes from start of loop
Solutions to Chapter 3 | Stacks and Queues
Cracking the Coding Interview | Data Structures
111
3.1 Describe how you could use a single array to implement three stacks.
pg 52
SOLUTION
Approach 1:
Divide the array in three equal parts and allow the individual stack to grow in that limited space (note: “[“ means inclusive, while “(“ means exclusive of the end point).
»»for stack 1, we will use [0, n/3)
»»for stack 2, we will use [n/3, 2n/3)
»»for stack 3, we will use [2n/3, n)
This solution is based on the assumption that we do not have any extra information about the usage of space by individual stacks and that we can’t either modify or use any extra space. With these constraints, we are left with no other choice but to divide equally.
1 int stackSize = 300;
2 int[] buffer = new int [stackSize * 3];
3 int[] stackPointer = {0, 0, 0}; // stack pointers to track top elem
4
5 void push(int stackNum, int value) {
6 /* Find the index of the top element in the array + 1, and
7 * increment the stack pointer */
8 int index = stackNum * stackSize + stackPointer[stackNum] + 1;
9 stackPointer[stackNum]++;
10 buffer[index] = value;
11 }
12
13 int pop(int stackNum) {
14 int index = stackNum * stackSize + stackPointer[stackNum];
15 stackPointer[stackNum]--;
16 int value = buffer[index];
17 buffer[index]=0;
18 return value;
19 }
20
21 int peek(int stackNum) {
22 int index = stackNum * stackSize + stackPointer[stackNum];
23 return buffer[index];
24 }
25
26 boolean isEmpty(int stackNum) {
27 return stackPointer[stackNum] == stackNum*stackSize;
28 }
Solutions to Chapter 3 | Stacks and Queues
112
CareerCup.com
Approach 2:
In this approach, any stack can grow as long as there is any free space in the array.
We sequentially allocate space to the stacks and we link new blocks to the previous block. This means any new element in a stack keeps a pointer to the previous top element of that particular stack.
In this implementation, we face a problem of unused space. For example, if a stack deletes some of its elements, the deleted elements may not necessarily appear at the end of the array. So, in that case, we would not be able to use those newly freed spaces.
To overcome this deficiency, we can maintain a free list and the whole array space would be given initially to the free list. For every insertion, we would delete an entry from the free list. In case of deletion, we would simply add the index of the free cell to the free list.
In this implementation we would be able to have flexibility in terms of variable space utilization but we would need to increase the space complexity.
1 int stackSize = 300;
2 int indexUsed = 0;
3 int[] stackPointer = {-1,-1,-1};
4 StackNode[] buffer = new StackNode[stackSize * 3];
5 void push(int stackNum, int value) {
6 int lastIndex = stackPointer[stackNum];
7 stackPointer[stackNum] = indexUsed;
8 indexUsed++;
9 buffer[stackPointer[stackNum]]=new StackNode(lastIndex,value);
10 }
11 int pop(int stackNum) {
12 int value = buffer[stackPointer[stackNum]].value;
13 int lastIndex = stackPointer[stackNum];
14 stackPointer[stackNum] = buffer[stackPointer[stackNum]].previous;
15 buffer[lastIndex] = null;
16 indexUsed--;
17 return value;
18 }
19 int peek(int stack) { return buffer[stackPointer[stack]].value; }
20 boolean isEmpty(int stackNum) { return stackPointer[stackNum] == -1; }
21
22 class StackNode {
23 public int previous;
24 public int value;
25 public StackNode(int p, int v){
26 value = v;
27 previous = p;
28 }
29 }
Solutions to Chapter 3 | Stacks and Queues
113 Cracking the Coding Interview | Data Structures
3.2 How would you design a stack which, in addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time.
pg 52
SOLUTION
You can implement this by having each node in the stack keep track of the minimum beneath itself. Then, to find the min, you just look at what the top element thinks is the min.
When you push an element onto the stack, the element is given the current minimum. It sets its “local min” to be the min.
1 public class StackWithMin extends Stack<NodeWithMin> {
2 public void push(int value) {
3 int newMin = Math.min(value, min());
4 super.push(new NodeWithMin(value, newMin));
5 }
6
7 public int min() {
8 if (this.isEmpty()) {
9 return Integer.MAX_VALUE;
10 } else {
11 return peek().min;
12 }
13 }
14 }
15
16 class NodeWithMin {
17 public int value;
18 public int min;
19 public NodeWithMin(int v, int min){
20 value = v;
21 this.min = min;
22 }
23 }
There’s just one issue with this: if we have a large stack, we waste a lot of space by keeping track of the min for every single element. Can we do better?
We can (maybe) do a bit better than this by using an additional stack which keeps track of the mins.
1 public class StackWithMin2 extends Stack<Integer> {
2 Stack<Integer> s2;
3 public StackWithMin2() {
4 s2 = new Stack<Integer>();
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 114
5 }
6 public void push(int value){
7 if (value <= min()) {
8 s2.push(value);
9 }
10 super.push(value);
11 }
12 public Integer pop() {
13 int value = super.pop();
14 if (value == min()) {
15 s2.pop();
16 }
17 return value;
18 }
19 public int min() {
20 if (s2.isEmpty()) {
21 return Integer.MAX_VALUE;
22 } else {
23 return s2.peek();
24 }
25 }
26 }
Why might this be more space efficient? If many elements have the same local min, then we’re keeping a lot of duplicate data. By having the mins kept in a separate stack, we don’t have this duplicate data (although we do use up a lot of extra space because we have a stack node instead of a single int).
Solutions to Chapter 3 | Stacks and Queues
115 Cracking the Coding Interview | Data Structures
3.3 Imagine a (literal) stack of plates. If the stack gets too high, it might topple. Therefore, in real life, we would likely start a new stack when the previous stack exceeds some threshold. Implement a data structure SetOfStacks that mimics this. SetOfStacks should be composed of several stacks, and should create a new stack once the previous one exceeds capacity. SetOfStacks.push() and SetOfStacks.pop() should behave identically to a single stack (that is, pop() should return the same values as it would if there were just a single stack).
FOLLOW UP
Implement a function popAt(int index) which performs a pop operation on a specific sub-stack.
pg 52
SOLUTION
In this problem, we’ve been told what our data structure should look like:
1 class SetOfStacks {
2 ArrayList<Stack> stacks = new ArrayList<Stack>();
3 public void push(int v) { ... }
4 public int pop() { ... }
5 }
We know that push() should behave identically to a single stack, which means that we need push() to call push on the last stack. We have to be a bit careful here though: if the last stack is at capacity, we need to create a new stack. Our code should look something like this:
1 public void push(int v) {
2 Stack last = getLastStack();
3 if (last != null && !last.isAtCapacity()) { // add to last stack
4 last.push(v);
5 } else { // must create new stack
6 Stack stack = new Stack(capacity);
7 stack.push(v);
8 stacks.add(stack);
9 }
10 }
What should pop() do? It should behave similarly to push(), in that it should operate on the last stack. If the last stack is empty (after popping), then we should remove it from the list of stacks.
1 public int pop() {
2 Stack last = getLastStack();
3 int v = last.pop();
4 if (last.size == 0) stacks.remove(stacks.size() - 1);
5 return v;
6 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 116
What about the follow up question? This is a bit trickier to implement, but essentially we should imagine a “rollover” system. If we pop an element from stack 1, we need to remove the bottom of stack 2 and push it onto stack 1. We then need to rollover from stack 3 to stack 2, stack 4 to stack 3, etc.
NOTE: You could make an argument that, rather than “rolling over,” we should be OK with some stacks not being at full capacity. This would improve the time complexity (by a fair amount, with a large number of elements), but it might get us into tricky situations later on if someone assumes that all stacks (other than the last) operate at full capacity. There’s no “right answer” here; discuss this trade-off with your interviewer!
1 public class SetOfStacks {
2 ArrayList<Stack> stacks = new ArrayList<Stack>();
3 public int capacity;
4 public SetOfStacks(int capacity) { this.capacity = capacity; }
5
6 public Stack getLastStack() {
7 if (stacks.size() == 0) return null;
8 return stacks.get(stacks.size() - 1);
9 }
10
11 public void push(int v) { /* see earlier code */ }
12 public int pop() {
13 Stack last = getLastStack();
14 System.out.println(stacks.size());
15 int v = last.pop();
16 if (last.size == 0) stacks.remove(stacks.size() - 1);
17 return v;
18 }
19
20 public int popAt(int index) {
21 return leftShift(index, true);
22 }
23
24 public int leftShift(int index, boolean removeTop) {
25 Stack stack = stacks.get(index);
26 int removed_item;
27 if (removeTop) removed_item = stack.pop();
28 else removed_item = stack.removeBottom();
29 if (stack.isEmpty()) {
30 stacks.remove(index);
31 } else if (stacks.size() > index + 1) {
32 int v = leftShift(index + 1, false);
Solutions to Chapter 3 | Stacks and Queues
117 Cracking the Coding Interview | Data Structures
33 stack.push(v);
34 }
35 return removed_item;
36 }
37 }
38
39 public class Stack {
40 private int capacity;
41 public Node top, bottom;
42 public int size = 0;
43
44 public Stack(int capacity) { this.capacity = capacity; }
45 public boolean isAtCapacity() { return capacity == size; }
46
47 public void join(Node above, Node below) {
48 if (below != null) below.above = above;
49 if (above != null) above.below = below;
50 }
51
52 public boolean push(int v) {
53 if (size >= capacity) return false;
54 size++;
55 Node n = new Node(v);
56 if (size == 1) bottom = n;
57 join(n, top);
58 top = n;
59 return true;
60 }
61
62 public int pop() {
63 Node t = top;
64 top = top.below;
65 size--;
66 return t.value;
67 }
68
69 public boolean isEmpty() { return size == 0; }
70 public int removeBottom() {
71 Node b = bottom;
72 bottom = bottom.above;
73 if (bottom != null) bottom.below = null;
74 size--;
75 return b.value;
76 }
77 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 118
3.4 In the classic problem of the Towers of Hanoi, you have 3 rods and N disks of different sizes which can slide onto any tower. The puzzle starts with disks sorted in ascending order of size from top to bottom (e.g., each disk sits on top of an even larger one). You have the following constraints:
(A) Only one disk can be moved at a time.
(B) A disk is slid off the top of one rod onto the next rod.
(C) A disk can only be placed on top of a larger disk.
Write a program to move the disks from the first rod to the last using Stacks.
pg 52
SOLUTION
We need to move N disks from Rod 1 to Rod 3, but let’s start from the beginning. Moving the top disk is easy - we just move it to Disk 3.
Can we move the top two disks? Yes:
1. Move Disk 1 from Rod 1 to Rod 2
2. Move Disk 2 from Rod 1 to Rod 3
3. Move Disk 1 from Rod 2 to Rod 3
Can we move the top three disks?
1. We know we can move the top two disks around from one Rod to another (as shown earlier), so let’s assume we have moved Disk 1 and 2 to Rod 2.
2. Move Disk 3 to Rod 3
3. Again we know we can move the top two disks around, so let’s move them from Rod 2 to Rod 3.
This approach leads to a natural recursive algorithm:
1 public static void main(String[] args)
2 int n = 5;
3 Tower[] towers = new Tower[n];
4 for (int i = 0; i < 3; i++) towers[i] = new Tower(i);
5 for (int i = n - 1; i >= 0; i--) towers[0].add(i);
6 towers[0].moveDisks(n, towers[2], towers[1]);
7 }
8
9 public class Tower {
10 private Stack<Integer> disks;
11 private int index;
12 public Tower(int i) {
Solutions to Chapter 3 | Stacks and Queues
119 Cracking the Coding Interview | Data Structures
13 disks = new Stack<Integer>();
14 index = i;
15 }
16
17 public int index() {
18 return index;
19 }
20
21 public void add(int d) {
22 if (!disks.isEmpty() && disks.peek() <= d) {
23 System.out.println(“Error placing disk ” + d);
24 } else {
25 disks.push(d);
26 }
27 }
28
29 public void moveTopTo(Tower t) {
30 int top = disks.pop();
31 t.add(top);
32 System.out.println(“Move disk ” + top + “ from ” + index() +
33 “ to ” + t.index());
34 }
35
36 public void print() {
37 System.out.println(“Contents of Tower “ + index());
38 for (int i = disks.size() - 1; i >= 0; i--) {
39 System.out.println(“ “ + disks.get(i));
40 }
41 }
42
43 public void moveDisks(int n, Tower destination, Tower buffer) {
44 if (n > 0) {
45 moveDisks(n - 1, buffer, destination);
46 moveTopTo(destination);
47 buffer.moveDisks(n - 1, destination, this);
48 }
49 }
50 }
Solutions to Chapter 3 | Stacks and Queues
. CareerCup com 120
3.5 Implement a MyQueue class which implements a queue using two stacks.
pg 52
SOLUTION
Since the major difference between a queue and a stack is the order (first-in-first-out vs. last-in-first-out), we know that we need to modify peek() and pop() to go in reverse order. We can use our second stack to reverse the order of the elements (by popping s1 and pushing the elements on to s2). In such an implementation, on each peek() and pop() operation, we would pop everything from s1 onto s2, perform the peek / pop operation, and then push everything back.
This will work, but if two pop / peeks are performed back-to-back, we’re needlessly moving elements. We can implement a “lazy” approach where we let the elements sit in s2.
s1 will thus be ordered with the newest elements on the top, while s2 will have the oldest elements on the top. We push the new elements onto s1, and peek and pop from s2. When s2 is empty, we’ll transfer all the elements from s1 onto s2, in reverse order.
1 public class MyQueue<T> {
2 Stack<T> s1, s2;
3 public MyQueue() {
4 s1 = new Stack<T>();
5 s2 = new Stack<T>();
6 }
7
8 public int size() {
9 return s1.size() + s2.size();
10 }
11
12 public void add(T value) {
13 s1.push(value);
14 }
15
16 public T peek() {
17 if (!s2.empty()) return s2.peek();
18 while (!s1.empty()) s2.push(s1.pop());
19 return s2.peek();
20 }
21
22 public T remove() {
23 if (!s2.empty()) return s2.pop();
24 while (!s1.empty()) s2.push(s1.pop());
25 return s2.pop();
26 }
27 }
Solutions to Chapter 3 | Stacks and Queues
121 Cracking the Coding Interview | Data Structures
3.6 Write a program to sort a stack in ascending order. You should not make any assumptions about how the stack is implemented. The following are the only functions that should be used to write this program: push | pop | peek | isEmpty.
pg 52
SOLUTION
Sorting can be performed with one more stack. The idea is to pull an item from the original stack and push it on the other stack. If pushing this item would violate the sort order of the new stack, we need to remove enough items from it so that it’s possible to push the new item. Since the items we removed are on the original stack, we’re back where we started. The algorithm is O(N^2) and appears below.
1 public static Stack<Integer> sort(Stack<Integer> s) {
2 Stack<Integer> r = new Stack<Integer>();
3 while(!s.isEmpty()) {
4 int tmp = s.pop();
5 while(!r.isEmpty() && r.peek() > tmp) {
6 s.push(r.pop());
7 }
8 r.push(tmp);
9 }
10 return r;
11 }

Solutions to Chapter 4 | Trees and Graphs
Cracking the Coding Interview | Data Structures
123
4.1 Implement a function to check if a tree is balanced. For the purposes of this question, a balanced tree is defined to be a tree such that no two leaf nodes differ in distance from the root by more than one.
pg 54
SOLUTION
The idea is very simple: the difference of min depth and max depth should not exceed 1, since the difference of the min and the max depth is the maximum distance difference possible in the tree.
1 public static int maxDepth(TreeNode root) {
2 if (root == null) {
3 return 0;
4 }
5 return 1 + Math.max(maxDepth(root.left), maxDepth(root.right));
6 }
7
8 public static int minDepth(TreeNode root) {
9 if (root == null) {
10 return 0;
11 }
12 return 1 + Math.min(minDepth(root.left), minDepth(root.right));
13 }
14
15 public static boolean isBalanced(TreeNode root){
16 return (maxDepth(root) - minDepth(root) <= 1);
17 }
Solutions to Chapter 4 | Trees and Graphs
124
CareerCup.com
4.2 Given a directed graph, design an algorithm to find out whether there is a route between two nodes.
pg 54
SOLUTION
This problem can be solved by just simple graph traversal, such as depth first search or breadth first search. We start with one of the two nodes and, during traversal, check if the other node is found. We should mark any node found in the course of the algorithm as ‘already visited’ to avoid cycles and repetition of the nodes.
1 public enum State {
2 Unvisited, Visited, Visiting;
3 }
4
5 public static boolean search(Graph g, Node start, Node end) {
6 LinkedList<Node> q = new LinkedList<Node>(); // operates as Stack
7 for (Node u : g.getNodes()) {
8 u.state = State.Unvisited;
9 }
10 start.state = State.Visiting;
11 q.add(start);
12 Node u;
13 while(!q.isEmpty()) {
14 u = q.removeFirst(); // i.e., pop()
15 if (u != null) {
16 for (Node v : u.getAdjacent()) {
17 if (v.state == State.Unvisited) {
18 if (v == end) {
19 return true;
20 } else {
21 v.state = State.Visiting;
22 q.add(v);
23 }
24 }
25 }
26 u.state = State.Visited;
27 }
28 }
29 return false;
30 }
Solutions to Chapter 4 | Trees and Graphs
125 Cracking the Coding Interview | Data Structures
4.3 Given a sorted (increasing order) array, write an algorithm to create a binary tree with minimal height.
pg 54
SOLUTION
We will try to create a binary tree such that for each node, the number of nodes in the left subtree and the right subtree are equal, if possible.
Algorithm:
1. Insert into the tree the middle element of the array.
2. Insert (into the left subtree) the left subarray elements
3. Insert (into the right subtree) the right subarray elements
4. Recurse
1 public static TreeNode addToTree(int arr[], int start, int end){
2 if (end < start) {
3 return null;
4 }
5 int mid = (start + end) / 2;
6 TreeNode n = new TreeNode(arr[mid]);
7 n.left = addToTree(arr, start, mid - 1);
8 n.right = addToTree(arr, mid + 1, end);
9 return n;
10 }
11
12 public static TreeNode createMinimalBST(int array[]) {
13 return addToTree(array, 0, array.length - 1);
14 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 126
4.4 Given a binary search tree, design an algorithm which creates a linked list of all the nodes at each depth (eg, if you have a tree with depth D, you’ll have D linked lists).
pg 54
SOLUTION
We can do a simple level by level traversal of the tree, with a slight modification of the breath-first traversal of the tree.
In a usual breath first search traversal, we simply traverse the nodes without caring which level we are on. In this case, it is critical to know the level. We thus use a dummy node to indicate when we have finished one level and are starting on the next.
1 ArrayList<LinkedList<TreeNode>> findLevelLinkList(TreeNode root) {
2 int level = 0;
3 ArrayList<LinkedList<TreeNode>> result =
4 new ArrayList<LinkedList<TreeNode>>();
5 LinkedList<TreeNode> list = new LinkedList<TreeNode>();
6 list.add(root);
7 result.add(level, list);
8 while (true) {
9 list = new LinkedList<TreeNode>();
10 for (int i = 0; i < result.get(level).size(); i++) {
11 TreeNode n = (TreeNode) result.get(level).get(i);
12 if (n != null) {
13 if(n.left != null) list.add(n.left);
14 if(n.right!= null) list.add(n.right);
15 }
16 }
17 if (list.size() > 0) {
18 result.add(level + 1, list);
19 } else {
20 break;
21 }
22 level++;
23 }
24 return result;
25 }
Solutions to Chapter 4 | Trees and Graphs
127 Cracking the Coding Interview | Data Structures
4.5 Write an algorithm to find the ‘next’ node (e.g., in-order successor) of a given node in a binary search tree where each node has a link to its parent.
pg 54
SOLUTION
We approach this problem by thinking very, very carefully about what happens on an in-order traversal. On an in-order traversal, we visit X.left, then X, then X.right.
So, if we want to find X.successor(), we do the following:
1. If X has a right child, then the successor must be on the right side of X (because of the order in which we visit nodes). Specifically, the left-most child must be the first node visited in that subtree.
2. Else, we go to X’s parent (call it P).
2.a. If X was a left child (P.left = X), then P is the successor of X
2.b. If X was a right child (P.right = X), then we have fully visited P, so we call successor(P).
1 public static TreeNode inorderSucc(TreeNode e) {
2 if (e != null) {
3 TreeNode p;
4 // Found right children -> return 1st inorder node on right
5 if (e.parent == null || e.right != null) {
6 p = leftMostChild(e.right);
7 } else {
8 // Go up until we’re on left instead of right (case 2b)
9 while ((p = e.parent) != null) {
10 if (p.left == e) {
11 break;
12 }
13 e = p;
14 }
15 }
16 return p;
17 }
18 return null;
19 }
20
21 public static TreeNode leftMostChild(TreeNode e) {
22 if (e == null) return null;
23 while (e.left != null) e = e.left;
24 return e;
25 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 128
4.6 Design an algorithm and write code to find the first common ancestor of two nodes in a binary tree. Avoid storing additional nodes in a data structure. NOTE: This is not necessarily a binary search tree.
pg 54
SOLUTION
If this were a binary search tree, we could do a modified find on the two nodes and see where the paths diverge. Unfortunately, this is not a binary search tree, so we much try other approaches.
Attempt #1:
If each node has a link to its parent, we could trace p and q’s paths up until they intersect.
Attempt #2:
Alternatively, you could follow a chain in which p and q are on the same side. That is, if p and q are both on the left of the node, branch left to look for the common ancestor. When p and q are no longer on the same side, you must have found the first common ancestor.
1 public Tree commonAncestor(Tree root, Tree p, Tree q) {
2 if (covers(root.left, p) && covers(root.left, q))
3 return commonAncestor(root.left, p, q);
4 if (covers(root.right, p) && covers(root.right, q))
5 return commonAncestor(root.right, p, q);
6 return root;
7 }
8 private boolean covers(Tree root, Tree p) { /* is p a child of root? */
9 if (root == null) return false;
10 if (root == p) return true;
11 return covers(root.left, p) || covers(root.right, p);
12 }
What is the running time of this algorithm? One way of looking at this is to see how many times each node is touched. Covers touches every child node, so we know that every single node in the tree must be touched at least once, and many nodes are touched multiple times.
Attempt #3:
For any node r, we know the following:
1. If p is on one side and q is on the other, r is the first common ancestor.
2. Else, the first common ancestor is on the left or the right side.
So, we can create a simple recursive algorithm called search that calls search(left side) and search(right side) looking at how many nodes (p or q) are placed from the left side and from the right side of the current node. If there are two nodes on one of the sides, then we have
Solutions to Chapter 4 | Trees and Graphs
129 Cracking the Coding Interview | Data Structures
to check if the child node on this side is p or q (because in this case the current node is the common ancestor). If the child node is neither p nor q, we should continue to search further (starting from the child).
If one of the searched nodes (p or q) is located on the right side of the current node, then the other node is located on the other side. Thus the current node is the common ancestor.
1 static int TWO_NODES_FOUND = 2;
2 static int ONE_NODE_FOUND = 1;
3 static int NO_NODES_FOUND = 0;
4
5 // Checks how many “special” nodes are located under this root
6 int covers(TreeNode root, TreeNode p, TreeNode q) {
7 int ret = NO_NODES_FOUND;
8 if (root == null) return ret;
9 if (root == p || root == q) ret += 1;
10 ret += covers(root.left, p, q);
11 if(ret == TWO_NODES_FOUND) // Found p and q
12 return ret;
13 return ret + covers(root.right, p, q);
14 }
15
16 TreeNode commonAncestor(TreeNode root, TreeNode p, TreeNode q) {
17 if (q == p && (root.left == q || root.right == q)) return root;
18 int nodesFromLeft = covers(root.left, p, q); // Check left side
19 if (nodesFromLeft == TWO_NODES_FOUND) {
20 if(root.left == p || root.left == q) return root.left;
21 else return commonAncestor(root.left, p, q);
22 } else if (nodesFromLeft == ONE_NODE_FOUND) {
23 if (root == p) return p;
24 else if (root == q) return q;
25 }
26 int nodesFromRight = covers(root.right, p, q); // Check right side
27 if(nodesFromRight == TWO_NODES_FOUND) {
28 if(root.right == p || root.right == q) return root.right;
29 else return commonAncestor(root.right, p, q);
30 } else if (nodesFromRight == ONE_NODE_FOUND) {
31 if (root == p) return p;
32 else if (root == q) return q;
33 }
34 if (nodesFromLeft == ONE_NODE_FOUND &&
35 nodesFromRight == ONE_NODE_FOUND) return root;
36 else return null;
37 }
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 130
4.7 You have two very large binary trees: T1, with millions of nodes, and T2, with hundreds of nodes. Create an algorithm to decide if T2 is a subtree of T1.
pg 54
SOLUTION
Note that the problem here specifies that T1 has millions of nodes—this means that we should be careful of how much space we use. Let’s say, for example, T1 has 10 million nodes—this means that the data alone is about 40 mb. We could create a string representing the inorder and preorder traversals. If T2’s preorder traversal is a substring of T1’s preorder traversal, and T2’s inorder traversal is a substring of T1’s inorder traversal, then T2 is a substring of T1. We can check this using a suffix tree. However, we may hit memory limitations because suffix trees are extremely memory intensive. If this become an issue, we can use an alternative approach.
Alternative Approach: The treeMatch procedure visits each node in the small tree at most once and is called no more than once per node of the large tree. Worst case runtime is at most O(n * m), where n and m are the sizes of trees T1 and T2, respectively. If k is the number of occurrences of T2’s root in T1, the worst case runtime can be characterized as O(n + k * m).
1 boolean containsTree(TreeNode t1, TreeNode t2) {
2 if (t2 == null) return true; // The empty tree is always a subtree
3 else return subTree(t1, t2);
4 }
5
6 boolean subTree(TreeNode r1, TreeNode r2) {
7 if (r1 == null)
8 return false; // big tree empty & subtree still not found.
9 if (r1.data == r2.data) {
10 if (matchTree(r1,r2)) return true;
11 }
12 return (subTree(r1.left, r2) || subTree(r1.right, r2));
13 }
14
15 boolean matchTree(TreeNode r1, TreeNode r2) {
16 if (r2 == null && r1 == null)
17 return true; // nothing left in the subtree
18 if (r1 == null || r2 == null)
19 return false; // big tree empty & subtree still not found
20 if (r1.data != r2.data)
21 return false; // data doesn’t match
22 return (matchTree(r1.left, r2.left) &&
23 matchTree(r1.right, r2.right));
24 }
25 }
Solutions to Chapter 4 | Trees and Graphs
131 Cracking the Coding Interview | Data Structures
4.8 You are given a binary tree in which each node contains a value. Design an algorithm to print all paths which sum up to that value. Note that it can be any path in the tree - it does not have to start at the root.
pg 54
SOLUTION
Let’s approach this problem by simplifying it. What if the path had to start at the root? In that case, we would have a much easier problem:
Start from the root and branch left and right, computing the sum thus far on each path. When we find the sum, we print the current path. Note that we don’t stop just because we found the sum. Why? Because we could have the following path (assume we are looking for the sum 5): 2 + 3 + –4 + 3 + 1 + 2. If we stopped once we hit 2 + 3, we’d miss several paths (2 + 3 + -4 + 3 + 1 and 3 + -4 + 3 + 1 + 2). So, we keep going along every possible path.
Now, what if the path can start anywhere? In that case, we make a small modification. On every node, we look “up” to see if we’ve found the sum. That is—rather than asking “does this node start a path with the sum?,” we ask “does this node complete a path with the sum?”
1 void findSum(TreeNode head, int sum, ArrayList<Integer> buffer,
2 int level) {
3 if (head == null) return;
4 int tmp = sum;
5 buffer.add(head.data);
6 for (int i = level;i >- 1; i--){
7 tmp -= buffer.get(i);
8 if (tmp == 0) print(buffer, i, level);
9 }
10 ArrayList<Integer> c1 = (ArrayList<Integer>) buffer.clone();
11 ArrayList<Integer> c2 = (ArrayList<Integer>) buffer.clone();
12 findSum(head.left, sum, c1, level + 1);
13 findSum(head.right, sum, c2, level + 1);
14 }
15
16 void print(ArrayList<Integer> buffer, int level, int i2) {
17 for (int i = level; i <= i2; i++) {
18 System.out.print(buffer.get(i) + “ ”);
19 }
20 System.out.println();
21 }
What is the time complexity of this algorithm? Well, if a node is at level r, we do r amount of work (that’s in the looking “up” step). We can take a guess at O(n lg n) (n nodes, doing an
Solutions to Chapter 4 | Trees and Graphs
. CareerCup com 132
average of lg n amount of work on each step), or we can be super mathematical:
There are 2^r nodes at level r.
1*2^1 + 2*2^2 + 3*2^3 + 4*2^4 + ... d * 2^d
= sum(r * 2^r, r from 0 to depth)
= 2 (d-1) * 2^d + 2
n = 2^d ==> d = lg n
NOTE: 2^lg(x) = x
O(2 (lg n - 1) * 2^(lg n) + 2) = O(2 (lg n - 1) * n ) = O(n lg n)
Following similar logic, our space complexity is O(n lg n).
Solutions to Chapter 5 | Bit Manipulation
Cracking the Coding Interview | Concepts and Algorithms
133
5.1 You are given two 32-bit numbers, N and M, and two bit positions, i and j. Write a method to set all bits between i and j in N equal to M (e.g., M becomes a substring of N located at i and starting at j).
EXAMPLE:
Input: N = 10000000000, M = 10101, i = 2, j = 6
Output: N = 10001010100
pg 58
SOLUTION
This code operates by clearing all bits in N between position i and j, and then ORing to put M in there.
1 public static int updateBits(int n, int m, int i, int j) {
2 int max = ~0; /* All 1’s */
3
4 // 1’s through position j, then 0’s
5 int left = max - ((1 << j) - 1);
6
7 // 1’s after position i
8 int right = ((1 << i) - 1);
9
10 // 1’s, with 0s between i and j
11 int mask = left | right;
12
13 // Clear i through j, then put m in there
14 return (n & mask) | (m << i);
15 }
Solutions to Chapter 5 | Bit Manipulation
134
CareerCup.com
5.2 Given a (decimal - e.g. 3.72) number that is passed in as a string, print the binary representation. If the number can not be represented accurately in binary, print “ERROR”
pg 58
SOLUTION
First, let’s start off by asking ourselves what a non-integer number in binary looks like. By analogy to a decimal number, the number n = 0.101 = 1 * (1/2^1) + 0 * (1/2^2) + 1 * (1/2^3).
Printing the int part of n is straight-forward (see below). To print the decimal part, we can multiply by 2 and check if the 2*n is greater than or equal to one. This is essentially “shifting” the fractional sum. That is:
r = 2*n = 2*0.101 = 1*(1 / 2^0) + 0*(1 / 2^1) + 1*(1 / 2^2) = 1.01
If r >= 1, then we know that n had a 1 right after the decimal point. By doing this continuously, we can check every digit.
1 public static String printBinary(String n) {
2 int intPart = Integer.parseInt(n.substring(0, n.indexOf(‘.’)));
3 double decPart = Double.parseDouble(
4 n.substring(n.indexOf(‘.’), n.length()));
5 String int_string = “”;
6 while (intPart > 0) {
7 int r = intPart % 2;
8 intPart >>= 1;
9 int_string = r + int_string;
10 }
11 StringBuffer dec_string = new StringBuffer();
12 while (decPart > 0) {
13 if (dec_string.length() > 32) return “ERROR”;
14 if (decPart == 1) {
15 dec_string.append((int)decPart);
16 break;
17 }
18 double r = decPart * 2;
19 if (r >= 1) {
20 dec_string.append(1);
21 decPart = r - 1;
22 } else {
23 dec_string.append(0);
24 decPart = r;
25 }
26 }
27 return int_string + “.” + dec_string.toString();
28 }
Solutions to Chapter 5 | Bit Manipulation
135 Cracking the Coding Interview | Concepts and Algorithms
5.3 Given an integer, print the next smallest and next largest number that have the same number of 1 bits in their binary representation.
pg 58
SOLUTION
The Brute Force Approach:
An easy approach is simply brute force: count the number of 1’s in n, and then increment (or decrement) until you find a number with the same number of 1’s. Easy - but not terribly interesting. Can we do something a bit more optimal? Yes!
Number Properties Approach for Next Number
Observations:
»»If we “turn on” a 0, we need to “turn off” a 1
»»If we turn on a 0 at bit i and turn off a 1 at bit j, the number changes by 2^i - 2^j.
»»If we want to get a bigger number with the same number of 1s and 0s, i must be bigger than j.
Solution:
1. Traverse from right to left. Once we’ve passed a 1, turn on the next 0. We’ve now increased the number by 2^i. Yikes! Example: xxxxx011100 becomes xxxxx111100
2. Turn off the one that’s just to the right side of that. We’re now bigger by 2^i - 2^(i-1) Example: xxxxx111100 becomes xxxxx101100
3. Make the number as small as possible by rearranging all the 1s to be as far right as possible: Example: xxxxx101100 becomes xxxxx100011
To get the previous number, we do the reverse.
1. Traverse from right to left. Once we’ve passed a zero, turn off the next 1. Example: xxxxx100011 becomes xxxxx000011.
2. Turn on the 0 that is directly to the right. Example: xxxxx000011 becomes xxxxx010011.
3. Make the number as big as possible by shifting all the ones as far to the left as possible. Example: xxxxx010011 becomes xxxxx011100 .
And now, for the code. Note the emphasis on pulling common code out into a reusable function. Your interviewer will look for “clean code” like this.
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 136
1 public static boolean GetBit(int n, int index) {
2 return ((n & (1 << index)) > 0);
3 }
4
5 public static int SetBit(int n, int index, boolean b) {
6 if (b) {
7 return n | (1 << index);
8 } else {
9 int mask = ~(1 << index);
10 return n & mask;
11 }
12 }
13
14 public static int GetNext_NP(int n) {
15 if (n <= 0) return -1;
16
17 int index = 0;
18 int countOnes = 0;
19
20 // Find first one.
21 while (!GetBit(n, index)) index++;
22
23 // Turn on next zero.
24 while (GetBit(n, index)) {
25 index++;
26 countOnes++;
27 }
28 n = SetBit(n, index, true);
29
30 // Turn off previous one
31 index--;
32 n = SetBit(n, index, false);
33 countOnes--;
34
35 // Set zeros
36 for (int i = index - 1; i >= countOnes; i--) {
37 n = SetBit(n, i, false);
38 }
39
40 // Set ones
41 for (int i = countOnes - 1; i >= 0; i--) {
42 n = SetBit(n, i, true);
43 }
44
45 return n;
Solutions to Chapter 5 | Bit Manipulation
137 Cracking the Coding Interview | Concepts and Algorithms
46 }
47
48 public static int GetPrevious_NP(int n) {
49 if (n <= 0) return -1; // Error
50
51 int index = 0;
52 int countZeros = 0;
53
54 // Find first zero.
55 while (GetBit(n, index)) index++;
56
57 // Turn off next 1.
58 while (!GetBit(n, index)) {
59 index++;
60 countZeros++;
61 }
62 n = SetBit(n, index, false);
63
64 // Turn on previous zero
65 index--;
66 n = SetBit(n, index, true);
67 countZeros--;
68
69 // Set ones
70 for (int i = index - 1; i >= countZeros; i--) {
71 n = SetBit(n, i, true);
72 }
73
74 // Set zeros
75 for (int i = countZeros - 1; i >= 0; i--) {
76 n = SetBit(n, i, false);
77 }
78
79 return n;
80 }
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 138
5.4 Explain what the following code does: ((n & (n-1)) == 0).
pg 58
SOLUTION
We can work backwards to solve this question.
What does it mean if A & B == 0?
It means that A and B never have a 1 bit in the same place. So if n & (n-1) == 0, then n and n-1 never share a 1.
What does n-1 look like (as compared with n)?
Try doing subtraction by hand (in base 2 or 10). What happens?
1101011000 [base 2]
- 1
= 1101010111 [base 2]
593100 [base 10]
- 1
= 593099 [base 10]
When you subtract 1 from a number, you look at the least significant bit. If it’s a 1 you change it to zero and you are done. If it’s a zero, you must “borrow” from a larger bit. So, you go to increasingly larger bits, changing each bit from a 0 to a 1, until you find a 1. You flip that one to a 0 and you are done.
Thus, n-1 will look like n, except that n’s initial 0s will be 1’s in n-1, and n’s least significant 1 will be a 0 in (n-1). That is:
if n = abcde1000
then n-1 = abcde0111
So what does n & (n-1) == 0 indicate?
n and (n-1) must have no 1s in common. Given that they look like this:
if n = abcde1000
then n-1 = abcde0111
abcde must be all 0s, which means that n must look like this: 00001000. n is therefore a power of two.
So, we have our answer: ((n & (n-1)) == 0) checks if n is a power of 2 (or 0).
Solutions to Chapter 5 | Bit Manipulation
139 Cracking the Coding Interview | Concepts and Algorithms
5.5 Write a function to determine the number of bits required to convert integer A to integer B.
Input: 31, 14
Output: 2
pg 58
SOLUTION
This seemingly complex problem is actually rather straightforward. To approach this, ask yourself how you would figure out which bits in two numbers are different. Simple: with an xor.
Each 1 in the xor will represent one different bit between A and B. We then simply need to count the number of bits that are 1.
1 public static int bitSwapRequired(int a, int b) {
2 int count = 0;
3 for (int c = a ^ b; c != 0; c = c >> 1) {
4 count += c & 1;
5 }
6 return count;
7 }
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 140
5.6 Write a program to swap odd and even bits in an integer with as few instructions as possible (e.g., bit 0 and bit 1 are swapped, bit 2 and bit 3 are swapped, etc).
pg 58
SOLUTION
Mask all odd bits with 10101010 in binary (which is 0xAA), then shift them left to put them in the even bits. Then, perform a similar operation for even bits. This takes a total 5 instructions.
1 public static int swapOddEvenBits(int x) {
2 return ( ((x & 0xaaaaaaaa) >> 1) | ((x & 0x55555555) << 1) );
3 }
Solutions to Chapter 5 | Bit Manipulation
141 Cracking the Coding Interview | Concepts and Algorithms
5.7 An array A[1... n] contains all the integers from 0 to n except for one number which is missing. In this problem, we cannot access an entire integer in A with a single operation. The elements of A are represented in binary, and the only operation we can use to access them is “fetch the jth bit of A[i]”, which takes constant time. Write code to find the missing integer. Can you do it in O(n) time?
pg 58
SOLUTION
Picture a list of binary numbers between 0 to n. What will change when we remove one number? We’ll get an imbalance of 1s and 0s in the least significant bit. That is, before removing the number k, we have this list of least significant bits (in some order):
0 0 0 0 0 1 1 1 1 1 OR 0 0 0 0 0 1 1 1 1
Suppose we secretly removed either a 1 or a 0 from this list. Could we tell which one was removed?
remove(0 from 0 0 0 0 0 1 1 1 1 1) --> 0 0 0 0 1 1 1 1 1
remove(1 from 0 0 0 0 0 1 1 1 1 1) --> 0 0 0 0 0 1 1 1 1
remove(0 from 0 0 0 0 0 1 1 1 1) --> 0 0 0 0 1 1 1 1
remove(1 from 0 0 0 0 0 1 1 1 1) --> 0 0 0 0 0 1 1 1
Note that if 0 is removed, we always wind up with count(1) >= count(0). If 1 is removed, we wind up with count(1) < count(0). Therefore, we can look at the least significant bit to figure out in O(N) time whether the missing number has a 0 or a 1 in the least significant bit (LSB). If LSB(missing) == 0, then we can discard all numbers with LSB = 1. If LSB(missing) == 1, we can discard all numbers with LSB = 0.
What about the next iteration, with the second least significant bit (SLSB)? We’ve discarded all the numbers with LSB = 1, so our list looks something like this (if n = 5, and missing = 3):
00000
00001
00010
-----
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
Our SLSBs now look like 0 0 1 0 1 0. Using the same logic as we applied for LSB, we can figure out that the missing number must have SLSB = 1. Our number must look like xxxx11.
Third iteration, discarding numbers with SLSB = 0:
00000
00001
00010
-----
00100
00101
00110
00111
01000
01001
01010
01011
01100
01101
We can now compute that count(TLSB = 1) = 1 and count(TLSB = 1) = 1. Therefore, TLSB = 0. We can recurse repeatedly, building our number bit by bit:
Solutions to Chapter 5 | Bit Manipulation
. CareerCup com 142
1 int findMissing(ArrayList<BitInteger> array) {
2 return findMissing(array, BitInteger.INTEGER_SIZE - 1);
3 }
4
5 int findMissing(ArrayList<BitInteger> input, int column) {
6 if (column < 0) { // Base case and error condition
7 return 0;
8 }
9 ArrayList<BitInteger> oddIndices = new ArrayList<BitInteger>();
10 ArrayList<BitInteger> evenIndices = new ArrayList<BitInteger>();
11 for (BitInteger t : input) {
12 if (t.fetch(column) == 0) {
13 evenIndices.add(t);
14 } else {
15 oddIndices.add(t);
16 }
17 }
18 if (oddIndices.size() >= evenIndices.size()) {
19 return (findMissing(evenIndices, column - 1)) << 1 | 0;
20 } else {
21 return (findMissing(oddIndices, column - 1)) << 1 | 1;
22 }
23 }
24
What is the run-time of this algorithm? On the first pass, we look at O(N) bits. On the second pass, we’ve eliminated N/2 numbers, so we then look at O(N/2) bits. On the third pass, we have eliminated another half of the numbers, so we then look at O(N/4) bits. If we keep going, we get an equation that looks like:
O(N) + O(N/2) + O(N/4) + O(N/8) + ... = O(2N) = O(N)
Our run-time is O(N).
Solutions to Chapter 6 | Brain Teasers
Cracking the Coding Interview | Concepts and Algorithms
143
6.1 Add arithmetic operators (plus, minus, times, divide) to make the following expression true: 3 1 3 6 = 8. You can use any parentheses you’d like.
pg 60
SOLUTION
An interviewer is asking this problem to see how you think and approach problems—so don’t just guess randomly.
Try approaching this the following way: What sorts of operations would get us to 8? I can think of a few:
4 * 2 = 8
16 / 2 = 8
4 + 4 = 8
Let’s start with the first one. Is there any way to make 3 1 3 6 produce 4 * 2? We can easily notice that 3 + 1 = 4 (the first two numbers). We can also notice that 6 / 3 = 2. If we had “3 1 6 3”, we’d be done, since (3 + 1)*(6 / 3) = 8. Although it seems a little unconventional to do this, we can, in fact, just flip the 6 and the 3 to get the solution:
(( 3 + 1 ) / 3) * 6 = 8
Solutions to Chapter 6 | Brain Teasers
144
CareerCup.com
6.2 There is an 8x8 chess board in which two diagonally opposite corners have been cut off. You are given 31 dominos, and a single domino can cover exactly two squares. Can you use the 31 dominos to cover the entire board? Prove your answer (by providing an example, or showing why it’s impossible).
pg 60
SOLUTION
Impossible. Here’s why: The chess board initially has 32 black and 32 white squares. By removing opposite corners (which must be the same color), we’re left with 30 of one color and 32 of the other color. Let’s say, for the sake of argument, that we have 30 black and 32 white squares.
When we lay down each domino, we’re taking up one white and one black square. Therefore, 31 dominos will take up 31 white squares and 31 black squares exactly. On this board, however, we must have 30 black squares and 32 white squares. Hence, it is impossible.
Solutions to Chapter 6 | Brain Teasers
145 Cracking the Coding Interview | Concepts and Algorithms
6.3 You have a five quart jug and a three quart jug, and an unlimited supply of water (but no measuring cups). How would you come up with exactly four quarts of water?
NOTE: The jugs are oddly shaped, such that filling up exactly ‘half’ of the jug would be impossible.
pg 60
SOLUTION
We can pour water back and forth between the two jugs as follows:
5 Quart Contents
3 Quart Contents
Note
5
0
Filled 5 quart jug
2
3
Filled 3Q with 5Q’s contents
0
2
Dumped 3Q
5
2
Filled 5Q
4
3
Fill remainder of 3Q with 5Q
4
Done! We have four quarts.
OBSERVATIONS AND SUGGESTIONS:
»»Many brain teasers have a math / CS root to them—this is one of them! Note that as long as the two jug sizes are relatively prime (i.e., have no common prime factors), you can find a pour sequence for any value between 1 and the sum of the jug sizes.
Solutions to Chapter 6 | Brain Teasers
. CareerCup com 146
6.4 A bunch of men are on an island. A genie comes down and gathers everyone together and places a magical hat on some people’s heads (i.e., at least one person has a hat). The hat is magical: it can be seen by other people, but not by the wearer of the hat himself. To remove the hat, those (and only those who have a hat) must dunk themselves underwater at exactly midnight. If there are n people and c hats, how long does it take the men to remove the hats? The men cannot tell each other (in any way) that they have a hat.
FOLLOW UP
Prove that your solution is correct.
pg 60
SOLUTION
This problem seems hard, so let’s simplify it by looking at specific cases.
Case c = 1: Exactly one man is wearing a hat.
Assuming all the men are intelligent, the man with the hat should look around and realize that no one else is wearing a hat. Since the genie said that at least one person is wearing a hat, he must conclude that he is wearing a hat. Therefore, he would be able to remove it that night.
Case c = 2: Exactly two men are wearing hats.
The two men with hats see one hat, and are unsure whether c = 1 or c = 2. They know, from the previous case, that if c = 1, the hats would be removed on Night #1. Therefore, if the other man still has a hat, he must deduce that c = 2, which means that he has a hat. Both men would then remove the hats on Night #2
Case General: If c = 3, then each man is unsure whether c = 2 or 3. If it were 2, the hats would be removed on Night #2. If they are not, they must deduce that c = 3, and therefore they have a hat. We can follow this logic for c = 4, 5, …
Proof by Induction
Using induction to prove a statement P(n)
If (1) P(1) = TRUE (e.g., the statement is true when n = 1)
AND (2) if P(n) = TRUE -> P(n+1) = TRUE (e.g., P(n+1) is true whenever P(2) is true).
THEN P(n) = TRUE for all n >= 1.
Explanation
»»Condition 2 sets up an infinite deduction chain: P(1) implies P(2) implies P(3) implies ... P(n) implies P(n+1) implies ...
Solutions to Chapter 6 | Brain Teasers
147 Cracking the Coding Interview | Concepts and Algorithms
»»Condition one (P(1) is true) ignites this chain, with truth cascading off into infinity.
Base Case: c = 1 (See previous page).
Assume true for c hats. i.e., if there are c hats, it will take c nights to remove all of them.
Prove true for c+1 hats.
Each man with a hat sees c hat, and can not be immediately sure whether there are c hats or c+1 hats. However, he knows that if there are c hats, it will take exactly c nights to remove them. Therefore, when c nights have passed and everyone still has a hat, he can only conclude that there are c+1 hats. He must know that he is wearing a hat. Each man makes the same conclusion and simultaneously removes the hats on night c+1.
Therefore, we have met the principles of induction. We have proven that it will take c nights to remove c hats.
Solutions to Chapter 6 | Brain Teasers
. CareerCup com 148
6.5 There is a building of 100 floors. If an egg drops from the Nth floor or above it will break. If it’s dropped from any floor below, it will not break. You’re given 2 eggs. Find N, while minimizing the number of drops for the worst case.
pg 60
SOLUTION
Observation: Regardless of how we drop Egg1, Egg2 must do a linear search. i.e., if Egg1 breaks between floor 10 and 15, we have to check every floor in between with the Egg2
The Approach:
A First Try: Suppose we drop an egg from the 10th floor, then the 20th, …
»»If the first egg breaks on the first drop (Floor 10), then we have at most 10 drops total.
»»If the first egg breaks on the last drop (Floor 100), then we have at most 19 drops total (floors 10, 20, ... ,90, 100, then 91 through 99).
»»That’s pretty good, but all we’ve considered is the absolute worst case. We should do some “load balancing” to make those two cases more even.
Goal: Create a system for dropping Egg1 so that the most drops required is consistent, whether Egg1 breaks on the first drop or the last drop.
1. A perfectly load balanced system would be one in which Drops of Egg1 + Drops of Egg2 is always the same, regardless of where Egg1 broke.
2. For that to be the case, since each drop of Egg1 takes one more step, Egg2 is allowed one fewer step.
3. We must, therefore, reduce the number of steps potentially required by Egg2 by one drop each time. For example, if Egg1 is dropped on Floor 20 and then Floor 30, Egg2 is potentially required to take 9 steps. When we drop Egg1 again, we must reduce potential Egg2 steps to only 8. That is, we must drop Egg1 at floor 39.
4. We know, therefore, Egg1 must start at Floor X, then go up by X-1 floors, then X-2, …, until it gets to 100.
5. Solve for X+(X-1)+(X-2)+…+1 = 100. X(X+1)/2 = 100 -> X = 14
We go to Floor 14, then 27, then 39, … This takes 14 steps maximum.
Solutions to Chapter 6 | Brain Teasers
149 Cracking the Coding Interview | Concepts and Algorithms
6.6 There are one hundred closed lockers in a hallway. A man begins by opening all one hundred lockers. Next, he closes every second locker. Then he goes to every third locker and closes it if it is open or opens it if it is closed (e.g., he toggles every third locker). After his one hundredth pass in the hallway, in which he toggles only locker number one hundred, how many lockers are open?
pg 60
SOLUTION
Question: For which rounds is a door toggled (open or closed)?
A door n is toggled once for each factor of n, including itself and 1. That is, door 15 is toggled on round 1, 3, 5, and 15.
Question: When would a door be left open?
Answer: A door is left open if the number of factors (x) is odd. You can think about this by pairing factors off as an open and a close. If there’s one remaining, the door will be open.
Question: When would x be odd?
Answer: x is odd if n is a perfect square. Here’s why: pair n’s factors by their complements. For example, if n is 36, the factors are (1, 36), (2, 18), (3, 12), (4, 9), (6, 6). Note that (6, 6) only contributes 1 factor, thus giving n an odd number of factors.
Question: How many perfect squares are there?
Answer: There are 10 perfect squares. You could count them (1, 4, 9, 16, 25, 36, 49, 64, 81, 100), or you could simply realize that you can take the numbers 1 through 10 and square them (1*1, 2*2, 3*3, ... , 10*10).
Therefore, there are 10 lockers open.

Solutions to Chapter 7 | Object Oriented Design
Cracking the Coding Interview | Concepts and Algorithms
151
7.1 Design the data structures for a generic deck of cards. Explain how you would subclass it to implement particular card games.
pg 62
SOLUTION
1 public class Card {
2 public enum Suit {
3 CLUBS (1), SPADES (2), HEARTS (3), DIAMONDS (4);
4 int value;
5 private Suit(int v) { value = v; }
6 };
7
8 private int card;
9 private Suit suit;
10
11 public Card(int r, Suit s) {
12 card = r;
13 suit = s;
14 }
15
16 public int value() { return card; }
17 public Suit suit() { return suit; }
18 }
Assume that we’re building a blackjack game, so we need to know the value of the cards. Face cards are ten and an ace is 11 (most of the time, but that’s the job of the Hand class, not the following class).
1 public class BlackJackCard extends Card {
2 public BlackJackCard(int r, Suit s) { super(r, s); }
3
4 public int value() {
5 int r = super.value();
6 if (r == 1) return 11; // aces are 11
7 if (r < 10) return r;
8 return 10;
9 }
10
11 boolean isAce() {
12 return super.value() == 1;
13 }
14 }
Solutions to Chapter 7 | Object Oriented Design
152
CareerCup.com
7.2 Imagine you have a call center with three levels of employees: fresher, technical lead (TL), product manager (PM). There can be multiple employees, but only one TL or PM. An incoming telephone call must be allocated to a fresher who is free. If a fresher can’t handle the call, he or she must escalate the call to technical lead. If the TL is not free or not able to handle it, then the call should be escalated to PM. Design the classes and data structures for this problem. Implement a method getCallHandler().
pg 62
SOLUTION
All three ranks of employees have different work to be done, so those specific functions are profile specific. We should keep these specific things within their respective class.
There are a few things which are common to them, like address, name, job title, age, etc. These things can be kept in one class and can be extended / inherited by others.
Finally, there should be one CallHandler class which would route the calls to the concerned person.
NOTE: On any object oriented design question, there are many ways to design the objects. Discuss the trade-offs of different solutions with your interviewer. You should usually design for long term code flexibility and maintenance.
1 public class CallHandler {
2 static final int LEVELS = 3; // we have 3 levels of employees
3 static final int NUM_FRESHERS = 5; // we have 5 freshers
4 ArrayList<Employee>[] employeeLevels = new ArrayList[LEVELS];
5 // queues for each call’s rank
6 Queue<Call>[] callQueues = new LinkedList[LEVELS];
7
8 public CallHandler() { ... }
9
10 Employee getCallHandler(Call call) {
11 for (int level = call.rank; level < LEVELS - 1; level++) {
12 ArrayList<Employee> employeeLevel = employeeLevels[level];
13 for (Employee emp : employeeLevel) {
14 if (emp.free) {
15 return emp;
16 }
17 }
18 }
19 return null;
20 }
21
22 // routes the call to an available employee, or adds to a queue
Solutions to Chapter 7 | Object Oriented Design
153 Cracking the Coding Interview | Concepts and Algorithms
23 void dispatchCall(Call call) {
24 // try to route the call to an employee with minimal rank
25 Employee emp = getCallHandler(call);
26 if (emp != null) {
27 emp.ReceiveCall(call);
28 } else {
29 // place the call into queue according to its rank
30 callQueues[call.rank].add(call);
31 }
32 }
33 void getNextCall(Employee e) {...} // look for call for e’s rank
34 }
35
36 class Call {
37 int rank = 0; // minimal rank of employee who can handle this call
38 public void reply(String message) { ... }
39 public void disconnect() { ... }
40 }
41
42 class Employee {
43 CallHandler callHandler;
44 int rank; // 0- fresher, 1 - technical lead, 2 - product manager
45 boolean free;
46 Employee(int rank) { this.rank = rank; }
47 void ReceiveCall(Call call) { ... }
48 void CallHandled(Call call) { ... } // call is complete
49 void CannotHandle(Call call) { // escalate call
50 call.rank = rank + 1;
51 callHandler.dispatchCall(call);
52 free = true;
53 callHandler.getNextCall(this); // look for waiting call
54 }
55 }
56
57 class Fresher extends Employee {
58 public Fresher() { super(0); }
59 }
60 class TechLead extends Employee {
61 public TechLead() { super(1); }
62 }
63 class ProductManager extends Employee {
64 public ProductManager() { super(2); }
65 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 154
7.3 Design a musical juke box using object oriented principles.
pg 62
SOLUTION
Let’s first understand the basic system components:
»»CD player
»»CD
»»Display () (displays length of song, remaining time and playlist)
Now, let’s break this down further:
»»Playlist creation (includes add, delete, shuffle etc sub functionalities)
»»CD selector
»»Track selector
»»Queueing up a song
»»Get next song from playlist
A user also can be introduced:
»»Adding
»»Deleting
»»Credit information
How do we group this functionality based on Objects (data + functions which go together)?
Object oriented design suggests wrapping up data with their operating functions in a single entity class.
1 public class CD { }
2 public class CDPlayer {
3 private Playlist p;
4 private CD c;
5 public Playlist getPlaylist() { return p; }
6 public void setPlaylist(Playlist p) { this.p = p; }
7 public CD getCD() { return c; }
8 public void setCD(CD c) { this.c = c; }
9 public CDPlayer(Playlist p) { this.p = p; }
10 public CDPlayer(CD c, Playlist p) { ... }
11 public CDPlayer(CD c) { this.c = c; }
12 public void playTrack(Song s) { ... }
13 }
14
15 public class JukeBox {
Solutions to Chapter 7 | Object Oriented Design
155 Cracking the Coding Interview | Concepts and Algorithms
16 private CDPlayer cdPlayer;
17 private User user;
18 private Set<CD> cdCollection;
19 private TrackSelector ts;
20
21 public JukeBox(CDPlayer cdPlayer, User user, Set<CD> cdCollection,
22 TrackSelector ts) { ... }
23 public Song getCurrentTrack() { return ts.getCurrentSong(); }
24 public void processOneUser(User u) { this.user = u; }
25 }
26
27 public class Playlist {
28 private Song track;
29 private Queue<Song> queue;
30 public Playlist(Song track, Queue<Song> queue) { ... }
31 public Song getNextTrackToPlay(){ return queue.peek(); }
32 public void queueUpTrack(Song s){ queue.add(s); }
33 }
34
35 public class Song {
36 private String songName;
37 }
38
39 public class TrackSelector {
40 private Song currentSong;
41 public TrackSelector(Song s) { currentSong=s; }
42 public void setTrack(Song s) { currentSong = s; }
43 public Song getCurrentSong() { return currentSong; }
44 }
45
46 public class User {
47 private String name;
48 public String getName() { return name; }
49 public void setName(String name) { this.name = name; }
50 public long getID() { return ID; }
51 public void setID(long iD) { ID = iD; }
52 private long ID;
53 public User(String name, long iD) { ... }
54 public User getUser() { return this; }
55 public static User addUser(String name, long iD) { ... }
56 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 156
7.4 Design a chess game using object oriented principles.
pg 62
SOLUTION
1 public class ChessPieceTurn { };
2 public class GameManager {
3 void processTurn(PlayerBase player) { };
4 boolean acceptTurn(ChessPieceTurn turn) { return true; };
5 Position currentPosition;
6 }
7
8 public abstract class PlayerBase {
9 public abstract ChessPieceTurn getTurn(Position p);
10 }
11 class ComputerPlayer extends PlayerBase {
12 public ChessPieceTurn getTurn(Position p) { return null; }
13 public void setDifficulty() { };
14 public PositionEstimator estimater;
15 public PositionBackTracker backtracter;
16 }
17 public class HumanPlayer extends PlayerBase {
18 public ChessPieceTurn getTurn(Position p) { return null; }
19 }
20
21 public abstract class ChessPieceBase {
22 abstract boolean canBeChecked();
23 abstract boolean isSupportCastle();
24 }
25 public class King extends ChessPieceBase { ... }
26 public class Queen extends ChessPieceBase { ... }
27
28 public class Position { // represents chess positions in compact form
29 ArrayList<ChessPieceBase> black;
30 ArrayList<ChessPieceBase> white;
31 }
32
33 public class PositionBackTracker {
34 public static Position getNext(Position p) { return null; }
35 }
36 public class PositionEstimator {
37 public static PositionPotentialValue estimate(Position p) { ... }
38 }
39 public abstract class PositionPotentialValue {
40 abstract boolean lessThan(PositionPotentialValue pv);
41 }
Solutions to Chapter 7 | Object Oriented Design
157 Cracking the Coding Interview | Concepts and Algorithms
7.5 Design the data structures for an online book reader system.
pg 62
SOLUTION
Since the problem doesn’t describe much about the functionality, let’s assume we want to design a basic online reading system which provides the following functionality:
»»User membership creation and extension.
»»Searching the database of books
»»Reading the books
To implement these we may require many other functions, like get, set, update, etc. Objects required would likely include User, Book, and Library.
The following code / object oriented design describes this functionality:
1 public class Book {
2 private long ID;
3 private String details;
4 private static Set<Book> books;
5
6 public Book(long iD, String details) { ... }
7 public static void addBook(long iD, String details){
8 books.add(new Book(iD, details));
9 }
10
11 public void update() { }
12 public static void delete(Book b) { books.remove(b); }
13 public static Book find(long id){
14 for (Book b : books)
15 if(b.getID() == id) return b;
16 return null;
17 }
18 }
19
20 public class User {
21 private long ID;
22 private String details;
23 private int accountType;
24 private static Set<User> users;
25
26 public Book searchLibrary(long id) { return Book.find(id); }
27 public void renewMembership() { ... }
28
29 public static User find(long ID) {
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 158
30 for (User u : users) {
31 if (u.getID() == ID) return u;
32 }
33 return null;
34 }
35
36 public static void addUser(long ID, String details,
37 int accountType) {
38 users.add(new User(ID, details, accountType));
39 }
40
41 public User(long iD, String details, int accountType) { ... }
42 }
43
44 public class OnlineReaderSystem {
45 private Book b;
46 private User u;
47 public OnlineReaderSystem(Book b, User u) { ... }
48 public void listenRequest() { }
49 public Book searchBook(long ID) { return Book.find(ID); }
50 public User searchUser(long ID){ return User.find(ID); }
51 public void display() { }
52 }
This design is a very simplistic implementation of such a system. We have a class for User to keep all the information regarding the user, and an identifier to identify each user uniquely. We can add functionality like registering the user, charging a membership amount and monthly / daily quota, etc.
Next, we have book class where we will keep all the book’s information. We would also implement functions like add / delete / update books.
Finally, we have a manager class for managing the online book reader system which would have a listen function to listen for any incoming requests to log in. It also provides book search functionality and display functionality. Because the end user interacts through this class, search must be implemented here.
Solutions to Chapter 7 | Object Oriented Design
159 Cracking the Coding Interview | Concepts and Algorithms
7.6 Implement a jigsaw puzzle. Design the data structures and explain an algorithm to solve the puzzle.
pg 62
SOLUTION
1 class Edge {
2 enum Type { inner, outer, flat }
3 Piece parent;
4 Type type;
5 bool fitsWith(Edge type) { ... }; // Inners & outer fit together.
6 }
7 class Piece {
8 Edge left, right, top, bottom;
9 Orientation solvedOrientation = ...; // 90, 180, etc
10 }
11 class Puzzle {
12 Piece[][] pieces; /* Remaining pieces left to put away. */
13 Piece[][] solution;
14 Edge[] inners, outers, flats;
15 /* We’re going to solve this by working our way in-wards, starting
16 * with the corners. This is a list of the inside edges. */
17 Edge[] exposed_edges;
18
19 void sort() {
20 /* Iterate through all edges, adding each to inners, outers,
21 * etc, as appropriate. Look for the corners—add those to
22 * solution. Add each non-flat edge of the corner to
23 * exposed_edges. */
24 }
25
26 void solve() {
27 foreach edge1 in exposed_edges {
28 /* Look for a match to edge1 */
29 if (edge1.type == Edge.Type.inner) {
30 foreach edge2 in outers {
31 if edge1.fitsWith(edge2) {
32 /* We found a match! Remove edge1 from
33 * exposed_edges. Add edge2’s piece to
34 * solution. Check which edges of edge2 are
35 * exposed, and add those to exposed_edges. */
36 }
37 }
38 /* Do the same thing, swapping inner & outer. */
39 }
40 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 160
41 }
42 }
Overview:
1. We grouped the edges by their type. Because inners go with outers, and vice versa, this enables us to go straight to the potential matches.
We keep track of the inner perimeter of the puzzle (exposed_edges) as we work our way inwards. exposed_edges is initialized to be the corner’s edges.
Solutions to Chapter 7 | Object Oriented Design
161 Cracking the Coding Interview | Concepts and Algorithms
7.7 Explain how you would design a chat server. In particular, provide details about the various backend components, classes, and methods. What would be the hardest problems to solve?
pg 62
SOLUTION
What is our chat server?
This is something you should discuss with your interviewer, but let’s make a couple of assumptions: imagine we’re designing a basic chat server that needs to support a small number of users. People have a contact list, they see who is online vs offline, and they can send text-based messages to them. We will not worry about supporting group chat, voice chat, etc. We will also assume that contact lists are mutual: I can only talk to you if you can talk to me. Let’s keep it simple.
What specific actions does it need to support?
»»User A signs online
»»User A asks for their contact list, with each person’s current status.
»»Friends of User A now see User A as online
»»User A adds User B to contact list
»»User A sends text-based message to User B
»»User A changes status message and/or status type
»»User A removes User B
»»User A signs offline
What can we learn about these requirements?
We must have a concept of users, add request status, online status, and messages.
What are the core components? We’ll need a database to store items and an “always online” application as the server. We might recommend using XML for the communication between the chat server and the clients, as it’s easy for a person and a machine to read.
What are the key objects and methods?
We have listed the key objects and methods below. Note that we have hidden many of the details, such as how to actually push the data out to a client.
1 enum StatusType {
2 online, offline, away;
3 }
4
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 162
5 class Status {
6 StatusType status_type;
7 String status_message;
8 }
9
10 class User {
11 String username;
12 String display_name;
13 User[] contact_list;
14 AddRequest[] requests;
15 boolean updateStatus(StatusType stype, String message) { … };
16 boolean addUserWithUsername(String name);
17 boolean approveRequest(String username);
18 boolean denyRequest(String username);
19 boolean removeContact(String username);
20 boolean sendMessage(String username, String message);
21 }
22 /* Holds data that from_user would like to add to_user */
23 class AddRequest {
24 User from_user;
25 User to_user;
26 }
27 class Server {
28 User getUserByUsername(String username);
29 }
What problems would be the hardest to solve (or the most interesting)?
Q1 How do we know if someone is online—I mean, really, really know?
While we would like users to tell us when they sign off, we can’t know for sure. A user’s connection might have died, for example. To make sure that we know when a user has signed off, we might try regularly pinging the client to make sure it’s still there.
Q2 How do we deal with conflicting information?
We have some information stored in the computer’s memory and some in the database. What happens if they get out of sync? Which one is “right”?
Q3 How do we make our server scale?
While we designed out chat server without worrying—too much– about scalability, in real life this would be a concern. We’d need to split our data across many servers, which would increase our concern about out of sync data.
Q4 How we do prevent denial of service attacks?
Clients can push data to us—what if they try to DOS us? How do we prevent that?
Solutions to Chapter 7 | Object Oriented Design
163 Cracking the Coding Interview | Concepts and Algorithms
7.8 Othello is played as follows: Each Othello piece is white on one side and black on the other. When a piece is surrounded by its opponents on both the left and right sides, or both the top and bottom, it is said to be captured and its color is flipped. On your turn, you must capture at least one of your opponent’s pieces. The game ends when either user has no more valid moves, and the win is assigned to the person with the most pieces. Implement the object oriented design for Othello.
pg 62
SOLUTION
Othello has these major steps:
2. Game () which would be the main function to manage all the activity in the game:
3. Initialize the game which will be done by constructor
4. Get first user input
5. Validate the input
6. Change board configuration
7. Check if someone has won the game
8. Get second user input
9. Validate the input
10. Change the board configuration
11. Check if someone has won the game...
NOTE: The full code for Othello is contained in the code attachment.
1 public class Question {
2 private final int white = 1;
3 private final int black = 2;
4 private int[][] board;
5
6 /* Sets up the board in the standard othello starting positions,
7 * and starts the game */
8 public void start () { ... }
9
10 /* Returns the winner, if any. If there are no winners, returns
11 * 0 */
12 private int won() {
13 if (!canGo (white) && !canGo (black)) {
14 int count = 0;
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 164
15 for (int i = 0; i < 8; i++) {
16 for (int j = 0; j < 8; j++) {
17 if (board [i] [j] == white) {
18 count++;
19 }
20 if (board [i] [j] == black) {
21 count--;
22 }
23 }
24 }
25 if (count > 0) return white;
26 if (count < 0) return black;
27 return 3;
28 }
29 return 0;
30 }
31
32 /* Returns whether the player of the specified color has a valid
33 * move in his turn. This will return false when
34 * 1. none of his pieces are present
35 * 2. none of his moves result in him gaining new pieces
36 * 3. the board is filled up
37 */
38 private boolean canGo(int color) { ... }
39
40 /* Returns if a move at coordinate (x,y) is a valid move for the
41 * specified player */
42 private boolean isValid(int color, int x, int y) { ... }
43
44 /* Prompts the player for a move and the coordinates for the move.
45 * Throws an exception if the input is not valid or if the entered
46 * coordinates do not make a valid move. */
47 private void getMove (int color) throws Exception { ... }
48
49 /* Adds the move onto the board, and the pieces gained from that
50 * move. Assumes the move is valid. */
51 private void add (int x, int y, int color) { ... }
52
53 /* The actual game: runs continuously until a player wins */
54 private void game() {
55 printBoard();
56 while (won() == 0) {
57 boolean valid = false;
58 while (!valid) {
59 try {
60 getMove(black);
Solutions to Chapter 7 | Object Oriented Design
165 Cracking the Coding Interview | Concepts and Algorithms
61 valid = true;
62 } catch (Exception e) {
63 System.out.println (“Enter a valid coordinate!”);
64 }
65 }
66 valid = false;
67 printBoard();
68 while (!valid) {
69 try {
70 getMove(white);
71 valid = true;
72 } catch (Exception e) {
73 System.out.println (“Enter a valid coordinate!”);
74 }
75 }
76 printBoard ();
77 }
78
79 if (won()!=3) {
80 System.out.println (won () == 1 ? “white” : “black” +
81 “ won!”);
82 } else {
83 System.out.println(“It’s a draw!”);
84 }
85 }
86 }
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 166
7.9 Explain the data structures and algorithms that you would use to design an in-memory file system. Illustrate with an example in code where possible.
pg 62
SOLUTION
For data block allocation, we can use bitmask vector and linear search (see “Practical File System Design”) or B+ trees (see Wikipedia).
1 struct DataBlock { char data[DATA_BLOCK_SIZE]; };
2 DataBlock dataBlocks[NUM_DATA_BLOCKS];
3 struct INode { std::vector<int> datablocks; };
4 struct MetaData {
5 int size;
6 Date last_modifed, created;
7 char extra_attributes;
8 };
9 std::vector<bool> dataBlockUsed(NUM_DATA_BLOCKS);
10 std::map<string, INode *> mapFromName;
11 struct FSBase;
12 struct File : public FSBase {
13 private:
14 std::vector<INode> * nodes;
15 MetaData metaData;
16 };
17
18 struct Directory : pubic FSBase { std::vector<FSBase* > content; };
19 struct FileSystem {
20 init();
21 mount(FileSystem*);
22 unmount(FileSystem*);
23 File createFile(cosnt char* name) { ... }
24 Directory createDirectory(const char* name) { ... }
25 // mapFromName to find INode corresponding to file
26 void openFile(File * file, FileMode mode) { ... }
27 void closeFile(File * file) { ... }
28 void writeToFile(File * file, void * data, int num) { ... }
29 void readFromFile(File* file, void* res, int numbutes,
30 int position) { ... }
31 };
Solutions to Chapter 7 | Object Oriented Design
167 Cracking the Coding Interview | Concepts and Algorithms
7.10 Describe the data structures and algorithms that you would use to implement a garbage collector in C++.
pg 62
SOLUTION
In C++, garbage collection with reference counting is almost always implemented with smart pointers, which perform reference counting. The main reason for using smart pointers over raw ordinary pointers is the conceptual simplicity of implementation and usage.
With smart pointers, everything related to garbage collection is performed behind the scenes - typically in constructors / destructors / assignment operator / explicit object management functions.
There are two types of functions, both of which are very simple:
1 RefCountPointer::type1() {
2 /* implementation depends on reference counting organisation.
3 * There can also be no ref. counter at all (see approach #4) */
4 incrementRefCounter(); }
5
6 RefCountPointer::type2() {
7 /* Implementation depends on reference counting organisation.
8 * There can also be no ref. counter at all (see approach #4). */
9 decrementRefCounter();
10 if (referenceCounterIsZero()) {
11 destructObject();
12 }
13 }
There are several approaches for reference counting implementation in C++:
1. Simple reference counting.
1 struct Object { };
2 struct RefCount {
3 int count;
4 };
5 struct RefCountPtr {
6 Object * pointee;
7 RefCount * refCount;
8 };
Advantages: performance.
Disadvantages: memory overhead because of two pointers.
2. Alternative reference counting.
Solutions to Chapter 7 | Object Oriented Design
. CareerCup com 168
1 struct Object { … };
2 struct RefCountPtrImpl {
3 int count;
4 Object * object;
5 };
6 struct RefCountPtr {
7 RefCountPtrImpl * pointee;
8 };
Advantages: no memory overhead because of two pointers.
Disadvantages: performance penalty because of extra level of indirection.
3. Intrusive reference counting.
1 struct Object { … };
2 struct ObjectIntrusiveReferenceCounting {
3 Object object;
4 int count;
5 };
6 struct RefCountPtr {
7 ObjectIntrusiveReferenceCounting * pointee;
8 };
Advantages: no previous disadvantages.
Disadvantages: class for intrusive reference counting should be modified.
4. Ownership list reference counting. It is an alternative for approach 1-3. For 1-3 it is only important to determine that counter is zero—its actual value is not important. This is the main idea of approach # 4.
All Smart-Pointers for given objects are stored in doubly-linked lists. The constructor of a smart pointer adds the new node to a list, and the destructor removes a node from the list and checks if the list is empty or not. If it is empty, the object is deleted.
1 struct Object { };
2 struct ListNode {
3 Object * pointee;
4 ListNode * next;
5 }
Solutions to Chapter 8 | Recursion
Cracking the Coding Interview | Concepts and Algorithms
169
8.1 Write a method to generate the nth Fibonacci number.
pg 64
SOLUTION
There are three potential approaches: (1) recursive approach (2) iterative approach (3) using matrix math. We have described the recursive and iterative approach below, as you would not be expected to be able to derive the matrix-based approach in an interview. For the interested math-geeks, you may read about the (most efficient) matrix-based algorithm at http://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form.
Recursive Solution:
1 int fibo(int n) {
2 if (n == 0) {
3 return 0; // f(0) = 0
4 } else if (n == 1) {
5 return 1; // f(1) = 1
6 } else if (n > 1) {
7 return fibo(n-1) + fibo(n-2); // f(n) = f(n—1) + f(n-2)
8 } else {
9 return –1; // Error condition
10 }
11 }
Iterative Solution:
1 int fibo(int n) {
2 if (n < 0) return -1; // Error condition.
3 if (n == 0) return 0;
4 int a = 1, b = 1;
5 for (int i = 3; i <= n; i++) {
6 int c = a + b;
7 a = b;
8 b = c;
9 }
10 return b;
11 }
Solutions to Chapter 8 | Recursion
170
CareerCup.com
8.2 Imagine a robot sitting on the upper left hand corner of an NxN grid. The robot can only move in two directions: right and down. How many possible paths are there for the robot?
FOLLOW UP
Imagine certain squares are “off limits”, such that the robot can not step on them. Design an algorithm to get all possible paths for the robot.
pg 64
SOLUTION
Part 1: (For clarity, we will solve this part assuming an X by Y grid)
Each path has (X-1)+(Y-1) steps. Imagine the following paths:
X X Y Y X (move right -> right -> down -> down -> right)
X Y X Y X (move right -> down -> right -> down -> right)
...
Each path can be fully represented by the moves at which we move right. That is, if I were to ask you which path you took, you could simply say “I moved right on step 3 and 4.”
Since you must always move right X-1 times, and you have X-1 + Y-1 total steps, you have to pick X-1 times to move right out of X-1+Y-1 choices. Thus, there are C(X-1, X-1+Y-1) paths (e.g., X-1+Y-1 choose X-1):
(X-1 + Y-1)! / ((X-1)! * (Y-1)!)
Part 2: Code
We can implement a simple recursive algorithm with backtracking:
1 ArrayList<Point> current_path = new ArrayList<Point>();
2 public static boolean getPaths(int x, int y) {
3 Point p = new Point(x, y);
4 current_path.add(p);
5 if (0 == x && 0 == y) return true; // current_path
6 boolean success = false;
7 if (x >= 1 && is_free(x - 1, y)) { // Try right
8 success = getPaths(x - 1, y); // Free! Go right
9 }
10 if (!success && y >= 1 && is_free(x, y - 1)) { // Try down
11 success = getPaths(x, y - 1); // Free! Go down
12 }
13 if (!success) {
14 current_path.remove(p); // Wrong way!
15 }
16 return success;
17 }
Solutions to Chapter 8 | Recursion
171 Cracking the Coding Interview | Concepts and Algorithms
8.3 Write a method that returns all subsets of a set.
pg 64
SOLUTION
We should first have some reasonable expectations of our time and space complexity. How many subsets of a set are there? We can compute this by realizing that when we generate a subset, each element has the “choice” of either being in there or not. That is, for the first element, there are 2 choices. For the second, there are two, etc. So, doing 2 * 2 * ... * 2 n times gives us 2^n subsets. We will not be able to do better than this in time or space complexity.
Approach #1: Recursion
This is a great problem to implement with recursion since we can build all subsets of a set using all subsets of a smaller set. Specifically, given a set S, we can do the following recursively:
»»Let first = S[0]. Let smallerSet = S[1, ... , n].
»»Compute all subsets of smallerSet and put them in allsubsets.
»»For each subset in allsubsets, clone it and add first to the subset.
The following code implements this algorithm:
1 ArrayList<ArrayList<Integer>> getSubsets(ArrayList<Integer> set,
2 int index) {
3 ArrayList<ArrayList<Integer>> allsubsets;
4 if (set.size() == index) {
5 allsubsets = new ArrayList<ArrayList<Integer>>();
6 allsubsets.add(new ArrayList<Integer>()); // Empty set
7 } else {
8 allsubsets = getSubsets(set, index + 1);
9 int item = set.get(index);
10 ArrayList<ArrayList<Integer>> moresubsets =
11 new ArrayList<ArrayList<Integer>>();
12 for (ArrayList<Integer> subset : allsubsets) {
13 ArrayList<Integer> newsubset = new ArrayList<Integer>();
14 newsubset.addAll(subset); //
15 newsubset.add(item);
16 moresubsets.add(newsubset);
17 }
18 allsubsets.addAll(moresubsets);
19 }
20 return allsubsets;
21 }
Approach #2: Combinatorics
»»When we’re generating a set, we have two choices for each element: (1) the element is
Solutions to Chapter 8 | Recursion
. CareerCup com 172
in the set (the “yes” state) or (2) the element is not in the set (the “no” state). This means that each subset is a sequence of yesses / nos—e.g., “yes, yes, no, no, yes, no”
»»This gives us 2^n possible subsets. How can we iterate through all possible sequences of “yes” / “no” states for all elements? If each “yes” can be treated as a 1 and each “no” can be treated as a 0, then each subset can be represented as a binary string.
»»Generating all subsets then really just comes down to generating all binary numbers (that is, all integers). Easy!
1 ArrayList<ArrayList<Integer>> getSubsets2(ArrayList<Integer> set) {
2 ArrayList<ArrayList<Integer>> allsubsets =
3 new ArrayList<ArrayList<Integer>>();
4 int max = 1 << set.size();
5 for (int i = 0; i < max; i++) {
6 ArrayList<Integer> subset = new ArrayList<Integer>();
7 int k = i;
8 int index = 0;
9 while (k > 0) {
10 if ((k & 1) > 0) {
11 subset.add(set.get(index));
12 }
13 k >>= 1;
14 index++;
15 }
16 allsubsets.add(subset);
17 }
18 return allsubsets;
19 }
Solutions to Chapter 8 | Recursion
173 Cracking the Coding Interview | Concepts and Algorithms
8.4 Write a method to compute all permutations of a string
pg 64
SOLUTION
Let’s assume a given string S represented by the letters A1, A2, A3, ... , An
To permute set S, we can select the first character, A1, permute the remainder of the string to get a new list. Then, with that new list, we can “push” A1 into each possible position.
For example, if our string is “abc”, we would do the following:
1. Let first = “a” and let remainder = “bc”
2. Let list = permute(bc) = {“bc”, “cd”}
3. Push “a” into each location of “bc” (--> “abc”, “bac”, “bca”) and “cb” (--> “acb”, “cab”, “cba”)
4. Return our new list
Now, the code to do this:
1 public static ArrayList<String> getPerms(String s) {
2 ArrayList<String> permutations = new ArrayList<String>();
3 if (s == null) { // error case
4 return null;
5 } else if (s.length() == 0) { // base case
6 permutations.add(“”);
7 return permutations;
8 }
9
10 char first = s.charAt(0); // get the first character
11 String remainder = s.substring(1); // remove the first character
12 ArrayList<String> words = getPerms(remainder);
13 for (String word : words) {
14 for (int j = 0; j <= word.length(); j++) {
15 permutations.add(insertCharAt(word, first, j));
16 }
17 }
18 return permutations;
19 }
20
21 public static String insertCharAt(String word, char c, int i) {
22 String start = word.substring(0, i);
23 String end = word.substring(i);
24 return start + c + end;
25 }
This solution takes O(n!) time, since there are n! permutations.
Solutions to Chapter 8 | Recursion
. CareerCup com 174
8.5 Implement an algorithm to print all valid (e.g., properly opened and closed) combinations of n-pairs of parentheses.
EXAMPLE:
input: 3 (e.g., 3 pairs of parentheses)
output: ()()(), ()(()), (())(), ((()))
pg 64
SOLUTION
We can solve this problem recursively by recursing through the string. On each iteration, we have the index for a particular character in the string. We need to select either a left or a right paren. When can we use left, and when can we use a right paren?
»»Left: As long as we haven’t used up all the left parentheses, we can always insert a left paren.
»»Right: We can insert a right paren as long as it won’t lead to a syntax error. When will we get a syntax error? We will get a syntax error if there are more right parentheses than left.
So, we simply keep track of the number of left and right parentheses allowed. If there are left parens remaining, we’ll insert a left paren and recurse. If there are more right parens remaining than left (eg, if there are more left parens used), then we’ll insert a right paren and recurse.
1 public static void printPar(int l, int r, char[] str, int count) {
2 if (l < 0 || r < l) return; // invalid state
3 if (l == 0 && r == 0) {
4 System.out.println(str); // found one, so print it
5 } else {
6 if (l > 0) { // try a left paren, if there are some available
7 str[count] = ‘(‘;
8 printPar(l - 1, r, str, count + 1);
9 }
10 if (r > l) { // try a right paren, if there’s a matching left
11 str[count] = ‘)’;
12 printPar(l, r - 1, str, count + 1);
13 }
14 }
15 }
16
17 public static void printPar(int count) {
18 char[] str = new char[count*2];
19 printPar(count, count, str, 0);
20 }
Solutions to Chapter 8 | Recursion
175 Cracking the Coding Interview | Concepts and Algorithms
8.6 Implement the “paint fill” function that one might see on many image editing programs. That is, given a screen (represented by a 2-dimensional array of Colors), a point, and a new color, fill in the surrounding area until you hit a border of that color.
pg 64
SOLUTION
First, let’s visualize how this method works. When we call Paint Fill (eg, “click” paint fill in the image editing application) on, say, a green pixel, we want to “bleed” outwards. Pixel by pixel, we expand outwards calling PaintFill on the surrounding pixel. When we hit a pixel that is not green, we stop. Surrounding green pixels may still be painted if they are touched by another Paint Fill operation.
We can implement this algorithm recursively:
1 enum Color {
2 Black, White, Red, Yellow, Green
3 }
4 boolean PaintFill(Color[][] screen, int x, int y, Color ocolor,
5 Color ncolor) {
6 if (x < 0 || x >= screen[0].length ||
7 y < 0 || y >= screen.length) {
8 return false;
9 }
10 if (screen[y][x] == ocolor) {
11 screen[y][x] = ncolor;
12 PaintFill(screen, x - 1, y, ocolor, ncolor); // left
13 PaintFill(screen, x + 1, y, ocolor, ncolor); // right
14 PaintFill(screen, x, y - 1, ocolor, ncolor); // top
15 PaintFill(screen, x, y + 1, ocolor, ncolor); // bottom
16 }
17 return true;
18 }
19
20 boolean PaintFill(Color[][] screen, int x, int y, Color ncolor) {
21 return PaintFill(screen, x, y, screen[y][x], ncolor);
22 }
Solutions to Chapter 8 | Recursion
. CareerCup com 176
8.7 Given an infinite number of quarters (25 cents), dimes (10 cents), nickels (5 cents) and pennies (1 cent), write code to calculate the number of ways of representing n cents.
pg 64
SOLUTION
This is a recursive problem, so let’s figure out how to do makeChange(n) using prior solutions (i.e., sub-problems). Let’s say n = 100, so we want to compute the number of ways of making change of 100 cents. What’s the relationship to its sub-problems?
We know that makeChange(100):
= makeChange(100 using 0 quarters) + makeChange(100 using 1 quarter) + makeChange(100 using 2 quarter) + makeChange(100 using 3 quarter) + makeChange(100 using 4 quarter)
Can we reduce this further? Yes!
= makeChange(100 using 0 quarters) + makeChange(75 using 0 quarter) + makeChange(50 using 0 quarters) + makeChange(25 using 0 quarters) + 1
Now what? We’ve used up all our quarters, so now we can start applying our next biggest denomination: dimes.
This leads to a recursive algorithm that looks like this:
1 public static int makeChange(int n, int denom) {
2 int next_denom = 0;
3 switch (denom) {
4 case 25:
5 next_denom = 10;
6 break;
7 case 10:
8 next_denom = 5;
9 break;
10 case 5:
11 next_denom = 1;
12 break;
13 case 1:
14 return 1;
15 }
16 int ways = 0;
17 for (int i = 0; i * denom <= n; i++) {
18 ways += makeChange(n - i * denom, next_denom);
19 }
20 return ways;
21 }
22
23 System.out.writeln(makeChange(n, 25));
Solutions to Chapter 8 | Recursion
177 Cracking the Coding Interview | Concepts and Algorithms
8.8 Write an algorithm to print all ways of arranging eight queens on a chess board so that none of them share the same row, column or diagonal.
pg 64
SOLUTION
We will use a backtracking algorithm. For each row, the column where we want to put the queen is based on checking that it does not violate the required condition.
1. For this, we need to store the column of the queen in each row as soon as we have finalized it. Let ColumnForRow[] be the array which stores the column number for each row.
2. The checks that are required for the three given conditions are:
»»On same Column : ColumnForRow[i] == ColumnForRow[j]
»»On same Diagonal: (ColumnForRow[i] - ColumnForRow[j] ) == ( i- j) or
(ColumnForRow[j] - ColumnForRow[i]) == (i - j)
1 int columnForRow[] = new int [8];
2 boolean check(int row) {
3 for (int i = 0; i < row; i++) {
4 int diff = Math.abs(columnForRow[i] - columnForRow[row]);
5 if (diff == 0 || diff == row - i) return false;
6 }
7 return true;
8 }
9
10 void PlaceQueen(int row){
11 if (row == 8) {
12 printBoard();
13 return;
14 }
15 for (int i = 0; i < 8; i++) {
16 columnForRow[row]=i;
17 if(check(row)){
18 PlaceQueen(row+1);
19 }
20 }
21 }

Solutions to Chapter 9 | Sorting and Searching
Cracking the Coding Interview | Concepts and Algorithms
179
9.1 You are given two sorted arrays, A and B, and A has a large enough buffer at the end to hold B. Write a method to merge B into A in sorted order.
pg 66
SOLUTION
This code is a part of the standard merge-sort code. We merge A and B from the back, by comparing each element.
1 public static void merge(int[] a, int[] b, int n, int m) {
2 int k = m + n - 1; // Index of last location of array b
3 int i = n - 1; // Index of last element in array b
4 int j = m - 1; // Index of last element in array a
5
6 // Start comparing from the last element and merge a and b
7 while (i >= 0 && j >= 0) {
8 if (a[i] > b[j]) {
9 a[k--] = a[i--];
10 } else {
11 a[k--] = b[j--];
12 }
13 }
14 while (j >= 0) {
15 a[k--] = b[j--];
16 }
17 }
Note: You don’t need to copy the contents of a after running out of b’s. They are already in place.
Solutions to Chapter 9 | Sorting and Searching
180
CareerCup.com
9.2 Write a method to sort an array of strings so that all the anagrams are next to each other.
pg 66
SOLUTION
The basic idea is to implement a normal sorting algorithm where you override the compareTo method to compare the “signature” of each string. In this case, the signature is the alphabetically sorted string.
1 public class AnagramComparator implements Comparator<String> {
2 public String sortChars(String s) {
3 char[] content = s.toCharArray();
4 Arrays.sort(content);
5 return new String(content);
6 }
7
8 public int compare(String s1, String s2) {
9 return sortChars(s1).compareTo(sortChars(s2));
10 }
11 }
Now, just sort the arrays, using this compareTo method instead of the usual one.
12 Arrays.sort(array, new AnagramComparator());
Solutions to Chapter 9 | Sorting and Searching
181 Cracking the Coding Interview | Concepts and Algorithms
9.3 Given a sorted array of n integers that has been rotated an unknown number of times, give an O(log n) algorithm that finds an element in the array. You may assume that the array was originally sorted in increasing order.
EXAMPLE:
Input: find 5 in array (15 16 19 20 25 1 3 4 5 7 10 14)
Output: 8 (the index of 5 in the array)
pg 66
SOLUTION
We can do this with a modification of binary search.
1 public static int search(int a[], int l, int u, int x) {
2 while (l <= u) {
3 int m = (l + u) / 2;
4 if (x == a[m]) {
5 return m;
6 } else if (a[l] <= a[m]) {
7 if (x > a[m]) {
8 l = m+1;
9 } else if (x >=a [l]) {
10 u = m-1;
11 } else {
12 l = m+1;
13 }
14 }
15 else if (x < a[m]) u = m-1;
16 else if (x <= a[u]) l = m+1;
17 else u = m - 1;
18 }
19 return -1;
20 }
21
22 public static int search(int a[], int x) {
23 return search(a, 0, a.length - 1, x);
24 }
What about duplicates? You may observe that the above function doesn’t give you an efficient result in case of duplicate elements. However, if your array has duplicate entries then we can’t do better than O(n) which is as good as linear search.
For example, if the array is [2,2,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,2], there is no way to find element 3 until you do a linear search.
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 182
9.4 If you have a 2 GB file with one string per line, which sorting algorithm would you use to sort the file and why?
pg 66
SOLUTION
When an interviewer gives a size limit of 2GB, it should tell you something - in this case, it suggests that they don’t want you to bring all the data into memory.
So what do we do? We only bring part of the data into memory..
Algorithm:
How much memory do we have available? Let’s assume we have X MB of memory available.
1. Divide the file into K chunks, where X * K = 2 GB. Bring each chunk into memory and sort the lines as usual using any O(n log n) algorithm. Save the lines back to the file.
2. Now bring the next chunk into memory and sort.
3. Once we’re done, merge them one by one.
The above algorithm is also known as external sort. Step 3 is known as N-way merge
The rationale behind using external sort is the size of data. Since the data is too huge and we can’t bring it all into memory, we need to go for a disk based sorting algorithm.
Solutions to Chapter 9 | Sorting and Searching
183 Cracking the Coding Interview | Concepts and Algorithms
9.5 Given a sorted array of strings which is interspersed with empty strings, write a method to find the location of a given string.
Example: find “ball” in [“at”, “”, “”, “”, “ball”, “”, “”, “car”, “”, “”, “dad”, “”, “”] will return 4
Example: find “ballcar” in [“at”, “”, “”, “”, “”, “ball”, “car”, “”, “”, “dad”, “”, “”] will return -1
pg 66
SOLUTION
Use ordinary binary search, but when you hit an empty string, advance to the next non-empty string; if there is no next non-empty string, search the left half.
1 public int search(String[] strings, String str, int first, int last) {
2 while (first <= last) {
3 // Ensure there is something at the end
4 while (first <= last && strings[last] == “”) {
5 --last;
6 }
7 if (last < first) {
8 return -1; // this block was empty, so fail
9 }
10 int mid = (last + first) >> 1;
11 while (strings[mid] == “”) {
12 ++mid; // will always find one
13 }
14 int r = strings[mid].compareTo(str);
15 if (r == 0) return mid;
16 if (r < 0) {
17 first = mid + 1;
18 } else {
19 last = mid - 1;
20 }
21 }
22 return -1;
23 }
24
25 public int search(String[] strings, String str) {
26 if (strings == null || str == null) return -1;
27 if (str == “”) {
28 for (int i = 0; i < strings.length; i++) {
29 if (strings[i] == “”) return i;
30 }
31 return -1;
32 }
33 return search(strings, str, 0, strings.length - 1);
34 }
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 184
9.6 Given a matrix in which each row and each column is sorted, write a method to find an element in it.
pg 66
SOLUTION
Assumptions:
»»Rows are sorted left to right in ascending order. Columns are sorted top to bottom in ascending order.
»»Matrix is of size MxN.
This algorithm works by elimination. Every move to the left (--col) eliminates all the elements below the current cell in that column. Likewise, every move down eliminates all the elements to the left of the cell in that row.
1 boolean FindElem(int[][] mat, int elem, int M, int N) {
2 int row = 0;
3 int col = N-1;
4 while (row < M && col >= 0) {
5 if (mat[row][col] == elem) {
6 return true;
7 } else if (mat[row][col] > elem) {
8 col--;
9 } else {
10 row++;
11 }
12 }
13 return false;
14 }
Solutions to Chapter 9 | Sorting and Searching
185 Cracking the Coding Interview | Concepts and Algorithms
9.7 A circus is designing a tower routine consisting of people standing atop one another’s shoulders. For practical and aesthetic reasons, each person must be both shorter and lighter than the person below him or her. Given the heights and weights of each person in the circus, write a method to compute the largest possible number of people in such a tower.
EXAMPLE:
Input (ht, wt): (65, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68, 110)
Output: The longest tower is length 6 and includes from top to bottom: (56, 90) (60,95) (65,100) (68,110) (70,150) (75,190)
pg 66
SOLUTION
Step 1. Sort all items by height first, and then by weight. This means that if all the heights are unique, then the items will be sorted by their height. If heights are the same, items will be sorted by their weight.
Example:
»»Before sorting: (60, 100) (70, 150) (56, 90) (75, 190) (60, 95) (68,110).
»»After sorting: (56, 90), (60, 95), (60,100), (68, 110), (70,150), (75,190).
Step 2. Find the longest sequence which contains increasing heights and increasing weights.
To do this, we:
a) Start at the beginning of the sequence. Currently, max_sequence is empty.
b) If, for the next item, the height and the weight is not greater than those of the previous item, we mark this item as “unfit” .
(60,95)
(65,100)
(75,80)
(80, 100)
(unfit item)
c) If the sequence found has more items than “max sequence”, it becomes “max sequence”.
d) After that the search is repeated from the “unfit item”, until we reach the end of the original sequence.
1 public class Question {
2 ArrayList<HtWt> items;
3 ArrayList<HtWt> lastFoundSeq;
4 ArrayList<HtWt> maxSeq;
5
Solutions to Chapter 9 | Sorting and Searching
. CareerCup com 186
6 // Returns longer sequence
7 ArrayList<HtWt> seqWithMaxLength(ArrayList<HtWt> seq1,
8 ArrayList<HtWt> seq2) {
9 return seq1.size() > seq2.size() ? seq1 : seq2;
10 }
11
12 // Fills next seq w decreased wts&returns index of 1st unfit item.
13 int fillNextSeq(int startFrom, ArrayList<HtWt> seq) {
14 int firstUnfitItem = startFrom;
15 if (startFrom < items.size()) {
16 for (int i = 0; i < items.size(); i++) {
17 HtWt item = items.get(i);
18 if (i == 0 || items.get(i-1).isBefore(item)) {
19 seq.add(item);
20 } else {
21 firstUnfitItem = i;
22 }
23 }
24 }
25 return firstUnfitItem;
26 }
27
28 // Find the maximum length sequence
29 void findMaxSeq() {
30 Collections.sort(items);
31 int currentUnfit = 0;
32 while (currentUnfit < items.size()) {
33 ArrayList<HtWt> nextSeq = new ArrayList<HtWt>();
34 int nextUnfit = fillNextSeq(currentUnfit, nextSeq);
35 maxSeq = seqWithMaxLength(maxSeq, nextSeq);
36 if (nextUnfit == currentUnfit) break;
37 else currentUnfit = nextUnfit;
38 }
39 }
40 }
Solutions to Chapter 10 | Mathematical
Cracking the Coding Interview | Concepts and Algorithms
187
10.1 You have a basketball hoop and someone says that you can play 1 of 2 games.
Game #1: You get one shot to make the hoop.
Game #2: You get three shots and you have to make 2 of 3 shots.
If p is the probability of making a particular shot, for which values of p should you pick one game or the other?
pg 68
SOLUTION
Probability of winning Game 1: p
Probability of winning Game 2:
Let s(k,n) be the probability of making exactly k shots out of n. The probability of winning game 2 is s(2, 3)+s(3, 3). Since, s(k, n) = C(n, k) ( 1- p)^(n - k) p^k, the probability of winning is 3 * (1 - p) * p^2 + p^3.
Simplified, it becomes 3 * p^2 - 2 * p^3.
You should play Game1 if P(Game1) > P(Game2):
p > 3*p^2 - 2*p^3.
1 > 3*p - 2*p^2
2*p^2 - 3*p + 1 > 0
(2p - 1)(p - 1) > 0
Both terms must be positive or both must be negative. But we know p < 1, so (p - 1) < 0. This means both terms must be negative.
(2p - 1) < 0
2p < 1
p < .5
So, we should play Game1 if p < .5.
Solutions to Chapter 10 | Mathematical
188
CareerCup.com
10.2 There are three ants on different vertices of a triangle. What is the probability of collision (between any two or all of them) if they start walking on the sides of the triangle?
Similarly find the probability of collision with ‘n’ ants on an ‘n’ vertex polygon.
pg 68
SOLUTION
None of the three ants will collide if all three are moving in clockwise direction, or all three are moving in a counter-clockwise direction. Otherwise, there will definitely be a collision.
How many ways are there for the three ants to move? Each ant can move in 2 directions, so there are 2^3 ways the ant can move. There are only two ways which will avoid a collision, therefore the probability of collision is (2^3 – 2) / (2^3) = 6 / 8 = 3 / 4.
To generalize this to an n-vertex polygon: there are still only 2 ways in which the ants can move to avoid a collision, but there are 2^n ways they can move total. Therefore, in general, probability of collision is (2^n – 2) / 2^n = 1 – 1/2^(n-1).
Solutions to Chapter 10 | Mathematical
189 Cracking the Coding Interview | Concepts and Algorithms
10.3 Given two lines on a Cartesian plane, determine whether the two lines would intersect.
pg 68
SOLUTION
There are a lot of unknowns in this problem (what format are the lines in? What if they are the same line?), but let’s assume:
»»If two lines are the same (same line = same slope and y-intercept), they are considered to intersect.
»»We get to decide the data structure.
1 public class Line {
2 static double epsilon = 0.000001;
3 public double slope;
4 public double yintercept;
5
6 public Line(double s, double y) {
7 slope = s;
8 yintercept = y;
9 }
10
11 public boolean intersect(Line line2) {
12 return Math.abs(slope - line2.slope) > epsilon ||
13 Math.abs(yintercept - line2.yintercept) < epsilon;
14 }
15 }
OBSERVATIONS AND SUGGESTIONS:
»»Ask questions. This question has a lot of unknowns—ask questions to clarify them. Many interviewers intentionally ask vague questions to see if you’ll clarify your assumptions.
»»When possible, design and use data structures. It shows that you understand and care about object oriented design.
»»Think through which data structures you design to represent a line. There are a lot of options, with lots of trade offs. Pick one and explain your choice.
»»Don’t assume that the slope and y-intercept are integers.
»»Understand limitations of floating point representations. Never check for equality with ==.
Solutions to Chapter 10 | Mathematical
. CareerCup com 190
10.4 Write a method to implement *, - , / operations. You should use only the + operator.
pg 68
SOLUTION
With an understanding of what each operation (minus, times, divide) does, this problem can be approached logically.
»»Subtraction should be relatively straightforward, as we all know that a - b is the same thing as a + (-1)*b.
»»Multiplication: we have to go back to what we learned in grade school: 21 * 3 = 21 + 21 + 21. It’s slow, but it works.
»»Division is the trickiest, because we usually think of 21 / 3 as something like “if you divide a 21 foot board into 3 pieces, how big is each piece?” If we think about it the other way around, it’s a little easier: “I divided a 21 foot board in x pieces and got pieces of 3 feet each, how many pieces were there?” From here, we can see that if we continuously subtract 3 feet from 21 feet, we’ll know how many pieces there are. That is, we continuously subtract b from a and count how many times we can do that.
1 /* Flip a positive sign to negative, or a negative sign to pos */
2 public static int FnNegate(int a) {
3 int neg = 0;
4 int d = a < 0 ? 1 : -1;
5 while (a != 0) {
6 neg += d;
7 a += d;
8 }
9 return neg;
10 }
11
12 /* Subtract two numbers by negating b and adding them */
13 public static int FnMinus(int a, int b) {
14 return a + FnNegate(b);
15 }
16
17 /* Check if a and b are different signs */
18 public static boolean DifferentSigns(int a, int b) {
19 return ((a < 0 && b > 0) || (a > 0 && b < 0)) ? true : false;
20 }
21
22 /* Return absolute value */
23 public static int abs(int a) {
24 if (a < 0) return FnNegate(a);
25 else return a;
26 }
Solutions to Chapter 10 | Mathematical
191 Cracking the Coding Interview | Concepts and Algorithms
27
28 /* Multiply a by b by adding a to itself b times */
29 public static int FnTimes(int a, int b) {
30 if (a < b) return FnTimes(b, a); // algo is faster if b < a
31 int sum = 0;
32 for (int iter = abs(b); iter > 0; --iter) sum += a;
33 if (b < 0) sum = FnNegate(sum);
34 return sum;
35 }
36
37 /* Divide a by b by literally counting how many times does b go into
38 * a. That is, count how many times you can subtract b from a until
39 * you hit 0. */
40 public static int FnDivide(int a, int b) throws
41 java.lang.ArithmeticException {
42 if (b == 0) {
43 throw new java.lang.ArithmeticException(“Divide by 0.”);
44 }
45 int quotient = 0;
46 int divisor = FnNegate(abs(b));
47 int divend; /* dividend */
48 for (divend = abs(a); divend >= abs(divisor); divend += divisor) {
49 ++quotient;
50 }
51 if (DifferentSigns(a, b)) quotient = FnNegate(quotient);
52 return quotient;
53 }
OBSERVATIONS AND SUGGESTIONS
»»A logical approach of going back to what exactly multiplication and division do comes in handy. Remember that. All (good) interview problems can be approached in a logical, methodical way!
»»The interviewer is looking for this sort of logical work-your-way-through-it approach.
»»This is a great problem to demonstrate your ability to write clean code—specifically, to show your ability to re-use code. For example, if you were writing this solution and didn’t put FnNegate in its own method, you should move it out once you see that you’ll use it multiple times.
»»Be careful about making assumptions while coding. Don’t assume that the numbers are all positive, or that a is bigger than b.
Solutions to Chapter 10 | Mathematical
. CareerCup com 192
10.5 Given two squares on a two dimensional plane, find a line that would cut these two squares in half.
pg 68
SOLUTION
Any line that goes through the center of a rectangle must cut it in half. Therefore, if you drew a line connecting the centers of the two squares, it would cut both in half.
1 public class Square {
2 public double left;
3 public double top;
4 public double bottom;
5 public double right;
6 public Square(double left, double top, double size) {
7 this.left = left;
8 this.top = top;
9 this.bottom = top + size;
10 this.right = left + size;
11 }
12
13 public Point middle() {
14 return new Point((this.left + this.right) / 2,
15 (this.top + this.bottom) / 2);
16 }
17
18 public Line cut(Square other) {
19 Point middle_s = this.middle();
20 Point middle_t = other.middle();
21 if (middle_s == middle_t) {
22 return new Line(new Point(left, top),
23 new Point(right, bottom));
24 } else {
25 return new Line(middle_s, middle_t);
26 }
27 }
28 }
SUGGESTIONS AND OBSERVATIONS
The main point of this problem is to see how careful you are about coding. It’s easy to glance over the special cases (e.g., the two squares having the same middle). Make a list of these special cases before you start the problem and make sure to handle them appropriately.
Solutions to Chapter 10 | Mathematical
193 Cracking the Coding Interview | Concepts and Algorithms
10.6 Given a two dimensional graph with points on it, find a line which passes the most number of points.
pg 68
SOLUTION
If we draw a line between every two points, we can check to see which line is the most common. A brute force approach would be to simply iterate through each line segment (formed by pairs of points) and count how many points fall on it. This would take O(N^3) time.
Before we discuss if we can do better, let’s figure out how we can represent a line. A line can be represented in (at least) two different ways: (1) as a pairing of points or (2) as a slope and a y-intercept.
Because our line is infinite, the slope and y-intercept approach seems more appropriate. The slope and y-intercept approach has an additional advantage: every line segment on the same greater line will have identical slopes and y-intercepts.
Let’s re-think our solution. We have a bunch of line segments, represented as a slope and y-intercept, and we want to find the most common slope and y-intercept. How can we find the most common one?
This is really no different than the old “find the most common number in a list of numbers” problem. We just iterate through the lines segments and use a hash table to count the number of times we’ve seen each line.
1 public static Line findBestLine(GraphPoint[] points) {
2 Line bestLine = null;
3 HashMap<Line, Integer> line_count = new HashMap<Line, Integer>();
4 for (int i = 0; i < points.length; i++) {
5 for (int j = i + 1; j < points.length; j++) {
6 Line line = new Line(points[i], points[j]);
7 if (!line_count.containsKey(line)) {
8 line_count.put(line, 0);
9 }
10 line_count.put(line, line_count.get(line) + 1);
11 if (bestLine == null ||
12 line_count.get(line) > line_count.get(bestLine)) {
13 bestLine = line;
14 }
15 }
16 }
17 return bestLine;
18 }
19
20 public class Line {
21 private static double epsilon = .0001;
Solutions to Chapter 10 | Mathematical
. CareerCup com 194
22 public double slope;
23 public double intercept;
24 private boolean infinite_slope = false;
25 public Line(GraphPoint p, GraphPoint q) {
26 if (Math.abs(p.x - q.x) > epsilon) { // if x’s are different
27 slope = (p.y - q.y) / (p.x - q.x); // compute slope
28 intercept = p.y - slope * p.x; // y intercept from y=mx+b
29 } else {
30 infinite_slope = true;
31 intercept = p.x; // x-intercept, since slope is infinite
32 }
33 }
34
35 public boolean isEqual(double a, double b) {
36 return (Math.abs(a - b) < epsilon);
37 }
38
39 @Override
40 public int hashCode() {
41 int sl = (int)(slope * 1000);
42 int in = (int)(intercept * 1000);
43 return sl | in;
44 }
45
46 @Override
47 public boolean equals(Object o) {
48 Line l = (Line) o;
49 if (isEqual(l.slope, slope) && isEqual(l.intercept, intercept)
50 && (infinite_slope == l.infinite_slope)) {
51 return true;
52 }
53 return false;
54 }
55 }
OBSERVATIONS AND SUGGESTIONS
»»Be careful about the calculation of the slope of a line. The line might be completely vertical. We can keep track of this in a separate flag (infinite_slope). We need to check this condition in the equals method.
»»Remember that when we perform division to calculate the slope, division is not exact. Therefore, rather than checking to see if two slopes are exactly equal, we need to check if they’re different by greater than epsilon.
Solutions to Chapter 10 | Mathematical
195 Cracking the Coding Interview | Concepts and Algorithms
10.7 Design an algorithm to find the kth number such that the only prime factors are 3, 5, and 7.
pg 68
SOLUTION
Any such number will look like (3^i)*(5^j)*(7^k). Here are the first 13 numbers:
1
-
3^0 * 5^0 * 7 ^ 0
3
3
3^1 * 5^0 * 7 ^ 0
5
5
3^0 * 5^1 * 7 ^ 0
7
7
3^0 * 5^0 * 7 ^ 1
9
3*3
3^2 * 5^0 * 7 ^ 0
15
3*5
3^1 * 5^1 * 7 ^ 0
21
3*7
3^1 * 5^0 * 7 ^ 1
25
5*5
3^0 * 5^2 * 7 ^ 0
27
3*9
3^3 * 5^0 * 7 ^ 0
35
5*7
3^0 * 5^1 * 7 ^1
45
5*9
3^2 * 5^1 * 7 ^0
49
7*7
3^0 * 5^0 * 7 ^2
63
3*21
3^2 * 5^0 * 7 ^1
»»3 * (previous number in list)
»»5 * (previous number in list)
»»7 * (previous number in list)
How would we find the next number in the list? Well, we could multiply 3, 5 and 7 times each number in the list and find the smallest element that has not yet been added to our list. This solution is O(n^2). Not bad, but I think we can do better.
In our current algorithm, we’re doing 3*1, 3*3, 3*5, 3*7, 3*9, 3*15, 3*21, 3*25 …, and the same for 5 and 7. We’ve already done almost all this work before—why are we doing it again?
We can fix this by multiplying each number we add to our list by 3, 5, 7 and putting the results in one of the three first-in-first-out queues. To look for the next “magic” number, we pick the smallest element in the three queues. Here is the algorithm:
1. Initialize array magic and queues Q3, Q5 and Q7
2. Insert 1 into magic.
3. Insert 1*3, 1*5 and 1*7 into Q3, Q5 and Q7 respectively.
4. Let x be the minimum element in Q3, Q5 and Q7. Append x to magic.
5. If x was found in:
Solutions to Chapter 10 | Mathematical
. CareerCup com 196
Q3 -> append x*3, x*5 and x*7 to Q3, Q5 and Q7. Remove x from Q3.
Q5 -> append x*5 and x*7 to Q5 and Q7. Remove x from Q5.
Q7 -> only append x*7 to Q7. Remove x from Q7.
Note: we do not need to append x*3 and x*5 to all lists because they will already be found in another list.
6. Repeat steps 4 - 6 until we’ve found k elements.
1 public static int getKthMagicNumber(int k) {
2 if (k <= 0) return 0;
3 int val = 1;
4 Queue<Integer> Q3 = new LinkedList<Integer>();
5 Queue<Integer> Q5 = new LinkedList<Integer>();
6 Queue<Integer> Q7 = new LinkedList<Integer>();
7 Q3.add(3);
8 Q5.add(5);
9 Q7.add(7);
10 for (--k; k > 0; --k) { // We’ve done one iteration already.
11 val = Math.min(Q3.peek().intValue(),
12 Math.min(Q5.peek().inValue(), Q7.peek().intValue()));
13 if (val == Q7.peek()) {
14 Q7.remove();
15 } else {
16 if (val == Q5.peek()) {
17 Q5.remove();
18 } else { // must be from Q3
19 Q3.remove();
20 Q3.add(val * 3);
21 }
22 Q5.add(val * 5);
23 }
24 Q7.add(val * 7);
25 }
26 return val;
27 }
OBSERVATIONS AND SUGGESTIONS:
When you get this question, do your best to solve it—even though it’s really difficult. Explain a brute force approach (not as tricky) and then start thinking about how you can optimize it. Or, try to find a pattern in the numbers.
Chances are, your interviewer will help you along when you get stuck. Whatever you do, don’t give up! Think out loud, wonder aloud, explain your thought process. Your interviewer will probably jump in to guide you.
Solutions to Chapter 11 | System Design and Memory Limits
Cracking the Coding Interview | Concepts and Algorithms
197
11.1 If you were integrating a feed of end of day stock price information (open, high, low, and closing price) for 5,000 companies, how would you do it? You are responsible for the development, rollout and ongoing monitoring and maintenance of the feed. Describe the different methods you considered and why you would recommend your approach. The feed is delivered once per trading day in a comma-separated format via an FTP site. The feed will be used by 1000 daily users in a web application.
pg 72
SOLUTION
Let’s assume we have some scripts which are scheduled to get the data via FTP at the end of the day. Where do we store the data? How do we store the data in such a way that we can do various analyses of it?
Proposal #1
Keep the data in text files. This would be very difficult to manage and update, as well as very hard to query. Keeping unorganized text files would lead to a very inefficient data model.
Proposal #2
We could use a database. This provides the following benefits:
»»Logical storage of data.
»»Facilitates an easy way of doing query processing over the data.
Example: return all stocks having open > N AND closing price < M
Advantages:
»»Makes the maintenance easy once installed properly.
»»Roll back, backing up data, and security could be provided using standard database features. We don’t have to “reinvent the wheel.”
Proposal #3
If requirements are not that broad and we just want to do a simple analysis and distribute the data, then XML could be another good option.
Our data has fixed format and fixed size: company_name, open, high, low, closing price. The XML could look like this:
<root>
<date value=“2008-10-12”>
<company name=“foo”>
<open>126.23</open>
<high>130.27</high>
<low>122.83</low>
Solutions to Chapter 11 | System Design and Memory Limits
198
CareerCup.com
<closingPrice>127.30</closingPrice>
</company>
<company name=“bar”>
<open>52.73</open>
<high>60.27</high>
<low>50.29</low>
<closingPrice>54.91</closingPrice>
</company>
</date>
<date value=“2008-10-11”> . . . </date>
</root>
Benefits:
»»Very easy to distribute. This is one reason that XML is a standard data model to share /distribute data.
»»Efficient parsers are available to parse the data and extract out only desired data.
»»We can add new data to the XML file by carefully appending data. We would not have to re-query the database.
However, querying the data could be difficult.
Solutions to Chapter 11 | System Design and Memory Limits
199 Cracking the Coding Interview | Concepts and Algorithms
11.2 How would you design the data structures for a very large social network (Facebook, LinkedIn, etc)? Describe how you would design an algorithm to show the connection, or path, between two people (e.g., Me -> Bob -> Susan -> Jason -> You).
pg 72
SOLUTION
Approach:
Forget that we’re dealing with millions of users at first. Design this for the simple case.
We can construct a graph by assuming every person is a node and if there is an edge between two nodes, then the two people are friends with each other.
class Person {
Person[] friends;
// Other info
}
If I want to find the connection between two people, I would start with one person and do a simple breadth first search.
But... oh no! Millions of users!
When we deal with a service the size of Orkut or Facebook, we cannot possibly keep all of our data on one machine. That means that our simple Person data structure from above doesn’t quite work—our friends may not live on the same machine as us. Instead, we can replace our list of friends with a list of their IDs, and traverse as follows:
1. For each friend ID: int machine_index = lookupMachineForUserID(id);
2. Go to machine machine_index
3. Person friend = lookupFriend(machine_index);
There are more optimizations and follow up questions here than we could possibly discuss, but here are just a few thoughts.
Optimization: Reduce Machine Jumps
Jumping from one machine to another is expensive. Instead of randomly jumping from machine to machine with each friend, try to batch these jumps—e.g., if 5 of my friends live on one machine, I should look them up all at once.
Optimization: Smart Division of People and Machines
People are much more likely to be friends with people who live in the same country as them. Rather than randomly dividing people up across machines, try to divvy them up by country, city, state, etc. This will reduce the number of jumps.
Question: Breadth First Search usually requires “marking” a node as visited. How do you do that in
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 200
this case?
Usually, in BFS, we mark a node as visited by setting a flag visited in its node class. Here, we don’t want to do that (there could be multiple searches going on at the same time, so it’s bad to just edit our data). In this case, we could mimic the marking of nodes with a hash table to lookup a node id and whether or not it’s been visited.
Other Follow-Up Questions:
»»In the real world, servers fail. How does this affect you?
»»How could you take advantage of caching?
»»Do you search until the end of the graph (infinite)? How do you decide when to give up?
»»In real life, some people have more friends of friends than others, and are therefore more likely to make a path between you and someone else. How could you use this data to pick where you start traversing?
The following code demonstrates our algorithm:
1 public class Server {
2 ArrayList<Machine> machines = new ArrayList<Machine>();
3 }
4
5 public class Machine {
6 public ArrayList<Person> persons = new ArrayList<Person>();
7 public int machineID;
8 }
9
10 public class Person {
11 private ArrayList<Integer> friends;
12 private int ID;
13 private int machineID;
14 private String info;
15 private Server server = new Server();
16
17 public String getInfo() { return info; }
18 public void setInfo(String info) {
19 this.info = info;
20 }
21
22 public int[] getFriends() {
23 int[] temp = new int[friends.size()];
24 for (int i = 0; i < temp.length; i++) {
25 temp[i] = friends.get(i);
26 }
27 return temp;
28 }
Solutions to Chapter 11 | System Design and Memory Limits
201 Cracking the Coding Interview | Concepts and Algorithms
29 public int getID() { return ID; }
30 public int getMachineID() { return machineID; }
31 public void addFriend(int id) { friends.add(id); }
32
33 // Look up a person given their ID and Machine ID
34 public Person lookUpFriend(int machineID, int ID) {
35 for (Machine m : server.machines) {
36 if (m.machineID == machineID) {
37 for (Person p : m.persons) {
38 if (p.ID == ID){
39 return p;
40 }
41 }
42 }
43 }
44 return null;
45 }
46
47 // Look up a machine given the machine ID
48 public Machine lookUpMachine(int machineID) {
49 for (Machine m:server.machines) {
50 if (m.machineID == machineID)
51 return m;
52 }
53 return null;
54 }
55
56 public Person(int iD, int machineID) {
57 ID = iD;
58 this.machineID = machineID;
59 }
60 }
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 202
11.3 Given an input file with four billion integers, provide an algorithm to generate an integer which is not contained in the file. Assume you have 1 GB of memory.
FOLLOW UP
What if you have only 10 MB of memory?
pg 72
SOLUTION
There are a total of 2^32, or 4 billion, distinct integers possible. We have 1 GB of memory, or 8 billion bits.
Thus, with 8 billion bits, we can map all possible integers to a distinct bit with the available memory. The logic is as follows:
1. Create a bit vector (BV) of size 4 billion.
2. Initialize BV with all 0’s
3. Scan all numbers (num) from the file and write BV[num] = 1;
4. Now scan again BV from 0th index
5. Return the first index which has 0 value.
1 byte[] bitfield = new byte [0xFFFFFFF/8];
2 void findOpenNumber2() throws FileNotFoundException {
3 Scanner in = new Scanner(new FileReader(“input_file_q11_4.txt”));
4 while (in.hasNextInt()) {
5 int n = in.nextInt ();
6 /* Finds the corresponding number in the bitfield by using the
7 * OR operator to set the nth bit of a byte (e.g.. 10 would
8 * correspond to the 2nd bit of index 2 in the byte array). */
9 bitfield [n / 8] |= 1 << (n % 8);
10 }
11
12 for (int i = 0 ; i < bitfield.length; i++) {
13 for (int j = 0; j < 8; j++) {
14 /* Retrieves the individual bits of each byte. When 0 bit
15 * is found, finds the corresponding value. */
16 if ((bitfield[i] & (1 << j)) == 0) {
17 System.out.println (i * 8 + j);
18 return;
19 }
20 }
21 }
22 }
Solutions to Chapter 11 | System Design and Memory Limits
203 Cracking the Coding Interview | Concepts and Algorithms
Follow Up: What if we have only 10 MB memory?
It’s possible to find a missing integer with just two passes of the data set. We can divide up the integers into blocks of some size (we’ll discuss how to decide on a size later). Let’s just assume that we divide up the integers into blocks of 1000. So, block 0 represents the numbers 0 through 999, block 1 represents blocks 1000 - 1999, etc. Since the range of ints is finite, we know that the number of blocks needed is finite.
In the first pass, we count how many ints are in each block. That is, if we see 552, we know that that is in block 0, we increment counter[0]. If we see 1425, we know that that is in block 1, so we increment counter[1].
At the end of the first pass, we’ll be able to quickly spot a block that is missing a number. If our block size is 1000, then any block which has fewer than 1000 numbers must be missing a number. Pick any one of those blocks.
In the second pass, we’ll actually look for which number is missing. We can do this by creating a simple bit vector of size 1000. We iterate through the file, and for each number that should be in our block, we set the appropriate bit in the bit vector. By the end, we’ll know which number (or numbers) is missing.
Now we just have to decide what the block size is.
A quick answer is 2^20 values per block. We will need an array with 2^12 block counters and a bit vector in 2^17 bytes. Both of these can comfortably fit in 10*2^20 bytes.
What’s the smallest footprint? When the array of block counters occupies the same memory as the bit vector. Let N = 2^32.
counters (bytes): blocks * 4
bit vector (bytes): (N / blocks) / 8
blocks * 4 = (N / blocks) / 8
blocks^2 = N / 32
blocks = sqrt(N/2)/4
It’s possible to find a missing integer with just under 65KB (or, more exactly, sqrt(2)*2^15 bytes).
1 int bitsize = 1048576; // 2^20 bits (2^17 bytes)
2 int blockNum = 4096; // 2^12
3 byte[] bitfield = new byte[bitsize/8];
4 int[] blocks = new int[blockNum];
5
6 void findOpenNumber() throws FileNotFoundException {
7 int starting = -1;
8 Scanner in = new Scanner (new FileReader (“input_file_q11_4.txt”));
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 204
9 while (in.hasNextInt()) {
10 int n = in.nextInt();
11 blocks[n / (bitfield.length * 8)]++;
12 }
13
14 for (int i = 0; i < blocks.length; i++) {
15 if (blocks[i] < bitfield.length * 8){
16 /* if value < 2^20, then at least 1 number is missing in
17 * that section. */
18 starting = i * bitfield.length * 8;
19 break;
20 }
21 }
22
23 in = new Scanner(new FileReader(“input_file_q11_4.txt”));
24 while (in.hasNextInt()) {
25 int n = in.nextInt();
26 /* If the number is inside the block that’s missing numbers,
27 * we record it */
28 if( n >= starting && n < starting + bitfield.length * 8){
29 bitfield [(n-starting) / 8] |= 1 << ((n - starting) % 8);
30 }
31 }
32
33 for (int i = 0 ; i < bitfield.length; i++) {
34 for (int j = 0; j < 8; j++) {
35 /* Retrieves the individual bits of each byte. When 0 bit
36 * is found, finds the corresponding value. */
37 if ((bitfield[i] & (1 << j)) == 0) {
38 System.out.println(i * 8 + j + starting);
39 return;
40 }
41 }
42 }
43 }
Solutions to Chapter 11 | System Design and Memory Limits
205 Cracking the Coding Interview | Concepts and Algorithms
11.4 You have an array with all the numbers from 1 to N, where N is at most 32,000. The array may have duplicate entries and you do not know what N is. With only 4KB of memory available, how would you print all duplicate elements in the array?
pg 72
SOLUTION
We have 4KB of memory which means we can address up to 8 * 4 * (2^10) bits. Note that 32* (2^10) bits is greater than 32000. We can create a bit vector with 32000 bits, where each bit represents one integer.
NOTE: While this isn’t an especially difficult problem, it’s important to implement this cleanly. We will define our own bit vector class to hold a large bit vector.
1 public static void checkDuplicates(int[] array) {
2 BitSet bs = new BitSet(32000);
3 for (int i = 0; i < array.length; i++) {
4 int num = array[i];
5 int num0 = num - 1; // bitset starts at 0, numbers start at 1
6 if (bs.get(num0)) {
7 System.out.println(num);
8 } else {
9 bs.set(num0);
10 }
11 }
12 }
13
14 class BitSet {
15 int[] bitset;
16
17 public BitSet(int size) {
18 bitset = new int[size >> 5]; // divide by 32
19 }
20
21 boolean get(int pos) {
22 int wordNumber = (pos >> 5); // divide by 32
23 int bitNumber = (pos & 0x1F); // mod 32
24 return (bitset[wordNumber] & (1 << bitNumber)) != 0;
25 }
26
27 void set(int pos) {
28 int wordNumber = (pos >> 5); // divide by 32
29 int bitNumber = (pos & 0x1F); // mod 32
30 bitset[wordNumber] |= 1 << bitNumber;
31 }
32 }
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 206
11.5 If you were designing a web crawler, how would you avoid getting into infinite loops?
pg 72
SOLUTION
First, how does the crawler get into a loop? The answer is very simple: when we re-parse an already parsed page. This would mean that we revisit all the links found in that page, and this would continue in a circular fashion.
Be careful about what the interviewer considers the “same” page. Is it URL or content? One could easily get redirected to a previously crawled page.
So how do we stop visiting an already visited page? The web is a graph-based structure, and we commonly use DFS (depth first search) and BFS (breadth first search) for traversing graphs. We can mark already visited pages the same way that we would in a BFS/DFS.
We can easily prove that this algorithm will terminate in any case. We know that each step of the algorithm will parse only new pages, not already visited pages. So, if we assume that we have N number of unvisited pages, then at every step we are reducing N (N-1) by 1. That proves that our algorithm will continue until they are only N steps.
SUGGESTIONS AND OBSERVATIONS
»»This question has a lot of ambiguity. Ask clarifying questions!
»»Be prepared to answer questions about coverage.
»»What kind of pages will you hit with a DFS versus a BFS?
»»What will you do when your crawler runs into a honey pot that generates an infinite subgraph for you to wander about?
Solutions to Chapter 11 | System Design and Memory Limits
207 Cracking the Coding Interview | Concepts and Algorithms
11.6 You have a billion urls, where each is a huge page. How do you detect the duplicate documents?
pg 72
SOLUTION
Observations:
1. Pages are huge, so bringing all of them in memory is a costly affair. We need a shorter representation of pages in memory. A hash is an obvious choice for this.
2. Billions of urls exist so we don’t want to compare every page with every other page (that would be O(n^2)).
Based on the above two observations we can derive an algorithm which is as follows:
1. Iterate through the pages and compute the hash table of each one.
2. Check if the hash value is in the hash table. If it is, throw out the url as a duplicate. If it is not, then keep the url and insert it in into the hash table.
This algorithm will provide us a list of unique urls. But wait, can this fit on one computer?
»»How much space does each page take up in the hash table?
»»Each page hashes to a four byte value.
»»Each url is an average of 30 characters, so that’s another 30 bytes at least.
»»Each url takes up roughly 34 bytes.
»»34 bytes * 1 billion = 31.6 gigabytes. We’re going to have trouble holding that all in memory!
What do we do?
»»We could split this up into files. We’ll have to deal with the file loading / unloading—ugh.
»»We could hash to disk. Size wouldn’t be a problem, but access time might. A hash table on disk would require a random access read for each check and write to store a viewed url. This could take msecs waiting for seek and rotational latencies. Elevator algorithms could elimate random bouncing from track to track.
»»Or, we could split this up across machines, and deal with network latency. Let’s go with this solution, and assume we have n machines.
»»First, we hash the document to get a hash value v
»»v%n tells us which machine this document’s hash table can be found on.
»»v / n is the value in the hash table that is located on its machine.
Solutions to Chapter 11 | System Design and Memory Limits
. CareerCup com 208
11.7 You have to design a database that can store terabytes of data. It should support efficient range queries. How would you do it?
pg 72
SOLUTION
Construct an index for each field that requires range queries. Use a B+ tree to implement the index. A B+ tree organizes sorted data for efficient insertion, retrieval and removal of records. Each record is identified by a key (for this problem, it is the field value). Since it is a dynamic, multilevel index, finding the beginning of the range depends only on the height of the tree, which is usually quite small. Record references are stored in the leaves, sorted by the key. Additional records can be found by following a next block reference. Records will be sequentially available until the key value reaches the maximum value specified in the query. Thus, runtimes will be dominated by the number of elements in a range.
Avoid using trees that store data at interior nodes, as traversing the tree will be expensive since it won’t be resident in memory.
Solutions to Chapter 12 | Testing
Cracking the Coding Interview | Concepts and Algorithms
209
12.1 Find the mistake(s) in the following code:
1 unsigned int i;
2 for (i = 100; i <= 0; --i)
3 printf(“%d\n”, i);
pg 70
SOLUTION
The printf will never get executed, as “i” is initialized to 100, so condition check “i <= 0” will fail.
Suppose the code is changed to “i >= 0.” Then, it will become an infinite loop, because “i” is an unsigned int which can’t be negative.
The correct code to print all numbers from 100 to 1, is “i > 0”.
1 unsigned int i;
2 for (i = 100; i > 0; --i)
3 printf(“%d\n”, i);
One additional correction is to use %u in place of %d, as we are printing unsigned int.
1 unsigned int i;
2 for (i = 100; i > 0; --i)
3 printf(“%u\n”, i);
Solutions to Chapter 12 | Testing
210
CareerCup.com
12.2 You are given the source to an application which crashes when it is run. After running it ten times in a debugger, you find it never crashes in the same place. The application is single threaded, and uses only the C standard library. What programming errors could be causing this crash? How would you test each one?
pg 70
SOLUTION
The question largely depends on the type of application being diagnosed. However, we can give some general causes of random crashes.
1. Random variable: The application uses some random number or variable component which may not be fixed for every execution of the program. Examples include: user input, a random number generated by the program, or the time of day.
2. Memory Leak: The program may have run out of memory. Other culprits are totally random for each run since it depends on the number of processes running at that particular time. This also includes heap overflow or corruption of data on the stack.
It is also possible that the program depends on another application / external module that could lead to the crash. If our application, for example, depends on some system attributes and they are modified by another program, then this interference may lead to a crash. Programs which interact with hardware are more prone to these errors.
In an interview, we should ask about which kind of application is being run. This information may give you some idea about the kind of error the interviewer is looking for. For example, a web server is more prone to memory leakage, whereas a program that runs close to the system level is more prone to crashes due to system dependencies.
Solutions to Chapter 12 | Testing
211 Cracking the Coding Interview | Concepts and Algorithms
12.3 We have the following method used in a chess game: boolean canMoveTo(int x, int y) x and y are the coordinates of the chess board and it returns whether or not the piece can move to that position. Explain how you would test this method.
pg 70
SOLUTION
There are two primary types of testing we should do:
Validation of input/output:
We should validate both the input and output to make sure that each are valid. This might entail:
1. Checking whether input is within the board limit.
»»Attempt to pass in negative numbers
»»Attempt to pass in x which is larger than the width
»»Attempt to pass in y which is larger than the width
Depending on the implementation, these should either return false or throw an exception.
2. Checking if output is within the valid set of return values. (Not an issue in this case, since there are no “invalid” boolean values.)
Functional testing:
Ideally, we would like to test every possible board, but this is far too big. We can do a reasonable coverage of boards however. There are 6 pieces in chess, so we need to do something like this:
1 foreach piece a:
2 for each other type of piece b (6 types + empty space)
3 foreach direction d
4 Create a board with piece a.
5 Place piece b in direction d.
6 Try to move – check return value.
Solutions to Chapter 12 | Testing
. CareerCup com 212
12.4 How would you load test a webpage without using any test tools?
pg 70
SOLUTION
Load testing helps to identify a web application’s maximum operating capacity, as well as any bottlenecks that may interfere with its performance. Similarly, it can check how an application responds to variations in load.
To perform load testing, we must first identify the performance-critical scenarios and the metrics which fulfill our performance objectives. Typical criteria include:
»»response time
»»throughput
»»resource utilization
»»maximum load that the system can bear.
Then, we design tests to simulate the load, taking care to measure each of these criteria.
In the absence of formal testing tools, we can basically create our own. For example, we could simulate concurrent users by creating thousands of virtual users. We would write a multi-threaded program with thousands of threads, where each thread acts as a real-world user loading the page. For each user, we would programmatically measure response time, data I/O, etc.
We would then analyze the results based on the data gathered during the tests and compare it with the accepted values.
Solutions to Chapter 12 | Testing
213 Cracking the Coding Interview | Concepts and Algorithms
12.5 How would you test a pen?
pg 70
SOLUTION
This problem is largely about understand the constraints: what exactly is the pen? You should ask a lot of questions to understand what exactly you are trying to test. To illustrate the technique in this problem, let us guide you through a mock-conversation.
Interviewer: How would you test a pen?
Candidate: Let me find out a bit about the pen. Who is going to use the pen?
Interviewer: Probably children.
Candidate: Ok, that’s interesting. What will they be doing with it? Will they be writing, drawing, or doing something else with it?
Interviewer: Drawing.
Candidate: Ok, great. On what? Paper? Clothing? Walls?
Interviewer: On clothing.
Candidate: Great. What kind of tip does the pen have? Felt? Ball point? Is it intended to wash off, or is it intended to be permanent?
Interviewer: It’s intended to wash off.
…. many questions later ...
Candidate: Ok, so as I understand it, we have a pen that is being targeted at 5—10 year olds. The pen has a felt tip and comes in red, green, blue and black. It’s intended to wash off clothing. Is that correct?
…
The candidate now has a problem that is significantly different from what it initially seemed to be. Thus, the candidate might now want to test:
1. Does the pen wash off with warm water, cold water, and luke warm water?
2. Does the pen wash off after staying on the clothing for several weeks? What happens if you wash the clothing while the pen is still wet?
3. Is the pen safe (e.g.—non-toxic) for children?
and so on...
Solutions to Chapter 12 | Testing
. CareerCup com 214
12.6 How would you test an ATM in a distributed banking system?
pg 70
SOLUTION
The first thing to do on this question is to clarify assumptions. Ask the following questions:
»»Who is going to use the ATM? Answers might be “anyone,” or it might be “blind people” - or any number of other answers.
»»What are they going to use it for? Answers might be “withdrawing money,” “transferring money,” “checking their balance,” or many other answers.
»»What tools do we have to test? Do we have access to the code, or just the ATM machine?
Remember: a good tester makes sure she knows what she’s testing!
Here are a few test cases for how to test just the withdrawing functionality:
»»Withdrawing money less than the account balance
»»Withdrawing money greater than the account balance
»»Withdrawing money equal to the account balance
»»Withdrawing money from an ATM and from the internet at the same time
»»Withdrawing money when the connection to the bank’s network is lost
»»Withdrawing money from multiple ATMs simultaneously
Solutions to Chapter 13 | C++
Cracking the Coding Interview | Knowledge Based
215
13.1 Write a method to print the last K lines of an input file using C++.
pg 76
SOLUTION
One brute force way could be to count the number of lines (N) and then print from N-10 to Nth line. But, this requires two reads of the file – potentially very costly if the file is large.
We need a solution which allows us to read just once and be able to print the last K lines. We can create extra space for K lines and then store each set of K lines in the array. So, initially, our array has lines 0 through 9, then 1 through 10, then 2 through 11, etc (if K = 10). Each time that we read a new line, we purge the oldest line from the array. Instead of shifting the array each time (very inefficient), we will use a circular array. This will allow us to always find the oldest element in O(1) time.
Example of inserting elements into a circular array:
step 1 (initially): array = {a, b, c, d, e, f}. p = 0
step 2 (insert g): array = {g, b, c, d, e, f}. p = 1
step 3 (insert h): array = {g, h, c, d, e, f}. p = 2
step 4 (insert i): array = {g, h, i, d, e, f}. p = 3
Code:
1 string L[K];
2 int lines = 0;
3 while (file.good()) {
4 getline(file, L[lines % K]); // read file line by line
5 ++lines;
6 }
7 // if less than K lines were read, print them all
8 int start, count;
9 if (lines < K) {
10 start = 0;
11 count = lines;
12 } else {
13 start = lines % K;
14 count = K;
15 }
16 for (int i = 0; i < count; ++i) {
17 cout << L[(start + i) % K] << endl;
18 }
OBSERVATIONS AND SUGGESTIONS:
»»Note, if you do printf(L[(index + i) % K]) when there are %’s in the string, bad things will happen.
Solutions to Chapter 13 | C++
216
CareerCup.com
13.2 Compare and contrast a hash table vs. an STL map. How is a hash table implemented? If the number of inputs is small, what data structure options can be used instead of a hash table?
pg 76
SOLUTION
Compare and contrast Hash Table vs. STL map
In a hash table, a value is stored by applying hash function on a key. Thus, values are not stored in a hash table in sorted order. Additionally, since hash tables use the key to find the index that will store the value, an insert/lookup can be done in amortised O(1) time (assuming only a few collisions in the hashtable). One must also handle potential collisions in a hashtable.
In an STL map, insertion of key/value pair is in sorted order of key. It uses a tree to store values, which is why an O(log N) insert/lookup is required. There is also no need to handle collisions. An STL map works well for things like:
»»find min element
»»find max element
»»print elements in sorted order
»»find the exact element or, if the element is not found, find the next smallest number
How is a hash table implemented?
1. A good hash function is required (e.g.: operation % prime number) to ensure that the hash values are uniformly distributed.
2. A collision resolving method is also needed: chaining (good for dense table entries), probing (good for sparse table entries), etc.
3. Implement methods to dynamically increase or decrease the hash table size on a given criterion. For example, when the [number of elements] by [table size] ratio is greater than the fixed threshold, increase the hash table size by creating a new hash table and transfer the entries from the old table to the new table by computing the index using new hash function.
What can be used instead of a hash table, if the number of inputs is small?
You can use an STL map. Although this takes O(log n) time, since the number of inputs is small, this time is negligible.
Solutions to Chapter 13 | C++
217 Cracking the Coding Interview | Knowledge Based
13.3 How do virtual functions work in C++?
pg 76
SOLUTION
A virtual function depends on a “vtable” or “Virtual Table”. If any function of a class is declared as virtual, a v-table is constructed which stores addresses of the virtual functions of this class. The compiler also adds a hidden vptr variable in all such classes which points to the vtable of that class. If a virtual function is not overridden in the derived class, the vtable of the derived class stores the address of the function in his parent class. The v-table is used to resolve the address of the function, for whenever the virtual function is called. Dynamic binding in C++ is therefore performed through the vtable mechanism.
Thus, when we assign the derived class object to the base class pointer, the vptr points to the vtable of the derived class. This assignment ensures that the most derived virtual function gets called.
1 class Shape {
2 public:
3 int edge_length;
4 virtual int circumference () {
5 cout << “Circumference of Base Class\n”;
6 return 0;
7 }
8 };
9 class Triangle: public Shape {
10 public:
11 int circumference () {
12 cout<< “Circumference of Triangle Class\n”;
13 return 3 * edge_length;
14 }
15 };
16 void main() {
17 Shape * x = new Shape();
18 x->circumference(); // prints “Circumference of Base Class”
19 Shape *y = new Triangle();
20 y->circumference(); // prints “Circumference of Triangle Class”
21 }
In the above example, circumference is a virtual function in shape class, so it becomes virtual in each of the derived classes (triangle, rectangle). C++ non-virtual function calls are resolved at compile time with static binding, while virtual function calls are resolved at run time with dynamic binding.
Solutions to Chapter 13 | C++
. CareerCup com 218
13.4 What is the difference between deep copy and shallow copy? Explain how you would use each.
pg 76
SOLUTION
1 struct Test {
2 char * ptr;
3 };
4 void shallow_copy(Test & src, Test & dest) {
5 dest.ptr = src.ptr;
6 }
7 void deep_copy(Test & src, Test & dest) {
8 dest.ptr = malloc(strlen(src.ptr) + 1);
9 memcpy(dest.ptr, src.ptr);
10 }
Note that shallow_copy may cause a lot of programming run-time errors, especially with the creation and deletion of objects. Shallow copy should be used very carefully and only when a programmer really understands what he wants to do. In most cases shallow copy is used when there is a need to pass information about a complex structure without actual duplication of data (e.g., call by reference). One must also be careful with destruction of shallow copy.
In real life, shallow copy is rarely used. There is an important programming concept called “smart pointer” that, in some sense, is an enhancement of the shallow copy concept.
Deep copy should be used in most cases, especially when the size of the copied structure is small.
Solutions to Chapter 13 | C++
219 Cracking the Coding Interview | Knowledge Based
13.5 What is the significance of the keyword “volatile” in C?
pg 76
SOLUTION
Volatile informs the compiler that the value of the variable can change from the outside, without any update done by the code.
Declaring a simple volatile variable:
volatile int x;
int volatile x;
Declaring a pointer variable for a volatile memory (only the pointer address is volatile):
volatile int * x;
int volatile * x;
Declaring a volatile pointer variable for a non-volatile memory (only memory contained is volatile):
int * volatile x;
Declaring a volatile variable pointer for a volatile memory (both pointer address and memory contained are volatile):
volatile int * volatile x;
int volatile * volatile x;
Volatile variables are not optimized, but this can actually be useful. Imagine this function:
1 int opt = 1;
2 void Fn(void) {
3 start:
4 if (opt == 1) goto start;
5 else break;
6 }
At first glance, our code appears to loop infinitely. The compiler will try to optimize it to:
1 void Fn(void) {
2 start:
3 int opt = 1;
4 if (true)
5 goto start;
6 }
This becomes an infinite loop. However, an external program might write ‘0’ to the location of variable opt. Volatile variables are also useful when multi-threaded programs have global variables and any thread can modify these shared variables. Of course, we don’t want optimization on them.
Solutions to Chapter 13 | C++
. CareerCup com 220
13.6 What is name hiding in C++?
pg 76
SOLUTION
Let us explain through an example. In C++, when you have a class with an overloaded method, and you then extend and override that method, you must override all of the overloaded methods.
For example:
1 class FirstClass {
2 public:
3 virtual void MethodA (int);
4 virtual void MethodA (int, int);
5 };
6 void FirstClass::MethodA (int i) {
7 std::cout << “ONE!!\n”;
8 }
9 void FirstClass::MethodA (int i, int j) {
10 std::cout << “TWO!!\n”;
11 }
This is a simple class with two methods (or one overloaded method). If you want to override the one-parameter version, you can do the following:
1 class SecondClass : public FirstClass {
2 public:
3 void MethodA (int);
4 };
5 void SecondClass::MethodA (int i) {
6 std::cout << “THREE!!\n”;
7 }
8 void main () {
9 SecondClass a;
10 a.MethodA (1);
11 a.MethodA (1, 1);
12 }
However, the second call won’t work, since the two-parameter MethodA is not visible. That is name hiding.
Solutions to Chapter 13 | C++
221 Cracking the Coding Interview | Knowledge Based
13.7 Why does a destructor in base class need to be declared virtual?
pg 76
SOLUTION
Calling a method with an object pointer always invokes:
»»the most derived class function, if a method is virtual.
»»the function implementation corresponding to the object pointer type (used to call the method), if a method is non-virtual.
A virtual destructor works in the same way. A destructor gets called when an object goes out of scope or when we call delete on an object pointer.
When any derived class object goes out of scope, the destructor of that derived class gets called first. It then calls its parent class destructor so memory allocated to the object is properly released.
But, if we call delete on a base pointer which points to a derived class object, the base class destructor gets called first (for non-virtual function). For example:
1 class Base {
2 public:
3 Base() { cout << “Base Constructor “ << endl; }
4 ~Base() { cout << “Base Destructor “ << endl; } /* see below */
5 };
6 class Derived: public Base {
7 public:
8 Derived() { cout << ”Derived Constructor “ << endl; }
9 ~Derived() { cout << ”Derived Destructor “ << endl; }
10 };
11 void main() {
12 Base *p = new Derived();
13 delete p;
14 }
Output:
Base Constructor
Derived Constructor
Base Destructor
If we declare the base class destructor as virtual, this makes all the derived class destructors virtual as well.
If we replace the above destructor with:
1 virtual ~Base() {
2 cout << “Base Destructor” << endl;
3 }
Solutions to Chapter 13 | C++
. CareerCup com 222
Then the output becomes:
Base Constructor
Derived Constructor
Derived Destructor
Base Destructor
So we should use virtual destructors if we call delete on a base class pointer which points to a derived class.
Solutions to Chapter 13 | C++
223 Cracking the Coding Interview | Knowledge Based
13.8 Write a method that takes a pointer to a Node structure as a parameter and returns a complete copy of the passed-in data structure. The Node structure contains two pointers to other Node structures.
pg 76
SOLUTION
The algorithm will maintain a mapping from a node address in the original structure to the corresponding node in the new structure. This mapping will allow us to discover previously copied nodes during a traditional depth first traversal of the structure. (Traversals often mark visited nodes--the mark can take many forms and does not necessarily need to be stored in the node.) Thus, we have a simple recursive algorithm:
1 typedef map<Node*, Node*> NodeMap;
2
3 Node * copy_recursive(Node * cur, NodeMap & nodeMap) {
4 if(cur == NULL) {
5 return NULL;
6 }
7 NodeMap::iterator i = nodeMap.find(cur);
8 if (i != nodeMap.end()) {
9 // we’ve been here before, return the copy
10 return i->second;
11 }
12 Node * node = new Node;
13 nodeMap[cur] = node; // map current node before traversing links
14 node->ptr1 = copy_recursive(cur->ptr1, nodeMap);
15 node->ptr2 = copy_recursive(cur->ptr2, nodeMap);
16 return node;
17 }
18 Node * copy_structure(Node * root) {
19 NodeMap nodeMap; // we will need an empty map
20 return copy_recursive(root, nodeMap);
21 }
22
Solutions to Chapter 13 | C++
. CareerCup com 224
13.9 Write a smart pointer (smart_ptr) class.
pg 76
SOLUTION
Smart_ptr is the same as a normal pointer, but it provides safety via automatic memory. It avoids dangling pointers, memory leaks, allocation failures etc. The smart pointer must maintain a single reference count for all instances.
1 template <class T> class SmartPointer {
2 public:
3 SmartPointer(T * ptr) {
4 ref = ptr;
5 ref_count = (unsigned*)malloc(sizeof(unsigned));
6 *ref_count = 1;
7 }
8 SmartPointer(SmartPointer<T> & sptr) {
9 ref = sptr.ref;
10 ref_count = sptr.ref_count;
11 ++*ref_count;
12 }
13 SmartPointer<T> & operator=(SmartPointer<T> & sptr) {
14 if (this != &sptr) {
15 ref = sptr.ref;
16 ref_count = sptr.ref_count;
17 ++*ref_count;
18 }
19 return *this;
20 }
21 ~SmartPointer() {
22 --*ref_count;
23 if (*ref_count == 0) {
24 delete ref;
25 free(ref_count);
26 ref = NULL;
27 ref_count = NULL;
28 }
29 }
30 T getValue() { return *ref; }
31 protected:
32 T * ref;
33 unsigned * ref_count;
34 };
Solutions to Chapter 14 | Java
Cracking the Coding Interview | Knowledge Based
225
14.1 In terms of inheritance, what is the effect of keeping a constructor private?
pg 78
SOLUTION
Declaring the constructor private will ensure that no one outside of the class can directly instantiate the class. In this case, the only way to create an instance of the class is by providing a static public method, as is done when using the Factory Method Pattern.
Additionally, because the constructor is private, the class also cannot be inherited.
Solutions to Chapter 14 | Java
226
CareerCup.com
14.2 In Java, does the finally block gets executed if we insert a return statement inside the try block of a try-catch-finally?
pg 78
SOLUTION
Yes, it will get executed.
The finally block gets executed when the try block exists. However, even when we attempt to exit within the try block (normal exit, return, continue, break or any exception), the finally block will still be executed.
Note: There are some cases in which the finally block will not get executed: if the virtual machine exits in between try/catch block execution, or the thread which is executing try/catch block gets killed.
Solutions to Chapter 14 | Java
227 Cracking the Coding Interview | Knowledge Based
14.3 What is the difference between final, finally, and finalize?
pg 78
SOLUTIONS
Final
When applied to a variable (primitive): The value of the variable cannot change.
When applied to a variable (reference): The reference variable cannot point to any other object on the heap.
When applied to a method: The method cannot be overridden.
When applied to a class: The class cannot be subclassed.
Finally
There is an optional finally block after the try block or after the catch block. Statements in the finally block will always be executed (except if JVM exits from the try block). The finally block is used to write the clean up code.
Finalize
This is the method that the JVM runs before running the garbage collector.
Solutions to Chapter 14 | Java
. CareerCup com 228
14.4 Explain the difference between templates in C++ and generics in Java.
pg 78
SOLUTION
C++ Templates
Java Generics
Classes and functions can be templated.
Classes and methods can be genericized.
Parameters can be any type or integral value.
Parameters can only be reference types (not primitive types).
Separate copies of the class or function are likely to be generated for each type parameter when compiled.
One version of the class or function is compiled, works for all type parameters.
Objects of a class with different type parameters are different types at run time.
Type parameters are erased when compiled; objects of a class with different type parameters are the same type at run time.
Implementation source code of the templated class or function must be included in order to use it (declaration insufficient).
Signature of the class or function from a compiled class file is sufficient to use it.
Templates can be specialized - a separate implementation could be provided for a particular template parameter.
Generics cannot be specialized.
Does not support wildcards. Instead, return types are often available as nested typedefs.
Supports wildcard as type parameter if it is only used once.
Does not directly support bounding of type parameters, but metaprogramming provides this.
Supports bounding of type parameters with "extends" and "super" for upper and lower bounds, respectively; allows enforcement of relationships between type parameters.
Allows instantiation of class of type parameter type.
Does not allow instantiation of class of type parameter type.
Type parameter of templated class can be used for static methods and variables.
Type parameter of templated class cannot be used for static methods and variables.
Static variables are not shared between classes of different type parameters.
Static variables are shared between instances of a classes of different type parameters.
From http://en.wikipedia.org/wiki/Comparison_of_Java_and_C%2B%2B#Templates_vs._Generics
Solutions to Chapter 14 | Java
229 Cracking the Coding Interview | Knowledge Based
14.5 Explain what object reflection is in Java and why it is useful.
pg 78
SOLUTION
Object Reflection is a feature in Java which provides a way to get reflective information about Java classes and objects, such as:
1. Getting information about methods and fields present inside the class at run time.
2. Creating a new instance of a class.
3. Getting and setting the object fields directly by getting field reference, regardless of what the access modifier is.
1 import java.lang.reflect.*;
2
3 public class Sample {
4 public static void main(String args[]) {
5 try {
6 Class c = Class.forName(“java.sql.Connection”);
7 Method m[] = c.getDeclaredMethods();
8 for (int i = 0; i < 3; i++) {
9 System.out.println(m[i].toString());
10 }
11 } catch (Throwable e) {
12 System.err.println(e);
13 }
14 }
15 }
This code’s output is the names of the first 3 methods inside the “java.sql.Connection” class (with fully qualified parameters).
Why it is useful:
1. Helps in observing or manipulating the runtime behavior of applications.
2. Useful while debugging and testing applications, as it allows direct access to methods, constructors, fields, etc.
Solutions to Chapter 14 | Java
. CareerCup com 230
14.6 Suppose you are using a map in your program, how would you count the number of times the program calls the put() and get() functions?
pg 78
SOLUTION
One simple solution is to put count variables for get() and put() methods and, whenever they are called, increment the count. We can also achieve this by extending the existing library map and overriding the get() and put() functions.
At first glance, this seems to work. However, what if we created multiple instances of the map? How would you sum up the total count for each map object?
The simplest solution for this is to keep the count variables static. We know static variables have only one copy for all objects of the class so the total count would be reflected in count variables.
Solutions to Chapter 15 | Databases
Cracking the Coding Interview | Knowledge Based
231
15.1 Write a method to find the number of employees in each department.
pg 80
SOLUTION
This problem uses a straight-forward join of Departments and Employees. Note that we use a left join instead of an inner join because we want to include Departments with 0 employees.
1 select Dept_Name, Departments.Dept_ID, count(*) as ‘num_employees’
2 from Departments
3 left join Employees
4 on Employees.Dept_ID = Departments.Dept_ID
5 group by Departments.Dept_ID, Dept_Name
Solutions to Chapter 15 | Databases
232
CareerCup.com
15.2 What are the different types of joins? Please explain how they differ and why certain types are better in certain situations.
pg 80
SOLUTION
JOIN is used to combine the results of two tables. To perform a join, each of the tables must have at least one field which will be used to find matching records from the other table. The join type defines which records will go into the result set.
Let’s take for example two tables: one table lists “regular” beverages, and another lists the calorie-free beverages. Each table has two fields: the beverage name and its product code. The “code” field will be used to perform the record matching.
Regular Beverages:
Name
Code
Budweiser
BUDWEISER
Coca-Cola
COCACOLA
Pepsi
PEPSI
Calorie-Free Beverages:
Code
Name
COCACOLA
Diet Coca-Cola
FRESCA
Fresca
PEPSI
Diet Pepsi
PEPSI
Pepsi Light
Water
Purified Water
Let’s join this table by the code field. Whereas the order of the joined tables makes sense in some cases, we will consider the following statement:
[Beverage] JOIN [Calorie-Free Beverage]
i.e. [Beverage] is from the left of the join operator, and [Calorie-Free Beverage] is from the right.
1. INNER JOIN: Result set will contain only those data where the criteria match. In our example we will get 3 records: 1 with COCACOLA and 2 with PEPSI codes.
2. OUTER JOIN: OUTER JOIN will always contain the results of INNER JOIN, however it can contain some records that have no matching record in other table. OUTER JOINs are divided to following subtypes:
Solutions to Chapter 15 | Databases
233 Cracking the Coding Interview | Knowledge Based
2.1. LEFT OUTER JOIN, or simply LEFT JOIN: The result will contain all records from the left table. If no matching records were found in the right table, then its fields will contain the NULL values. In our example, we would get 4 records. In addition to INNER JOIN results, BUDWEISER will be listed, because it was in the left table.
2.2. RIGHT OUTER JOIN, or simply RIGHT JOIN: This type of join is the opposite of LEFT JOIN; it will contain all records from the right table, and missing fields from the left table will contain NULL. If we have two tables A and B, then we can say that statement A LEFT JOIN B is equivalent to statement B RIGHT JOIN A.
In our example, we will get 5 records. In addition to INNER JOIN results, FRESCA and WATER records will be listed.
2.3. FULL OUTER JOIN
This type of join combines the results of LEFT and RIGHT joins. All records from both tables will be part of the result set, whether the matching record exists in the other table or not. If no matching record was found then the corresponding result fields will have a NULL value.
In our example, we will get 6 records.
Solutions to Chapter 15 | Databases
. CareerCup com 234
15.3 What is denormalization? Explain the pros and cons.
pg 80
SOLUTION
Denormalization is the process of attempting to optimize the performance of a database by adding redundant data or by grouping data. In some cases, denormalization helps cover up the inefficiencies inherent in relational database software. A relational normalized database imposes a heavy access load over physical storage of data even if it is well tuned for high performance.
A normalized design will often store different but related pieces of information in separate logical tables (called relations). If these relations are stored physically as separate disk files, completing a database query that draws information from several relations (a join operation) can be slow. If many relations are joined, it may be prohibitively slow. There are two strategies for dealing with this. The preferred method is to keep the logical design normalized, but allow the database management system (DBMS) to store additional redundant information on disk to optimize query response. In this case, it is the DBMS software’s responsibility to ensure that any redundant copies are kept consistent. This method is often implemented in SQL as indexed views (Microsoft SQL Server) or materialized views (Oracle). A view represents information in a format convenient for querying, and the index ensures that queries against the view are optimized.
The more usual approach is to denormalize the logical data design. With care, this can achieve a similar improvement in query response, but at a cost—it is now the database designer’s responsibility to ensure that the denormalized database does not become inconsistent. This is done by creating rules in the database called constraints, that specify how the redundant copies of information must be kept synchronized. It is the increase in logical complexity of the database design and the added complexity of the additional constraints that make this approach hazardous. Moreover, constraints introduce a trade-off, speeding up reads (SELECT in SQL) while slowing down writes (INSERT, UPDATE, and DELETE). This means a denormalized database under heavy write load may actually offer worse performance than its functionally equivalent normalized counterpart.
A denormalized data model is not the same as a data model that has not been normalized, and denormalization should only take place after a satisfactory level of normalization has taken place and that any required constraints and/or rules have been created to deal with the inherent anomalies in the design. For example, all the relations are in third normal form and any relations with join and multivalued dependencies are handled appropriately.
From http://en.wikipedia.org/wiki/Denormalization
Solutions to Chapter 15 | Databases
235 Cracking the Coding Interview | Knowledge Based
15.4 Draw an entity-relationship diagram for a database with companies, people, and professionals (people who work for companies).
pg 80
SOLUTION
People who work for companies are Professionals. So there is an ISA (is a) relationship between People and Professionals (or we could say that a Professional is derived from People).
Each Professional has additional information such as degree, work experiences, etc, in addition to the properties derived from People.
A Professional works for one company at a time, but Companies can hire many Professionals, so there is a Many to One relationship between Professionals and Companies. This “Works For” relationship can store attributes such as date of joining the company, salary, etc. These attributes are only defined when we relate a Professional with a Company.
A Person can have multiple phone numbers, which is why Phone is a multi-valued attribute.
Professional
People
Companies
Works For
Degree
Experience
Salary
Address
CName
CID
Date of Joining
Address
Phone
ISA
N
1
DOB
Sex
PName
PID
Solutions to Chapter 15 | Databases
. CareerCup com 236
15.5 Imagine a simple database storing information for students’ grades. Design what this database might look like, and provide a SQL query to return a list of the honor roll students (top 10%), sorted by their grade point average.
pg 80
SOLUTION
In a simplistic database, we’ll have at least these three objects: Students, Courses, and CourseEnrollment. Students will have at least the student name and ID, and will likely have other personal information. Courses will contain the course name and ID, and will likely contain the course description, professor, etc. CourseEnrollment will pair Students and Courses, and will also contain a field for CourseGrade. We will assume that CourseGrade is an integer.
Our SQL query to get the list of honor roll students might look like this:
1 SELECT StudentName, GPA
2 FROM (
3 SELECT top 10 percent Avg(CourseEnrollment.Grade) AS GPA,
4 CourseEnrollment.StudentID
5 FROM CourseEnrollment
6 GROUP BY CourseEnrollment.StudentID
7 ORDER BY Avg(CourseEnrollment.Grade)) Honors
8 INNER JOIN Students ON Honors.StudentID = Students.StudentID
This database could get arbitrarily more complicated if we wanted to add in professor information, billing, etc.
Solutions to Chapter 16 | Low Level
Cracking the Coding Interview | Knowledge Based
237
16.1 Explain the following terms: virtual memory, page fault, thrashing.
pg 82
SOLUTION
Virtual memory is a computer system technique which gives an application program the impression that it has contiguous working memory (an address space), while in fact it may be physically fragmented and may even overflow on to disk storage. Systems that use this technique make programming of large applications easier and use real physical memory (e.g. RAM) more efficiently than those without virtual memory.
http://en.wikipedia.org/wiki/Virtual_memory
Page Fault: A page is a fixed-length block of memory that is used as a unit of transfer between physical memory and external storage like a disk, and a page fault is an interrupt (or exception) to the software raised by the hardware, when a program accesses a page that is mapped in address space, but not loaded in physical memory.
http://en.wikipedia.org/wiki/Page_fault
Thrash is the term used to describe a degenerate situation on a computer where increasing resources are used to do a decreasing amount of work. In this situation the system is said to be thrashing. Usually it refers to two or more processes accessing a shared resource repeatedly such that serious system performance degradation occurs because the system is spending a disproportionate amount of time just accessing the shared resource. Resource access time may generally be considered as wasted, since it does not contribute to the advancement of any process. In modern computers, thrashing may occur in the paging system (if there is not ‘sufficient’ physical memory or the disk access time is overly long), or in the communications system (especially in conflicts over internal bus access), etc.
http://en.wikipedia.org/wiki/Thrash_(computer_science)
Solutions to Chapter 16 | Low Level
238
CareerCup.com
16.2 What is a Branch Target buffer? Explain how it can be used in reducing bubble cycles in cases of branch misprediction.
pg 82
SOLUTION
Branch misprediction occurs when the CPU mispredicts the next instruction to be executed.
The CPU uses pipelining which allows several instructions to be processed simultaneously. But during a conditional jump, the next instruction to be executed depends on the result of the condition. Branch Prediction tries to guess the next instruction. However, if the guess is wrong, we are penalized because the instruction which was executed must be discarded.
Branch Target Buffer (BTB) reduces the penalty by predicting the path of the branch, computing the target of the branch and caching the information used by the branch. There will be no stalls if the branch entry found on BTB and the prediction is correct, otherwise the penalty will be at least two cycles.
Solutions to Chapter 16 | Low Level
239 Cracking the Coding Interview | Knowledge Based
16.3 Describe direct memory access (DMA). Can a user level buffer / pointer be used by kernel or drivers?
pg 82
SOLUTION
Direct Memory is a feature which provides direct access (read/write) to system memory without interaction from the CPU. The “DMA Controller” manages this by requesting the System bus access (DMA request) from CPU. CPU completes its current task and grants access by asserting DMA acknowledgement signal. Once it gets the access, it reads/writes the data and returns back the system bus to the CPU by asserting the bus release signal. This transfer is faster than the usual transfer by CPU. Between this time CPU is involved with processing task which doesn’t require memory access.
By using DMA, drivers can access the memory allocated to the user level buffer / pointer.
Solutions to Chapter 16 | Low Level
. CareerCup com 240
16.4 Write a step by step execution of things that happen after a user presses a key on the keyboard. Use as much detail as possible.
pg 82
SOLUTION
1. The keyboard sends a scan code of the key to the keyboard controller (Scan code for key pressed and key released is different).
2. The keyboard controller interprets the scan code and stores it in a buffer.
3. The keyboard controller sends a hardware interrupt to the processor. This is done by putting signal on “interrupt request line”: IRQ 1.
4. The interrupt controller maps IRQ 1 into INT 9.
5. An interrupt is a signal which tells the processor to stop what it was doing currently and do some special task.
6. The processor invokes the “Interrupt handler”. CPU fetches the address of “Interrupt Service Routine” (ISR) from “Interrupt Vector Table” maintained by the OS (Processor use the IRQ number for this).
7. The ISR reads the scan code from port 60h and decides whether to process it or pass the control to program for taking action.
Solutions to Chapter 16 | Low Level
241 Cracking the Coding Interview | Knowledge Based
16.5 Write a program to find whether a machine is big endian or little endian.
pg 82
SOLUTION
1 #define BIG_ENDIAN 0
2 #define LITTLE_ENDIAN 1
3 int TestByteOrder() {
4 short int word = 0x0001;
5 char *byte = (char *) &word;
6 return (byte[0] ? LITTLE_ENDIAN : BIG_ENDIAN);
7 }
Solutions to Chapter 16 | Low Level
. CareerCup com 242
16.6 Discuss how would you make sure that a process doesn’t access an unauthorized part of the stack.
pg 82
SOLUTION
As with any ambiguously worded interview question, it may help to probe the interviewer to understand what specifically you’re intended to solve. Are you trying to prevent code that has overflowed a buffer from compromising the execution by overwriting stack values? Are you trying to maintain some form of thread-specific isolation between threads? Is the code of interest native code like C++ or running under a virtual machine like Java?
Remember that, in a multi-threaded environment, there can be multiple stacks in a process.
NATIVE CODE
One threat to the stack is malicious program input, which can overflow a buffer and overwrite stack pointers, thus circumventing the intended execution of the program.
If the interviewer is looking for a simple method to reduce the risk of buffer overflows in native code, modern compilers provide this sort of stack protection through a command line option. With Microsoft’s CL, you just pass /GS to the compiler. With GCC, you can use -fstack-protector-all.
For more complex schemes, you could set individual permissions on the range of memory pages representing the stack section you care about. In the Win32 API, you’d use the VirtualProtect API to mark the page PAGE_READONLY or PAGE_NOACCESS. This will cause the code accessing the region to go through an exception on access to the specific section of the stack.
Alternately, you could use the HW Debug Registers (DRs) to set a read or write breakpoint on the specific memory addresses of interest. A separate process could be used to debug the process of interest, catch the HW exception that would be generated if this section of the stack were accessed.
However, it’s very important to note that under normal circumstances, threads and processes are not means of access control. Nothing can prevent native code from writing anywhere within the address space of its process, including to the stack. Specifically, there is nothing to prevent malicious code in the process from calling VirtualProtect and marking the stack sections of interest PAGE_EXECUTE_READWRITE. Equally so, nothing prevents code from zeroing out the HW debug registers, eliminating your breakpoints. In summary, nothing can fully prevent native code from accessing memory addresses, including the stack, within its own process space.
MANAGED CODE
Solutions to Chapter 16 | Low Level
243 Cracking the Coding Interview | Knowledge Based
A final option is to consider requiring this code that should be “sandboxed” to run in a managed language like Java or C# / .NET. By default, the virtual machines running managed code in these languages make it impossible to gain complete access to the stack from within the process.
One can use further security features of the runtimes to prevent the code from spawning additional processes or running “unsafe” code to inspect the stack. With .NET, for example, you can use Code Access Security (CAS) or appdomain permissions to control such execution.
Solutions to Chapter 16 | Low Level
. CareerCup com 244
16.7 What are the best practices to prevent reverse engineering of DLLs?
pg 82
SOLUTION
Best practices include the following:
»»Use obfuscators.
»»Do not store any data (string, etc) in open form. Always compress or encode it.
»»Use a static link so there is no DLL to attack.
»»Strip all symbols.
»»Use a .DEF file and an import library to have anonymous exports known only by their export ids.
»»Keep the DLL in a resource and expose it in the file system (under a suitably obscure name, perhaps even generated at run time) only when running.
»»Hide all real functions behind a factory method that exchanges a secret (better, proof of knowledge of a secret) for a table of function pointers to the real methods.
»»Use anti-debugging techniques borrowed from the malware world to prevent reverse engineering. (Note that this will likely get you false positives from AV tools.)
»»Use protectors.
Solutions to Chapter 16 | Low Level
245 Cracking the Coding Interview | Knowledge Based
16.8 A device boots with an empty FIFO queue. In the first 400 ns period after startup, and in each subsequent 400 ns period, a maximum of 80 words will be written to the queue. Each write takes 4 ns. A worker thread requires 3 ns to read a word, and 2 ns to process it before reading the next word. What is the shortest depth of the FIFO such that no data is lost?
pg 82
SOLUTION
While a perfectly optimal solution is complex, an interviewer is most interested in how you approach the problem.
THEORY
First, note that writes do not have to be evenly distributed within a period. Thus a likely worst case is 80 words are written at the end of the first period, followed by 80 more at the start of the next.
Note that the maximum write rate for a full period is exactly matched by a full period of processing (400 ns / ((3 ns + 2 ns)/process) = 80 processed words/period).
As the 2nd period in our example is fully saturated, adding writes from a 3rd period would not add additional stress, and this example is a true worst case for the conditions.
A SAFE QUEUE DEPTH
For an estimate of maximum queue size, notice that these 160 writes take 640 ns (160 writes * 4 ns / write = 640 ns), during which time only 128 words have been read (640 ns / ((3 ns + 2 ns) / word) = 128 words). However, the first read cannot start until the first write has finished, which fills an extra slot in the queue.
Also, depending on the interactions between read and write timing, a second additional slot may be necessary to ensure a write does not trash the contents of a concurrently occurring read. Thus, a safe estimate is that the queue must be at least 34 words deep (160 - 128 + 1 + 1 = 34) to accommodate the unread words.
FINDING AN OPTIMAL (MINIMAL) QUEUE DEPTH
Depending on the specifics of the problem, it’s possible that the final queue spot could be safely removed. In many cases, the time required to do an edge case analysis to determine safety is not worth the effort. However, if the interviewer is interested, the full analysis follows.
We are interested in the exact queue load during the final (160th) consecutive write to the queue. We can approach this by graphing the queue load from time = 0 ns, observing the pattern, and extending it to time = 716 ns, the time of the final consecutive write.
The graph below shows that the queue load increases as each write begins, and decreases
Solutions to Chapter 16 | Low Level
. CareerCup com 246
3 ns after a read begins. Uninteresting time segments are surrounded by [brackets]. Each character represents 1 ns.
0 - 79 ns
80 - 99 ns
100 - 707 ns
708 - 723 ns
>= 724 ns
Writer
AAAABBBBCCCCDDDDEEEE
XXXXYYYYZZZZ____
Worker
____aaaaabbbbbcccccd
opppppqqqqqrrrrr
Queue Load
11112221222222223222
3333333343333322 *
Y = Writing word 159 @ 712 ns
Z = Writing word 160 @ 716 ns
q = Processing word 127 @ 714 ns
r = Processing word 128
* = Between 708 and 723 ns, the queue load is shown as 30 plus the digit shown at each ns.
Note that the queue load does in fact reach a maximum of 34 at time = 716 ns.
As an interesting note, if the problem had required only 2 ns of the 5 ns processing time to complete a read, the optimal queue depth would decrease to 33.
The below graphs are unnecessary, but show empirically that adding writes from the 3rd period does not change the queue depth required.
< 796 ns
797 - 807 ns
808 - 873 ns
874 - 885 ns
Writer
____AAAABBBB
!!@@@@####$$
Worker
^^^&&&&&****
yyyyyzzzzzaa
Queue Load
877788778887
112111221122 *
A = Writing word 161
& = Processing word 144
# = Writing word 181
z = Processing word 160 @ 779 ns
* = Between 874 and 885 ns, the queue load is shown as 20 plus the digit shown at each ns.
< 1112 ns
1112 - 1123 ns
Writer
YYYYZZZZ____
Worker
^^&&&&&%%%%%
Queue Load
333343333322 *
Z = Writing word 240 @ 1116 ns
& = Processing word 207 @ 1114 ns
* = Between 1112 and 1123 ns, the queue load is shown as 30 plus the digit shown at each ns.
Solutions to Chapter 16 | Low Level
247 Cracking the Coding Interview | Knowledge Based
16.9 Write an aligned malloc & free function that takes number of bytes and aligned byte (which is always power of 2)
EXAMPLE
align_malloc (1000,128) will return a memory address that is a multiple of 128 and that points to memory of size 1000 bytes.
aligned_free() will free memory allocated by align_malloc.
pg 82
SOLUTION
1. We will use malloc routine provided by C to implement the functionality.
Allocate memory of size (bytes required + alignment – 1 + sizeof(void*)) using malloc.
alignment: malloc can give us any address and we need to find a multiple of alignment.
(Therefore, at maximum multiple of alignment, we will be alignment-1 bytes away from any location.)
sizeof(size_t): We are returning a modified memory pointer to user, which is different from the one that would be returned by malloc. We also need to extra space to store the address given by malloc, so that we can free memory in aligned_free by calling free routine provided by C.
2. If it returns NULL, then aligned_malloc will fail and we return NULL.
3. Else, find the aligned memory address which is a multiple of alignment (call this p2).
4. Store the address returned by malloc (e.g., p1 is just size_t bytes ahead of p2), which will be required by aligned_free.
5. Return p2.
1 void* aligned_malloc(size_t required_bytes, size_t alignment) {
2 void* p1; // original block
3 void** p2; // aligned block
4 int offset = alignment - 1 + sizeof(void*);
5 if ((p1 = (void*)malloc(required_bytes + offset)) == NULL) {
6 return NULL;
7 }
8 p2 = (void**)(((size_t)(p1) + offset) & ~(alignment - 1));
9 p2[-1] = p1;
10 return p2;
11 }
12 void aligned_free(void *p) {
13 free(((void**)p)[-1]);
14 }
Solutions to Chapter 16 | Low Level
. CareerCup com 248
16.10 Write a function called my2DAlloc which allocates a two dimensional array. Minimize the number of calls to malloc and make sure that the memory is accessible by the notation arr[i][j].
pg 82
SOLUTION
We will use one call to malloc.
Allocate one block of memory to hold the row vector and the array data. The row vector will reside in rows * sizeof(int*) bytes. The integers in the array will take up another rows * cols * sizeof(int) bytes.
Constructing the array in a single malloc has the added benefit of allowing disposal of the array with a single free call rather than using a special function to free the subsidiary data blocks.
1 #include <malloc.h>
2
3 int** My2DAlloc(int rows, int cols) {
4 int header = rows * sizeof(int*);
5 int data = rows * cols * sizeof(int);
6 int** rowptr = (int**)malloc(header + data);
7 int* buf = (int*)(rowptr + rows);
8 int k;
9 for (k = 0; k < rows; ++k) {
10 rowptr[k] = buf + k*cols;
11 }
12 return rowptr;
13 }
Solutions to Chapter 17 | Networking
Cracking the Coding Interview | Knowledge Based
249
17.1 Explain what happens, step by step, after you type a URL into a browser. Use as much detail as possible.
pg 84
SOLUTION
There’s no right, or even complete, answer for this question. This question allows you to go into arbitrary amounts of detail depending on what you’re comfortable with. Here’s a start though:
1. Browser contacts the DNS server to find the IP address of URL.
2. DNS returns back the IP address of the site.
3. Browser opens TCP connection to the web server at port 80.
4. Browser fetches the html code of the page requested.
5. Browser renders the HTML in the display window.
6. Browser terminates the connection when window is closed.
One of the most interesting steps is Step 1 and 2 - “Domain Name Resolution.” The web addresses we type are nothing but an alias to an IP address in human readable form. Mapping of domain names and their associated Internet Protocol (IP) addresses is managed by the Domain Name System (DNS), which is a distributed but hierarchical entity.
Each domain name server is divided into zones. A single server may only be responsible for knowing the host names and IP addresses for a small subset of a zone, but DNS servers can work together to map all domain names to their IP addresses. That means if one domain name server is unable to find the IP addresses of a requested domain then it requests the information from other domain name servers.
Solutions to Chapter 17 | Networking
250
CareerCup.com
17.2 Explain any common routing protocol in detail. For example: BGP, OSPF, RIP.
pg 84
SOLUTION
Depending on the reader’s level of understanding, knowledge, interest or career aspirations, he or she may wish to explore beyond what is included here. Wikipedia and other websites are great places to look for a deeper understanding. We will provide only a short summary.
BGP: Border Gateway Protocol
BGP is the core routing protocol of the Internet. “When a BGP router first comes up on the Internet, either for the first time or after being turned off, it establishes connections with the other BGP routers with which it directly communicates. The first thing it does is download the entire routing table of each neighboring router. After that it only exchanges much shorter update messages with other routers.
BGP routers send and receive update messages to indicate a change in the preferred path to reach a computer with a given IP address. If the router decides to update its own routing tables because this new path is better, then it will subsequently propagate this information to all of the other neighboring BGP routers to which it is connected, and they will in turn decide whether to update their own tables and propagate the information further.”
Borrowed from http://www.livinginternet.com/i/iw_route_egp_bgp.htm.
RIP: Routing Information Protocol
“RIP provides the standard IGP protocol for local area networks, and provides great network stability, guaranteeing that if one network connection goes down the network can quickly adapt to send packets through another connection. “
“What makes RIP work is a routing database that stores information on the fastest route from computer to computer, an update process that enables each router to tell other routers which route is the fastest from its point of view, and an update algorithm that enables each router to update its database with the fastest route communicated from neighboring routers.”
Borrowing from http://www.livinginternet.com/i/iw_route_igp_rip.htm.
OSPF: Open Shortest Path First
“Open Shortest Path First (OSPF) is a particularly efficient IGP routing protocol that is faster than RIP, but also more complex.”
The main difference between OSPF and RIP is that RIP only keeps track of the closest router for each destination address, while OSPF keeps track of a complete topological database of all connections in the local network. The OSPF algorithm works as described below.
Solutions to Chapter 17 | Networking
251 Cracking the Coding Interview | Knowledge Based
»»Startup. When a router is turned on it sends Hello packets to all of its neighbors, re-ceives their Hello packets in return, and establishes routing connections by synchroniz-ing databases with adjacent routers that agree to synchronize.
»»Update. At regular intervals each router sends an update message called its “link state” describing its routing database to all the other routers, so that all routers have the same description of the topology of the local network.
»»Shortest path tree. Each router then calculates a mathematical data structure called a “shortest path tree” that describes the shortest path to each destination address and therefore indicates the closest router to send to for each communication; in other words -- “open shortest path first”.
See http://www.livinginternet.com/i/iw_route_igp_ospf.htm.
Solutions to Chapter 17 | Networking
. CareerCup com 252
17.3 Compare and contrast the IPv4 and IPv6 protocols.
pg 84
SOLUTION
IPv4 and IPv6 are the internet protocols applied at the network layer. IPv4 is the most widely used protocol right now and IPv6 is the next generation protocol for internet.
»»IPv4 is the fourth version of Internet protocol which uses 32 bit addressing whereas IPv6 is a next generation internet protocol which uses 128 bits addressing.
»»IPv4 allows 4,294,967,296 unique addresses where as IPv6 can hold 340-undecillion (34, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000, 000) unique IP addresses.
»»IPv4 has different class types: A,B,C,D and E. Class A, Class B, and Class C are the three classes of addresses used on IP networks in common practice. Class D addresses are reserved for multicast. Class E addresses are simply reserved, meaning they should not be used on IP networks (used on a limited basis by some research organizations for experimental purposes).
»»IPv6 addresses are broadly classified into three categories:
1. Unicast addresses: A Unicast address acts as an identifier for a single interface. An IPv6 packet sent to a Unicast address is delivered to the interface identified by that address.
2. Multicast addresses: A Multicast address acts as an identifier for a group / set of interfaces that may belong to the different nodes. An IPv6 packet delivered to a multicast address is delivered to the multiple interfaces.
3. Anycast addresses: Anycast addresses act as identifiers for a set of interfaces that may belong to the different nodes. An IPv6 packet destined for an Anycast address is delivered to one of the interfaces identified by the address.
»»IPv4 address notation: 239.255.255.255, 255.255.255.0
»»IPv6 addresses are denoted by eight groups of hexadecimal quartets separated by colons in between them.
»»An example of a valid IPv6 address: 2001:cdba:0000:0000:0000:0000:3257:9652
Because of the increase in the population, there is a need of Ipv6 protocol which can provide solution for:
1. Increased address space
2. More efficient routing
3. Reduced management requirement
Solutions to Chapter 17 | Networking
253 Cracking the Coding Interview | Knowledge Based
4. Improved methods to change ISP
5. Better mobility support
6. Multi-homing
7. Security
8. Scoped address: link-local, site-local and global-address space
Solutions to Chapter 17 | Networking
. CareerCup com 254
17.4 What is a network / subnet mask? Explain how host A sends a message / packet to host B when: (a) both are on same network and (b) both are on different networks. Explain which layer makes the routing decision and how.
pg 84
SOLUTION
A mask is a bit pattern used to identify the network/subnet address. The IP address consists of two components: the network address and the host address.
The IP addresses are categorized into different classes which are used to identify the network address.
Example: Consider IP address 152.210.011.002. This address belongs to Class B, so:
Network Mask: 11111111.11111111.00000000.00000000
Given Address: 10011000.11010101.00001011.00000010
By ANDing Network Mask and IP Address, we get the following network address:
10011000.11010101.00000000.00000000 (152.210.0.0)
Host address: 00001011.00000010
Similarly, a network administrator can divide any network into sub-networks by using subnet mask. To do this, we further divide the host address into two or more subnets.
For example, if the above network is divided into 18 subnets (requiring a minimum of 5 bits to represent 18 subnets), the first 5 bits will be used to identify the subnet address.
Subnet Mask: 11111111.11111111.11111000.00000000 (255.255.248.0)
Given Address: 10011000.11010101.00001011.00000010
So, by ANDing the subnet mask and the given address, we get the following subnet address: 10011000.11010101.00001000.00000000 (152.210.1.0)
How Host A sends a message/packet to Host B:
When both are on same network: the host address bits are used to identify the host within the network.
Both are on different networks: the router uses the network mask to identify the network and route the packet. The host can be identified using the network host address.
The network layer is responsible for making routing decisions. A routing table is used to store the path information and the cost involved with that path, while a routing algorithm uses the routing table to decide the path on which to route the packets.
Routing is broadly classified into Static and Dynamic Routing based on whether the table is fixed or it changes based on the current network condition.
Solutions to Chapter 17 | Networking
255 Cracking the Coding Interview | Knowledge Based
17.5 What are the differences between TCP and UDP? Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s / receiver’s window) and congestion control.
pg 84
SOLUTION
TCP (Transmission Control Protocol): TCP is a connection-oriented protocol. A connection can be made from client to server, and from then on any data can be sent along that connection.
»»Reliable - when you send a message along a TCP socket, you know it will get there unless the connection fails completely. If it gets lost along the way, the server will re-request the lost part. This means complete integrity; data will not get corrupted.
»»Ordered - if you send two messages along a connection, one after the other, you know the first message will get there first. You don’t have to worry about data arriving in the wrong order.
»»Heavyweight - when the low level parts of the TCP “stream” arrive in the wrong order, resend requests have to be sent. All the out of sequence parts must be put back together, which requires a bit of work.
UDP(User Datagram Protocol): UDP is connectionless protocol. With UDP you send messages (packets) across the network in chunks.
»»Unreliable - When you send a message, you don’t know if it’ll get there; it could get lost on the way.
»»Not ordered - If you send two messages out, you don’t know what order they’ll arrive in.
»»Lightweight - No ordering of messages, no tracking connections, etc. It’s just fire and forget! This means it’s a lot quicker, and the network card / OS have to do very little work to translate the data back from the packets.
Explain how TCP handles reliable delivery (explain ACK mechanism), flow control (explain TCP sender’s/receiver’s window).
For each TCP packet, the receiver of a packet must acknowledge that the packet is received. If there is no acknowledgement, the packet is sent again. These guarantee that every single packet is delivered. ACK is a packet used in TCP to acknowledge receipt of a packet. A TCP window is the amount of outstanding (unacknowledged by the recipient) data a sender can send on a particular connection before it gets an acknowledgment back from the receiver that it has gotten some of it.
For example, if a pair of hosts are talking over a TCP connection that has a TCP window with a size of 64 KB, the sender can only send 64 KB of data and then it must wait for an acknowledgment from the receiver that some or all of the data has been received. If the receiver
Solutions to Chapter 17 | Networking
. CareerCup com 256
acknowledges that all the data has been received, then the sender is free to send another 64 KB. If the sender gets back an acknowledgment from the receiver that it received the first 32 KB (which could happen if the second 32 KB was still in transit or it could happen if the second 32 KB got lost), then the sender can only send another additional 32 KB since it can’t have more than 64 KB of unacknowledged data outstanding (the second 32 KB of data plus the third).
Congestion Control
The TCP uses a network congestion avoidance algorithm that includes various aspects of an additive-increase-multiplicative-decrease scheme, with other schemes such as slow-start in order to achieve congestion avoidance.
There are different algorithms to solve the problem; Tahoe and Reno are the most well known. To avoid congestion collapse, TCP uses a multi-faceted congestion control strategy. For each connection, TCP maintains a congestion window, limiting the total number of unacknowledged packets that may be in transit end-to-end. This is somewhat analogous to TCP’s sliding window used for flow control. TCP uses a mechanism called slow start to increase the congestion window after a connection is initialized and after a timeout. It starts with a window of two times the maximum segment size (MSS). Although the initial rate is low, the rate of increase is very rapid: for every packet acknowledged, the congestion window increases by 1 MSS so that for every round trip time (RTT), the congestion window has doubled. When the congestion window exceeds a threshold ssthresh the algorithm enters a new state, called congestion avoidance. In some implementations (i.e., Linux), the initial ssthresh is large, and so the first slow start usually ends after a loss. However, ssthresh is updated at the end of each slow start, and will often affect subsequent slow starts triggered by timeouts.
Solutions to Chapter 18 | Threads and Locks
Cracking the Coding Interview | Knowledge Based
257
18.1 What’s the difference between a thread and a process?
pg 86
SOLUTION
Processes and threads are related to each other but are fundamentally different.
A process can be thought of as an instance of a program in execution. Each process is an independent entity to which system resources (CPU time, memory, etc.) are allocated and each process is executed in a separate address space. One process cannot access the variables and data structures of another process. If you wish to access another process’ resources, inter-process communications have to be used such as pipes, files, sockets etc.
A thread uses the same stack space of a process. A process can have multiple threads. A key difference between processes and threads is that multiple threads share parts of their state. Typically, one allows multiple threads to read and write the same memory (no processes can directly access the memory of another process). However, each thread still has its own registers and its own stack, but other threads can read and write the stack memory.
A thread is a particular execution path of a process; when one thread modifies a process resource, the change is immediately visible to sibling threads.
Solutions to Chapter 18 | Threads and Locks
258
CareerCup.com
18.2 How can you measure the time spent in a context switch?
pg 86
SOLUTION
This is a tricky question, but let’s start with a possible solution.
A context switch is the time spent switching between two processes (e.g., bringing a waiting process into execution and sending an executing process into waiting/terminated state). This happens in multitasking. The operating system must bring the state information of waiting processes into memory and save the state information of the running process.
In order to solve this problem, we would like to record timestamps of the last and first instruction of the swapping processes. The context switching time would be the difference in the timestamps between the two processes.
Let’s take an easy example: Assume there are only two processes, P1 and P2.
P1 is executing and P2 is waiting for execution. At some point, the OS must swap P1 and P2—let’s assume it happens at the Nth instruction of P1. So, the context switch time for this would be Time_Stamp(P2_1) – Time_Stamp(P2_N)
Easy enough. The tricky part is this: how do we know when this swapping occurs? Swapping is governed by the scheduling algorithm of the OS. We can not, of course, record the timestamp of every instruction in the process.
Another issue: there are many kernel level threads which are also doing context switches, and the user does not have any control over them.
Overall, we can say that this is mostly an approximate calculation which depends on the underlying OS. One approximation could be to record the end instruction timestamp of a process and start timestamp of a process and waiting time in queue.
If the total timeof execution of all the processes was T, then the context switch time = T – (SUM for all processes (waiting time + execution time)).
Solutions to Chapter 18 | Threads and Locks
259 Cracking the Coding Interview | Knowledge Based
18.3 Implement a singleton design pattern as a template such that, for any given class Foo, you can call Singleton::instance() and get a pointer to an instance of a singleton of type Foo. Assume the existence of a class Lock which has acquire() and release() methods. How could you make your implementation thread safe and exception safe?
pg 86
SOLUTION
1 using namespace std;
2 /* Place holder for thread synchronization lock */
3 class Lock {
4 public:
5 Lock() { /* placeholder code to create the lock */ }
6 ~Lock() { /* placeholder code to deallocate the lock */ }
7 void AcquireLock() { /* placeholder to acquire the lock */ }
8 void ReleaseLock() { /* placeholder to release the lock */ }
9 };
10
11 /* Singleton class with a method that creates a new instance of the
12 * class of the type of the passed in template if it does not
13 * already exist. */
14 template <class T> class Singleton {
15 private:
16 static Lock lock;
17 static T* object;
18 protected:
19 Singleton() { };
20 public:
21 static T * instance();
22 };
23 Lock Singleton::lock;
24
25 T * Singleton::Instance() {
26 /* if object is not initialized, acquire lock */
27 if (object == 0) {
28 lock.AcquireLock();
29 /* If two threads simultaneously check and pass the first “if”
30 * condition, then only the one who acquired the lock first
31 * should create the instance */
32 if (object == 0) {
33 object = new T;
34 }
35 lock.ReleaseLock();
36 }
37 return object;
38 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 260
39
40 int main() {
41 /* foo is any class defined for which we want singleton access */
42 Foo* singleton_foo = Singleton<Foo>::Instance();
43 return 0;
44 }
The general method to make a program thread safe is to lock shared resources whenever write permission is given. This way, if one thread is modifying the resource, other threads can not modify it.
Solutions to Chapter 18 | Threads and Locks
261 Cracking the Coding Interview | Knowledge Based
18.4 Design a class which provides a lock only if there are no possible deadlocks.
pg 86
SOLUTION
For our solution, we implement a wait / die deadlock prevention scheme.
1 class MyThread extends Thread {
2 long time;
3 ArrayList<Resource> res = new ArrayList<Resource>();
4 public ArrayList<Resource> getRes() { return res; }
5
6 public void run() {
7 /* Run infinitely */
8 time = System.currentTimeMillis();
9 int count = 0;
10 while (true) {
11 if (count < 4) {
12 if (Question.canAcquireResource(this,
13 Question.r[count])) {
14 res.add(Question.r[count]);
15 count++;
16 System.out.println(“Resource: [“ +
17 Question.r[count - 1].getId() + “] acquired by
18 thread: [“ + this.getName() + “]”);
19 try {
20 sleep(1000);
21 } catch (InterruptedException e) {
22 e.printStackTrace();
23 }
24 }
25 }
26 else {
27 this.stop();
28 }
29 }
30 }
31
32 public long getTime() { return time; }
33 public void setRes(ArrayList<Resource> res) { this.res = res; }
34 MyThread(String name) {
35 super(name);
36 }
37 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 262
18.5 Suppose we have the following code:
class Foo {
public:
A
(.....); /* If A is called, a new thread will be created and
* the corresponding function will be executed. */
B(.....); /* same as above */
C(.....); /* same as above */
}
Foo f;
f.A(.....);
f.B(.....);
f.C(.....);
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
iii) Suppose we have the following code to use class Foo. We do not know how the threads will be scheduled in the OS.
Foo f;
f.A(.....); f.B(.....); f.C(.....);
f.A(.....); f.B(.....); f.C(.....);
Can you design a mechanism to make sure that all the methods will be executed in sequence?
pg 86
SOLUTION
i) Can you design a mechanism to make sure that B is executed after A, and C is executed after B?
1 Semaphore s_a(0);
2 Semaphore s_b(0);
3 A {
4 /***/
5 s_a.release(1);
6 }
7 B {
8 s_a.acquire(1);
9 /****/
10 s_b.release(1);
11 }
12 C {
13 s_b.acquire(1);
14 /******/
15 }
ii) Can you design a mechanism to make sure that all the methods will be executed in sequence?
1 Semaphore s_a(0);
Solutions to Chapter 18 | Threads and Locks
263 Cracking the Coding Interview | Knowledge Based
2 Semaphore s_b(0);
3 Semaphore s_c(1);
4 A {
5 s_c.acquire(1);
6 /***/
7 s_a.release(1);
8 }
9 B {
10 s_a.acquire(1);
11 /****/
12 s_b.release(1);
13 }
14 C {
15 s_b.acquire(1);
16 /******/
17 s_c.release(1);
18 }
Solutions to Chapter 18 | Threads and Locks
. CareerCup com 264
18.6 You are given a class with synchronized method A, and a normal method C. If you have two threads in one instance of a program, can they call A at the same time? Can they call A and C at the same time?
pg 86
SOLUTION
Java provides two ways to achieve synchronization: synchronized method and synchronized statement.
Synchronized method: Methods of a class which need to be synchronized are declared with “synchronized” keyword. If one thread is executing a synchronized method, all other threads which want to execute any of the synchronized methods on the same objects get blocked.
Syntax: method1 and method2 need to be synchronized
1 public class SynchronizedMethod {
2 // Variables declaration
3 public synchronized returntype Method1() {
4 // Statements
5 }
6 public synchronized returntype method2() {
7 // Statements
8 }
9 // Other methods
10 }
Synchronized statement: It provides the synchronization for a group of statements rather than a method as a whole. It needs to provide the object on which these synchronized statements will be applied, unlike in a synchronized method.
Syntax: synchronized statements on “this” object
1 synchronized(this) {
2 /* statement 1
3 * ...
4 * statement N */
5 }
i) If you have two threads in one instance of a program, can they call A at the same time?
Not possible; read the above paragraph.
ii) Can they call A and C at the same time?
Yes. Only methods of the same object which are declared with the keyword synchronized can’t be interleaved.
Solutions to Chapter 19 | Moderate
Cracking the Coding Interview | Additional Review Problems
265
19.1 Write a function to swap a number in place without temporary variables.
pg 89
SOLUTION
This is a classic interview problem. If you haven’t heard this problem before, you can approach it by taking the difference between a and b:
1 public static void swap(int a, int b) {
2 a = b - a; // 9 - 5 = 4
3 b = b - a; // 9 - 4 = 5
4 a = a + b; // 4 + 5 = 9
5
6 System.out.println(“a: “ + a);
7 System.out.println(“b: “ + b);
8 }
You can then optimize it as follows:
1 public static void swap_opt(int a, int b) {
2 a = a^b;
3 b = a^b;
4 a = a^b;
5
6 System.out.println(“a: “ + a);
7 System.out.println(“b: “ + b);
8 }
Solutions to Chapter 19 | Moderate
266
CareerCup.com
19.2 Design an algorithm to figure out if someone has won in a game of tic-tac-toe.
pg 89
SOLUTION
The first thing to ask your interviewer is whether the hasWon function will be called just once, or multiple times. If it will be called multiple times, you can get a very fast algorithm by amortizing the cost (especially if you can design your own data storage system for the tic-tac-toe board).
Approach #1: If hasWon is called many times
There are only 3^9, or about twenty thousand tic-tac-toe boards. We can thus represent our tic-tac-toe board as an int, with each digit representing a piece (0 means Empty, 1 means Red, 2 means Blue). We set up a hashtable or array in advance with all possible boards as keys, and the values are 0, 1, and 2. Our function then is simply this:
int hasWon(int board) {
return winnerHashtable[board];
}
Easy!
Approach #2: If hasWon is only called once
1 enum Piece { Empty, Red, Blue };
2 enum Check { Row, Column, Diagonal, ReverseDiagonal }
3
4 Piece getIthColor(Piece[][] board, int index, int var, Check check) {
5 if (check == Check.Row) return board[index][var];
6 else if (check == Check.Column) return board[var][index];
7 else if (check == Check.Diagonal) return board[var][var];
8 else if (check == Check.ReverseDiagonal)
9 return board[board.length - 1 - var][var];
10 return Piece.Empty;
11 }
12
13 Piece getWinner(Piece[][] board, int fixed_index, Check check) {
14 Piece color = getIthColor(board, fixed_index, 0, check);
15 if (color == Piece.Empty) return Piece.Empty;
16 for (int var = 1; var < board.length; var++) {
17 if (color != getIthColor(board, fixed_index, var, check)) {
18 return Piece.Empty;
19 }
20 }
21 return color;
22 }
23
Solutions to Chapter 19 | Moderate
267 Cracking the Coding Interview | Additional Review Problems
24 Piece hasWon(Piece[][] board) {
25 int N = board.length;
26 Piece winner = Piece.Empty;
27
28 // Check rows and columns
29 for (int i = 0; i < N; i++) {
30 winner = getWinner(board, i, Check.Row);
31 if (winner != Piece.Empty) {
32 return winner;
33 }
34
35 winner = getWinner(board, i, Check.Column);
36 if (winner != Piece.Empty) {
37 return winner;
38 }
39 }
40
41 winner = getWinner(board, -1, Check.Diagonal);
42 if (winner != Piece.Empty) {
43 return winner;
44 }
45
46 // Check diagonal
47 winner = getWinner(board, -1, Check.ReverseDiagonal);
48 if (winner != Piece.Empty) {
49 return winner;
50 }
51
52 return Piece.Empty;
53 }
SUGGESTIONS AND OBSERVATIONS:
»»Note that the runtime could be reduced to O(N) with the addition of row and column count arrays (and two sums for the diagonals)
»»A common follow up (or tweak) to this question is to write this code for an NxN board.
Solutions to Chapter 19 | Moderate
. CareerCup com 268
19.3 Write an algorithm which computes the number of trailing zeros in n factorial.
pg 89
SOLUTION
Trailing zeros are contributed by pairs of 5 and 2, because 5*2 = 10. To count the number of pairs, we just have to count the number of multiples of 5. Note that while 5 contributes to one multiple of 10, 25 contributes two (because 25 = 5*5).
1 public static int numZeros(int num) {
2 int count = 0;
3 if (num < 0) {
4 System.out.println(“Factorial is not defined for < 0”);
5 return 0;
6 }
7 for (int i = 5; num / i > 0; i *= 5) {
8 count += num / i;
9 }
10 return count;
11 }
Let’s walk through an example to see how this works: Suppose num = 26. In the first loop, we count how many multiples of five there are by doing 26 / 5 = 5 (these multiples are 5, 10, 15, 20, and 25). In the next loop, we count how many multiples of 25 there are: 26 / 25 = 1 (this multiple is 25). Thus, we see that we get one zero from 5, 10, 15 and 20, and two zeros from 25 (note how it was counted twice in the loops). Therefore, 26! has six zeros.
OBSERVATIONS AND SUGGESTIONS:
»»This is a bit of a brain teaser, but it can be approached logically (as shown above). By thinking through what exactly will contribute a zero, and what doesn’t matter, you can come up with a solution. Again, be very clear in your rules up front so that you can implement this correctly.
Solutions to Chapter 19 | Moderate
269 Cracking the Coding Interview | Additional Review Problems
19.4 Write a method which finds the maximum of two numbers. You should not use if-else or any other comparison operator.
EXAMPLE
Input: 5, 10
Output: 10
pg 89
SOLUTION
Let’s try to solve this by “re-wording” the problem. We will re-word the problem until we get something that has removed all if statements.
Rewording 1: If a > b, return a; else, return b.
Rewording 2: If (a - b) is negative, return b; else, return a.
Rewording 3: If (a - b) is negative, let k = 1; else, let k = 0. Return a - k * (a - b).
Rewording 4: Let c = a - b. Let k = the most significant bit of c. Return a - k * c.
We have now reworded the problem into something that fits the requirements. The code for this is below.
1 int getMax(int a, int b) {
2 int c = a - b;
3 int k = (c >> 31) & 0x1;
4 int max = a - k * c;
5 return max;
6 }
Solutions to Chapter 19 | Moderate
. CareerCup com 270
19.5 The Game of Master Mind is played as follows:
The computer has four slots containing balls that are red (R), yellow (Y), green (G) or blue (B). For example, the computer might have RGGB (e.g., Slot #1 is red, Slots #2 and #3 are green, Slot #4 is blue).
You, the user, are trying to guess the solution. You might, for example, guess YRGB.
When you guess the correct color for the correct slot, you get a “hit”. If you guess a color that exists but is in the wrong slot, you get a “pseudo-hit”. For example, the guess YRGB has 2 hits and one pseudo hit.
For each guess, you are told the number of hits and pseudo-hits.
Write a method that, given a guess and a solution, returns the number of hits and pseudo hits.
pg 89
SOLUTION
This problem is straight-forward. We simply check the number of hits and pseudo-hits. We will store the number of each in a class. To do a quick lookup to see it an element is a pseudo-hit, we will use a bit mask.
1 public static class Result {
2 public int hits;
3 public int pseudoHits;
4 };
5
6 public static Result estimate(String guess, String solution) {
7 Result res = new Result();
8 int solution_mask = 0;
9 for (int i = 0; i < 4; ++i) {
10 solution_mask |= 1 << (1 + solution.charAt(i) - ‘A’);
11 }
12 for (int i = 0; i < 4; ++i) {
13 if (guess.charAt(i) == solution.charAt(i)) {
14 ++res.hits;
15 } else if ((solution_mask &
16 (1 << (1 + guess.charAt(i) - ‘A’))) >= 1) {
17 ++res.pseudoHits;
18 }
19 }
20 return res;
21 }
Solutions to Chapter 19 | Moderate
271 Cracking the Coding Interview | Additional Review Problems
19.6 Given an integer between 0 and 999,999, print an English phrase that describes the integer (eg, “One Thousand, Two Hundred and Thirty Four”).
pg 89
SOLUTION
This is not an especially challenging problem, but it is a long and tedious one. Your interviewer is unlikely to ask to see every detail, but he / she will be interested in how you approach the problem.
1 public static String numtostring(int num) {
2 StringBuilder sb = new StringBuilder();
3
4 // Count number of digits in num.
5 int len = 1;
6 while (Math.pow((double)10, (double)len ) < num) {
7 len++;
8 }
9
10 String[] wordarr1 = {“”,”One ”, “Two ”, “Three ”, “Four ”,
11 “Five ”, “Six ”, “Seven ”, “Eight ”,”Nine ”};
12 String[] wordarr11 = {“”, “Eleven ”, “Twelve ”, “Thirteen ”,
13 “Fourteen ”, “Fifteen ”, “Sixteen ”,
14 “Seventeen ”, “Eighteen ”, “Nineteen ”};
15 String[] wordarr10 = {“”,”Ten ”, “Twenty ”, “Thirty ”, “Forty ”,
16 “Fifty ”, “Sixty ”, “Seventy ”, “Eighty ”,
17 “Ninety “};
18 String[] wordarr100 = {“”, “Hundred ”, “Thousand ”};
19 int tmp;
20 if (num == 0) {
21 sb.append(“Zero”);
22 } else {
23 if (len > 3 && len % 2 == 0) {
24 len++;
25 }
26 do {
27 // Number greater than 999
28 if (len > 3) {
29 tmp = (num / (int)Math.pow((double)10,(double)len-2));
30 // If tmp is 2 digit number and not a multiple of 10
31 if (tmp / 10 == 1 && tmp%10 != 0) {
32 sb.append(wordarr11[tmp % 10]) ;
33 } else {
34 sb.append(wordarr10[tmp / 10]);
35 sb.append(wordarr1[tmp % 10]);
36 }
Solutions to Chapter 19 | Moderate
. CareerCup com 272
37 if (tmp > 0) {
38 sb.append(wordarr100[len / 2]);
39 }
40 num = num % (int)(Math.pow((double)10,(double)len-2));
41 len = len-2;
42 } else { // Number is less than 1000
43 tmp = num / 100;
44 if (tmp != 0) {
45 sb.append(wordarr1[tmp]);
46 sb.append(wordarr100[len / 2]);
47 }
48 tmp = num % 100 ;
49 if(tmp / 10 == 1 && tmp % 10 != 0) {
50 sb.append(wordarr11[tmp % 10]) ;
51 } else {
52 sb.append(wordarr10[tmp / 10]);
53 sb.append(wordarr1[tmp % 10]);
54 }
55 len = 0;
56 }
57 } while(len > 0);
58 }
59 return sb.toString();
60 }
Solutions to Chapter 19 | Moderate
273 Cracking the Coding Interview | Additional Review Problems
19.7 You are given an array of integers (both positive and negative). Find the continuous sequence with the largest sum. Return the sum.
EXAMPLE
Input: {2, -8, 3, -2, 4, -10}
Output: 5 (i.e., {3, -2, 4} )
pg 89
SOLUTION
A simple linear algorithm will work by keeping track of the current subsequence sum. If that sum ever drops below zero, that subsequence will not contribute to the subsequent maximal subsequence since it would reduce it by adding the negative sum.
1 public static int getMaxSum(int[] a) {
2 int maxsum = 0;
3 int sum = 0;
4 for (int i = 0; i < a.length; i++) {
5 sum += a[i];
6 if (maxsum < sum) {
7 maxsum = sum;
8 } else if (sum < 0) {
9 sum = 0;
10 }
11 }
12 return maxsum;
13 }
NOTE: If the array is all negative numbers, what is the correct behavior? Consider this simple array {-3, -10, -5}. You could make a good argument that the maximum sum is either: (A) -3 (if you assume the subsequence can’t be empty) (B) 0 (the subsequence has length 0) or (C) MINIMUM_INT (essentially the error case). We went with option B (max sum = 0), but there’s no “correct” answer. This is a great thing to discuss with your interviewer to show how careful you are.
Solutions to Chapter 19 | Moderate
. CareerCup com 274
19.8 Design a method to find the frequency of occurrences of any given word in a book.
pg 89
SOLUTION
The first question – which you should ask your interviewer – is if you’re just asking for a single word (“single query”) or if you might, eventually, use the same method for many different words (“repetitive queries”)? That is, are you simply asking for the frequency of “dog”, or might you ask for “dog,” and then “cat,” “mouse,” etc?
Solution: Single Query
In this case, we simply go through the book, word by word, and count the number of times that a word appears. This will take O(n) time. We know we can’t do better than that, as we must look at every word in the book.
Solution: Repetitive Queries
In this case, we create a hash table which maps from a word to a frequency. Our code is then like this:
1 Hashtable<String, Integer> setupDictionary(String[] book) {
2 Hashtable<String, Integer> table =
3 new Hashtable<String, Integer>();
4 for (String word : book) {
5 word = word.toLowerCase();
6 if (word.trim() != “”) {
7 if (!table.containsKey(word)) table.put(word, 0);
8 table.put(word, table.get(word) + 1);
9 }
10 }
11 return table;
12 }
13
14 int getFrequency(Hashtable<String, Integer> table, String word) {
15 if (table == null || word == null) return -1;
16 word = word.toLowerCase();
17 if (table.containsKey(word)) {
18 return table.get(word);
19 }
20 return 0;
21 }
Note: a problem like this is relatively easy. Thus, the interviewer is going to be looking heavily at how careful you are. Did you check for error conditions?
Solutions to Chapter 19 | Moderate
275 Cracking the Coding Interview | Additional Review Problems
19.9 Since XML is very verbose, you are given a way of encoding it where each tag gets mapped to a pre-defined integer value. The language/grammar is as follows:
Element --> Element Attr* END Element END [aka, encode the element
t
ag, then its attributes, then tack on an END character, then
e
ncode its children, then another end tag]
Attr --> Tag Value [assume all values are strings]
END --> 01
Tag --> some predefined mapping to int
Value --> string value END
Write code to print the encoded version of an xml element (passed in as string).
FOLLOW UP
Is there anything else you could do to (in many cases) compress this even further?
pg 90
SOLUTION
Part 1: Solution
This solution tokenizes the input and then encodes the items, element by element.
NOTE: See code attachment for full, executable code. We have included an abbreviated section here.
1 private Map<String, Byte> tagMap;
2 private static final Byte[] END = { 0, 1 };
3 private List<String> tokens;
4 private int currentTokenIndex;
5
6 byte[] encode(char[] input) throws IOException {
7 tokenize(input);
8 currentTokenIndex = 0;
9 ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
10 encodeTokens(outputStream);
11 return outputStream.toByteArray();
12 }
13
14 void encodeTokens(ByteArrayOutputStream output) {
15 nextToken(“<”);
16
17 // read tag name
18 String tagName = nextToken();
19 output.write(getTagCode(tagName));
20
21 // read attributes
Solutions to Chapter 19 | Moderate
. CareerCup com 276
22 while (!hasNextToken(“>”) && !hasNextTokens(“/”, “>”)) {
23 // read next attribute
24 String key = nextToken();
25 nextToken(“=”);
26 String value = nextToken();
27 output.write(getTagCode(key));
28 for (char c : value.toCharArray()) {
29 output.write(c);
30 }
31 output.write(END[0]);
32 output.write(END[1]);
33 }
34 // end of attributes
35 output.write(END[0]);
36 output.write(END[1]);
37 // finish this element
38 if (hasNextTokens(“/”, “>”)) {
39 nextToken(“/”);
40 nextToken(“>”);
41 } else {
42 nextToken(“>”);
43 // while not the end tag
44 while (!hasNextTokens(“<”, “/”)) {
45 encodeTokens(output); // encode child
46 }
47 // ending tag
48 nextToken(“<”);
49 nextToken(“/”);
50 nextToken(tagName);
51 nextToken(“>”);
52 }
53 output.write(END[0]);
54 output.write(END[1]);
55 }
Part 2: Is there anything you can do to compress this further?
You can treat the file as a general stream of characters and use any number of compression techniques: Shannon–Fano coding, Huffman coding or Arithmetic coding.
Solutions to Chapter 19 | Moderate
277 Cracking the Coding Interview | Additional Review Problems
19.10 Write a method to generate a random number between 1 and 7, given a method that generates a random number between 1 and 5 (i.e., implement rand7() using rand5()). .
pg 90
SOLUTION
First, observe that we cannot do this in a guaranteed finite amount of time. Why? Let’s see by a parallel example: How would you use rand2() to create rand3()?
Observe that each call of rand2() and the corresponding decision you make can be represented by a decision tree. On each node, you have two branches. You take the left one when rand2() equals 0 (which happens with 1/2 probability). You take the right one when rand2() equals 1 (which happens with 1/2 probability). You continue branching left and right as you continue to call 1/2. When you reach a leaf, you return a result of 1, 2 or 3 (your rand3() results).
»»What’s the probability of taking each branch? 1/2.
»»What’s the probability to reach a particular leaf node? 1/2^j (for some j).
»»What the probability of returning 3 (for example)? We could compute this by summing up the probabilities of reaching each leaf node with value 3. Each of these paths has probability 1/2^j, so we know that the total probability of returning 3 must be a series of terms of reciprocal powers of 2 (e.g., 1/2^x + 1/2^y + 1/2^z + …).
We also know, however, that the probability of returning 3 must be 1/3 (because rand3() should be perfectly random). Can you find a series of reciprocal powers of 2 that sum to 1/3? No, because 3 and 2 are relatively prime.
We can similarly conclude that to solve this problem, we will need to accept a small (infinitesimally small) chance that this process will repeat forever. That’s ok.
So, how do we solve this?
In order to generate a random number between 1 and 7, we just need to uniformly generate a larger range than we are looking for and then repeatedly sample until we get a number that is good for us. We will generate a base 5 number with two places with two calls to the RNG.
public static int rand7() {
while (true) {
i
nt num = 5 * (rand5() - 1) + (rand5() - 1);
i
f (num < 21) return (num % 7 + 1);
}
}
Solutions to Chapter 19 | Moderate
. CareerCup com 278
19.11 Design an algorithm to find all pairs of integers within an array which sum to a specified value.
pg 90
SOLUTION
One easy and (time) efficient solution involves a hash map from integers to integers. This algorithm works by iterating through the array. On each element x, look up sum - x in the hash table and, if it exists, print (x, sum - x). Add x to the hash table, and go to the next element.
Alternate Solution
Definition of Complement: If we’re trying to find a pair of numbers that sums to z, the complement of x will be z - x (that is, the number that can be added to x to make z). For example, if we’re trying to find a pair of numbers that sum to 12, the complement of –5 would be 17.
The Algorithm: Imagine we have the following sorted array: {-2 -1 0 3 5 6 7 9 13 14 }. Let first point to the head of the array and last point to the end of the array. To find the complement of first, we just move last backwards until we find it. If first + last < sum, then there is no complement for first. We can therefore move first forward. We stop when first is greater than last.
Why must this find all complements for first? Because the array is sorted and we’re trying progressively smaller numbers. When the sum of first and last is less than the sum, we know that trying even smaller numbers (as last) won’t help us find a complement.
Why must this find all complements for last? Because all pairs must be made up of a first and a last. We’ve found all complements for first, therefore we’ve found all complements of last.
1 public static void printPairSums(int[] array, int sum) {
2 Arrays.sort(array);
3 int first = 0;
4 int last = array.length - 1;
5 while (first < last) {
6 int s = array[first] + array[last];
7 if (s == sum) {
8 System.out.println(array[first] + “ “ + array[last]);
9 ++first;
10 --last;
11 } else {
12 if (s < sum) ++first;
13 else --last;
14 }
15 }
16 }
Solutions to Chapter 20 | Hard
Cracking the Coding Interview | Additional Review Problems
279
20.1 Write a function that adds two numbers. You should not use + or any arithmetic operators.
pg 91
SOLUTION
To investigate this problem, let’s start off by gaining a deeper understanding of how we add numbers. We’ll work in Base 10 so that it’s easier to see. To add 759 + 674, I would usually add digit[0] from each number, carry the one, add digit[1] from each number, carry the one, etc. You could take the same approach in binary: add each digit, and carry the one as necessary.
Can we make this a little easier? Yes! Imagine I decided to split apart the “addition” and “carry” steps. That is, I do the following:
1. Add 759 + 674, but “forget” to carry. I then get 323.
2. Add 759 + 674 but only do the carrying, rather than the addition of each digit. I then get 1110.
3. Add the result of the first two operations (recursively, using the same process described in step 1 and 2): 1110 + 323 = 1433.
Now, how would we do this in binary?
1. If I add two binary numbers together but forget to carry, bit[i] will be 0 if bit[i] in a and b are both 0 or both 1. This is an XOR.
2. If I add two numbers together but only carry, I will have a 1 in bit[i] if bit[i-1] in a and b are both 1’s. This is an AND, shifted.
3. Now, recurse until there’s nothing to carry.
1 int add_no_arithm(int a, int b) {
2 if (b == 0) return a;
3 int sum = a ^ b; // add without carrying
4 int carry = (a & b) << 1; // carry, but don’t add
5 return add_no_arithm(sum, carry); // recurse
6 }
OBSERVATIONS AND SUGGESTIONS:
The Approach: There are a couple of suggestions for figuring out this problem:
1. Our first instinct in problems like these should be that we’re going to have to work with bits. Why? Because when you take away the + sign, what other choice do we have? Plus, that’s how computers do it.
Solutions to Chapter 20 | Hard
280
CareerCup.com
2. Our next thought in problems like these should be to really, really understand how you add. Walk through an addition problem to see if you can understand something new—some pattern—and then see if you can replicate that with code.
Your interviewer is looking for two things in this problem:
1. Can you break down a problem and solve it?
2. Do you understand how to work with bits?
Solutions to Chapter 20 | Hard
281 Cracking the Coding Interview | Additional Review Problems
20.2 Write a method to shuffle a deck of cards. It must be a perfect shuffle - in other words, each 52! permutations of the deck has to be equally likely. Assume that you are given a random number generator which is perfect.
pg 91
SOLUTION
This is a very well known interview question, and a well known algorithm. If you aren’t one of the lucky few to have already know this algorithm, read on.
Let’s start with a brute force approach: we could randomly selecting items and put them into a new array. We must make sure that we don’t pick the same item twice though by somehow marking the node as dead.
Array: [1] [2] [3] [4] [5]
Randomly select 4: [4] [?] [?] [?] [?]
Mark element as dead: [1] [2] [3] [X] [5]
The tricky part is, how do we mark [4] as dead such that we prevent that element from being picked again? One way to do it is to swap the now-dead [4] with the first element in the array:
Array: [1] [2] [3] [4] [5]
Randomly select 4: [4] [?] [?] [?] [?]
Swap dead element: [X] [2] [3] [1] [5]
Array: [X] [2] [3] [1] [5]
Randomly select 3: [4] [3] [?] [?] [?]
Swap dead element: [X] [X] [2] [1] [5]
By doing it this way, it’s much easier for the algorithm to “know” that the first k elements are dead than that the third, fourth, nineth, etc elements are dead. We can also optimize this by merging the shuffled array and the original array.
Randomly select 4: [4] [2] [3] [1] [5]
Randomly select 3: [4] [3] [2] [1] [5]
This is an easy algorithm to implement iteratively:
1 public static void shuffleArray(int[] cards) {
2 int temp, index;
3 for (int i = 0; i < cards.length; i++){
4 index = (int) (Math.random() * (cards.length - i)) + i;
5 temp = cards[i];
6 cards[i] = cards[index];
7 cards[index] = temp;
8 }
9 }
Solutions to Chapter 20 | Hard
. CareerCup com 282
20.3 Write a method to randomly generate a set of m integers from an array of size n. Each element must have equal probability of being chosen.
pg 91
SOLUTION
Our first instinct on this problem might be to randomly pick elements from the array and put them into our new subset array. But then, what if we pick the same element twice? Ideally, we’d want to somehow “shrink” the array to no longer contain that element. Shrinking is expensive though because of all the shifting required.
Instead of shrinking / shifting, we can swap the element with an element at the beginning of the array and then “remember” that the array now only includes elements j and greater. That is, when we pick subset[0] to be array[k], we replace array[k] with the first element in the array. When we pick subset[1], we consider array[0] to be “dead” and we pick a random element y between 1 and array.size(). We then set subset[1] equal to array[y], and set array[y] equal to array[1]. Elements 0 and 1 are now “dead.” Subset[2] is now chosen from array[2] through array[array.size()], and so on.
1 /* Random number between lower and higher, inclusive */
2 public static int rand(int lower, int higher) {
3 return lower + (int)(Math.random() * (higher - lower + 1));
4 }
5
6 /* pick M elements from original array. Clone original array so that
7 * we don’t destroy the input. */
8 public static int[] pickMRandomly(int[] original, int m) {
9 int[] subset = new int[m];
10 int[] array = original.clone();
11 for (int j = 0; j < m; j++) {
12 int index = rand(j, array.length - 1);
13 subset[j] = array[index];
14 array[index] = array[j]; // array[j] is now “dead”
15 }
16 return subset;
17 }
Solutions to Chapter 20 | Hard
283 Cracking the Coding Interview | Additional Review Problems
20.4 Write a method to count the number of 2s between 0 and n.
pg 91
SOLUTION
Picture a sequence of numbers:
0 1 2 3 4 5 6 7 8 9
10 11 12 13 14 15 16 17 18 19
20 21 22 23 24 25 26 27 28 29
...
110 111 112 113 114 115 116 117 118 119
The last digit will be repeated every 10 numbers, the last two digits will be repeated every 10^2 numbers, the last 3 digits will be repeated every 10^3 numbers, etc.
So, if there are X 2s between 0 and 99, then we know there are 2x twos between 0 and 199. Between 0 and 299, we have 3x twos from the last two digits, and another 100 2s from the first digit.
In other words, we can look at a number like this:
f(513) = 5 * f(99) + f(13) + 100
To break this down individually:
»»The sequence of the last two digits are repeated 5 times, so add 5 * f(99)
»»We need to account for the last two digits in 500 -> 513, so add f(13)
»»We need to account for the first digit being two between 200 -> 299, so add 100
Of course, if n is, say, 279, we’ll need to account for this slightly differently:
f(279) = 2 * f(99) + f(79) + 79 + 1
To break this down individually:
»»The sequence of the last two digits are repeated 2 times, so add 2 * f(99)
»»We need to account for the last two digits in 200 -> 279, so add f(79)
»»We need to account for the first digit being two between 200 -> 279, so add 79 + 1
Recu rsive Code:
1 public static int count2sR(int n) {
2 // Base case
3 if (n == 0) return 0;
4
5 // 513 into 5 * 100 + 13. [Power = 100; First = 5; Remainder = 13]
6 int power = 1;
7 while (10 * power < n) power *= 10;
8 int first = n / power;
9 int remainder = n % power;
Solutions to Chapter 20 | Hard
. CareerCup com 284
10
11 // Counts 2s from first digit
12 int nTwosFirst = 0;
13 if (first > 2) nTwosFirst += power;
14 else if (first == 2) nTwosFirst += remainder + 1;
15
16 // Count 2s from all other digits
17 int nTwosOther = first * count2sR(power - 1) + count2sR(remainder);
18
19 return nTwosFirst + nTwosOther;
20 }
We can also implement this algorithm iteratively:
1 public static int count2sI(int num) {
2 int countof2s = 0, digit = 0;
3 int j = num, seendigits=0, position=0, pow10_pos = 1;
4 /* maintaining this value instead of calling pow() is an 6x perf
5 * gain (48s -> 8s) pow10_posMinus1. maintaining this value
6 * instead of calling Numof2s is an 2x perf gain (8s -> 4s).
7 * overall > 10x speedup */
8 while (j > 0) {
9 digit = j % 10;
10 int pow10_posMinus1 = pow10_pos / 10;
11 countof2s += digit * position * pow10_posMinus1;
12 /* we do this if digit <, >, or = 2
13 * Digit < 2 implies there are no 2s contributed by this
14 * digit.
15 * Digit == 2 implies there are 2 * numof2s contributed by
16 * the previous position + num of 2s contributed by the
17 * presence of this 2 */
18 if (digit == 2) {
19 countof2s += seendigits + 1;
20 }
21 /* Digit > 2 implies there are digit * num of 2s by the prev.
22 * position + 10^position */
23 else if(digit > 2) {
24 countof2s += pow10_pos;
25 }
26 seendigits = seendigits + pow10_pos * digit;
27 pow10_pos *= 10;
28 position++;
29 j = j / 10;
30 }
31 return(countof2s);
32 }
Solutions to Chapter 20 | Hard
285 Cracking the Coding Interview | Additional Review Problems
20.5 You have a large text file containing words. Given any two words, find the shortest distance (in terms of number of words) between them in the file. Can you make the searching operation in O(1) time? What about the space complexity for your solution?
pg 91
SOLUTION
We will assume for this question that the word order does not matter. This is a question you should ask your interviewer. If the word order does matter, we can make the small modification shown in the code below.
To solve this problem, simply traverse the file and for every occurrence of word1 and word2, compare difference of positions and update the current minimum.
1 int shortest(String[] words, String word1, String word2) {
2 int pos = 0;
3 int min = Integer.MAX_VALUE / 2;
4 int word1_pos = -min;
5 int word2_pos = -min;
6 for (int i = 0; i < words.length; i++) {
7 String current_word = words[i];
8 if (current_word.equals(word1)) {
9 word1_pos = pos;
10 // Comment following 3 lines if word order matters
11 int distance = word1_pos - word2_pos;
12 if (min > distance)
13 min = distance;
14 } else if (current_word.equals(word2)) {
15 word2_pos = pos;
16 int distance = word2_pos - word1_pos;
17 if (min > distance) min = distance;
18 }
19 ++pos;
20 }
21 return min;
22 }
To solve this problem in less time (but more space), we can create a hash table with each word and the locations where it occurs. We then just need to find the minimum (arithmetic) difference in the locations (e.g., abs(word0.loc[1] - word1.loc[5])).
To find the minimum arithmetic difference, we take each location for word1 (e.g.: 0, 3} and do a modified binary search for it in word2’s location list, returning the closest number. Our search for 3, for example, in {2, 7, 9} would return 1. The minimum of all these binary searches is the shortest distance.
Solutions to Chapter 20 | Hard
. CareerCup com 286
20.6 Describe an algorithm to find the largest 1 million numbers in 1 billion numbers. Assume that the computer memory can hold all one billion numbers.
pg 91
SOLUTION
Approach 1: Sorting
Sort the elements and then take the first million numbers from that. Complexity is O(n log n).
Approach 2: Max Heap
1. Create a Min Heap with the first million numbers.
2. For each remaining number, insert it in the Min Heap and then delete the minimum value from the heap.
3. The heap now contains the largest million numbers.
4. This algorithm is O(n log m), where m is the number of values we are looking for.
Approach 3: Selection Rank Algorithm (if you can modify the original array)
Selection Rank is a well known algorithm in computer science to find the ith smallest (or largest) element in an array in expected linear time. The basic algorithm for finding the ith smallest elements goes like this:
»»Pick a random element in the array and use it as a ‘pivot’. Move all elements smaller than that element to one side of the array, and all elements larger to the other side.
»»If there are exactly i elements on the right, then you just find the smallest element on that side.
»»Otherwise, if the right side is bigger than i, repeat the algorithm on the right. If the right side is smaller than i, repeat the algorithm on the left for i – right.size().
Given this algorithm, you can either:
»»Tweak it to use the existing partitions to find the largest i elements (where i = one million).
»»Or, once you find the ith largest element, run through the array again to return all elements greater than or equal to it.
This algorithm has expected O(n) time.
Solutions to Chapter 20 | Hard
287 Cracking the Coding Interview | Additional Review Problems
20.7 Write a program to find the longest word made of other words.
pg 91
SOLUTION
The solution below does the following:
1. Sort the array by size, putting the longest word at the front
2. For each word, split it in all possible ways. That is, for “test”, split it into {“t”, “est”}, {“te”, “st”} and {“tes”, “t”}.
3. Then, for each pairing, check if the first half and the second both exist elsewhere in the array.
4. “Short circuit” by returning the first string we find that fits condition #3.
What is the time complexity of this?
»»Time to sort array: O(n log n)
»»Time to check if first / second half of word exists: O(d) per word, where d is the average length of a word.
»»Total complexity: O(n log n + n * d). Note that d is fixed (probably around 5—10 characters). Thus, we can guess that for short arrays, the time is estimated by O(n * d) , which also equals O(number of characters in the array). For longer arrays, the time will be better estimated by O(n log n).
»»Space complexity: O(n).
Optimizations: If we didn’t want to use additional space, we could cut out the hash table. This would mean:
»»Sorting the array in alphabetical order
»»Rather than looking up the word in a hash table, we would use binary search in the array
»»We would no longer be able to short circuit.
1 class LengthComparator implements Comparator<String> {
2 @Override
3 public int compare(String o1, String o2) {
4 if (o1.length() < o2.length()) return 1;
5 if (o1.length() > o2.length()) return -1;
6 return 0;
7 }
8 }
Solutions to Chapter 20 | Hard
. CareerCup com 288
20.8 Given a string s and an array of smaller strings T, design a method to search s for each small string in T.
pg 91
SOLUTION
First, create a suffix tree for s. For example, if your word were bibs, you would create the following tree:
Then, all you need to do is search for each string in T in the suffix tree. Note that if “B” were a word, you would come up with two locations.
1 public class SuffixTree {
2 SuffixTreeNode root = new SuffixTreeNode();
3 public SuffixTree(String s) {
4 for (int i = 0; i < s.length(); i++) {
5 String suffix = s.substring(i);
6 root.insertString(suffix, i);
7 }
8 }
9
10 public ArrayList<Integer> getIndexes(String s) {
11 return root.getIndexes(s);
12 }
13 }
14
15 public class SuffixTreeNode {
16 HashMap<Character, SuffixTreeNode> children = new
17 HashMap<Character, SuffixTreeNode>();
18 char value;
19 ArrayList<Integer> indexes = new ArrayList<Integer>();
S
B
I
B
S
S
B
I
S
Solutions to Chapter 20 | Hard
289 Cracking the Coding Interview | Additional Review Problems
20 public SuffixTreeNode() { }
21
22 public void insertString(String s, int index) {
23 indexes.add(index);
24 if (s != null && s.length() > 0) {
25 value = s.charAt(0);
26 SuffixTreeNode child = null;
27 if (children.containsKey(value)) {
28 child = children.get(value);
29 } else {
30 child = new SuffixTreeNode();
31 children.put(value, child);
32 }
33 String remainder = s.substring(1);
34 child.insertString(remainder, index);
35 }
36 }
37
38 public ArrayList<Integer> getIndexes(String s) {
39 if (s == null || s.length() == 0) {
40 return indexes;
41 } else {
42 char first = s.charAt(0);
43 if (children.containsKey(first)) {
44 String remainder = s.substring(1);
45 return children.get(first).getIndexes(remainder);
46 }
47 }
48 return null;
49 }
50 }
51
52 public class Question {
53 public static void main(String[] args) {
54 String testString = “mississippi”;
55 String[] stringList = {“is”, “sip”, “hi”, “sis”};
56 SuffixTree tree = new SuffixTree(testString);
57 for (String s : stringList) {
58 ArrayList<Integer> list = tree.getIndexes(s);
59 if (list != null) {
60 System.out.println(s + “: “ + list.toString());
61 }
62 }
63 }
64 }
Solutions to Chapter 20 | Hard
. CareerCup com 290
20.9 Numbers are randomly generated and passed to a method. Write a program to find and maintain the median value as new values are generated.
pg 91
SOLUTIONS
One solution is to use two priority heaps: a max heap for the values below the median, and a min heap for the values above the median. The median will be largest value of the max heap. When a new value arrives it is placed in the below heap if the value is less than or equal to the median, otherwise it is placed into the above heap. The heap sizes can be equal or the below heap has one extra. This constraint can easily be restored by shifting an element from one heap to the other. The median is available in constant time, so updates are O(lg n).
1 private Comparator<Integer> maxHeapComparator, minHeapComparator;
2 private PriorityQueue<Integer> maxHeap, minHeap;
3 public void addNewNumber(int randomNumber) {
4 if (maxHeap.size() == minHeap.size()) {
5 if ((minHeap.peek() != null) &&
6 randomNumber > minHeap.peek()) {
7 maxHeap.offer(minHeap.poll());
8 minHeap.offer(randomNumber);
9 } else {
10 maxHeap.offer(randomNumber);
11 }
12 }
13 else {
14 if(randomNumber < maxHeap.peek()){
15 minHeap.offer(maxHeap.poll());
16 maxHeap.offer(randomNumber);
17 }
18 else {
19 minHeap.offer(randomNumber);
20 }
21 }
22 }
23 public static double getMedian() {
24 if (maxHeap.isEmpty()) return minHeap.peek();
25 else if (minHeap.isEmpty()) return maxHeap.peek();
26 if (maxHeap.size() == minHeap.size()) {
27 return (minHeap.peek() + maxHeap.peek()) / 2;
28 } else if (maxHeap.size() > minHeap.size()) {
29 return maxHeap.peek();
30 } else {
31 return minHeap.peek();
32 }
33 }
Solutions to Chapter 20 | Hard
291 Cracking the Coding Interview | Additional Review Problems
20.10 Given two words of equal length that are in a dictionary, write a method to transform one word into another word by changing only one letter at a time. The new word you get in each step must be in the dictionary.
EXAMPLE:
Input: DAMP, LIKE
Output: DAMP -> LAMP -> LIMP -> LIME -> LIKE
pg 91
SOLUTION
Though this problem seems tough, it’s actually a straightforward modification of breadth-first-search. Each word in our “graph” branches to all words in the dictionary that are one edit away. The interesting part is how to implement this—should we build a graph as we go? We could, but there’s an easier way. We can instead use a “backtrack map.” In this backtrack map, if B[v] = w, then you know that you edited v to get w. When we reach our end word, we can use this backtrack map repeatedly to reverse our path. See the code below:
1 LinkedList<String> transform(String startWord, String stopWord,
2 Set<String> dictionary) {
3 startWord = startWord.toUpperCase();
4 stopWord = stopWord.toUpperCase();
5 Queue<String> actionQueue = new LinkedList<String>();
6 Set<String> visitedSet = new HashSet<String>();
7 Map<String, String> backtrackMap = new TreeMap<String, String>();
8
9 actionQueue.add(startWord);
10 visitedSet.add(startWord);
11
12 while (!actionQueue.isEmpty()) {
13 String w = actionQueue.poll();
14 // For each possible word v from w with one edit operation
15 for (String v : getOneEditWords(w)) {
16 if (v.equals(stopWord)) {
17 // Found our word! Now, back track.
18 LinkedList<String> list = new LinkedList<String>();
19 // Append v to list
20 list.add(v);
21 while (w != null) {
22 list.add(0, w);
23 w = backtrackMap.get(w);
24 }
25 return list;
26 }
27 // If v is a dictionary word
Solutions to Chapter 20 | Hard
. CareerCup com 292
28 if (dictionary.contains(v)) {
29 if (!visitedSet.contains(v)) {
30 actionQueue.add(v);
31 visitedSet.add(v); // mark visited
32 backtrackMap.put(v, w);
33 }
34 }
35 }
36 }
37 return null;
38 }
39
40 Set<String> getOneEditWords(String word) {
41 Set<String> words = new TreeSet<String>();
42 for (int i = 0; i < word.length(); i++) {
43 char[] wordArray = word.toCharArray();
44 // change that letter to something else
45 for (char c = ‘A’; c <= ‘Z’; c++) {
46 if (c != word.charAt(i)) {
47 wordArray[i] = c;
48 words.add(new String(wordArray));
49 }
50 }
51 }
52 return words;
53 }
Let n be the length of the start word and m be the number of like sized words in the dictionary. The runtime of this algorithm is O(n*m) since the while loop will dequeue at most m unique words. The for loop is O(n) as it walks down the string applying a fixed number of replacements for each character.
Solutions to Chapter 20 | Hard
293 Cracking the Coding Interview | Additional Review Problems
20.11 Imagine you have a square matrix, where each cell is filled with either black or white. Design an algorithm to find the maximum subsquare such that all four borders are filled with black pixels.
pg 92
SOLUTION
Assumption: Square is of size NxN.
This algorithm does the following:
1. Iterate through every (full) column from left to right.
2. At each (full) column (call this currentColumn), look at the subcolumns (from biggest to smallest).
3. At each subcolumn, see if you can form a square with the subcolumn as the left side. If so, update currentMaxSize and go to the next (full) column.
4. If N - currentColumn <= currentMaxSize, then break completely. We’ve found the largest square possible. Why? At each column, we’re trying to create a square with that column as the left side. The largest such square we could possibly create is N - currentColumn. Thus, if N-currentColumn <= currentMaxSize, then we have no need to proceed.
Time complexity: O(N^2).
1 public static Subsquare findSquare(int[][] matrix){
2 assert(matrix.length > 0);
3 for (int row = 0; row < matrix.length; row++){
4 assert(matrix[row].length == matrix.length);
5 }
6
7 int N = matrix.length;
8
9 int currentMaxSize = 0;
10 Subsquare sq = null;
11 int col = 0;
12
13 // Iterate through each column from left to right
14 while (N - col > currentMaxSize) { // See step 4 above
15 for (int row = 0; row < matrix.length; row++){
16 // starting from the biggest
17 int size = N - Math.max(row, col);
18 while (size > currentMaxSize){
19 if (isSquare(matrix, row, col, size)){
20 currentMaxSize = size;
21 sq = new Subsquare(row, col, size);
22 break; // go to next (full) column
Solutions to Chapter 20 | Hard
. CareerCup com 294
23 }
24 size--;
25 }
26 }
27 col++;
28 }
29 return sq;
30 }
31
32 private static boolean isSquare(int[][] matrix, int row, int col,
33 int size) {
34 // Check top and bottom border.
35 for (int j = 0; j < size; j++){
36 if (matrix[row][col+j] == 1) {
37 return false;
38 }
39 if (matrix[row+size-1][col+j] == 1){
40 return false;
41 }
42 }
43
44 // Check left and right border.
45 for (int i = 1; i < size - 1; i++){
46 if (matrix[row+i][col] == 1){
47 return false;
48 }
49 if (matrix[row+i][col+size-1] == 1){
50 return false;
51 }
52 }
53 return true;
54 }
55
56 public class Subsquare {
57 public int row, column, size;
58 public Subsquare(int r, int c, int sz) {
59 row = r;
60 column = c;
61 size = sz;
62 }
63 }
Solutions to Chapter 20 | Hard
295 Cracking the Coding Interview | Additional Review Problems
20.12 Given an NxN matrix of positive and negative integers, write code to find the sub-matrix with the largest possible sum.
pg 92
SOLUTION
Brute Force: Complexity O(N^6)
Like many “maximizing” problems, this problem has a straight forward brute force solution. The brute force solution simply iterates through all possible sub-matrixes, computes the sum, and finds the biggest.
To iterate through all possible sub-matrixes (with no duplicates), we simply need to iterate through all order pairings of rows, and then all ordered pairings of columns.
This solution is O(N^6), since we iterate through O(N^4) sub-matrixes, and it takes O(N^2) time to compute the area of each.
Optimized Solution: O(N^4)
Notice that the earlier solution is made slower by a factor of O(N^2) simply because computing the sum of a matrix is so slow. Can we reduce the time to compute the area? Yes! In fact, we can reduce the time of computeSum to O(1).
Consider the following:
If we had the sum of the smaller rectangle (the one including A, B, C, D), and we could compute the sum of D as follows: area(D) = area(A through D) - area(A) - area(B) - area(C).
What if, instead, we had the following:
with the following values (notice that each Val_* starts at the origin):
x1
x2
y1
y2
A
B
C
D
A
B
C
D
Solutions to Chapter 20 | Hard
. CareerCup com 296
Val_D = area(point(0, 0) -> point(x2, y2))
Val_C = area(point(0, 0) -> point(x2, y1))
Val_B = area(point(0, 0) -> point(x1, y2))
Val_A = area(point(0, 0) -> point(x1, y1))
With these values, we know the following:
area(D) = Val_D - area(A union C) - area(A union B) + area(A).
Or, written another way:
area(D) = Val_D - Val_B - Val_C + Val_A
Can we efficiently compute these Val_* values for all points in the matrix? Yes, by using similar logic:
Val_(x, y) = Val(x - 1, y) + Val(y - 1, x) - Val(x - 1, y - 1)
We can precompute all such values, and then efficiently find the maximum submatrix. See the following code for this implementation
1 public static int getMaxMatrix(int[][] original) {
2 int maxArea = Integer.MIN_VALUE; // Important! Max could be < 0
3 int rowCount = original.length;
4 int columnCount = original[0].length;
5 int[][] matrix = precomputeMatrix(original);
6 for (int row1 = 0; row1 < rowCount; row1++) {
7 for (int row2 = row1; row2 < rowCount; row2++) {
8 for (int col1 = 0; col1 < columnCount; col1++) {
9 for (int col2 = col1; col2 < columnCount; col2++) {
10 maxArea = Math.max(maxArea, computeSum(matrix,
11 row1, row2, col1, col2));
12 }
13 }
14 }
15 }
16 return maxArea;
17 }
18
19 private static int[][] precomputeMatrix(int[][] matrix) {
20 int[][] sumMatrix = new int[matrix.length][matrix[0].length];
21 for (int i = 0; i < matrix.length; i++) {
22 for (int j = 0; j < matrix.length; j++) {
23 if (i == 0 && j == 0) { // first cell
24 sumMatrix[i][j] = matrix[i][j];
25 } else if (j == 0) { // cell in first column
26 sumMatrix[i][j] = sumMatrix[i - 1][j] + matrix[i][j];
27 } else if (i == 0) { // cell in first row
28 sumMatrix[i][j] = sumMatrix[i][j - 1] + matrix[i][j];
29 } else {
30 sumMatrix[i][j] = sumMatrix[i - 1][j] +
31 sumMatrix[i][j - 1] - sumMatrix[i - 1][j - 1] +
Solutions to Chapter 20 | Hard
297 Cracking the Coding Interview | Additional Review Problems
32 matrix[i][j];
33 }
34 }
35 }
36 return sumMatrix;
37 }
38
39 private static int computeSum(int[][] sumMatrix, int i1, int i2,
40 int j1, int j2) {
41 if (i1 == 0 && j1 == 0) { // starts at row 0, column 0
42 return sumMatrix[i2][j2];
43 } else if (i1 == 0) { // start at row 0
44 return sumMatrix[i2][j2] - sumMatrix[i2][j1 - 1];
45 } else if (j1 == 0) { // start at column 0
46 return sumMatrix[i2][j2] - sumMatrix[i1 - 1][j2];
47 } else {
48 return sumMatrix[i2][j2] - sumMatrix[i2][j1 - 1]
49 - sumMatrix[i1 - 1][j2] + sumMatrix[i1 - 1][j1 - 1];
50 }
51 }
Solutions to Chapter 20 | Hard
. CareerCup com 298
20.13 Given a dictionary of millions of words, give an algorithm to find the largest possible rectangle of letters such that every row forms a word (reading left to right) and every column forms a word (reading top to bottom).
pg 92
SOLUTION
Many problems involving a dictionary can be solved by doing some preprocessing. Where can we do preprocessing?
Well, if we’re going to create a rectangle of words, we know that each row must be the same length and each column must have the same length. So, let’s group the words of the dictionary based on their sizes. Let’s call this grouping D, where D[i] provides a list of words of length i.
Next, observe that we’re looking for the largest rectangle. What is the absolute largest rectangle that could be formed? It’s (length of largest word) * (length of largest word).
1 int max_rectangle = longest_word * longest_word;
2 for z = max_rectangle to 1 {
3 for each pair of numbers (i, j) where i*j = z {
4 /* attempt to make rectangle. return if successful. */
5 }
6 }
By iterating in this order, we ensure that the first rectangle we find will be the largest.
Now, for the hard part: make_rectangle. Our approach is to rearrange words in list1 into rows and check if the columns are valid words in list2. However, instead of creating, say, a particular 10x20 rectangle, we check if the columns created after inserting the first two words are even valid pre-fixes. A trie becomes handy here.
1 WordGroup[] groupList = WordGroup.createWordGroups(list);
2 private int maxWordLength = groupList.length;
3 private Trie trieList[] = new Trie[maxWordLength];
4
5 public Rectangle maxRectangle() {
6 int maxSize = maxWordLength * maxWordLength;
7 for (int z = maxSize; z > 0; z--) {
8 for (int i = 1; i <= maxWordLength; i ++ ) {
9 if (z % i == 0) {
10 int j = z / i;
11 if (j <= maxWordLength) {
12 Rectangle rectangle = makeRectangle(i,j);
13 if (rectangle != null) {
14 return rectangle;
15 }
16 }
Solutions to Chapter 20 | Hard
299 Cracking the Coding Interview | Additional Review Problems
17 }
18 }
19 }
20 return null;
21 }
22
23 private Rectangle makeRectangle(int length, int height) {
24 if (groupList[length - 1] == null ||
25 groupList[height - 1] == null) {
26 return null;
27 }
28 if (trieList[height - 1] == null) {
29 LinkedList<String> words = groupList[height - 1].getWords();
30 trieList[height - 1] = new Trie(words);
31 }
32 return makePartialRectangle(length, height,
33 new Rectangle(length));
34 }
35
36 private Rectangle makePartialRectangle(int l, int h,
37 Rectangle rectangle) {
38 if (rectangle.height == h) { // Check if complete rectangle
39 if (rectangle.isComplete(l, h, groupList[h - 1])) {
40 return rectangle;
41 } else {
42 return null;
43 }
44 }
45
46 // Compare columns to trie to see if potentially valid rect */
47 if (!rectangle.isPartialOK(l, trieList[h - 1])) return null;
48
49 for (int i = 0; i < groupList[l-1].length(); i++) {
50 Rectangle org_plus =
51 rectangle.append(groupList[l-1].getWord(i));
52 Rectangle rect = makePartialRectangle(l, h, org_plus);
53 if (rect != null) {
54 return rect;
55 }
56 }
57 return null;
58 }
NOTE: See code attachment for full code.

Cracking the Coding Interview
301
Index
A
arithmetic 108, 131, 143, 190, 265, 269, 271, 273, 278, 279, 283, 295
arraylists 126, 142, 152, 170, 171, 173, 185, 200, 261, 288
arrays 100, 102, 111, 179, 273, 278, 281, 282, 286
B
big-O 95, 97, 102, 113, 121, 130, 131, 141, 142, 172, 173, 181, 182, 193, 207, 216, 267, 274, 286, 287, 290, 293, 295
bit manipulation 95, 133, 134, 135, 138, 140, 141, 172, 202, 254, 265, 269, 270, 279
bit vectors 95, 142, 202, 205
breadth first search 124, 126, 199, 206, 291
C
C++ 166, 167, 215, 216, 217, 218, 219, 220, 221, 223, 224, 228, 241, 242, 247, 248, 259
combinatorics 170, 171, 266
D
databases 197, 208, 231, 232, 234
G
graphs 124, 199, 206, 291
H
hash tables 95, 98, 99, 105, 193, 200, 216, 230, 266, 270, 274, 275, 285, 287, 288, 291
heaps 286, 290
J
Java 225, 226, 227, 228, 229, 230, 264
L
lines 189, 192, 193
linked lists 105, 106, 107, 108, 109, 124, 126, 152, 196, 223, 291, 299
302
CareerCup.com
Index
M
matrixes 101, 102, 163, 169, 184, 293, 295
maximize and minimize 125, 148, 273, 293, 295, 298
O
object oriented design 113, 115, 118, 120, 124, 151, 152, 154, 156, 157, 159, 161, 163, 166, 167, 175, 189, 192, 199, 200, 217, 218, 221, 225, 259, 261, 266, 270, 288, 298
P
probability and randomness 187, 188, 277, 281, 282
Q
queues 113, 120, 152, 155, 195, 196, 291
R
recursion 106, 108, 113, 116, 118, 123, 125, 128, 130, 131, 141, 142, 146, 169, 170, 173, 174, 175, 176, 177, 223, 275, 279, 283, 295, 298
S
searching 148, 181, 183, 184, 285
sortings 99, 121, 159, 179, 180, 181, 182, 185, 278, 286, 287
stacks 111, 113, 115, 118, 120, 121, 124
strings 95, 96, 97, 99, 100, 103, 134, 173, 174, 180, 183, 271, 275, 287, 288
T
testing 97, 98, 209, 210, 211, 214
threading 219, 226, 242, 257, 258, 259, 262, 264
trees 123, 125, 126, 127, 128, 130, 131, 166, 208, 216, 286, 288, 290, 298
303 Cracking the Coding Interview
Mock Interviews
Mock Interviews
Studying helps, but nothing can prepare you like the real thing. Each CareerCup interviewer has given over a hundred interviews at Google, Microsoft, or Amazon. To nail your interview, sit down with a trained interviewer and get their experienced feedback.
See www.careercup.com/interview for more details.
One Hour Interview with Real Interviewers
Our interviewers will give you a real interview, just like you'd get at Google, Microsoft or Amazon. We'll test you on the same types of questions that they do. We'll grade you the same way they do. How can we do this? We’ve done over 100 interviews each for these companies. We’ve screened resumes. We’ve been part of their hiring committees. We know what they want.
We'll Also Give You...
»»An .mp3 recording of your interview.
»»Feedback on where you shined and where you struggled.
»»Specific suggestions on how to improve.
»»Instructions on how to approach tough problems
»»Lessons on what interviewers look for in your code.
Schedule Your Interview Today!
See www.careercup.com/interview for pricing and details. Check out our special student rates!
. CareerCup com 304
About the Author
Gayle Laakmann’s interviewing expertise comes from vast experience on both sides of the desk. She has completed Software Engineering interviews with - and received offers from - Microsoft, Google, Amazon, Apple, IBM, Goldman Sachs, Capital IQ, and a number of other firms.
Of these top companies, she has worked for Microsoft, Apple and Google, where she gained deep insight into each company’s hiring practices.
Most recently, Gayle spent three years at Google as a Software Engineer and was one of the company’s lead interviewers. She interviewed over 120 candidates in the U.S. and abroad, and led much of the recruiting for her alma mater, the University of Pennsylvania.
Additionally, she served on Google’s Hiring Committee, where she reviewed each candidate’s feedback and made hire / no-hire decisions. She assessed over 700 candidates in that role, and evaluated hundreds more resumes.
In 2005, Gayle founded CareerCup.com to bring her wealth of experience to candidates around the world. Launched first as a free forum for interview questions, CareerCup now offers a book, a video and mock interviews.
Gayle holds a bachelor’s and master’s degree in Computer Science from the University of Pennsylvania.
• Chapter 5 has been reorganized and streamlined, accounting for the ubiquity of
switched Ethernet in local area networks and the consequent increased use of
Ethernet in point-to-point scenarios. Also, a new section on data center networking
has been added.
• Chapter 6 has been updated to reflect recent advances in wireless networks, particularly
cellular data networks and 4G services and architecture.
• Chapter 7, which focuses on multimedia networking, has gone through a major
revision. The chapter now includes an in-depth discussion of streaming video,
including adaptive streaming, and an entirely new and modernized discussion of
CDNs. A newly added section describes the Netflix, YouTube, and Kankan video
streaming systems. The material that has been removed to make way for these
new topics is still available on the Companion Web site.
• Chapter 8 now contains an expanded discussion on endpoint authentication.
• Significant new material involving end-of-chapter problems has been added. As
with all previous editions, homework problems have been revised, added, and
removed.
Audience
This textbook is for a first course on computer networking. It can be used in both
computer science and electrical engineering departments. In terms of programming
languages, the book assumes only that the student has experience with C, C++, Java,
or Python (and even then only in a few places). Although this book is more precise
and analytical than many other introductory computer networking texts, it rarely
uses any mathematical concepts that are not taught in high school. We have made a
deliberate effort to avoid using any advanced calculus, probability, or stochastic
process concepts (although we’ve included some homework problems for students
with this advanced background). The book is therefore appropriate for undergraduate
courses and for first-year graduate courses. It should also be useful to practitioners
in the telecommunications industry.
What Is Unique about This Textbook?
The subject of computer networking is enormously complex, involving many
concepts, protocols, and technologies that are woven together in an intricate
manner. To cope with this scope and complexity, many computer networking texts
are often organized around the “layers” of a network architecture. With a layered
organization, students can see through the complexity of computer networking—
they learn about the distinct concepts and protocols in one part of the architecture
while seeing the big picture of how all parts fit together. From a pedagogical
perspective, our personal experience has been that such a layered approach
viii Preface
Preface
Welcome to the sixth edition of Computer Networking: A Top-Down Approach. Since
the publication of the first edition 12 years ago, our book has been adopted for use at
many hundreds of colleges and universities, translated into 14 languages, and used
by over one hundred thousand students and practitioners worldwide. We’ve heard
from many of these readers and have been overwhelmed by the positive response.
What’s New in the Sixth Edition?
We think one important reason for this success has been that our book continues to offer
a fresh and timely approach to computer networking instruction. We’ve made changes
in this sixth edition, but we’ve also kept unchanged what we believe (and the instructors
and students who have used our book have confirmed) to be the most important
aspects of this book: its top-down approach, its focus on the Internet and a modern
treatment of computer networking, its attention to both principles and practice, and its
accessible style and approach toward learning about computer networking. Nevertheless,
the sixth edition has been revised and updated substantially:
• The Companion Web site has been significantly expanded and enriched to
include VideoNotes and interactive exercises, as discussed later in this Preface.
• In Chapter 1, the treatment of access networks has been modernized, and the
description of the Internet ISP ecosystem has been substantially revised, accounting
for the recent emergence of content provider networks, such as Google’s. The
presentation of packet switching and circuit switching has also been reorganized,
providing a more topical rather than historical orientation.
• In Chapter 2, Python has replaced Java for the presentation of socket programming.
While still explicitly exposing the key ideas behind the socket API, Python
code is easier to understand for the novice programmer. Moreover, unlike Java,
Python provides access to raw sockets, enabling students to build a larger variety
of network applications. Java-based socket programming labs have been
replaced with corresponding Python labs, and a new Python-based ICMP Ping
lab has been added. As always, when material is retired from the book, such as
Java-based socket programming material, it remains available on the book’s
Companion Web site (see following text).
• In Chapter 3, the presentation of one of the reliable data transfer protocols has
been simplified and a new sidebar on TCP splitting, commonly used to optimize
the performance of cloud services, has been added.
• In Chapter 4, the section on router architectures has been significantly updated,
reflecting recent developments and practices in the field. Several new integrative
sidebars involving DNS, BGP, and OSPF are included.
• Chapter 5 has been reorganized and streamlined, accounting for the ubiquity of
switched Ethernet in local area networks and the consequent increased use of
Ethernet in point-to-point scenarios. Also, a new section on data center networking
has been added.
• Chapter 6 has been updated to reflect recent advances in wireless networks, particularly
cellular data networks and 4G services and architecture.
• Chapter 7, which focuses on multimedia networking, has gone through a major
revision. The chapter now includes an in-depth discussion of streaming video,
including adaptive streaming, and an entirely new and modernized discussion of
CDNs. A newly added section describes the Netflix, YouTube, and Kankan video
streaming systems. The material that has been removed to make way for these
new topics is still available on the Companion Web site.
• Chapter 8 now contains an expanded discussion on endpoint authentication.
• Significant new material involving end-of-chapter problems has been added. As
with all previous editions, homework problems have been revised, added, and
removed.
Audience
This textbook is for a first course on computer networking. It can be used in both
computer science and electrical engineering departments. In terms of programming
languages, the book assumes only that the student has experience with C, C++, Java,
or Python (and even then only in a few places). Although this book is more precise
and analytical than many other introductory computer networking texts, it rarely
uses any mathematical concepts that are not taught in high school. We have made a
deliberate effort to avoid using any advanced calculus, probability, or stochastic
process concepts (although we’ve included some homework problems for students
with this advanced background). The book is therefore appropriate for undergraduate
courses and for first-year graduate courses. It should also be useful to practitioners
in the telecommunications industry.
What Is Unique about This Textbook?
The subject of computer networking is enormously complex, involving many
concepts, protocols, and technologies that are woven together in an intricate
manner. To cope with this scope and complexity, many computer networking texts
are often organized around the “layers” of a network architecture. With a layered
organization, students can see through the complexity of computer networking—
they learn about the distinct concepts and protocols in one part of the architecture
while seeing the big picture of how all parts fit together. From a pedagogical
perspective, our personal experience has been that such a layered approach
viii Preface
Preface ix
indeed works well. Nevertheless, we have found that the traditional approach of
teaching—bottom up; that is, from the physical layer towards the application
layer—is not the best approach for a modern course on computer networking.
A Top-Down Approach
Our book broke new ground 12 years ago by treating networking in a top-down
manner—that is, by beginning at the application layer and working its way down
toward the physical layer. The feedback we received from teachers and students
alike have confirmed that this top-down approach has many advantages and does
indeed work well pedagogically. First, it places emphasis on the application layer
(a “high growth area” in networking). Indeed, many of the recent revolutions in
computer networking—including the Web, peer-to-peer file sharing, and media
streaming—have taken place at the application layer. An early emphasis on applicationlayer
issues differs from the approaches taken in most other texts, which have only a
small amount of material on network applications, their requirements, application-layer
paradigms (e.g., client-server and peer-to-peer), and application programming interfaces.
Second, our experience as instructors (and that of many instructors who have
used this text) has been that teaching networking applications near the beginning of
the course is a powerful motivational tool. Students are thrilled to learn about how
networking applications work—applications such as e-mail and the Web, which most
students use on a daily basis. Once a student understands the applications, the student
can then understand the network services needed to support these applications. The
student can then, in turn, examine the various ways in which such services might be
provided and implemented in the lower layers. Covering applications early thus provides
motivation for the remainder of the text.
Third, a top-down approach enables instructors to introduce network application
development at an early stage. Students not only see how popular applications
and protocols work, but also learn how easy it is to create their own
network applications and application-level protocols. With the top-down
approach, students get early exposure to the notions of socket programming, service
models, and protocols—important concepts that resurface in all subsequent
layers. By providing socket programming examples in Python, we highlight the
central ideas without confusing students with complex code. Undergraduates in
electrical engineering and computer science should not have difficulty following
the Python code.
An Internet Focus
Although we dropped the phrase “Featuring the Internet” from the title of this book
with the fourth edition, this doesn’t mean that we dropped our focus on the Internet!
Indeed, nothing could be further from the case! Instead, since the Internet has
become so pervasive, we felt that any networking textbook must have a significant
focus on the Internet, and thus this phrase was somewhat unnecessary. We continue
to use the Internet’s architecture and protocols as primary vehicles for studying fundamental
computer networking concepts. Of course, we also include concepts and
protocols from other network architectures. But the spotlight is clearly on the Internet,
a fact reflected in our organizing the book around the Internet’s five-layer architecture:
the application, transport, network, link, and physical layers.
Another benefit of spotlighting the Internet is that most computer science and
electrical engineering students are eager to learn about the Internet and its protocols.
They know that the Internet has been a revolutionary and disruptive technology and
can see that it is profoundly changing our world. Given the enormous relevance of
the Internet, students are naturally curious about what is “under the hood.” Thus, it
is easy for an instructor to get students excited about basic principles when using the
Internet as the guiding focus.
Teaching Networking Principles
Two of the unique features of the book—its top-down approach and its focus on the
Internet—have appeared in the titles of our book. If we could have squeezed a third
phrase into the subtitle, it would have contained the word principles. The field of
networking is now mature enough that a number of fundamentally important issues
can be identified. For example, in the transport layer, the fundamental issues include
reliable communication over an unreliable network layer, connection establishment/
teardown and handshaking, congestion and flow control, and multiplexing. Two fundamentally
important network-layer issues are determining “good” paths between
two routers and interconnecting a large number of heterogeneous networks. In the
link layer, a fundamental problem is sharing a multiple access channel. In network
security, techniques for providing confidentiality, authentication, and message
integrity are all based on cryptographic fundamentals. This text identifies fundamental
networking issues and studies approaches towards addressing these issues. The
student learning these principles will gain knowledge with a long “shelf life”—long
after today’s network standards and protocols have become obsolete, the principles
they embody will remain important and relevant. We believe that the combination of
using the Internet to get the student’s foot in the door and then emphasizing fundamental
issues and solution approaches will allow the student to quickly understand
just about any networking technology.
The Web Site
Each new copy of this textbook includes six months of access to a Companion Web site
for all book readers at http://www.pearsonhighered.com/kurose-ross, which includes:
• Interactive learning material. An important new component of the sixth edition
is the significantly expanded online and interactive learning material. The
book’s Companion Web site now contains VideoNotes—video presentations of
x Preface
Preface xi
important topics thoughout the book done by the authors, as well as walkthroughs
of solutions to problems similar to those at the end of the chapter.
We’ve also added Interactive Exercises that can create (and present solutions
for) problems similar to selected end-of-chapter problems. Since students can
generate (and view solutions for) an unlimited number of similar problem
instances, they can work until the material is truly mastered. We’ve seeded the
Web site with VideoNotes and online problems for chapters 1 through 5 and will
continue to actively add and update this material over time. As in earlier editions,
the Web site contains the interactive Java applets that animate many key
networking concepts. The site also has interactive quizzes that permit students
to check their basic understanding of the subject matter. Professors can integrate
these interactive features into their lectures or use them as mini labs.
• Additional technical material. As we have added new material in each edition of
our book, we’ve had to remove coverage of some existing topics to keep the
book at manageable length. For example, to make room for the new material in
this edition, we’ve removed material on ATM networks and the RTSP protocol
for multimedia. Material that appeared in earlier editions of the text is still of
interest, and can be found on the book’s Web site.
• Programming assignments. The Web site also provides a number of detailed
programming assignments, which include building a multithreaded Web
server, building an e-mail client with a GUI interface, programming the sender
and receiver sides of a reliable data transport protocol, programming a distributed
routing algorithm, and more.
• Wireshark labs. One’s understanding of network protocols can be greatly deepened
by seeing them in action. The Web site provides numerous Wireshark
assignments that enable students to actually observe the sequence of messages
exchanged between two protocol entities. The Web site includes separate Wireshark
labs on HTTP, DNS, TCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, SSL, and
on tracing all protocols involved in satisfying a request to fetch a web page.
We’ll continue to add new labs over time.
Pedagogical Features
We have each been teaching computer networking for more than 20 years.
Together, we bring more than 50 years of teaching experience to this text, during
which time we have taught many thousands of students. We have also been active
researchers in computer networking during this time. (In fact, Jim and Keith first
met each other as master’s students in a computer networking course taught by
Mischa Schwartz in 1979 at Columbia University.) We think all this gives us a
good perspective on where networking has been and where it is likely to go in the
future. Nevertheless, we have resisted temptations to bias the material in this book
towards our own pet research projects. We figure you can visit our personal Web
sites if you are interested in our research. Thus, this book is about modern computer
networking—it is about contemporary protocols and technologies as well as
the underlying principles behind these protocols and technologies. We also believe
that learning (and teaching!) about networking can be fun. A sense of humor, use
of analogies, and real-world examples in this book will hopefully make this material
more fun.
Supplements for Instructors
We provide a complete supplements package to aid instructors in teaching this course.
This material can be accessed from Pearson’s Instructor Resource Center
(http://www.pearsonhighered.com/irc). Visit the Instructor Resource Center or send
e-mail to computing@aw.com for information about accessing these instructor’s
supplements.
• PowerPoint® slides. We provide PowerPoint slides for all nine chapters. The
slides have been completely updated with this sixth edition. The slides cover
each chapter in detail. They use graphics and animations (rather than relying
only on monotonous text bullets) to make the slides interesting and visually
appealing. We provide the original PowerPoint slides so you can customize them
to best suit your own teaching needs. Some of these slides have been contributed
by other instructors who have taught from our book.
• Homework solutions. We provide a solutions manual for the homework problems
in the text, programming assignments, and Wireshark labs. As noted earlier, we’ve
introduced many new homework problems in the first five chapters of the book.
Chapter Dependencies
The first chapter of this text presents a self-contained overview of computer networking.
Introducing many key concepts and terminology, this chapter sets the stage
for the rest of the book. All of the other chapters directly depend on this first chapter.
After completing Chapter 1, we recommend instructors cover Chapters 2
through 5 in sequence, following our top-down philosophy. Each of these five chapters
leverages material from the preceding chapters. After completing the first five
chapters, the instructor has quite a bit of flexibility. There are no interdependencies
among the last four chapters, so they can be taught in any order. However, each of
the last four chapters depends on the material in the first five chapters. Many
instructors first teach the first five chapters and then teach one of the last four chapters
for “dessert.”
xii Preface
Preface xiii
One Final Note: We’d Love to Hear from You
We encourage students and instructors to e-mail us with any comments they might
have about our book. It’s been wonderful for us to hear from so many instructors
and students from around the world about our first four editions. We’ve incorporated
many of these suggestions into later editions of the book. We also encourage instructors
to send us new homework problems (and solutions) that would complement the
current homework problems. We’ll post these on the instructor-only portion of the
Web site. We also encourage instructors and students to create new Java applets that
illustrate the concepts and protocols in this book. If you have an applet that you
think would be appropriate for this text, please submit it to us. If the applet (including
notation and terminology) is appropriate, we’ll be happy to include it on the text’s
Web site, with an appropriate reference to the applet’s authors.
So, as the saying goes, “Keep those cards and letters coming!” Seriously,
please do continue to send us interesting URLs, point out typos, disagree with
any of our claims, and tell us what works and what doesn’t work. Tell us what
you think should or shouldn’t be included in the next edition. Send your e-mail
to kurose@cs.umass.edu and ross@poly.edu.
Acknowledgments
Since we began writing this book in 1996, many people have given us invaluable
help and have been influential in shaping our thoughts on how to best organize and
teach a networking course. We want to say A BIG THANKS to everyone who has
helped us from the earliest first drafts of this book, up to this fifth edition. We are also
very thankful to the many hundreds of readers from around the world—students, faculty,
practitioners—who have sent us thoughts and comments on earlier editions of
the book and suggestions for future editions of the book. Special thanks go out to:
Al Aho (Columbia University)
Hisham Al-Mubaid (University of Houston-Clear Lake)
Pratima Akkunoor (Arizona State University)
Paul Amer (University of Delaware)
Shamiul Azom (Arizona State University)
Lichun Bao (University of California at Irvine)
Paul Barford (University of Wisconsin)
Bobby Bhattacharjee (University of Maryland)
Steven Bellovin (Columbia University)
Pravin Bhagwat (Wibhu)
Supratik Bhattacharyya (previously at Sprint)
Ernst Biersack (Eurécom Institute)
Shahid Bokhari (University of Engineering & Technology, Lahore)
Jean Bolot (Technicolor Research)
Daniel Brushteyn (former University of Pennsylvania student)
Ken Calvert (University of Kentucky)
Evandro Cantu (Federal University of Santa Catarina)
Jeff Case (SNMP Research International)
Jeff Chaltas (Sprint)
Vinton Cerf (Google)
Byung Kyu Choi (Michigan Technological University)
Bram Cohen (BitTorrent, Inc.)
Constantine Coutras (Pace University)
John Daigle (University of Mississippi)
Edmundo A. de Souza e Silva (Federal University of Rio de Janeiro)
Philippe Decuetos (Eurécom Institute)
Christophe Diot (Technicolor Research)
Prithula Dhunghel (Akamai)
Deborah Estrin (University of California, Los Angeles)
Michalis Faloutsos (University of California at Riverside)
Wu-chi Feng (Oregon Graduate Institute)
Sally Floyd (ICIR, University of California at Berkeley)
Paul Francis (Max Planck Institute)
Lixin Gao (University of Massachusetts)
JJ Garcia-Luna-Aceves (University of California at Santa Cruz)
Mario Gerla (University of California at Los Angeles)
David Goodman (NYU-Poly)
Yang Guo (Alcatel/Lucent Bell Labs)
Tim Griffin (Cambridge University)
Max Hailperin (Gustavus Adolphus College)
Bruce Harvey (Florida A&M University, Florida State University)
Carl Hauser (Washington State University)
Rachelle Heller (George Washington University)
Phillipp Hoschka (INRIA/W3C)
Wen Hsin (Park University)
Albert Huang (former University of Pennsylvania student)
Cheng Huang (Microsoft Research)
Esther A. Hughes (Virginia Commonwealth University)
Van Jacobson (Xerox PARC)
Pinak Jain (former NYU-Poly student)
Jobin James (University of California at Riverside)
Sugih Jamin (University of Michigan)
Shivkumar Kalyanaraman (IBM Research, India)
Jussi Kangasharju (University of Helsinki)
Sneha Kasera (University of Utah)
Parviz Kermani (formerly of IBM Research)
xiv Preface
Preface xv
Hyojin Kim (former University of Pennsylvania student)
Leonard Kleinrock (University of California at Los Angeles)
David Kotz (Dartmouth College)
Beshan Kulapala (Arizona State University)
Rakesh Kumar (Bloomberg)
Miguel A. Labrador (University of South Florida)
Simon Lam (University of Texas)
Steve Lai (Ohio State University)
Tom LaPorta (Penn State University)
Tim-Berners Lee (World Wide Web Consortium)
Arnaud Legout (INRIA)
Lee Leitner (Drexel University)
Brian Levine (University of Massachusetts)
Chunchun Li (former NYU-Poly student)
Yong Liu (NYU-Poly)
William Liang (former University of Pennsylvania student)
Willis Marti (Texas A&M University)
Nick McKeown (Stanford University)
Josh McKinzie (Park University)
Deep Medhi (University of Missouri, Kansas City)
Bob Metcalfe (International Data Group)
Sue Moon (KAIST)
Jenni Moyer (Comcast)
Erich Nahum (IBM Research)
Christos Papadopoulos (Colorado Sate University)
Craig Partridge (BBN Technologies)
Radia Perlman (Intel)
Jitendra Padhye (Microsoft Research)
Vern Paxson (University of California at Berkeley)
Kevin Phillips (Sprint)
George Polyzos (Athens University of Economics and Business)
Sriram Rajagopalan (Arizona State University)
Ramachandran Ramjee (Microsoft Research)
Ken Reek (Rochester Institute of Technology)
Martin Reisslein (Arizona State University)
Jennifer Rexford (Princeton University)
Leon Reznik (Rochester Institute of Technology)
Pablo Rodrigez (Telefonica)
Sumit Roy (University of Washington)
Avi Rubin (Johns Hopkins University)
Dan Rubenstein (Columbia University)
Douglas Salane (John Jay College)
Despina Saparilla (Cisco Systems)
John Schanz (Comcast)
Henning Schulzrinne (Columbia University)
Mischa Schwartz (Columbia University)
Ardash Sethi (University of Delaware)
Harish Sethu (Drexel University)
K. Sam Shanmugan (University of Kansas)
Prashant Shenoy (University of Massachusetts)
Clay Shields (Georgetown University)
Subin Shrestra (University of Pennsylvania)
Bojie Shu (former NYU-Poly student)
Mihail L. Sichitiu (NC State University)
Peter Steenkiste (Carnegie Mellon University)
Tatsuya Suda (University of California at Irvine)
Kin Sun Tam (State University of New York at Albany)
Don Towsley (University of Massachusetts)
David Turner (California State University, San Bernardino)
Nitin Vaidya (University of Illinois)
Michele Weigle (Clemson University)
David Wetherall (University of Washington)
Ira Winston (University of Pennsylvania)
Di Wu (Sun Yat-sen University)
Shirley Wynn (NYU-Poly)
Raj Yavatkar (Intel)
Yechiam Yemini (Columbia University)
Ming Yu (State University of New York at Binghamton)
Ellen Zegura (Georgia Institute of Technology)
Honggang Zhang (Suffolk University)
Hui Zhang (Carnegie Mellon University)
Lixia Zhang (University of California at Los Angeles)
Meng Zhang (former NYU-Poly student)
Shuchun Zhang (former University of Pennsylvania student)
Xiaodong Zhang (Ohio State University)
ZhiLi Zhang (University of Minnesota)
Phil Zimmermann (independent consultant)
Cliff C. Zou (University of Central Florida)
We also want to thank the entire Addison-Wesley team—in particular, Michael Hirsch,
Marilyn Lloyd, and Emma Snider—who have done an absolutely outstanding job on
this sixth edition (and who have put up with two very finicky authors who seem congenitally
unable to meet deadlines!). Thanks also to our artists, Janet Theurer and
Patrice Rossi Calkin, for their work on the beautiful figures in this book, and to Andrea
Stefanowicz and her team at PreMediaGlobal for their wonderful production work on
this edition. Finally, a most special thanks go to Michael Hirsch, our editor at Addison-
Wesley, and Susan Hartman, our former editor at Addison-Wesley. This book would
not be what it is (and may well not have been at all) without their graceful management,
constant encouragement, nearly infinite patience, good humor, and perseverance.
xvi Preface
xvii
Table of Contents
Chapter 1 Computer Networks and the Internet 1
1.1 What Is the Internet? 2
1.1.1 A Nuts-and-Bolts Description 2
1.1.2 A Services Description 5
1.1.3 What Is a Protocol? 7
1.2 The Network Edge 9
1.2.1 Access Networks 12
1.2.2 Physical Media 18
1.3 The Network Core 22
1.3.1 Packet Switching 22
1.3.2 Circuit Switching 27
1.3.3 A Network of Networks 32
1.4 Delay, Loss, and Throughput in Packet-Switched Networks 35
1.4.1 Overview of Delay in Packet-Switched Networks 35
1.4.2 Queuing Delay and Packet Loss 39
1.4.3 End-to-End Delay 42
1.4.4 Throughput in Computer Networks 44
1.5 Protocol Layers and Their Service Models 47
1.5.1 Layered Architecture 47
1.5.2 Encapsulation 53
1.6 Networks Under Attack 55
1.7 History of Computer Networking and the Internet 60
1.7.1 The Development of Packet Switching: 1961–1972 60
1.7.2 Proprietary Networks and Internetworking: 1972–1980 62
1.7.3 A Proliferation of Networks: 1980–1990 63
1.7.4 The Internet Explosion: The 1990s 64
1.7.5 The New Millennium 65
1.8 Summary 66
Homework Problems and Questions 68
Wireshark Lab 78
Interview: Leonard Kleinrock 80
Chapter 2 Application Layer 83
2.1 Principles of Network Applications 84
2.1.1 Network Application Architectures 86
2.1.2 Processes Communicating 88
2.1.3 Transport Services Available to Applications 91
2.1.4 Transport Services Provided by the Internet 93
2.1.5 Application-Layer Protocols 96
2.1.6 Network Applications Covered in This Book 97
2.2 The Web and HTTP 98
2.2.1 Overview of HTTP 98
2.2.2 Non-Persistent and Persistent Connections 100
2.2.3 HTTP Message Format 103
2.2.4 User-Server Interaction: Cookies 108
2.2.5 Web Caching 110
2.2.6 The Conditional GET 114
2.3 File Transfer: FTP 116
2.3.1 FTP Commands and Replies 118
2.4 Electronic Mail in the Internet 118
2.4.1 SMTP 121
2.4.2 Comparison with HTTP 124
2.4.3 Mail Message Format 125
2.4.4 Mail Access Protocols 125
2.5 DNS—The Internet’s Directory Service 130
2.5.1 Services Provided by DNS 131
2.5.2 Overview of How DNS Works 133
2.5.3 DNS Records and Messages 139
2.6 Peer-to-Peer Applications 144
2.6.1 P2P File Distribution 145
2.6.2 Distributed Hash Tables (DHTs) 151
2.7 Socket Programming: Creating Network Applications 156
2.7.1 Socket Programming with UDP 157
2.7.2 Socket Programming with TCP 163
2.8 Summary 168
Homework Problems and Questions 169
Socket Programming Assignments 179
Wireshark Labs: HTTP, DNS 181
Interview: Marc Andreessen 182
xviii Table of Contents
Table of Contents xix
Chapter 3 Transport Layer 185
3.1 Introduction and Transport-Layer Services 186
3.1.1 Relationship Between Transport and Network Layers 186
3.1.2 Overview of the Transport Layer in the Internet 189
3.2 Multiplexing and Demultiplexing 191
3.3 Connectionless Transport: UDP 198
3.3.1 UDP Segment Structure 202
3.3.2 UDP Checksum 202
3.4 Principles of Reliable Data Transfer 204
3.4.1 Building a Reliable Data Transfer Protocol 206
3.4.2 Pipelined Reliable Data Transfer Protocols 215
3.4.3 Go-Back-N (GBN) 218
3.4.4 Selective Repeat (SR) 223
3.5 Connection-Oriented Transport: TCP 230
3.5.1 The TCP Connection 231
3.5.2 TCP Segment Structure 233
3.5.3 Round-Trip Time Estimation and Timeout 238
3.5.4 Reliable Data Transfer 242
3.5.5 Flow Control 250
3.5.6 TCP Connection Management 252
3.6 Principles of Congestion Control 259
3.6.1 The Causes and the Costs of Congestion 259
3.6.2 Approaches to Congestion Control 265
3.6.3 Network-Assisted Congestion-Control Example:
ATM ABR Congestion Control 266
3.7 TCP Congestion Control 269
3.7.1 Fairness 279
3.8 Summary 283
Homework Problems and Questions 285
Programming Assignments 300
Wireshark Labs: TCP, UDP 301
Interview: Van Jacobson 302
Chapter 4 The Network Layer 305
4.1 Introduction 306
4.1.1 Forwarding and Routing 308
4.1.2 Network Service Models 310
4.2 Virtual Circuit and Datagram Networks 313
4.2.1 Virtual-Circuit Networks 314
4.2.2 Datagram Networks 317
4.2.3 Origins of VC and Datagram Networks 319
4.3 What’s Inside a Router? 320
4.3.1 Input Processing 322
4.3.2 Switching 324
4.3.3 Output Processing 326
4.3.4 Where Does Queuing Occur? 327
4.3.5 The Routing Control Plane 331
4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet 331
4.4.1 Datagram Format 332
4.4.2 IPv4 Addressing 338
4.4.3 Internet Control Message Protocol (ICMP) 353
4.4.4 IPv6 356
4.4.5 A Brief Foray into IP Security 362
4.5 Routing Algorithms 363
4.5.1 The Link-State (LS) Routing Algorithm 366
4.5.2 The Distance-Vector (DV) Routing Algorithm 371
4.5.3 Hierarchical Routing 379
4.6 Routing in the Internet 383
4.6.1 Intra-AS Routing in the Internet: RIP 384
4.6.2 Intra-AS Routing in the Internet: OSPF 388
4.6.3 Inter-AS Routing: BGP 390
4.7 Broadcast and Multicast Routing 399
4.7.1 Broadcast Routing Algorithms 400
4.7.2 Multicast 405
4.8 Summary 412
Homework Problems and Questions 413
Programming Assignments 429
Wireshark Labs: IP, ICMP 430
Interview: Vinton G. Cerf 431
Chapter 5 The Link Layer: Links, Access Networks, and LANs 433
5.1 Introduction to the Link Layer 434
5.1.1 The Services Provided by the Link Layer 436
5.1.2 Where Is the Link Layer Implemented? 437
5.2 Error-Detection and -Correction Techniques 438
5.2.1 Parity Checks 440
5.2.2 Checksumming Methods 442
5.2.3 Cyclic Redundancy Check (CRC) 443
5.3 Multiple Access Links and Protocols 445
5.3.1 Channel Partitioning Protocols 448
5.3.2 Random Access Protocols 449
5.3.3 Taking-Turns Protocols 459
5.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access 460
xx Table of Contents
Table of Contents xxi
5.4 Switched Local Area Networks 461
5.4.1 Link-Layer Addressing and ARP 462
5.4.2 Ethernet 469
5.4.3 Link-Layer Switches 476
5.4.4 Virtual Local Area Networks (VLANs) 482
5.5 Link Virtualization: A Network as a Link Layer 486
5.5.1 Multiprotocol Label Switching (MPLS) 487
5.6 Data Center Networking 490
5.7 Retrospective: A Day in the Life of a Web Page Request 495
5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet 495
5.7.2 Still Getting Started: DNS and ARP 497
5.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server 498
5.7.4 Web Client-Server Interaction: TCP and HTTP 499
5.8 Summary 500
Homework Problems and Questions 502
Wireshark Labs: Ethernet and ARP, DHCP 510
Interview: Simon S. Lam 511
Chapter 6 Wireless and Mobile Networks 513
6.1 Introduction 514
6.2 Wireless Links and Network Characteristics 519
6.2.1 CDMA 522
6.3 WiFi: 802.11 Wireless LANs 526
6.3.1 The 802.11 Architecture 527
6.3.2 The 802.11 MAC Protocol 531
6.3.3 The IEEE 802.11 Frame 537
6.3.4 Mobility in the Same IP Subnet 541
6.3.5 Advanced Features in 802.11 542
6.3.6 Personal Area Networks: Bluetooth and Zigbee 544
6.4 Cellular Internet Access 546
6.4.1 An Overview of Cellular Network Architecture 547
6.4.2 3G Cellular Data Networks: Extending the Internet to Cellular
Subscribers 550
6.4.3 On to 4G: LTE 553
6.5 Mobility Management: Principles 555
6.5.1 Addressing 557
6.5.2 Routing to a Mobile Node 559
6.6 Mobile IP 564
6.7 Managing Mobility in Cellular Networks 570
6.7.1 Routing Calls to a Mobile User 571
6.7.2 Handoffs in GSM 572
6.8 Wireless and Mobility: Impact on Higher-Layer Protocols 575
6.9 Summary 578
Homework Problems and Questions 578
Wireshark Lab: IEEE 802.11 (WiFi) 583
Interview: Deborah Estrin 584
Chapter 7 Multimedia Networking 587
7.1 Multimedia Networking Applications 588
7.1.1 Properties of Video 588
7.1.2 Properties of Audio 590
7.1.3 Types of Multimedia Network Applications 591
7.2 Streaming Stored Video 593
7.2.1 UDP Streaming 595
7.2.2 HTTP Streaming 596
7.2.3 Adaptive Streaming and DASH 600
7.2.4 Content Distribution Networks 602
7.2.5 Case Studies: Netflix, YouTube, and Kankan 608
7.3 Voice-over-IP 612
7.3.1 Limitations of the Best-Effort IP Service 612
7.3.2 Removing Jitter at the Receiver for Audio 614
7.3.3 Recovering from Packet Loss 617
7.3.4 Case Study: VoIP with Skype 620
7.4 Protocols for Real-Time Conversational Applications 623
7.4.1 RTP 624
7.4.2 SIP 627
7.5 Network Support for Multimedia 632
7.5.1 Dimensioning Best-Effort Networks 634
7.5.2 Providing Multiple Classes of Service 636
7.5.3 Diffserv 648
7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees:
Resource Reservation and Call Admission 652
7.6 Summary 655
Homework Problems and Questions 656
Programming Assignment 666
Interview: Henning Schulzrinne 668
Chapter 8 Security in Computer Networks 671
8.1 What Is Network Security? 672
8.2 Principles of Cryptography 675
8.2.1 Symmetric Key Cryptography 676
8.2.2 Public Key Encryption 683
xxii Table of Contents
Table of Contents xxiii
8.3 Message Integrity and Digital Signatures 688
8.3.1 Cryptographic Hash Functions 689
8.3.2 Message Authentication Code 691
8.3.3 Digital Signatures 693
8.4 End-Point Authentication 700
8.4.1 Authentication Protocol ap1.0 700
8.4.2 Authentication Protocol ap2.0 701
8.4.3 Authentication Protocol ap3.0 702
8.4.4 Authentication Protocol ap3.1 703
8.4.5 Authentication Protocol ap4.0 703
8.5 Securing E-Mail 705
8.5.1 Secure E-Mail 706
8.5.2 PGP 710
8.6 Securing TCP Connections: SSL 711
8.6.1 The Big Picture 713
8.6.2 A More Complete Picture 716
8.7 Network-Layer Security: IPsec and Virtual Private Networks 718
8.7.1 IPsec and Virtual Private Networks (VPNs) 718
8.7.2 The AH and ESP Protocols 720
8.7.3 Security Associations 720
8.7.4 The IPsec Datagram 721
8.7.5 IKE: Key Management in IPsec 725
8.8 Securing Wireless LANs 726
8.8.1 Wired Equivalent Privacy (WEP) 726
8.8.2 IEEE 802.11i 728
8.9 Operational Security: Firewalls and Intrusion Detection Systems 731
8.9.1 Firewalls 731
8.9.2 Intrusion Detection Systems 739
8.10 Summary 742
Homework Problems and Questions 744
Wireshark Lab: SSL 752
IPsec Lab 752
Interview: Steven M. Bellovin 753
Chapter 9 Network Management 755
9.1 What Is Network Management? 756
9.2 The Infrastructure for Network Management 760
9.3 The Internet-Standard Management Framework 764
9.3.1 Structure of Management Information: SMI 766
9.3.2 Management Information Base: MIB 770
9.3.3 SNMP Protocol Operations and Transport Mappings 772
9.3.4 Security and Administration 775
9.4 ASN.1 778
9.5 Conclusion 783
Homework Problems and Questions 783
Interview: Jennifer Rexford 786
References 789
Index 823
xxiv Table of Contents
COMPUTER
NETWORKING
A Top-Down Approach
SIXTH EDITION
This page intentionally left blank
CHAPTER1
Computer
Networks and
the Internet
1
Today’s Internet is arguably the largest engineered system ever created by mankind,
with hundreds of millions of connected computers, communication links, and
switches; with billions of users who connect via laptops, tablets, and smartphones;
and with an array of new Internet-connected devices such as sensors, Web cams,
game consoles, picture frames, and even washing machines. Given that the Internet
is so large and has so many diverse components and uses, is there any hope of
understanding how it works? Are there guiding principles and structure that can provide
a foundation for understanding such an amazingly large and complex system?
And if so, is it possible that it actually could be both interesting and fun to learn
about computer networks? Fortunately, the answers to all of these questions is a
resounding YES! Indeed, it’s our aim in this book to provide you with a modern
introduction to the dynamic field of computer networking, giving you the principles
and practical insights you’ll need to understand not only today’s networks, but
tomorrow’s as well.
This first chapter presents a broad overview of computer networking and the
Internet. Our goal here is to paint a broad picture and set the context for the rest of
this book, to see the forest through the trees. We’ll cover a lot of ground in this introductory
chapter and discuss a lot of the pieces of a computer network, without losing
sight of the big picture.
We’ll structure our overview of computer networks in this chapter as follows.
After introducing some basic terminology and concepts, we’ll first examine the
basic hardware and software components that make up a network. We’ll begin at
the network’s edge and look at the end systems and network applications running
in the network. We’ll then explore the core of a computer network, examining the
links and the switches that transport data, as well as the access networks and physical
media that connect end systems to the network core. We’ll learn that the Internet
is a network of networks, and we’ll learn how these networks connect with
each other.
After having completed this overview of the edge and core of a computer network,
we’ll take the broader and more abstract view in the second half of this chapter.
We’ll examine delay, loss, and throughput of data in a computer network and
provide simple quantitative models for end-to-end throughput and delay: models
that take into account transmission, propagation, and queuing delays. We’ll then
introduce some of the key architectural principles in computer networking, namely,
protocol layering and service models. We’ll also learn that computer networks are
vulnerable to many different types of attacks; we’ll survey some of these attacks and
consider how computer networks can be made more secure. Finally, we’ll close this
chapter with a brief history of computer networking.
1.1 What Is the Internet?
In this book, we’ll use the public Internet, a specific computer network, as our principal
vehicle for discussing computer networks and their protocols. But what is the
Internet? There are a couple of ways to answer this question. First, we can describe
the nuts and bolts of the Internet, that is, the basic hardware and software components
that make up the Internet. Second, we can describe the Internet in terms of a networking
infrastructure that provides services to distributed applications. Let’s begin
with the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.
1.1.1 A Nuts-and-Bolts Description
The Internet is a computer network that interconnects hundreds of millions of computing
devices throughout the world. Not too long ago, these computing devices were
primarily traditional desktop PCs, Linux workstations, and so-called servers that store
and transmit information such as Web pages and e-mail messages. Increasingly,
however, nontraditional Internet end systems such as laptops, smartphones, tablets,
TVs, gaming consoles, Web cams, automobiles, environmental sensing devices,
picture frames, and home electrical and security systems are being connected to the
Internet. Indeed, the term computer network is beginning to sound a bit dated, given
the many nontraditional devices that are being hooked up to the Internet. In Internet jargon,
all of these devices are called hosts or end systems. As of July 2011, there were
2 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
1.1 • WHAT IS THE INTERNET? 3
Figure 1.1  Some pieces of the Internet
Key:
Host
(= end system)
Server Mobile Router Link-Layer
switch
Modem Base
station
Smartphone Cell phone
tower
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
nearly 850 million end systems attached to the Internet [ISC 2012], not counting
smartphones, laptops, and other devices that are only intermittently connected to the
Internet. Overall, more there are an estimated 2 billion Internet users [ITU 2011].
End systems are connected together by a network of communication links and
packet switches. We’ll see in Section 1.2 that there are many types of communication
links, which are made up of different types of physical media, including coaxial
cable, copper wire, optical fiber, and radio spectrum. Different links can transmit
data at different rates, with the transmission rate of a link measured in bits/second.
When one end system has data to send to another end system, the sending end system
segments the data and adds header bytes to each segment. The resulting packages
of information, known as packets in the jargon of computer networks, are then
sent through the network to the destination end system, where they are reassembled
into the original data.
A packet switch takes a packet arriving on one of its incoming communication
links and forwards that packet on one of its outgoing communication links. Packet
switches come in many shapes and flavors, but the two most prominent types in
today’s Internet are routers and link-layer switches. Both types of switches forward
packets toward their ultimate destinations. Link-layer switches are typically
used in access networks, while routers are typically used in the network core. The
sequence of communication links and packet switches traversed by a packet from
the sending end system to the receiving end system is known as a route or path
through the network. The exact amount of traffic being carried in the Internet is
difficult to estimate but Cisco [Cisco VNI 2011] estimates global Internet traffic will
be nearly 40 exabytes per month in 2012.
Packet-switched networks (which transport packets) are in many ways similar
to transportation networks of highways, roads, and intersections (which transport
vehicles). Consider, for example, a factory that needs to move a large
amount of cargo to some destination warehouse located thousands of kilometers
away. At the factory, the cargo is segmented and loaded into a fleet of trucks.
Each of the trucks then independently travels through the network of highways,
roads, and intersections to the destination warehouse. At the destination warehouse,
the cargo is unloaded and grouped with the rest of the cargo arriving from
the same shipment. Thus, in many ways, packets are analogous to trucks, communication
links are analogous to highways and roads, packet switches are analogous
to intersections, and end systems are analogous to buildings. Just as a truck
takes a path through the transportation network, a packet takes a path through a
computer network.
End systems access the Internet through Internet Service Providers (ISPs),
including residential ISPs such as local cable or telephone companies; corporate
ISPs; university ISPs; and ISPs that provide WiFi access in airports, hotels, coffee
shops, and other public places. Each ISP is in itself a network of packet switches
and communication links. ISPs provide a variety of types of network access to the
end systems, including residential broadband access such as cable modem or DSL,
4 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
high-speed local area network access, wireless access, and 56 kbps dial-up modem
access. ISPs also provide Internet access to content providers, connecting Web
sites directly to the Internet. The Internet is all about connecting end systems to
each other, so the ISPs that provide access to end systems must also be interconnected.
These lower-tier ISPs are interconnected through national and international
upper-tier ISPs such as Level 3 Communications, AT&T, Sprint, and NTT.
An upper-tier ISP consists of high-speed routers interconnected with high-speed
fiber-optic links. Each ISP network, whether upper-tier or lower-tier, is managed
independently, runs the IP protocol (see below), and conforms to certain naming
and address conventions. We’ll examine ISPs and their interconnection more
closely in Section 1.3.
End systems, packet switches, and other pieces of the Internet run protocols
that control the sending and receiving of information within the Internet. The
Transmission Control Protocol (TCP) and the Internet Protocol (IP) are two of
the most important protocols in the Internet. The IP protocol specifies the format of
the packets that are sent and received among routers and end systems. The Internet’s
principal protocols are collectively known as TCP/IP. We’ll begin looking into protocols
in this introductory chapter. But that’s just a start—much of this book is concerned
with computer network protocols!
Given the importance of protocols to the Internet, it’s important that everyone
agree on what each and every protocol does, so that people can create systems and
products that interoperate. This is where standards come into play. Internet standards
are developed by the Internet Engineering Task Force (IETF)[IETF 2012].
The IETF standards documents are called requests for comments (RFCs). RFCs
started out as general requests for comments (hence the name) to resolve network
and protocol design problems that faced the precursor to the Internet [Allman 2011].
RFCs tend to be quite technical and detailed. They define protocols such as TCP, IP,
HTTP (for the Web), and SMTP (for e-mail). There are currently more than 6,000
RFCs. Other bodies also specify standards for network components, most notably
for network links. The IEEE 802 LAN/MAN Standards Committee [IEEE 802
2012], for example, specifies the Ethernet and wireless WiFi standards.
1.1.2 A Services Description
Our discussion above has identified many of the pieces that make up the Internet.
But we can also describe the Internet from an entirely different angle—namely, as
an infrastructure that provides services to applications. These applications
include electronic mail, Web surfing, social networks, instant messaging, Voiceover-
IP (VoIP), video streaming, distributed games, peer-to-peer (P2P) file sharing,
television over the Internet, remote login, and much, much more. The
applications are said to be distributed applications, since they involve multiple
end systems that exchange data with each other. Importantly, Internet applications
1.1 • WHAT IS THE INTERNET? 5
run on end systems—they do not run in the packet switches in the network core.
Although packet switches facilitate the exchange of data among end systems, they
are not concerned with the application that is the source or sink of data.
Let’s explore a little more what we mean by an infrastructure that provides
services to applications. To this end, suppose you have an exciting new idea for a
distributed Internet application, one that may greatly benefit humanity or one that
may simply make you rich and famous. How might you go about transforming
this idea into an actual Internet application? Because applications run on end systems,
you are going to need to write programs that run on the end systems. You
might, for example, write your programs in Java, C, or Python. Now, because you
are developing a distributed Internet application, the programs running on the
different end systems will need to send data to each other. And here we get to a
central issue—one that leads to the alternative way of describing the Internet as a
platform for applications. How does one program running on one end system
instruct the Internet to deliver data to another program running on another end
system?
End systems attached to the Internet provide an Application Programming
Interface (API) that specifies how a program running on one end system asks
the Internet infrastructure to deliver data to a specific destination program running
on another end system. This Internet API is a set of rules that the sending
program must follow so that the Internet can deliver the data to the destination
program. We’ll discuss the Internet API in detail in Chapter 2. For now, let’s
draw upon a simple analogy, one that we will frequently use in this book. Suppose
Alice wants to send a letter to Bob using the postal service. Alice, of course,
can’t just write the letter (the data) and drop the letter out her window. Instead,
the postal service requires that Alice put the letter in an envelope; write Bob’s
full name, address, and zip code in the center of the envelope; seal the envelope;
put a stamp in the upper-right-hand corner of the envelope; and finally, drop the
envelope into an official postal service mailbox. Thus, the postal service has its
own “postal service API,” or set of rules, that Alice must follow to have the
postal service deliver her letter to Bob. In a similar manner, the Internet has an
API that the program sending data must follow to have the Internet deliver the
data to the program that will receive the data.
The postal service, of course, provides more than one service to its customers.
It provides express delivery, reception confirmation, ordinary use, and many more
services. In a similar manner, the Internet provides multiple services to its applications.
When you develop an Internet application, you too must choose one of the
Internet’s services for your application. We’ll describe the Internet’s services in
Chapter 2.
We have just given two descriptions of the Internet; one in terms of its hardware
and software components, the other in terms of an infrastructure for providing
services to distributed applications. But perhaps you are still confused as to what the
6 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Internet is. What are packet switching and TCP/IP? What are routers? What kinds of
communication links are present in the Internet? What is a distributed application?
How can a toaster or a weather sensor be attached to the Internet? If you feel a bit
overwhelmed by all of this now, don’t worry—the purpose of this book is to introduce
you to both the nuts and bolts of the Internet and the principles that govern how
and why it works. We’ll explain these important terms and questions in the following
sections and chapters.
1.1.3 What Is a Protocol?
Now that we’ve got a bit of a feel for what the Internet is, let’s consider another
important buzzword in computer networking: protocol. What is a protocol? What
does a protocol do?
A Human Analogy
It is probably easiest to understand the notion of a computer network protocol by
first considering some human analogies, since we humans execute protocols all of
the time. Consider what you do when you want to ask someone for the time of day.
A typical exchange is shown in Figure 1.2. Human protocol (or good manners, at
least) dictates that one first offer a greeting (the first “Hi” in Figure 1.2) to initiate
communication with someone else. The typical response to a “Hi” is a returned
“Hi” message. Implicitly, one then takes a cordial “Hi” response as an indication
that one can proceed and ask for the time of day. A different response to the initial
“Hi” (such as “Don’t bother me!” or “I don’t speak English,” or some unprintable
reply) might indicate an unwillingness or inability to communicate. In this case,
the human protocol would be not to ask for the time of day. Sometimes one gets no
response at all to a question, in which case one typically gives up asking that person
for the time. Note that in our human protocol, there are specific messages we
send, and specific actions we take in response to the received reply messages or
other events (such as no reply within some given amount of time). Clearly, transmitted
and received messages, and actions taken when these messages are sent or
received or other events occur, play a central role in a human protocol. If people
run different protocols (for example, if one person has manners but the other does
not, or if one understands the concept of time and the other does not) the protocols
do not interoperate and no useful work can be accomplished. The same is true in
networking—it takes two (or more) communicating entities running the same protocol
in order to accomplish a task.
Let’s consider a second human analogy. Suppose you’re in a college class (a
computer networking class, for example!). The teacher is droning on about protocols
and you’re confused. The teacher stops to ask, “Are there any questions?” (a
1.1 • WHAT IS THE INTERNET? 7
message that is transmitted to, and received by, all students who are not sleeping).
You raise your hand (transmitting an implicit message to the teacher). Your teacher
acknowledges you with a smile, saying “Yes . . .” (a transmitted message encouraging
you to ask your question—teachers love to be asked questions), and you then ask
your question (that is, transmit your message to your teacher). Your teacher hears
your question (receives your question message) and answers (transmits a reply to
you). Once again, we see that the transmission and receipt of messages, and a set of
conventional actions taken when these messages are sent and received, are at the
heart of this question-and-answer protocol.
Network Protocols
A network protocol is similar to a human protocol, except that the entities exchanging
messages and taking actions are hardware or software components of some
device (for example, computer, smartphone, tablet, router, or other network-capable
8 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
GET http://www.awl.com/kurose-ross
TCP connection request
Time Time
TCP connection reply
<file>
Hi
Got the time?
Time Time
Hi
2:00
Figure 1.2  A human protocol and a computer network protocol
device). All activity in the Internet that involves two or more communicating remote
entities is governed by a protocol. For example, hardware-implemented protocols in
two physically connected computers control the flow of bits on the “wire” between
the two network interface cards; congestion-control protocols in end systems control
the rate at which packets are transmitted between sender and receiver; protocols
in routers determine a packet’s path from source to destination. Protocols are running
everywhere in the Internet, and consequently much of this book is about computer
network protocols.
As an example of a computer network protocol with which you are probably
familiar, consider what happens when you make a request to a Web server, that is,
when you type the URL of a Web page into your Web browser. The scenario is illustrated
in the right half of Figure 1.2. First, your computer will send a connection
request message to the Web server and wait for a reply. The Web server will eventually
receive your connection request message and return a connection reply message.
Knowing that it is now OK to request the Web document, your computer then
sends the name of the Web page it wants to fetch from that Web server in a GET
message. Finally, the Web server returns the Web page (file) to your computer.
Given the human and networking examples above, the exchange of messages
and the actions taken when these messages are sent and received are the key defining
elements of a protocol:
A protocol defines the format and the order of messages exchanged between
two or more communicating entities, as well as the actions taken on the transmission
and/or receipt of a message or other event.
The Internet, and computer networks in general, make extensive use of protocols.
Different protocols are used to accomplish different communication tasks. As
you read through this book, you will learn that some protocols are simple and
straightforward, while others are complex and intellectually deep. Mastering the
field of computer networking is equivalent to understanding the what, why, and how
of networking protocols.
1.2 The Network Edge
In the previous section we presented a high-level overview of the Internet and networking
protocols. We are now going to delve a bit more deeply into the components
of a computer network (and the Internet, in particular). We begin in this
section at the edge of a network and look at the components with which we are most
familiar—namely, the computers, smartphones and other devices that we use on a
daily basis. In the next section we’ll move from the network edge to the network
core and examine switching and routing in computer networks.
1.2 • THE NETWORK EDGE 9
Recall from the previous section that in computer networking jargon, the computers
and other devices connected to the Internet are often referred to as end systems.
They are referred to as end systems because they sit at the edge of the Internet,
as shown in Figure 1.3. The Internet’s end systems include desktop computers (e.g.,
desktop PCs, Macs, and Linux boxes), servers (e.g., Web and e-mail servers), and
mobile computers (e.g., laptops, smartphones, and tablets). Furthermore, an increasing
number of non-traditional devices are being attached to the Internet as end systems
(see sidebar).
End systems are also referred to as hosts because they host (that is, run) application
programs such as a Web browser program, a Web server program, an e-mail
client program, or an e-mail server program. Throughout this book we will use the
terms hosts and end systems interchangeably; that is, host = end system. Hosts are
sometimes further divided into two categories: clients and servers. Informally,
clients tend to be desktop and mobile PCs, smartphones, and so on, whereas servers
tend to be more powerful machines that store and distribute Web pages, stream
video, relay e-mail, and so on. Today, most of the servers from which we receive
10 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
A DIZZYING ARRAY OF INTERNET END SYSTEMS
Not too long ago, the end-system devices connected to the Internet were primarily
traditional computers such as desktop machines and powerful servers. Beginning in
the late 1990s and continuing today, a wide range of interesting devices are being
connected to the Internet, leveraging their ability to send and receive digital data.
Given the Internet’s ubiquity, its well-defined (standardized) protocols, and the
availability of Internet-ready commodity hardware, it’s natural to use Internet technology
to network these devices together and to Internet-connected servers.
Many of these devices are based in the home—video game consoles (e.g.,
Microsoft’s Xbox), Internet-ready televisions, digital picture frames that download
and display digital pictures, washing machines, refrigerators, and even a toaster
that downloads meteorological information and burns an image of the day’s forecast
(e.g., mixed clouds and sun) on your morning toast [BBC 2001]. IP-enabled
phones with GPS capabilities put location-dependent services (maps, information
about nearby services or people) at your fingertips. Networked sensors embedded
into the physical environment allow monitoring of buildings, bridges, seismic activity,
wildlife habitats, river estuaries, and the weather. Biomedical devices can be
embedded and networked in a body-area network. With so many diverse devices
being networked together, the Internet is indeed becoming an “Internet of things”
[ITU 2005b].
CASE HISTORY
search results, e-mail, Web pages, and videos reside in large data centers. For
example, Google has 30–50 data centers, with many having more than one hundred
thousand servers.
Mobile Network
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Home Network
Figure 1.3  End-system interaction
1.2 • THE NETWORK EDGE 11
12 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
National or
Global ISP
Mobile Network
Local or
Regional ISP
Enterprise Network
Home Network
Figure 1.4  Access networks
1.2.1 Access Networks
Having considered the applications and end systems at the “edge of the network,”
let’s next consider the access network—the network that physically connects an end
system to the first router (also known as the “edge router”) on a path from the end
system to any other distant end system. Figure 1.4 shows several types of access
networks with thick, shaded lines, and the settings (home, enterprise, and wide-area
mobile wireless) in which they are used.
Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite
In developed countries today, more than 65 percent of the households have Internet
access, with Korea, Netherlands, Finland, and Sweden leading the way with more than
80 percent of households having Internet access, almost all via a high-speed broadband
connection [ITU 2011]. Finland and Spain have recently declared high-speed Internet
access to be a “legal right.” Given this intense interest in home access, let’s begin our
overview of access networks by considering how homes connect to the Internet.
Today, the two most prevalent types of broadband residential access are digital
subscriber line (DSL) and cable. A residence typically obtains DSL Internet access
from the same local telephone company (telco) that provides its wired local phone
access. Thus, when DSL is used, a customer’s telco is also its ISP. As shown in
Figure 1.5, each customer’s DSL modem uses the existing telephone line (twistedpair
copper wire, which we’ll discuss in Section 1.2.2) to exchange data with a digital
subscriber line access multiplexer (DSLAM) located in the telco’s local central
office (CO). The home’s DSL modem takes digital data and translates it to highfrequency
tones for transmission over telephone wires to the CO; the analog signals
from many such houses are translated back into digital format at the DSLAM.
The residential telephone line carries both data and traditional telephone signals
simultaneously, which are encoded at different frequencies:
• A high-speed downstream channel, in the 50 kHz to 1 MHz band
• A medium-speed upstream channel, in the 4 kHz to 50 kHz band
• An ordinary two-way telephone channel, in the 0 to 4 kHz band
This approach makes the single DSL link appear as if there were three separate
links, so that a telephone call and an Internet connection can share the DSL link at
the same time. (We’ll describe this technique of frequency-division multiplexing in
Home PC
Home
phone
DSL
modem
Internet
Telephone
network
Splitter
Existing phone line:
0-4KHz phone; 4-50KHz
upstream data; 50KHz–
1MHz downstream data
Central
office
DSLAM
Figure 1.5  DSL Internet access
1.2 • THE NETWORK EDGE 13
14 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Section 1.3.1). On the customer side, a splitter separates the data and telephone signals
arriving to the home and forwards the data signal to the DSL modem. On the
telco side, in the CO, the DSLAM separates the data and phone signals and sends
the data into the Internet. Hundreds or even thousands of households connect to a
single DSLAM [Dischinger 2007].
The DSL standards define transmission rates of 12 Mbps downstream and
1.8 Mbps upstream [ITU 1999], and 24 Mbps downstream and 2.5 Mbps upstream
[ITU 2003]. Because the downstream and upstream rates are different, the access is
said to be asymmetric. The actual downstream and upstream transmission rates
achieved may be less than the rates noted above, as the DSL provider may purposefully
limit a residential rate when tiered service (different rates, available at different
prices) are offered, or because the maximum rate can be limited by the distance
between the home and the CO, the gauge of the twisted-pair line and the degree of
electrical interference. Engineers have expressly designed DSL for short distances
between the home and the CO; generally, if the residence is not located within 5 to 10
miles of the CO, the residence must resort to an alternative form of Internet access.
While DSL makes use of the telco’s existing local telephone infrastructure,
cable Internet access makes use of the cable television company’s existing cable
television infrastructure. A residence obtains cable Internet access from the same
company that provides its cable television. As illustrated in Figure 1.6, fiber optics
connect the cable head end to neighborhood-level junctions, from which traditional
coaxial cable is then used to reach individual houses and apartments. Each
neighborhood junction typically supports 500 to 5,000 homes. Because both fiber
and coaxial cable are employed in this system, it is often referred to as hybrid
fiber coax (HFC).
Fiber
cable
Coaxial cable
Hundreds
of homes
Cable head end
Hundreds
of homes
Fiber
node
Fiber
node
Internet
CMTS
Figure 1.6  A hybrid fiber-coaxial access network
Cable internet access requires special modems, called cable modems. As with a
DSL modem, the cable modem is typically an external device and connects to the
home PC through an Ethernet port. (We will discuss Ethernet in great detail in
Chapter 5.) At the cable head end, the cable modem termination system (CMTS)
serves a similar function as the DSL network’s DSLAM—turning the analog signal
sent from the cable modems in many downstream homes back into digital format.
Cable modems divide the HFC network into two channels, a downstream and an
upstream channel. As with DSL, access is typically asymmetric, with the downstream
channel typically allocated a higher transmission rate than the upstream
channel. The DOCSIS 2.0 standard defines downstream rates up to 42.8 Mbps and
upstream rates of up to 30.7 Mbps. As in the case of DSL networks, the maximum
achievable rate may not be realized due to lower contracted data rates or media
impairments.
One important characteristic of cable Internet access is that it is a shared
broadcast medium. In particular, every packet sent by the head end travels downstream
on every link to every home and every packet sent by a home travels on the
upstream channel to the head end. For this reason, if several users are simultaneously
downloading a video file on the downstream channel, the actual rate at which
each user receives its video file will be significantly lower than the aggregate cable
downstream rate. On the other hand, if there are only a few active users and they
are all Web surfing, then each of the users may actually receive Web pages at the
full cable downstream rate, because the users will rarely request a Web page at
exactly the same time. Because the upstream channel is also shared, a distributed
multiple access protocol is needed to coordinate transmissions and avoid collisions.
(We’ll discuss this collision issue in some detail in Chapter 5.)
Although DSL and cable networks currently represent more than 90 percent of
residential broadband access in the United States, an up-and-coming technology that
promises even higher speeds is the deployment of fiber to the home (FTTH)
[FTTH Council 2011a]. As the name suggests, the FTTH concept is simple—
provide an optical fiber path from the CO directly to the home. In the United States,
Verizon has been particularly aggressive with FTTH with its FIOS service [Verizon
FIOS 2012].
There are several competing technologies for optical distribution from the
CO to the homes. The simplest optical distribution network is called direct fiber,
with one fiber leaving the CO for each home. More commonly, each fiber leaving
the central office is actually shared by many homes; it is not until the fiber
gets relatively close to the homes that it is split into individual customer-specific
fibers. There are two competing optical-distribution network architectures that
perform this splitting: active optical networks (AONs) and passive optical networks
(PONs). AON is essentially switched Ethernet, which is discussed in
Chapter 5.
Here, we briefly discuss PON, which is used in Verizon’s FIOS service.
Figure 1.7 shows FTTH using the PON distribution architecture. Each home has
1.2 • THE NETWORK EDGE 15
an optical network terminator (ONT), which is connected by dedicated optical
fiber to a neighborhood splitter. The splitter combines a number of homes (typically
less than 100) onto a single, shared optical fiber, which connects to an optical line
terminator (OLT) in the telco’s CO. The OLT, providing conversion between optical
and electrical signals, connects to the Internet via a telco router. In the home, users
connect a home router (typically a wireless router) to the ONT and access the Internet
via this home router. In the PON architecture, all packets sent from OLT to the
splitter are replicated at the splitter (similar to a cable head end).
FTTH can potentially provide Internet access rates in the gigabits per second
range. However, most FTTH ISPs provide different rate offerings, with the higher
rates naturally costing more money. The average downstream speed of US FTTH
customers was approximately 20 Mbps in 2011 (compared with 13 Mbps for cable
access networks and less than 5 Mbps for DSL) [FTTH Council 2011b].
Two other access network technologies are also used to provide Internet access
to the home. In locations where DSL, cable, and FTTH are not available (e.g., in
some rural settings), a satellite link can be used to connect a residence to the Internet
at speeds of more than 1 Mbps; StarBand and HughesNet are two such satellite
access providers. Dial-up access over traditional phone lines is based on the same
model as DSL—a home modem connects over a phone line to a modem in the ISP.
Compared with DSL and other broadband access networks, dial-up access is excruciatingly
slow at 56 kbps.
Access in the Enterprise (and the Home): Ethernet and WiFi
On corporate and university campuses, and increasingly in home settings, a local area
network (LAN) is used to connect an end system to the edge router. Although there
are many types of LAN technologies, Ethernet is by far the most prevalent access
technology in corporate, university, and home networks. As shown in Figure 1.8,
Ethernet users use twisted-pair copper wire to connect to an Ethernet switch, a
16 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Internet
Central office
Optical
splitter
ONT
ONT
ONT
OLT
Optical
fibers
Figure 1.7  FTTH Internet access
technology discussed in detail in Chapter 5. The Ethernet switch, or a network of
such interconnected switches, is then in turn connected into the larger Internet. With
Ethernet access, users typically have 100 Mbps access to the Ethernet switch,
whereas servers may have 1 Gbps or even 10 Gbps access.
Increasingly, however, people are accessing the Internet wirelessly from laptops,
smartphones, tablets, and other devices (see earlier sidebar on “A Dizzying
Array of Devices”). In a wireless LAN setting, wireless users transmit/receive packets
to/from an access point that is connected into the enterprise’s network (most
likely including wired Ethernet), which in turn is connected to the wired Internet. A
wireless LAN user must typically be within a few tens of meters of the access point.
Wireless LAN access based on IEEE 802.11 technology, more colloquially known
as WiFi, is now just about everywhere—universities, business offices, cafes, airports,
homes, and even in airplanes. In many cities, one can stand on a street corner
and be within range of ten or twenty base stations (for a browseable global map of
802.11 base stations that have been discovered and logged on a Web site by people
who take great enjoyment in doing such things, see [wigle.net 2012]). As discussed
in detail in Chapter 6, 802.11 today provides a shared transmission rate of up to
54 Mbps.
Even though Ethernet and WiFi access networks were initially deployed in enterprise
(corporate, university) settings, they have recently become relatively common
components of home networks. Many homes combine broadband residential access
(that is, cable modems or DSL) with these inexpensive wireless LAN technologies to
create powerful home networks [Edwards 2011]. Figure 1.9 shows a typical home
network. This home network consists of a roaming laptop as well as a wired PC; a
base station (the wireless access point), which communicates with the wireless PC; a
cable modem, providing broadband access to the Internet; and a router, which interconnects
the base station and the stationary PC with the cable modem. This network
allows household members to have broadband access to the Internet with one member
roaming from the kitchen to the backyard to the bedrooms.
Ethernet
switch
Institutional
router
100 Mbps
100 Mbps
100 Mbps
Server
To Institution’s
ISP
Figure 1.8  Ethernet Internet access
1.2 • THE NETWORK EDGE 17
Wide-Area Wireless Access: 3G and LTE
Increasingly, devices such as iPhones, BlackBerrys, and Android devices are being
used to send email, surf the Web, Tweet, and download music while on the run.
These devices employ the same wireless infrastructure used for cellular telephony
to send/receive packets through a base station that is operated by the cellular network
provider. Unlike WiFi, a user need only be within a few tens of kilometers (as
opposed to a few tens of meters) of the base station.
Telecommunications companies have made enormous investments in so-called
third-generation (3G) wireless, which provides packet-switched wide-area wireless
Internet access at speeds in excess of 1 Mbps. But even higher-speed wide-area
access technologies—a fourth-generation (4G) of wide-area wireless networks—are
already being deployed. LTE ( for “Long-Term Evolution”—a candidate for Bad
Acronym of the Year Award) has its roots in 3G technology, and can potentially
achieve rates in excess of 10 Mbps. LTE downstream rates of many tens of Mbps
have been reported in commercial deployments. We’ll cover the basic principles of
wireless networks and mobility, as well as WiFi, 3G, and LTE technologies (and
more!) in Chapter 6.
1.2.2 Physical Media
In the previous subsection, we gave an overview of some of the most important
network access technologies in the Internet. As we described these technologies,
we also indicated the physical media used. For example, we said that HFC uses a
combination of fiber cable and coaxial cable. We said that DSL and Ethernet use
copper wire. And we said that mobile access networks use the radio spectrum.
18 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Cable
head end
House
Internet
Figure 1.9  A typical home network
In this subsection we provide a brief overview of these and other transmission
media that are commonly used in the Internet.
In order to define what is meant by a physical medium, let us reflect on the
brief life of a bit. Consider a bit traveling from one end system, through a series of
links and routers, to another end system. This poor bit gets kicked around and
transmitted many, many times! The source end system first transmits the bit, and
shortly thereafter the first router in the series receives the bit; the first router then
transmits the bit, and shortly thereafter the second router receives the bit; and so
on. Thus our bit, when traveling from source to destination, passes through a series
of transmitter-receiver pairs. For each transmitter-receiver pair, the bit is sent by
propagating electromagnetic waves or optical pulses across a physical medium.
The physical medium can take many shapes and forms and does not have to be of
the same type for each transmitter-receiver pair along the path. Examples of physical
media include twisted-pair copper wire, coaxial cable, multimode fiber-optic
cable, terrestrial radio spectrum, and satellite radio spectrum. Physical media fall
into two categories: guided media and unguided media. With guided media, the
waves are guided along a solid medium, such as a fiber-optic cable, a twisted-pair
copper wire, or a coaxial cable. With unguided media, the waves propagate in the
atmosphere and in outer space, such as in a wireless LAN or a digital satellite
channel.
But before we get into the characteristics of the various media types, let us say
a few words about their costs. The actual cost of the physical link (copper wire,
fiber-optic cable, and so on) is often relatively minor compared with other networking
costs. In particular, the labor cost associated with the installation of the physical
link can be orders of magnitude higher than the cost of the material. For this reason,
many builders install twisted pair, optical fiber, and coaxial cable in every room in a
building. Even if only one medium is initially used, there is a good chance that
another medium could be used in the near future, and so money is saved by not having
to lay additional wires in the future.
Twisted-Pair Copper Wire
The least expensive and most commonly used guided transmission medium is
twisted-pair copper wire. For over a hundred years it has been used by telephone
networks. In fact, more than 99 percent of the wired connections from the telephone
handset to the local telephone switch use twisted-pair copper wire. Most of
us have seen twisted pair in our homes and work environments. Twisted pair consists
of two insulated copper wires, each about 1 mm thick, arranged in a regular
spiral pattern. The wires are twisted together to reduce the electrical interference
from similar pairs close by. Typically, a number of pairs are bundled together in a
cable by wrapping the pairs in a protective shield. A wire pair constitutes a single
communication link. Unshielded twisted pair (UTP) is commonly used for
1.2 • THE NETWORK EDGE 19
computer networks within a building, that is, for LANs. Data rates for LANs
using twisted pair today range from 10 Mbps to 10 Gbps. The data rates that can
be achieved depend on the thickness of the wire and the distance between transmitter
and receiver.
When fiber-optic technology emerged in the 1980s, many people disparaged
twisted pair because of its relatively low bit rates. Some people even felt that fiberoptic
technology would completely replace twisted pair. But twisted pair did not
give up so easily. Modern twisted-pair technology, such as category 6a cable, can
achieve data rates of 10 Gbps for distances up to a hundred meters. In the end,
twisted pair has emerged as the dominant solution for high-speed LAN networking.
As discussed earlier, twisted pair is also commonly used for residential Internet
access. We saw that dial-up modem technology enables access at rates of up to 56
kbps over twisted pair. We also saw that DSL (digital subscriber line) technology
has enabled residential users to access the Internet at tens of Mbps over twisted pair
(when users live close to the ISP’s modem).
Coaxial Cable
Like twisted pair, coaxial cable consists of two copper conductors, but the two conductors
are concentric rather than parallel. With this construction and special insulation
and shielding, coaxial cable can achieve high data transmission rates. Coaxial
cable is quite common in cable television systems. As we saw earlier, cable television
systems have recently been coupled with cable modems to provide residential
users with Internet access at rates of tens of Mbps. In cable television and cable
Internet access, the transmitter shifts the digital signal to a specific frequency band,
and the resulting analog signal is sent from the transmitter to one or more receivers.
Coaxial cable can be used as a guided shared medium. Specifically, a number of
end systems can be connected directly to the cable, with each of the end systems
receiving whatever is sent by the other end systems.
Fiber Optics
An optical fiber is a thin, flexible medium that conducts pulses of light, with each
pulse representing a bit. A single optical fiber can support tremendous bit rates, up
to tens or even hundreds of gigabits per second. They are immune to electromagnetic
interference, have very low signal attenuation up to 100 kilometers, and are
very hard to tap. These characteristics have made fiber optics the preferred longhaul
guided transmission media, particularly for overseas links. Many of the longdistance
telephone networks in the United States and elsewhere now use fiber optics
exclusively. Fiber optics is also prevalent in the backbone of the Internet. However,
the high cost of optical devices—such as transmitters, receivers, and switches—has
hindered their deployment for short-haul transport, such as in a LAN or into the
20 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
home in a residential access network. The Optical Carrier (OC) standard link speeds
range from 51.8 Mbps to 39.8 Gbps; these specifications are often referred to as OCn,
where the link speed equals n × 51.8 Mbps. Standards in use today include OC-1,
OC-3, OC-12, OC-24, OC-48, OC-96, OC-192, OC-768. [Mukherjee 2006,
Ramaswamy 2010] provide coverage of various aspects of optical networking.
Terrestrial Radio Channels
Radio channels carry signals in the electromagnetic spectrum. They are an attractive
medium because they require no physical wire to be installed, can penetrate walls,
provide connectivity to a mobile user, and can potentially carry a signal for long distances.
The characteristics of a radio channel depend significantly on the propagation
environment and the distance over which a signal is to be carried. Environmental considerations
determine path loss and shadow fading (which decrease the signal strength
as the signal travels over a distance and around/through obstructing objects), multipath
fading (due to signal reflection off of interfering objects), and interference (due
to other transmissions and electromagnetic signals).
Terrestrial radio channels can be broadly classified into three groups: those that
operate over very short distance (e.g., with one or two meters); those that operate in
local areas, typically spanning from ten to a few hundred meters; and those that
operate in the wide area, spanning tens of kilometers. Personal devices such as wireless
headsets, keyboards, and medical devices operate over short distances; the
wireless LAN technologies described in Section 1.2.1 use local-area radio channels;
the cellular access technologies use wide-area radio channels. We’ll discuss radio
channels in detail in Chapter 6.
Satellite Radio Channels
A communication satellite links two or more Earth-based microwave transmitter/
receivers, known as ground stations. The satellite receives transmissions on one frequency
band, regenerates the signal using a repeater (discussed below), and transmits
the signal on another frequency. Two types of satellites are used in communications:
geostationary satellites and low-earth orbiting (LEO) satellites.
Geostationary satellites permanently remain above the same spot on Earth. This
stationary presence is achieved by placing the satellite in orbit at 36,000 kilometers
above Earth’s surface. This huge distance from ground station through satellite back
to ground station introduces a substantial signal propagation delay of 280 milliseconds.
Nevertheless, satellite links, which can operate at speeds of hundreds of Mbps,
are often used in areas without access to DSL or cable-based Internet access.
LEO satellites are placed much closer to Earth and do not remain permanently
above one spot on Earth. They rotate around Earth (just as the Moon does) and may
communicate with each other, as well as with ground stations. To provide continuous
1.2 • THE NETWORK EDGE 21
22 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
coverage to an area, many satellites need to be placed in orbit. There are currently
many low-altitude communication systems in development. Lloyd’s satellite constellations
Web page [Wood 2012] provides and collects information on satellite
constellation systems for communications. LEO satellite technology may be used
for Internet access sometime in the future.
1.3 The Network Core
Having examined the Internet’s edge, let us now delve more deeply inside the network
core—the mesh of packet switches and links that interconnects the Internet’s
end systems. Figure 1.10 highlights the network core with thick, shaded lines.
1.3.1 Packet Switching
In a network application, end systems exchange messages with each other. Messages
can contain anything the application designer wants. Messages may perform a
control function (for example, the “Hi” messages in our handshaking example in
Figure 1.2) or can contain data, such as an email message, a JPEG image, or an MP3
audio file. To send a message from a source end system to a destination end system,
the source breaks long messages into smaller chunks of data known as packets.
Between source and destination, each packet travels through communication links
and packet switches (for which there are two predominant types, routers and linklayer
switches). Packets are transmitted over each communication link at a rate
equal to the full transmission rate of the link. So, if a source end system or a packet
switch is sending a packet of L bits over a link with transmission rate R bits/sec, then
the time to transmit the packet is L/R seconds.
Store-and-Forward Transmission
Most packet switches use store-and-forward transmission at the inputs to the
links. Store-and-forward transmission means that the packet switch must receive
the entire packet before it can begin to transmit the first bit of the packet onto the
outbound link. To explore store-and-forward transmission in more detail, consider
a simple network consisting of two end systems connected by a single router, as
shown in Figure 1.11. A router will typically have many incident links, since its job
is to switch an incoming packet onto an outgoing link; in this simple example, the
router has the rather simple task of transferring a packet from one (input) link to
the only other attached link. In this example, the source has three packets, each
consisting of L bits, to send to the destination. At the snapshot of time shown in
Figure 1.11, the source has transmitted some of packet 1, and the front of packet 1
has already arrived at the router. Because the router employs store-and-forwarding,
at this instant of time, the router cannot transmit the bits it has received; instead it
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Home Network
Mobile Network
Figure 1.10  The network core
1.3 • THE NETWORK CORE 23
24 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
must first buffer (i.e., “store”) the packet’s bits. Only after the router has received
all of the packet’s bits can it begin to transmit (i.e., “forward”) the packet onto the
outbound link. To gain some insight into store-and-forward transmission, let’s now
calculate the amount of time that elapses from when the source begins to send the
packet until the destination has received the entire packet. (Here we will ignore
propagation delay—the time it takes for the bits to travel across the wire at near the
speed of light—which will be discussed in Section 1.4.) The source begins to transmit
at time 0; at time L/R seconds, the source has transmitted the entire packet, and
the entire packet has been received and stored at the router (since there is no propagation
delay). At time L/R seconds, since the router has just received the entire
packet, it can begin to transmit the packet onto the outbound link towards the destination;
at time 2L/R, the router has transmitted the entire packet, and the entire
packet has been received by the destination. Thus, the total delay is 2L/R. If
the switch instead forwarded bits as soon as they arrive (without first receiving the
entire packet), then the total delay would be L/R since bits are not held up at
the router. But, as we will discuss in Section 1.4, routers need to receive, store, and
process the entire packet before forwarding.
Now let’s calculate the amount of time that elapses from when the source
begins to send the first packet until the destination has received all three packets.
As before, at time L/R, the router begins to forward the first packet. But also at time
L/R the source will begin to send the second packet, since it has just finished sending
the entire first packet. Thus, at time 2L/R, the destination has received the first
packet and the router has received the second packet. Similarly, at time 3L/R, the
destination has received the first two packets and the router has received the third
packet. Finally, at time 4L/R the destination has received all three packets!
Let’s now consider the general case of sending one packet from source to destination
over a path consisting of N links each of rate R (thus, there are N-1 routers
between source and destination). Applying the same logic as above, we see that the
end-to-end delay is:
(1.1)
You may now want to try to determine what the delay would be for P packets sent
over a series of N links.
dend@to@end = N
L
R
Source
R bps
2 1
Front of packet 1 Destination
stored in router,
awaiting remaining
bits before forwarding
3
Figure 1.11  Store-and-forward packet switching
Queuing Delays and Packet Loss
Each packet switch has multiple links attached to it. For each attached link, the
packet switch has an output buffer (also called an output queue), which stores
packets that the router is about to send into that link. The output buffers play a key
role in packet switching. If an arriving packet needs to be transmitted onto a link but
finds the link busy with the transmission of another packet, the arriving packet must
wait in the output buffer. Thus, in addition to the store-and-forward delays, packets
suffer output buffer queuing delays. These delays are variable and depend on the
level of congestion in the network. Since the amount of buffer space is finite, an
arriving packet may find that the buffer is completely full with other packets waiting
for transmission. In this case, packet loss will occur—either the arriving packet
or one of the already-queued packets will be dropped.
Figure 1.12 illustrates a simple packet-switched network. As in Figure 1.11,
packets are represented by three-dimensional slabs. The width of a slab represents
the number of bits in the packet. In this figure, all packets have the same width and
hence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts
A and B first send their packets along 10 Mbps Ethernet links to the first router. The
router then directs these packets to the 1.5 Mbps link. If, during a short interval of
time, the arrival rate of packets to the router (when converted to bits per second)
exceeds 1.5 Mbps, congestion will occur at the router as packets queue in the link’s
output buffer before being transmitted onto the link. For example, if Host A and B
each send a burst of five packets back-to-back at the same time, then most of these
packets will spend some time waiting in the queue. The situation is, in fact, entirely
analogous to many common-day situations—for example, when we wait in line for
a bank teller or wait in front of a tollbooth. We’ll examine this queuing delay in
more detail in Section 1.4.
1.3 • THE NETWORK CORE 25
10 Mbps Ethernet
Key:
Packets
A
B
C
D E
1.5 Mbps
Queue of
packets waiting
for output link
Figure 1.12  Packet switching
26 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Forwarding Tables and Routing Protocols
Earlier, we said that a router takes a packet arriving on one of its attached
communication links and forwards that packet onto another one of its attached communication
links. But how does the router determine which link it should forward
the packet onto? Packet forwarding is actually done in different ways in different
types of computer networks. Here, we briefly describe how it is done in the
Internet.
In the Internet, every end system has an address called an IP address. When a
source end system wants to send a packet to a destination end system, the source
includes the destination’s IP address in the packet’s header. As with postal addresses,
this address has a hierarchical structure. When a packet arrives at a router in the
network, the router examines a portion of the packet’s destination address and forwards
the packet to an adjacent router. More specifically, each router has a
forwarding table that maps destination addresses (or portions of the destination
addresses) to that router’s outbound links. When a packet arrives at a router, the
router examines the address and searches its forwarding table, using this destination
address, to find the appropriate outbound link. The router then directs the packet to
this outbound link.
The end-to-end routing process is analogous to a car driver who does not use
maps but instead prefers to ask for directions. For example, suppose Joe is driving
from Philadelphia to 156 Lakeside Drive in Orlando, Florida. Joe first drives to his
neighborhood gas station and asks how to get to 156 Lakeside Drive in Orlando,
Florida. The gas station attendant extracts the Florida portion of the address and
tells Joe that he needs to get onto the interstate highway I-95 South, which has an
entrance just next to the gas station. He also tells Joe that once he enters Florida,
he should ask someone else there. Joe then takes I-95 South until he gets to Jacksonville,
Florida, at which point he asks another gas station attendant for directions.
The attendant extracts the Orlando portion of the address and tells Joe that he
should continue on I-95 to Daytona Beach and then ask someone else. In Daytona
Beach, another gas station attendant also extracts the Orlando portion of the
address and tells Joe that he should take I-4 directly to Orlando. Joe takes I-4 and
gets off at the Orlando exit. Joe goes to another gas station attendant, and this time
the attendant extracts the Lakeside Drive portion of the address and tells Joe the
road he must follow to get to Lakeside Drive. Once Joe reaches Lakeside Drive, he
asks a kid on a bicycle how to get to his destination. The kid extracts the 156 portion
of the address and points to the house. Joe finally reaches his ultimate destination.
In the above analogy, the gas station attendants and kids on bicycles are
analogous to routers.
We just learned that a router uses a packet’s destination address to index a forwarding
table and determine the appropriate outbound link. But this statement begs
yet another question: How do forwarding tables get set? Are they configured by
hand in each and every router, or does the Internet use a more automated procedure?
This issue will be studied in depth in Chapter 4. But to whet your appetite here,
we’ll note now that the Internet has a number of special routing protocols that are
used to automatically set the forwarding tables. A routing protocol may, for example,
determine the shortest path from each router to each destination and use the
shortest path results to configure the forwarding tables in the routers.
How would you actually like to see the end-to-end route that packets take in the
Internet? We now invite you to get your hands dirty by interacting with the Traceroute
program. Simply visit the site www.traceroute.org, choose a source in a particular
country, and trace the route from that source to your computer. (For a discussion of
Traceroute, see Section 1.4.)
1.3.2 Circuit Switching
There are two fundamental approaches to moving data through a network of links
and switches: circuit switching and packet switching. Having covered packetswitched
networks in the previous subsection, we now turn our attention to circuitswitched
networks.
In circuit-switched networks, the resources needed along a path (buffers, link
transmission rate) to provide for communication between the end systems are
reserved for the duration of the communication session between the end systems. In
packet-switched networks, these resources are not reserved; a session’s messages
use the resources on demand, and as a consequence, may have to wait (that is,
queue) for access to a communication link. As a simple analogy, consider two
restaurants, one that requires reservations and another that neither requires reservations
nor accepts them. For the restaurant that requires reservations, we have to go
through the hassle of calling before we leave home. But when we arrive at the
restaurant we can, in principle, immediately be seated and order our meal. For the
restaurant that does not require reservations, we don’t need to bother to reserve a
table. But when we arrive at the restaurant, we may have to wait for a table before
we can be seated.
Traditional telephone networks are examples of circuit-switched networks.
Consider what happens when one person wants to send information (voice or facsimile)
to another over a telephone network. Before the sender can send the information,
the network must establish a connection between the sender and the
receiver. This is a bona fide connection for which the switches on the path
between the sender and receiver maintain connection state for that connection. In
the jargon of telephony, this connection is called a circuit. When the network
establishes the circuit, it also reserves a constant transmission rate in the network’s
links (representing a fraction of each link’s transmission capacity) for the
duration of the connection. Since a given transmission rate has been reserved for
this sender-to-receiver connection, the sender can transfer the data to the receiver
at the guaranteed constant rate.
1.3 • THE NETWORK CORE 27
28 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Figure 1.13  A simple circuit-switched network consisting of
four switches and four links
Figure 1.13 illustrates a circuit-switched network. In this network, the four circuit
switches are interconnected by four links. Each of these links has four circuits,
so that each link can support four simultaneous connections. The hosts (for example,
PCs and workstations) are each directly connected to one of the switches. When
two hosts want to communicate, the network establishes a dedicated end-to-end
connection between the two hosts. Thus, in order for Host A to communicate with
Host B, the network must first reserve one circuit on each of two links. In this example,
the dedicated end-to-end connection uses the second circuit in the first link and
the fourth circuit in the second link. Because each link has four circuits, for each
link used by the end-to-end connection, the connection gets one fourth of the link’s
total transmission capacity for the duration of the connection. Thus, for example, if
each link between adjacent switches has a transmission rate of 1 Mbps, then each
end-to-end circuit-switch connection gets 250 kbps of dedicated transmission rate.
In contrast, consider what happens when one host wants to send a packet to
another host over a packet-switched network, such as the Internet. As with circuit
switching, the packet is transmitted over a series of communication links. But different
from circuit switching, the packet is sent into the network without reserving any link
resources whatsoever. If one of the links is congested because other packets need to be
transmitted over the link at the same time, then the packet will have to wait in a buffer
at the sending side of the transmission link and suffer a delay. The Internet makes its
best effort to deliver packets in a timely manner, but it does not make any guarantees.
Multiplexing in Circuit-Switched Networks
A circuit in a link is implemented with either frequency-division multiplexing
(FDM) or time-division multiplexing (TDM). With FDM, the frequency spectrum
of a link is divided up among the connections established across the link.
Specifically, the link dedicates a frequency band to each connection for the
duration of the connection. In telephone networks, this frequency band typically
has a width of 4 kHz (that is, 4,000 hertz or 4,000 cycles per second). The width
of the band is called, not surprisingly, the bandwidth. FM radio stations also use
FDM to share the frequency spectrum between 88 MHz and 108 MHz, with each
station being allocated a specific frequency band.
For a TDM link, time is divided into frames of fixed duration, and each frame
is divided into a fixed number of time slots. When the network establishes a connection
across a link, the network dedicates one time slot in every frame to this connection.
These slots are dedicated for the sole use of that connection, with one time slot
available for use (in every frame) to transmit the connection’s data.
Figure 1.14 illustrates FDM and TDM for a specific network link supporting up
to four circuits. For FDM, the frequency domain is segmented into four bands, each
of bandwidth 4 kHz. For TDM, the time domain is segmented into frames, with four
time slots in each frame; each circuit is assigned the same dedicated slot in the
revolving TDM frames. For TDM, the transmission rate of a circuit is equal to the
frame rate multiplied by the number of bits in a slot. For example, if the link transmits
8,000 frames per second and each slot consists of 8 bits, then the transmission
rate of a circuit is 64 kbps.
Proponents of packet switching have always argued that circuit switching is
wasteful because the dedicated circuits are idle during silent periods. For example,
1.3 • THE NETWORK CORE 29
4KHz
TDM
FDM
Link Frequency
4KHz
Slot
Key:
All slots labeled “2” are dedicated
to a specific sender-receiver pair.
Frame
1
2
2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
Time
Figure 1.14  With FDM, each circuit continuously gets a fraction of the
bandwidth. With TDM, each circuit gets all of the bandwidth
periodically during brief intervals of time (that is, during slots)
30 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
when one person in a telephone call stops talking, the idle network resources (frequency
bands or time slots in the links along the connection’s route) cannot be used
by other ongoing connections. As another example of how these resources can be
underutilized, consider a radiologist who uses a circuit-switched network to
remotely access a series of x-rays. The radiologist sets up a connection, requests an
image, contemplates the image, and then requests a new image. Network resources
are allocated to the connection but are not used (i.e., are wasted) during the radiologist’s
contemplation periods. Proponents of packet switching also enjoy pointing out
that establishing end-to-end circuits and reserving end-to-end transmission capacity
is complicated and requires complex signaling software to coordinate the operation
of the switches along the end-to-end path.
Before we finish our discussion of circuit switching, let’s work through a numerical
example that should shed further insight on the topic. Let us consider how long it
takes to send a file of 640,000 bits from Host A to Host B over a circuit-switched network.
Suppose that all links in the network use TDM with 24 slots and have a bit rate
of 1.536 Mbps. Also suppose that it takes 500 msec to establish an end-to-end circuit
before Host A can begin to transmit the file. How long does it take to send the file?
Each circuit has a transmission rate of (1.536 Mbps)/24 = 64 kbps, so it takes (640,000
bits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds we add the circuit
establishment time, giving 10.5 seconds to send the file. Note that the transmission
time is independent of the number of links: The transmission time would be 10 seconds
if the end-to-end circuit passed through one link or a hundred links. (The actual
end-to-end delay also includes a propagation delay; see Section 1.4.)
Packet Switching Versus Circuit Switching
Having described circuit switching and packet switching, let us compare the two.
Critics of packet switching have often argued that packet switching is not suitable
for real-time services (for example, telephone calls and video conference calls)
because of its variable and unpredictable end-to-end delays (due primarily to variable
and unpredictable queuing delays). Proponents of packet switching argue that
(1) it offers better sharing of transmission capacity than circuit switching and (2) it
is simpler, more efficient, and less costly to implement than circuit switching. An
interesting discussion of packet switching versus circuit switching is [Molinero-
Fernandez 2002]. Generally speaking, people who do not like to hassle with restaurant
reservations prefer packet switching to circuit switching.
Why is packet switching more efficient? Let’s look at a simple example. Suppose
users share a 1 Mbps link. Also suppose that each user alternates between periods
of activity, when a user generates data at a constant rate of 100 kbps, and periods
of inactivity, when a user generates no data. Suppose further that a user is active
only 10 percent of the time (and is idly drinking coffee during the remaining 90 percent
of the time). With circuit switching, 100 kbps must be reserved for each user at
all times. For example, with circuit-switched TDM, if a one-second frame is divided
into 10 time slots of 100 ms each, then each user would be allocated one time slot
per frame.
Thus, the circuit-switched link can support only 10 (= 1 Mbps/100 kbps) simultaneous
users. With packet switching, the probability that a specific user is active is
0.1 (that is, 10 percent). If there are 35 users, the probability that there are 11 or
more simultaneously active users is approximately 0.0004. (Homework Problem P8
outlines how this probability is obtained.) When there are 10 or fewer simultaneously
active users (which happens with probability 0.9996), the aggregate arrival
rate of data is less than or equal to 1 Mbps, the output rate of the link. Thus, when
there are 10 or fewer active users, users’ packets flow through the link essentially
without delay, as is the case with circuit switching. When there are more than 10
simultaneously active users, then the aggregate arrival rate of packets exceeds the
output capacity of the link, and the output queue will begin to grow. (It continues to
grow until the aggregate input rate falls back below 1 Mbps, at which point the
queue will begin to diminish in length.) Because the probability of having more than
10 simultaneously active users is minuscule in this example, packet switching provides
essentially the same performance as circuit switching, but does so while
allowing for more than three times the number of users.
Let’s now consider a second simple example. Suppose there are 10 users and that
one user suddenly generates one thousand 1,000-bit packets, while other users
remain quiescent and do not generate packets. Under TDM circuit switching with 10
slots per frame and each slot consisting of 1,000 bits, the active user can only use its
one time slot per frame to transmit data, while the remaining nine time slots in each
frame remain idle. It will be 10 seconds before all of the active user’s one million bits
of data has been transmitted. In the case of packet switching, the active user can continuously
send its packets at the full link rate of 1 Mbps, since there are no other users
generating packets that need to be multiplexed with the active user’s packets. In this
case, all of the active user’s data will be transmitted within 1 second.
The above examples illustrate two ways in which the performance of packet
switching can be superior to that of circuit switching. They also highlight the crucial
difference between the two forms of sharing a link’s transmission rate among multiple
data streams. Circuit switching pre-allocates use of the transmission link regardless
of demand, with allocated but unneeded link time going unused. Packet
switching on the other hand allocates link use on demand. Link transmission capacity
will be shared on a packet-by-packet basis only among those users who have
packets that need to be transmitted over the link.
Although packet switching and circuit switching are both prevalent in today’s
telecommunication networks, the trend has certainly been in the direction of packet
switching. Even many of today’s circuit-switched telephone networks are slowly
migrating toward packet switching. In particular, telephone networks often use
packet switching for the expensive overseas portion of a telephone call.
1.3 • THE NETWORK CORE 31
1.3.3 A Network of Networks
We saw earlier that end systems (PCs, smartphones, Web servers, mail servers,
and so on) connect into the Internet via an access ISP. The access ISP can provide
either wired or wireless connectivity, using an array of access technologies
including DSL, cable, FTTH, Wi-Fi, and cellular. Note that the access ISP does
not have to be a telco or a cable company; instead it can be, for example, a university
(providing Internet access to students, staff, and faculty), or a company
(providing access for its employees). But connecting end users and content
providers into an access ISP is only a small piece of solving the puzzle of connecting
the billions of end systems that make up the Internet. To complete this
puzzle, the access ISPs themselves must be interconnected. This is done by creating
a network of networks—understanding this phrase is the key to understanding
the Internet.
Over the years, the network of networks that forms the Internet has evolved into
a very complex structure. Much of this evolution is driven by economics and
national policy, rather than by performance considerations. In order to understand
today’s Internet network structure, let’s incrementally build a series of network
structures, with each new structure being a better approximation of the complex
Internet that we have today. Recall that the overarching goal is to interconnect the
access ISPs so that all end systems can send packets to each other. One naive
approach would be to have each access ISP directly connect with every other access
ISP. Such a mesh design is, of course, much too costly for the access ISPs, as it
would require each access ISP to have a separate communication link to each of the
hundreds of thousands of other access ISPs all over the world.
Our first network structure, Network Structure 1, interconnects all of the
access ISPs with a single global transit ISP. Our (imaginary) global transit ISP is
a network of routers and communication links that not only spans the globe, but
also has at least one router near each of the hundreds of thousands of access ISPs.
Of course, it would be very costly for the global ISP to build such an extensive
network. To be profitable, it would naturally charge each of the access ISPs for
connectivity, with the pricing reflecting (but not necessarily directly proportional
to) the amount of traffic an access ISP exchanges with the global ISP. Since the
access ISP pays the global transit ISP, the access ISP is said to be a customer and
the global transit ISP is said to be a provider.
Now if some company builds and operates a global transit ISP that is profitable,
then it is natural for other companies to build their own global transit ISPs and compete
with the original global transit ISP. This leads to Network Structure 2, which
consists of the hundreds of thousands of access ISPs and multiple global transit
ISPs. The access ISPs certainly prefer Network Structure 2 over Network Structure
1 since they can now choose among the competing global transit providers as a
function of their pricing and services. Note, however, that the global transit ISPs
32 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
themselves must interconnect: Otherwise access ISPs connected to one of the global
transit providers would not be able to communicate with access ISPs connected to
the other global transit providers.
Network Structure 2, just described, is a two-tier hierarchy with global transit
providers residing at the top tier and access ISPs at the bottom tier. This assumes
that global transit ISPs are not only capable of getting close to each and every access
ISP, but also find it economically desirable to do so. In reality, although some ISPs
do have impressive global coverage and do directly connect with many access ISPs,
no ISP has presence in each and every city in the world. Instead, in any given region,
there may be a regional ISP to which the access ISPs in the region connect. Each
regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary)
global transit ISP; but tier-1 ISPs, which actually do exist, do not have a presence in
every city in the world. There are approximately a dozen tier-1 ISPs, including
Level 3 Communications, AT&T, Sprint, and NTT. Interestingly, no group officially
sanctions tier-1 status; as the saying goes—if you have to ask if you’re a member of
a group, you’re probably not.
Returning to this network of networks, not only are there multiple competing tier-
1 ISPs, there may be multiple competing regional ISPs in a region. In such a hierarchy,
each access ISP pays the regional ISP to which it connects, and each regional ISP
pays the tier-1 ISP to which it connects. (An access ISP can also connect directly to a
tier-1 ISP, in which case it pays the tier-1 ISP). Thus, there is customer-provider
relationship at each level of the hierarchy. Note that the tier-1 ISPs do not pay anyone
as they are at the top of the hierarchy. To further complicate matters, in some regions,
there may be a larger regional ISP (possibly spanning an entire country) to which the
smaller regional ISPs in that region connect; the larger regional ISP then connects to a
tier-1 ISP. For example, in China, there are access ISPs in each city, which connect to
provincial ISPs, which in turn connect to national ISPs, which finally connect to tier-1
ISPs [Tian 2012]. We refer to this multi-tier hierarchy, which is still only a crude
approximation of today’s Internet, as Network Structure 3.
To build a network that more closely resembles today’s Internet, we must add
points of presence (PoPs), multi-homing, peering, and Internet exchange points
(IXPs) to the hierarchical Network Structure 3. PoPs exist in all levels of the hierarchy,
except for the bottom (access ISP) level. A PoP is simply a group of one or
more routers (at the same location) in the provider’s network where customer ISPs
can connect into the provider ISP. For a customer network to connect to a
provider’s PoP, it can lease a high-speed link from a third-party telecommunications
provider to directly connect one of its routers to a router at the PoP. Any ISP
(except for tier-1 ISPs) may choose to multi-home, that is, to connect to two or
more provider ISPs. So, for example, an access ISP may multi-home with two
regional ISPs, or it may multi-home with two regional ISPs and also with a tier-1
ISP. Similarly, a regional ISP may multi-home with multiple tier-1 ISPs. When an
1.3 • THE NETWORK CORE 33
ISP multi-homes, it can continue to send and receive packets into the Internet even
if one of its providers has a failure.
As we just learned, customer ISPs pay their provider ISPs to obtain global
Internet interconnectivity. The amount that a customer ISP pays a provider ISP
reflects the amount of traffic it exchanges with the provider. To reduce these costs, a
pair of nearby ISPs at the same level of the hierarchy can peer, that is, they can
directly connect their networks together so that all the traffic between them passes
over the direct connection rather than through upstream intermediaries. When two
ISPs peer, it is typically settlement-free, that is, neither ISP pays the other. As noted
earlier, tier-1 ISPs also peer with one another, settlement-free. For a readable discussion
of peering and customer-provider relationships, see [Van der Berg 2008].
Along these same lines, a third-party company can create an Internet Exchange
Point (IXP) (typically in a stand-alone building with its own switches), which is a
meeting point where multiple ISPs can peer together. There are roughly 300 IXPs in
the Internet today [Augustin 2009]. We refer to this ecosystem—consisting of
access ISPs, regional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as
Network Structure 4.
We now finally arrive at Network Structure 5, which describes the Internet of
2012. Network Structure 5, illustrated in Figure 1.15, builds on top of Network
Structure 4 by adding content provider networks. Google is currently one of the
leading examples of such a content provider network. As of this writing, it is estimated
that Google has 30 to 50 data centers distributed across North America,
Europe, Asia, South America, and Australia. Some of these data centers house
over one hundred thousand servers, while other data centers are smaller, housing
only hundreds of servers. The Google data centers are all interconnected via
Google’s private TCP/IP network, which spans the entire globe but is nevertheless
separate from the public Internet. Importantly, the Google private network only
34 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
Regional
ISP
Tier 1
ISP
Content provider
(e.g., Google)
Tier 1
ISP
IXP
Regional
ISP
IXP IXP
Figure 1.15  Interconnection of ISPs
carries traffic to/from Google servers. As shown in Figure 1.15, the Google
private network attempts to “bypass” the upper tiers of the Internet by peering
(settlement free) with lower-tier ISPs, either by directly connecting with them or
by connecting with them at IXPs [Labovitz 2010]. However, because many access
ISPs can still only be reached by transiting through tier-1 networks, the Google
network also connects to tier-1 ISPs, and pays those ISPs for the traffic it
exchanges with them. By creating its own network, a content provider not only
reduces its payments to upper-tier ISPs, but also has greater control of how its
services are ultimately delivered to end users. Google’s network infrastructure is
described in greater detail in Section 7.2.4.
In summary, today’s Internet—a network of networks—is complex, consisting of
a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs are
diverse in their coverage, with some spanning multiple continents and oceans, and
others limited to narrow geographic regions. The lower-tier ISPs connect to the
higher-tier ISPs, and the higher-tier ISPs interconnect with one another. Users and
content providers are customers of lower-tier ISPs, and lower-tier ISPs are customers
of higher-tier ISPs. In recent years, major content providers have also created their
own networks and connect directly into lower-tier ISPs where possible.
1.4 Delay, Loss, and Throughput
in Packet-Switched Networks
Back in Section 1.1 we said that the Internet can be viewed as an infrastructure
that provides services to distributed applications running on end systems. Ideally,
we would like Internet services to be able to move as much data as we want
between any two end systems, instantaneously, without any loss of data. Alas, this
is a lofty goal, one that is unachievable in reality. Instead, computer networks necessarily
constrain throughput (the amount of data per second that can be transferred)
between end systems, introduce delays between end systems, and can
actually lose packets. On one hand, it is unfortunate that the physical laws of reality
introduce delay and loss as well as constrain throughput. On the other hand,
because computer networks have these problems, there are many fascinating
issues surrounding how to deal with the problems—more than enough issues to
fill a course on computer networking and to motivate thousands of PhD theses! In
this section, we’ll begin to examine and quantify delay, loss, and throughput in
computer networks.
1.4.1 Overview of Delay in Packet-Switched Networks
Recall that a packet starts in a host (the source), passes through a series of routers,
and ends its journey in another host (the destination). As a packet travels from one
node (host or router) to the subsequent node (host or router) along this path, the
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 35
packet suffers from several types of delays at each node along the path. The most
important of these delays are the nodal processing delay, queuing delay, transmission
delay, and propagation delay; together, these delays accumulate to give a total
nodal delay. The performance of many Internet applications—such as search, Web
browsing, email, maps, instant messaging, and voice-over-IP—are greatly affected
by network delays. In order to acquire a deep understanding of packet switching and
computer networks, we must understand the nature and importance of these delays.
Types of Delay
Let’s explore these delays in the context of Figure 1.16. As part of its end-to-end
route between source and destination, a packet is sent from the upstream node
through router A to router B. Our goal is to characterize the nodal delay at router A.
Note that router A has an outbound link leading to router B. This link is preceded by
a queue (also known as a buffer). When the packet arrives at router A from the
upstream node, router A examines the packet’s header to determine the appropriate
outbound link for the packet and then directs the packet to this link. In this example,
the outbound link for the packet is the one that leads to router B. A packet can be
transmitted on a link only if there is no other packet currently being transmitted on
the link and if there are no other packets preceding it in the queue; if the link is
currently busy or if there are other packets already queued for the link, the newly
arriving packet will then join the queue.
Processing Delay
The time required to examine the packet’s header and determine where to direct the
packet is part of the processing delay. The processing delay can also include other
factors, such as the time needed to check for bit-level errors in the packet that occurred
in transmitting the packet’s bits from the upstream node to router A. Processing delays
36 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
A
B
Nodal
processing
Queueing
(waiting for
transmission)
Transmission
Propagation
Figure 1.16  The nodal delay at router A
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 37
in high-speed routers are typically on the order of microseconds or less. After this
nodal processing, the router directs the packet to the queue that precedes the link to
router B. (In Chapter 4 we’ll study the details of how a router operates.)
Queuing Delay
At the queue, the packet experiences a queuing delay as it waits to be transmitted onto
the link. The length of the queuing delay of a specific packet will depend on the number
of earlier-arriving packets that are queued and waiting for transmission onto the
link. If the queue is empty and no other packet is currently being transmitted, then our
packet’s queuing delay will be zero. On the other hand, if the traffic is heavy and many
other packets are also waiting to be transmitted, the queuing delay will be long. We
will see shortly that the number of packets that an arriving packet might expect to find
is a function of the intensity and nature of the traffic arriving at the queue. Queuing
delays can be on the order of microseconds to milliseconds in practice.
Transmission Delay
Assuming that packets are transmitted in a first-come-first-served manner, as is common
in packet-switched networks, our packet can be transmitted only after all the
packets that have arrived before it have been transmitted. Denote the length of the
packet by L bits, and denote the transmission rate of the link from router A to router
B by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps;
for a 100 Mbps Ethernet link, the rate is R = 100 Mbps. The transmission delay is
L/R. This is the amount of time required to push (that is, transmit) all of the packet’s
bits into the link. Transmission delays are typically on the order of microseconds to
milliseconds in practice.
Propagation Delay
Once a bit is pushed into the link, it needs to propagate to router B. The time
required to propagate from the beginning of the link to router B is the propagation
delay. The bit propagates at the propagation speed of the link. The propagation
speed depends on the physical medium of the link (that is, fiber optics, twisted-pair
copper wire, and so on) and is in the range of
2 108 meters/sec to 3 108 meters/sec
which is equal to, or a little less than, the speed of light. The propagation delay is
the distance between two routers divided by the propagation speed. That is, the
propagation delay is d/s, where d is the distance between router A and router B and s
is the propagation speed of the link. Once the last bit of the packet propagates to
node B, it and all the preceding bits of the packet are stored in router B. The whole
process then continues with router B now performing the forwarding. In wide-area
networks, propagation delays are on the order of milliseconds.
Comparing Transmission and Propagation Delay
Newcomers to the field of computer networking sometimes have difficulty understanding
the difference between transmission delay and propagation delay. The difference
is subtle but important. The transmission delay is the amount of time required for
the router to push out the packet; it is a function of the packet’s length and the transmission
rate of the link, but has nothing to do with the distance between the two
routers. The propagation delay, on the other hand, is the time it takes a bit to propagate
from one router to the next; it is a function of the distance between the two routers, but
has nothing to do with the packet’s length or the transmission rate of the link.
An analogy might clarify the notions of transmission and propagation delay. Consider
a highway that has a tollbooth every 100 kilometers, as shown in Figure 1.17.
You can think of the highway segments between tollbooths as links and the tollbooths
as routers. Suppose that cars travel (that is, propagate) on the highway at a
rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously accelerates
to 100 km/hour and maintains that speed between tollbooths). Suppose next
that 10 cars, traveling together as a caravan, follow each other in a fixed order. You
can think of each car as a bit and the caravan as a packet. Also suppose that each
tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and
that it is late at night so that the caravan’s cars are the only cars on the highway.
Finally, suppose that whenever the first car of the caravan arrives at a tollbooth, it
waits at the entrance until the other nine cars have arrived and lined up behind it.
(Thus the entire caravan must be stored at the tollbooth before it can begin to be forwarded.)
The time required for the tollbooth to push the entire caravan onto the
highway is (10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the
transmission delay in a router. The time required for a car to travel from the exit of
one tollbooth to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is
analogous to propagation delay. Therefore, the time from when the caravan is stored
in front of a tollbooth until the caravan is stored in front of the next tollbooth is the
sum of transmission delay and propagation delay—in this example, 62 minutes.
38 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Ten-car
caravan
Toll
booth
Toll
booth
100 km 100 km
Figure 1.17  Caravan analogy
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 39
Let’s explore this analogy a bit more. What would happen if the tollbooth service
time for a caravan were greater than the time for a car to travel between tollbooths?
For example, suppose now that the cars travel at the rate of 1,000 km/hour and the tollbooth
services cars at the rate of one car per minute. Then the traveling delay between
two tollbooths is 6 minutes and the time to serve a caravan is 10 minutes. In this case,
the first few cars in the caravan will arrive at the second tollbooth before the last cars
in the caravan leave the first tollbooth. This situation also arises in packet-switched
networks—the first bits in a packet can arrive at a router while many of the remaining
bits in the packet are still waiting to be transmitted by the preceding router.
If a picture speaks a thousand words, then an animation must speak a million
words. The companion Web site for this textbook provides an interactive Java applet
that nicely illustrates and contrasts transmission delay and propagation delay. The
reader is highly encouraged to visit that applet. [Smith 2009] also provides a very
readable discussion of propagation, queueing, and transmission delays.
If we let dproc, dqueue, dtrans, and dprop denote the processing, queuing, transmission,
and propagation delays, then the total nodal delay is given by
dnodal = dproc + dqueue + dtrans + dprop
The contribution of these delay components can vary significantly. For example,
dprop can be negligible (for example, a couple of microseconds) for a link connecting
two routers on the same university campus; however, dprop is hundreds of milliseconds
for two routers interconnected by a geostationary satellite link, and can be
the dominant term in dnodal. Similarly, dtrans can range from negligible to significant.
Its contribution is typically negligible for transmission rates of 10 Mbps and higher
(for example, for LANs); however, it can be hundreds of milliseconds for large
Internet packets sent over low-speed dial-up modem links. The processing delay,
dproc, is often negligible; however, it strongly influences a router’s maximum
throughput, which is the maximum rate at which a router can forward packets.
1.4.2 Queuing Delay and Packet Loss
The most complicated and interesting component of nodal delay is the queuing
delay, dqueue. In fact, queuing delay is so important and interesting in computer networking
that thousands of papers and numerous books have been written about it
[Bertsekas 1991; Daigle 1991; Kleinrock 1975, 1976; Ross 1995]. We give only a
high-level, intuitive discussion of queuing delay here; the more curious reader may
want to browse through some of the books (or even eventually write a PhD thesis
on the subject!). Unlike the other three delays (namely, dproc, dtrans, and dprop), the
queuing delay can vary from packet to packet. For example, if 10 packets arrive
at an empty queue at the same time, the first packet transmitted will suffer no queuing
delay, while the last packet transmitted will suffer a relatively large queuing
delay (while it waits for the other nine packets to be transmitted). Therefore, when
40 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
characterizing queuing delay, one typically uses statistical measures, such as average
queuing delay, variance of queuing delay, and the probability that the queuing
delay exceeds some specified value.
When is the queuing delay large and when is it insignificant? The answer to this
question depends on the rate at which traffic arrives at the queue, the transmission
rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives
periodically or arrives in bursts. To gain some insight here, let a denote the average
rate at which packets arrive at the queue (a is in units of packets/sec). Recall that R is
the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of
the queue. Also suppose, for simplicity, that all packets consist of L bits. Then the
average rate at which bits arrive at the queue is La bits/sec. Finally, assume that the
queue is very big, so that it can hold essentially an infinite number of bits. The ratio
La/R, called the traffic intensity, often plays an important role in estimating the
extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at
the queue exceeds the rate at which the bits can be transmitted from the queue. In this
unfortunate situation, the queue will tend to increase without bound and the queuing
delay will approach infinity! Therefore, one of the golden rules in traffic engineering
is: Design your system so that the traffic intensity is no greater than 1.
Now consider the case La/R = 1. Here, the nature of the arriving traffic impacts
the queuing delay. For example, if packets arrive periodically—that is, one packet
arrives every L/R seconds—then every packet will arrive at an empty queue and
there will be no queuing delay. On the other hand, if packets arrive in bursts but
periodically, there can be a significant average queuing delay. For example, suppose
N packets arrive simultaneously every (L/R)N seconds. Then the first packet transmitted
has no queuing delay; the second packet transmitted has a queuing delay of
L/R seconds; and more generally, the nth packet transmitted has a queuing delay of
(n  1)L/R seconds. We leave it as an exercise for you to calculate the average queuing
delay in this example.
The two examples of periodic arrivals described above are a bit academic. Typically,
the arrival process to a queue is random; that is, the arrivals do not follow any
pattern and the packets are spaced apart by random amounts of time. In this more
realistic case, the quantity La/R is not usually sufficient to fully characterize the
queueing delay statistics. Nonetheless, it is useful in gaining an intuitive understanding
of the extent of the queuing delay. In particular, if the traffic intensity is close to
zero, then packet arrivals are few and far between and it is unlikely that an arriving
packet will find another packet in the queue. Hence, the average queuing delay will
be close to zero. On the other hand, when the traffic intensity is close to 1, there will
be intervals of time when the arrival rate exceeds the transmission capacity (due to
variations in packet arrival rate), and a queue will form during these periods of time;
when the arrival rate is less than the transmission capacity, the length of the queue
will shrink. Nonetheless, as the traffic intensity approaches 1, the average queue
length gets larger and larger. The qualitative dependence of average queuing delay
on the traffic intensity is shown in Figure 1.18.
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 41
One important aspect of Figure 1.18 is the fact that as the traffic intensity
approaches 1, the average queuing delay increases rapidly. A small percentage
increase in the intensity will result in a much larger percentage-wise increase in
delay. Perhaps you have experienced this phenomenon on the highway. If you regularly
drive on a road that is typically congested, the fact that the road is typically
congested means that its traffic intensity is close to 1. If some event causes an even
slightly larger-than-usual amount of traffic, the delays you experience can be huge.
To really get a good feel for what queuing delays are about, you are encouraged
once again to visit the companion Web site, which provides an interactive Java
applet for a queue. If you set the packet arrival rate high enough so that the traffic
intensity exceeds 1, you will see the queue slowly build up over time.
Packet Loss
In our discussions above, we have assumed that the queue is capable of holding an
infinite number of packets. In reality a queue preceding a link has finite capacity,
although the queuing capacity greatly depends on the router design and cost.
Because the queue capacity is finite, packet delays do not really approach infinity as
the traffic intensity approaches 1. Instead, a packet can arrive to find a full queue.
With no place to store such a packet, a router will drop that packet; that is, the
packet will be lost. This overflow at a queue can again be seen in the Java applet for
a queue when the traffic intensity is greater than 1.
From an end-system viewpoint, a packet loss will look like a packet having
been transmitted into the network core but never emerging from the network at the
destination. The fraction of lost packets increases as the traffic intensity increases.
Therefore, performance at a node is often measured not only in terms of delay, but
also in terms of the probability of packet loss. As we’ll discuss in the subsequent
Average queuing delay
La/R
1
Figure 1.18  Dependence of average queuing delay on traffic intensity
chapters, a lost packet may be retransmitted on an end-to-end basis in order to
ensure that all data are eventually transferred from source to destination
1.4.3 End-to-End Delay
Our discussion up to this point has focused on the nodal delay, that is, the delay at a
single router. Let’s now consider the total delay from source to destination. To get a
handle on this concept, suppose there are N  1 routers between the source host and
the destination host. Let’s also suppose for the moment that the network is uncongested
(so that queuing delays are negligible), the processing delay at each router
and at the source host is dproc, the transmission rate out of each router and out of the
source host is R bits/sec, and the propagation on each link is dprop. The nodal delays
accumulate and give an end-to-end delay,
dend-end = N (dproc + dtrans + dprop) (1.2)
where, once again, dtrans = L/R, where L is the packet size. Note that Equation 1.2 is a
generalization of Equation 1.1, which did not take into account processing and propagation
delays. We leave it to you to generalize Equation 1.2 to the case of heterogeneous
delays at the nodes and to the presence of an average queuing delay at each node.
Traceroute
To get a hands-on feel for end-to-end delay in a computer network, we can make use
of the Traceroute program. Traceroute is a simple program that can run in any Internet
host. When the user specifies a destination hostname, the program in the source
host sends multiple, special packets toward that destination. As these packets work
their way toward the destination, they pass through a series of routers. When a
router receives one of these special packets, it sends back to the source a short message
that contains the name and address of the router.
More specifically, suppose there are N  1 routers between the source and the
destination. Then the source will send N special packets into the network, with each
packet addressed to the ultimate destination. These N special packets are marked 1
through N, with the first packet marked 1 and the last packet marked N. When the nth
router receives the nth packet marked n, the router does not forward the packet
toward its destination, but instead sends a message back to the source. When the destination
host receives the Nth packet, it too returns a message back to the source. The
source records the time that elapses between when it sends a packet and when it
receives the corresponding return message; it also records the name and address of
the router (or the destination host) that returns the message. In this manner, the source
can reconstruct the route taken by packets flowing from source to destination, and
the source can determine the round-trip delays to all the intervening routers. Traceroute
actually repeats the experiment just described three times, so the source actually
sends 3 • N packets to the destination. RFC 1393 describes Traceroute in detail.
42 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
VideoNote
Using Traceroute to
discover network
paths and measure
network delay
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 43
Here is an example of the output of the Traceroute program, where the route
was being traced from the source host gaia.cs.umass.edu (at the University of Massachusetts)
to the host cis.poly.edu (at Polytechnic University in Brooklyn). The output
has six columns: the first column is the n value described above, that is, the
number of the router along the route; the second column is the name of the router;
the third column is the address of the router (of the form xxx.xxx.xxx.xxx); the last
three columns are the round-trip delays for three experiments. If the source receives
fewer than three messages from any given router (due to packet loss in the network),
Traceroute places an asterisk just after the router number and reports fewer than
three round-trip times for that router.
1 cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms
2 128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms
3 border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms 0.451 ms
4 acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460 ms
5 agr4-loopback.NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms 13.267 ms
6 acr2-loopback.NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms 12.148 ms
7 pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823 ms 11.793 ms
8 gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms 11.556 ms 13.297 ms
9 p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms
10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms
In the trace above there are nine routers between the source and the destination.
Most of these routers have a name, and all of them have addresses. For example, the
name of Router 3 is border4-rt-gi-1-3.gw.umass.edu and its address is
128.119.2.194. Looking at the data provided for this same router, we see that
in the first of the three trials the round-trip delay between the source and the router
was 1.03 msec. The round-trip delays for the subsequent two trials were 0.48 and
0.45 msec. These round-trip delays include all of the delays just discussed, including
transmission delays, propagation delays, router processing delays, and queuing
delays. Because the queuing delay is varying with time, the round-trip delay of
packet n sent to a router n can sometimes be longer than the round-trip delay of
packet n+1 sent to router n+1. Indeed, we observe this phenomenon in the above
example: the delays to Router 6 are larger than the delays to Router 7!
Want to try out Traceroute for yourself? We highly recommended that you visit
http://www.traceroute.org, which provides a Web interface to an extensive list of
sources for route tracing. You choose a source and supply the hostname for any destination.
The Traceroute program then does all the work. There are a number of free
software programs that provide a graphical interface to Traceroute; one of our
favorites is PingPlotter [PingPlotter 2012].
End System, Application, and Other Delays
In addition to processing, transmission, and propagation delays, there can be additional
significant delays in the end systems. For example, an end system wanting to
transmit a packet into a shared medium (e.g., as in a WiFi or cable modem scenario)
may purposefully delay its transmission as part of its protocol for sharing the
medium with other end systems; we’ll consider such protocols in detail in Chapter
5. Another important delay is media packetization delay, which is present in Voiceover-
IP (VoIP) applications. In VoIP, the sending side must first fill a packet with
encoded digitized speech before passing the packet to the Internet. This time to fill a
packet—called the packetization delay—can be significant and can impact the userperceived
quality of a VoIP call. This issue will be further explored in a homework
problem at the end of this chapter.
1.4.4 Throughput in Computer Networks
In addition to delay and packet loss, another critical performance measure in computer
networks is end-to-end throughput. To define throughput, consider transferring
a large file from Host A to Host B across a computer network. This transfer
might be, for example, a large video clip from one peer to another in a P2P file
sharing system. The instantaneous throughput at any instant of time is the rate
(in bits/sec) at which Host B is receiving the file. (Many applications, including
many P2P file sharing systems, display the instantaneous throughput during
downloads in the user interface—perhaps you have observed this before!) If the
file consists of F bits and the transfer takes T seconds for Host B to receive all F
bits, then the average throughput of the file transfer is F/T bits/sec. For some
applications, such as Internet telephony, it is desirable to have a low delay and an
instantaneous throughput consistently above some threshold (for example, over
24 kbps for some Internet telephony applications and over 256 kbps for some realtime
video applications). For other applications, including those involving file
transfers, delay is not critical, but it is desirable to have the highest possible
throughput.
To gain further insight into the important concept of throughput, let’s consider a
few examples. Figure 1.19(a) shows two end systems, a server and a client, connected
by two communication links and a router. Consider the throughput for a file
transfer from the server to the client. Let Rs denote the rate of the link between
the server and the router; and Rc denote the rate of the link between the router and
the client. Suppose that the only bits being sent in the entire network are those
from the server to the client. We now ask, in this ideal scenario, what is the serverto-
client throughput? To answer this question, we may think of bits as fluid and
communication links as pipes. Clearly, the server cannot pump bits through its link
at a rate faster than Rs bps; and the router cannot forward bits at a rate faster than Rc
bps. If Rs < Rc, then the bits pumped by the server will “flow” right through the
router and arrive at the client at a rate of Rs bps, giving a throughput of Rs bps. If, on
the other hand, Rc < Rs, then the router will not be able to forward bits as quickly as
it receives them. In this case, bits will only leave the router at rate Rc, giving an
44 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
1.4 • DELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS 45
end-to-end throughput of Rc. (Note also that if bits continue to arrive at the router at
rate Rs, and continue to leave the router at Rc, the backlog of bits at the router waiting
for transmission to the client will grow and grow—a most undesirable situation!)
Thus, for this simple two-link network, the throughput is min{Rc, Rs}, that is, it is
the transmission rate of the bottleneck link. Having determined the throughput, we
can now approximate the time it takes to transfer a large file of F bits from server to
client as F/min{Rs, Rc}. For a specific example, suppose you are downloading an
MP3 file of F = 32 million bits, the server has a transmission rate of Rs = 2 Mbps,
and you have an access link of Rc = 1 Mbps. The time needed to transfer the file is
then 32 seconds. Of course, these expressions for throughput and transfer time are
only approximations, as they do not account for store-and-forward and processing
delays as well as protocol issues.
Figure 1.19(b) now shows a network with N links between the server and the
client, with the transmission rates of the N links being R1, R2,..., RN. Applying the
same analysis as for the two-link network, we find that the throughput for a file
transfer from server to client is min{R1, R2,..., RN}, which is once again the transmission
rate of the bottleneck link along the path between server and client.
Now consider another example motivated by today’s Internet. Figure 1.20(a)
shows two end systems, a server and a client, connected to a computer network.
Consider the throughput for a file transfer from the server to the client. The server is
connected to the network with an access link of rate Rs and the client is connected to
the network with an access link of rate Rc. Now suppose that all the links in the core
of the communication network have very high transmission rates, much higher than
Rs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed
links that experience little congestion. Also suppose that the only bits being sent in
the entire network are those from the server to the client. Because the core of the
computer network is like a wide pipe in this example, the rate at which bits can flow
Server
Rs
R1 R2 RN
Rc
Client
Server
a.
b.
Client
Figure 1.19  Throughput for a file transfer from server to client
from source to destination is again the minimum of Rs and Rc, that is, throughput =
min{Rs, Rc}. Therefore, the constraining factor for throughput in today’s Internet is
typically the access network.
For a final example, consider Figure 1.20(b) in which there are 10 servers and
10 clients connected to the core of the computer network. In this example, there are
10 simultaneous downloads taking place, involving 10 client-server pairs. Suppose
that these 10 downloads are the only traffic in the network at the current time. As
shown in the figure, there is a link in the core that is traversed by all 10 downloads.
Denote R for the transmission rate of this link R. Let’s suppose that all server access
links have the same rate Rs, all client access links have the same rate Rc, and the
transmission rates of all the links in the core—except the one common link of rate
R—are much larger than Rs, Rc, and R. Now we ask, what are the throughputs of the
downloads? Clearly, if the rate of the common link, R, is large—say a hundred times
larger than both Rs and Rc—then the throughput for each download will once again
be min{Rs, Rc}. But what if the rate of the common link is of the same order as Rs
and Rc? What will the throughput be in this case? Let’s take a look at a specific
46 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Server
Rs
Rc
a. b.
Client 10 Clients
10 Servers
Bottleneck
link of
capacity R
Figure 1.20  End-to-end throughput: (a) Client downloads a file from
server; (b) 10 clients downloading with 10 servers
example. Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the common link
divides its transmission rate equally among the 10 downloads. Then the bottleneck
for each download is no longer in the access network, but is now instead the shared
link in the core, which only provides each download with 500 kbps of throughput.
Thus the end-to-end throughput for each download is now reduced to 500 kbps.
The examples in Figure 1.19 and Figure 1.20(a) show that throughput depends
on the transmission rates of the links over which the data flows. We saw that when
there is no other intervening traffic, the throughput can simply be approximated as
the minimum transmission rate along the path between source and destination. The
example in Figure 1.20(b) shows that more generally the throughput depends not
only on the transmission rates of the links along the path, but also on the intervening
traffic. In particular, a link with a high transmission rate may nonetheless be the bottleneck
link for a file transfer if many other data flows are also passing through that
link. We will examine throughput in computer networks more closely in the homework
problems and in the subsequent chapters.
1.5 Protocol Layers and Their Service Models
From our discussion thus far, it is apparent that the Internet is an extremely complicated
system. We have seen that there are many pieces to the Internet: numerous
applications and protocols, various types of end systems, packet switches, and various
types of link-level media. Given this enormous complexity, is there any hope of
organizing a network architecture, or at least our discussion of network architecture?
Fortunately, the answer to both questions is yes.
1.5.1 Layered Architecture
Before attempting to organize our thoughts on Internet architecture, let’s look for a
human analogy. Actually, we deal with complex systems all the time in our everyday
life. Imagine if someone asked you to describe, for example, the airline system.
How would you find the structure to describe this complex system that has ticketing
agents, baggage checkers, gate personnel, pilots, airplanes, air traffic control, and a
worldwide system for routing airplanes? One way to describe this system might be
to describe the series of actions you take (or others take for you) when you fly on an
airline. You purchase your ticket, check your bags, go to the gate, and eventually get
loaded onto the plane. The plane takes off and is routed to its destination. After your
plane lands, you deplane at the gate and claim your bags. If the trip was bad, you
complain about the flight to the ticket agent (getting nothing for your effort). This
scenario is shown in Figure 1.21.
Already, we can see some analogies here with computer networking: You are
being shipped from source to destination by the airline; a packet is shipped from
1.5 • PROTOCOL LAYERS AND THEIR SERVICE MODELS 47
source host to destination host in the Internet. But this is not quite the analogy we
are after. We are looking for some structure in Figure 1.21. Looking at Figure 1.21,
we note that there is a ticketing function at each end; there is also a baggage function
for already-ticketed passengers, and a gate function for already-ticketed and
already-baggage-checked passengers. For passengers who have made it through the
gate (that is, passengers who are already ticketed, baggage-checked, and through the
gate), there is a takeoff and landing function, and while in flight, there is an airplanerouting
function. This suggests that we can look at the functionality in Figure 1.21
in a horizontal manner, as shown in Figure 1.22.
Figure 1.22 has divided the airline functionality into layers, providing a framework
in which we can discuss airline travel. Note that each layer, combined with the
48 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing Airplane routing Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Ticket
Baggage
Gate
Takeoff/Landing
Departure airport Intermediate air-traffic
control centers
Figure 1.22  Horizontal layering of airline functionality
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Airplane routing
Figure 1.21  Taking an airplane trip: actions
1.5 • PROTOCOL LAYERS AND THEIR SERVICE MODELS 49
layers below it, implements some functionality, some service. At the ticketing layer
and below, airline-counter-to-airline-counter transfer of a person is accomplished.
At the baggage layer and below, baggage-check-to-baggage-claim transfer of a person
and bags is accomplished. Note that the baggage layer provides this service only
to an already-ticketed person. At the gate layer, departure-gate-to-arrival-gate transfer
of a person and bags is accomplished. At the takeoff/landing layer, runway-torunway
transfer of people and their bags is accomplished. Each layer provides its
service by (1) performing certain actions within that layer (for example, at the gate
layer, loading and unloading people from an airplane) and by (2) using the services
of the layer directly below it (for example, in the gate layer, using the runway-torunway
passenger transfer service of the takeoff/landing layer).
A layered architecture allows us to discuss a well-defined, specific part of a
large and complex system. This simplification itself is of considerable value by providing
modularity, making it much easier to change the implementation of the service
provided by the layer. As long as the layer provides the same service to the layer
above it, and uses the same services from the layer below it, the remainder of the
system remains unchanged when a layer’s implementation is changed. (Note that
changing the implementation of a service is very different from changing the service
itself!) For example, if the gate functions were changed (for instance, to have
people board and disembark by height), the remainder of the airline system would
remain unchanged since the gate layer still provides the same function (loading and
unloading people); it simply implements that function in a different manner after the
change. For large and complex systems that are constantly being updated, the ability
to change the implementation of a service without affecting other components of the
system is another important advantage of layering.
Protocol Layering
But enough about airlines. Let’s now turn our attention to network protocols. To
provide structure to the design of network protocols, network designers organize
protocols—and the network hardware and software that implement the protocols—
in layers. Each protocol belongs to one of the layers, just as each function in the
airline architecture in Figure 1.22 belonged to a layer. We are again interested in
the services that a layer offers to the layer above—the so-called service model of
a layer. Just as in the case of our airline example, each layer provides its service
by (1) performing certain actions within that layer and by (2) using the services of
the layer directly below it. For example, the services provided by layer n may
include reliable delivery of messages from one edge of the network to the other.
This might be implemented by using an unreliable edge-to-edge message delivery
service of layer n  1, and adding layer n functionality to detect and retransmit
lost messages.
A protocol layer can be implemented in software, in hardware, or in a combination
of the two. Application-layer protocols—such as HTTP and SMTP—are almost
always implemented in software in the end systems; so are transport-layer protocols.
Because the physical layer and data link layers are responsible for handling communication
over a specific link, they are typically implemented in a network interface
card (for example, Ethernet or WiFi interface cards) associated with a given link.
The network layer is often a mixed implementation of hardware and software. Also
note that just as the functions in the layered airline architecture were distributed
among the various airports and flight control centers that make up the system, so too
is a layer n protocol distributed among the end systems, packet switches, and other
components that make up the network. That is, there’s often a piece of a layer n protocol
in each of these network components.
Protocol layering has conceptual and structural advantages [RFC 3439]. As we
have seen, layering provides a structured way to discuss system components. Modularity
makes it easier to update system components. We mention, however, that
some researchers and networking engineers are vehemently opposed to layering
[Wakeman 1992]. One potential drawback of layering is that one layer may duplicate
lower-layer functionality. For example, many protocol stacks provide error
recovery on both a per-link basis and an end-to-end basis. A second potential drawback
is that functionality at one layer may need information (for example, a timestamp
value) that is present only in another layer; this violates the goal of
separation of layers.
When taken together, the protocols of the various layers are called the protocol
stack. The Internet protocol stack consists of five layers: the physical, link, network,
transport, and application layers, as shown in Figure 1.23(a). If you examine the
Table of Contents, you will see that we have roughly organized this book using the
layers of the Internet protocol stack. We take a top-down approach, first covering
the application layer and then proceeding downward.
50 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Transport
Application
Network
Link
Physical
a. Five-layer
Internet
protocol stack
Transport
Session
Application
Presentation
Network
Link
Physical
b. Seven-layer
ISO OSI
reference model
Figure 1.23  The Internet protocol stack (a) and OSI reference model (b)
1.5 • PROTOCOL LAYERS AND THEIR SERVICE MODELS 51
Application Layer
The application layer is where network applications and their application-layer protocols
reside. The Internet’s application layer includes many protocols, such as the HTTP
protocol (which provides for Web document request and transfer), SMTP (which provides
for the transfer of e-mail messages), and FTP (which provides for the transfer of
files between two end systems). We’ll see that certain network functions, such as the
translation of human-friendly names for Internet end systems like www.ietf.org to a
32-bit network address, are also done with the help of a specific application-layer protocol,
namely, the domain name system (DNS). We’ll see in Chapter 2 that it is very
easy to create and deploy our own new application-layer protocols.
An application-layer protocol is distributed over multiple end systems, with the
application in one end system using the protocol to exchange packets of information
with the application in another end system. We’ll refer to this packet of information
at the application layer as a message.
Transport Layer
The Internet’s transport layer transports application-layer messages between
application endpoints. In the Internet there are two transport protocols, TCP and
UDP, either of which can transport application-layer messages. TCP provides a
connection-oriented service to its applications. This service includes guaranteed
delivery of application-layer messages to the destination and flow control (that is,
sender/receiver speed matching). TCP also breaks long messages into shorter segments
and provides a congestion-control mechanism, so that a source throttles its
transmission rate when the network is congested. The UDP protocol provides a connectionless
service to its applications. This is a no-frills service that provides no
reliability, no flow control, and no congestion control. In this book, we’ll refer to a
transport-layer packet as a segment.
Network Layer
The Internet’s network layer is responsible for moving network-layer packets
known as datagrams from one host to another. The Internet transport-layer protocol
(TCP or UDP) in a source host passes a transport-layer segment and a destination
address to the network layer, just as you would give the postal service a letter
with a destination address. The network layer then provides the service of delivering
the segment to the transport layer in the destination host.
The Internet’s network layer includes the celebrated IP Protocol, which defines
the fields in the datagram as well as how the end systems and routers act on these
fields. There is only one IP protocol, and all Internet components that have a network
layer must run the IP protocol. The Internet’s network layer also contains routing
protocols that determine the routes that datagrams take between sources and
destinations. The Internet has many routing protocols. As we saw in Section 1.3, the
Internet is a network of networks, and within a network, the network administrator
can run any routing protocol desired. Although the network layer contains both the
IP protocol and numerous routing protocols, it is often simply referred to as the IP
layer, reflecting the fact that IP is the glue that binds the Internet together.
Link Layer
The Internet’s network layer routes a datagram through a series of routers between
the source and destination. To move a packet from one node (host or router) to the
next node in the route, the network layer relies on the services of the link layer. In
particular, at each node, the network layer passes the datagram down to the link
layer, which delivers the datagram to the next node along the route. At this next
node, the link layer passes the datagram up to the network layer.
The services provided by the link layer depend on the specific link-layer protocol
that is employed over the link. For example, some link-layer protocols provide
reliable delivery, from transmitting node, over one link, to receiving node. Note that
this reliable delivery service is different from the reliable delivery service of TCP,
which provides reliable delivery from one end system to another. Examples of linklayer
protocols include Ethernet, WiFi, and the cable access network’s DOCSIS protocol.
As datagrams typically need to traverse several links to travel from source to
destination, a datagram may be handled by different link-layer protocols at different
links along its route. For example, a datagram may be handled by Ethernet on one
link and by PPP on the next link. The network layer will receive a different service
from each of the different link-layer protocols. In this book, we’ll refer to the linklayer
packets as frames.
Physical Layer
While the job of the link layer is to move entire frames from one network element
to an adjacent network element, the job of the physical layer is to move the individual
bits within the frame from one node to the next. The protocols in this layer are
again link dependent and further depend on the actual transmission medium of the
link (for example, twisted-pair copper wire, single-mode fiber optics). For example,
Ethernet has many physical-layer protocols: one for twisted-pair copper wire,
another for coaxial cable, another for fiber, and so on. In each case, a bit is moved
across the link in a different way.
The OSI Model
Having discussed the Internet protocol stack in detail, we should mention that it is
not the only protocol stack around. In particular, back in the late 1970s, the International
Organization for Standardization (ISO) proposed that computer networks be
52 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
organized around seven layers, called the Open Systems Interconnection (OSI)
model [ISO 2012]. The OSI model took shape when the protocols that were to
become the Internet protocols were in their infancy, and were but one of many different
protocol suites under development; in fact, the inventors of the original OSI
model probably did not have the Internet in mind when creating it. Nevertheless,
beginning in the late 1970s, many training and university courses picked up on the
ISO mandate and organized courses around the seven-layer model. Because of its
early impact on networking education, the seven-layer model continues to linger on
in some networking textbooks and training courses.
The seven layers of the OSI reference model, shown in Figure 1.23(b), are:
application layer, presentation layer, session layer, transport layer, network layer,
data link layer, and physical layer. The functionality of five of these layers is
roughly the same as their similarly named Internet counterparts. Thus, let’s consider
the two additional layers present in the OSI reference model—the presentation layer
and the session layer. The role of the presentation layer is to provide services that
allow communicating applications to interpret the meaning of data exchanged.
These services include data compression and data encryption (which are selfexplanatory)
as well as data description (which, as we will see in Chapter 9, frees
the applications from having to worry about the internal format in which data are
represented/stored—formats that may differ from one computer to another). The
session layer provides for delimiting and synchronization of data exchange, including
the means to build a checkpointing and recovery scheme.
The fact that the Internet lacks two layers found in the OSI reference model
poses a couple of interesting questions: Are the services provided by these layers
unimportant? What if an application needs one of these services? The Internet’s
answer to both of these questions is the same—it’s up to the application developer.
It’s up to the application developer to decide if a service is important, and if the
service is important, it’s up to the application developer to build that functionality
into the application.
1.5.2 Encapsulation
Figure 1.24 shows the physical path that data takes down a sending end system’s
protocol stack, up and down the protocol stacks of an intervening link-layer switch
and router, and then up the protocol stack at the receiving end system. As we discuss
later in this book, routers and link-layer switches are both packet switches. Similar
to end systems, routers and link-layer switches organize their networking hardware
and software into layers. But routers and link-layer switches do not implement all of
the layers in the protocol stack; they typically implement only the bottom layers. As
shown in Figure 1.24, link-layer switches implement layers 1 and 2; routers implement
layers 1 through 3. This means, for example, that Internet routers are capable
of implementing the IP protocol (a layer 3 protocol), while link-layer switches are
not. We’ll see later that while link-layer switches do not recognize IP addresses, they
1.5 • PROTOCOL LAYERS AND THEIR SERVICE MODELS 53
54 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
are capable of recognizing layer 2 addresses, such as Ethernet addresses. Note that
hosts implement all five layers; this is consistent with the view that the Internet
architecture puts much of its complexity at the edges of the network.
Figure 1.24 also illustrates the important concept of encapsulation. At the
sending host, an application-layer message (M in Figure 1.24) is passed to the
transport layer. In the simplest case, the transport layer takes the message and
appends additional information (so-called transport-layer header information, Ht
in Figure 1.24) that will be used by the receiver-side transport layer. The application-
layer message and the transport-layer header information together constitute
the transport-layer segment. The transport-layer segment thus encapsulates the
application-layer message. The added information might include information
allowing the receiver-side transport layer to deliver the message up to the appropriate
application, and error-detection bits that allow the receiver to determine
whether bits in the message have been changed in route. The transport layer then
passes the segment to the network layer, which adds network-layer header information
(Hn in Figure 1.24) such as source and destination end system addresses,
M
M
M
M
Ht
Ht
Ht
Hn
Hl Hn
Hl Hn Ht
Link-layer switch
Router
Application
Transport
Network
Link
Physical
Message
Segment
Datagram
Frame
M
M
M
M
Ht
Ht
Ht
Hn
Hl Hn
Link
Physical
Source
Network
Link
Physical
Destination
Application
Transport
Network
Link
Physical
M Hl Hn Ht M
Hn Ht M Hn Ht M
Hl Hn Ht M Hl Hn Ht M
Figure 1.24  Hosts, routers, and link-layer switches; each contains a
different set of layers, reflecting their differences in
functionality
creating a network-layer datagram. The datagram is then passed to the link
layer, which (of course!) will add its own link-layer header information and create
a link-layer frame. Thus, we see that at each layer, a packet has two types of
fields: header fields and a payload field. The payload is typically a packet from
the layer above.
A useful analogy here is the sending of an interoffice memo from one corporate
branch office to another via the public postal service. Suppose Alice, who is
in one branch office, wants to send a memo to Bob, who is in another branch
office. The memo is analogous to the application-layer message. Alice puts the
memo in an interoffice envelope with Bob’s name and department written on the
front of the envelope. The interoffice envelope is analogous to a transport-layer
segment—it contains header information (Bob’s name and department number)
and it encapsulates the application-layer message (the memo). When the sending
branch-office mailroom receives the interoffice envelope, it puts the interoffice
envelope inside yet another envelope, which is suitable for sending through the
public postal service. The sending mailroom also writes the postal address of the
sending and receiving branch offices on the postal envelope. Here, the postal
envelope is analogous to the datagram—it encapsulates the transport-layer segment
(the interoffice envelope), which encapsulates the original message (the
memo). The postal service delivers the postal envelope to the receiving branchoffice
mailroom. There, the process of de-encapsulation is begun. The mailroom
extracts the interoffice memo and forwards it to Bob. Finally, Bob opens the envelope
and removes the memo.
The process of encapsulation can be more complex than that described above.
For example, a large message may be divided into multiple transport-layer segments
(which might themselves each be divided into multiple network-layer datagrams).
At the receiving end, such a segment must then be reconstructed from its constituent
datagrams.
1.6 Networks Under Attack
The Internet has become mission critical for many institutions today, including large
and small companies, universities, and government agencies. Many individuals also
rely on the Internet for many of their professional, social, and personal activities.
But behind all this utility and excitement, there is a dark side, a side where “bad
guys” attempt to wreak havoc in our daily lives by damaging our Internet-connected
computers, violating our privacy, and rendering inoperable the Internet services on
which we depend.
The field of network security is about how the bad guys can attack computer
networks and about how we, soon-to-be experts in computer networking, can
1.6 • NETWORKS UNDER ATTACK 55
56 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
defend networks against those attacks, or better yet, design new architectures that
are immune to such attacks in the first place. Given the frequency and variety of
existing attacks as well as the threat of new and more destructive future attacks,
network security has become a central topic in the field of computer networking.
One of the features of this textbook is that it brings network security issues to the
forefront.
Since we don’t yet have expertise in computer networking and Internet protocols,
we’ll begin here by surveying some of today’s more prevalent securityrelated
problems. This will whet our appetite for more substantial discussions in
the upcoming chapters. So we begin here by simply asking, what can go wrong?
How are computer networks vulnerable? What are some of the more prevalent
types of attacks today?
The bad guys can put malware into your host via the Internet
We attach devices to the Internet because we want to receive/send data from/to
the Internet. This includes all kinds of good stuff, including Web pages, e-mail
messages, MP3s, telephone calls, live video, search engine results, and so on.
But, unfortunately, along with all that good stuff comes malicious stuff—collectively
known as malware—that can also enter and infect our devices. Once malware
infects our device it can do all kinds of devious things, including deleting
our files; installing spyware that collects our private information, such as social
security numbers, passwords, and keystrokes, and then sends this (over the Internet,
of course!) back to the bad guys. Our compromised host may also be
enrolled in a network of thousands of similarly compromised devices, collectively
known as a botnet, which the bad guys control and leverage for spam email
distribution or distributed denial-of-service attacks (soon to be discussed)
against targeted hosts.
Much of the malware out there today is self-replicating: once it infects one
host, from that host it seeks entry into other hosts over the Internet, and from the
newly infected hosts, it seeks entry into yet more hosts. In this manner, selfreplicating
malware can spread exponentially fast. Malware can spread in the
form of a virus or a worm. Viruses are malware that require some form of user
interaction to infect the user’s device. The classic example is an e-mail attachment
containing malicious executable code. If a user receives and opens such an attachment,
the user inadvertently runs the malware on the device. Typically, such email
viruses are self-replicating: once executed, the virus may send an identical
message with an identical malicious attachment to, for example, every recipient in
the user’s address book. Worms are malware that can enter a device without any
explicit user interaction. For example, a user may be running a vulnerable network
application to which an attacker can send malware. In some cases, without any
user intervention, the application may accept the malware from the Internet and
run it, creating a worm. The worm in the newly infected device then scans the
Internet, searching for other hosts running the same vulnerable network application.
When it finds other vulnerable hosts, it sends a copy of itself to those hosts.
Today, malware, is pervasive and costly to defend against. As you work through
this textbook, we encourage you to think about the following question: What can
computer network designers do to defend Internet-attached devices from malware
attacks?
The bad guys can attack servers and network infrastructure
Another broad class of security threats are known as denial-of-service (DoS)
attacks. As the name suggests, a DoS attack renders a network, host, or other piece
of infrastructure unusable by legitimate users. Web servers, e-mail servers, DNS
servers (discussed in Chapter 2), and institutional networks can all be subject to DoS
attacks. Internet DoS attacks are extremely common, with thousands of DoS attacks
occurring every year [Moore 2001; Mirkovic 2005]. Most Internet DoS attacks fall
into one of three categories:
• Vulnerability attack. This involves sending a few well-crafted messages to a vulnerable
application or operating system running on a targeted host. If the right
sequence of packets is sent to a vulnerable application or operating system, the
service can stop or, worse, the host can crash.
• Bandwidth flooding. The attacker sends a deluge of packets to the targeted
host—so many packets that the target’s access link becomes clogged, preventing
legitimate packets from reaching the server.
• Connection flooding. The attacker establishes a large number of half-open or
fully open TCP connections (TCP connections are discussed in Chapter 3) at the
target host. The host can become so bogged down with these bogus connections
that it stops accepting legitimate connections.
Let’s now explore the bandwidth-flooding attack in more detail. Recalling our
delay and loss analysis discussion in Section 1.4.2, it’s evident that if the server has an
access rate of R bps, then the attacker will need to send traffic at a rate of approximately
R bps to cause damage. If R is very large, a single attack source may not be
able to generate enough traffic to harm the server. Furthermore, if all the traffic
emanates from a single source, an upstream router may be able to detect the attack and
block all traffic from that source before the traffic gets near the server. In a distributed
DoS (DDoS) attack, illustrated in Figure 1.25, the attacker controls multiple sources
and has each source blast traffic at the target. With this approach, the aggregate traffic
rate across all the controlled sources needs to be approximately R to cripple the
1.6 • NETWORKS UNDER ATTACK 57
service. DDoS attacks leveraging botnets with thousands of comprised hosts are a
common occurrence today [Mirkovic 2005]. DDos attacks are much harder to detect
and defend against than a DoS attack from a single host.
We encourage you to consider the following question as you work your way
through this book: What can computer network designers do to defend against DoS
attacks? We will see that different defenses are needed for the three types of DoS
attacks.
The bad guys can sniff packets
Many users today access the Internet via wireless devices, such as WiFi-connected
laptops or handheld devices with cellular Internet connections (covered in Chapter
6). While ubiquitous Internet access is extremely convenient and enables marvelous
new applications for mobile users, it also creates a major security vulnerability—by
placing a passive receiver in the vicinity of the wireless transmitter, that receiver can
obtain a copy of every packet that is transmitted! These packets can contain all kinds
of sensitive information, including passwords, social security numbers, trade
secrets, and private personal messages. A passive receiver that records a copy of
every packet that flies by is called a packet sniffer.
Sniffers can be deployed in wired environments as well. In wired broadcast environments,
as in many Ethernet LANs, a packet sniffer can obtain copies of broadcast
packets sent over the LAN. As described in Section 1.2, cable access technologies
also broadcast packets and are thus vulnerable to sniffing. Furthermore, a bad guy
who gains access to an institution’s access router or access link to the Internet may
58 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Attacker
“start
attack”
Slave
Slave
Slave
Victim
Slave
Slave
Figure 1.25  A distributed denial-of-service attack
1.6 • NETWORKS UNDER ATTACK 59
be able to plant a sniffer that makes a copy of every packet going to/from the organization.
Sniffed packets can then be analyzed offline for sensitive information.
Packet-sniffing software is freely available at various Web sites and as commercial
products. Professors teaching a networking course have been known to assign lab exercises
that involve writing a packet-sniffing and application-layer data reconstruction
program. Indeed, the Wireshark [Wireshark 2012] labs associated with this text (see the
introductory Wireshark lab at the end of this chapter) use exactly such a packet sniffer!
Because packet sniffers are passive—that is, they do not inject packets into the
channel—they are difficult to detect. So, when we send packets into a wireless channel,
we must accept the possibility that some bad guy may be recording copies of
our packets. As you may have guessed, some of the best defenses against packet
sniffing involve cryptography. We will examine cryptography as it applies to network
security in Chapter 8.
The bad guys can masquerade as someone you trust
It is surprisingly easy (you will have the knowledge to do so shortly as you proceed
through this text!) to create a packet with an arbitrary source address, packet content,
and destination address and then transmit this hand-crafted packet into the
Internet, which will dutifully forward the packet to its destination. Imagine the
unsuspecting receiver (say an Internet router) who receives such a packet, takes the
(false) source address as being truthful, and then performs some command embedded
in the packet’s contents (say modifies its forwarding table). The ability to inject
packets into the Internet with a false source address is known as IP spoofing, and is
but one of many ways in which one user can masquerade as another user.
To solve this problem, we will need end-point authentication, that is, a mechanism
that will allow us to determine with certainty if a message originates from
where we think it does. Once again, we encourage you to think about how this can
be done for network applications and protocols as you progress through the chapters
of this book. We will explore mechanisms for end-point authentication in Chapter 8.
In closing this section, it’s worth considering how the Internet got to be such an
insecure place in the first place. The answer, in essence, is that the Internet was originally
designed to be that way, based on the model of “a group of mutually trusting
users attached to a transparent network” [Blumenthal 2001]—a model in which (by
definition) there is no need for security. Many aspects of the original Internet architecture
deeply reflect this notion of mutual trust. For example, the ability for one
user to send a packet to any other user is the default rather than a requested/granted
capability, and user identity is taken at declared face value, rather than being authenticated
by default.
But today’s Internet certainly does not involve “mutually trusting users.”
Nonetheless, today’s users still need to communicate when they don’t necessarily
trust each other, may wish to communicate anonymously, may communicate indirectly
through third parties (e.g., Web caches, which we’ll study in Chapter 2, or
60 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
mobility-assisting agents, which we’ll study in Chapter 6), and may distrust the
hardware, software, and even the air through which they communicate. We now
have many security-related challenges before us as we progress through this book:
We should seek defenses against sniffing, end-point masquerading, man-in-themiddle
attacks, DDoS attacks, malware, and more. We should keep in mind that
communication among mutually trusted users is the exception rather than the rule.
Welcome to the world of modern computer networking!
1.7 History of Computer Networking and
the Internet
Sections 1.1 through 1.6 presented an overview of the technology of computer networking
and the Internet. You should know enough now to impress your family and
friends! However, if you really want to be a big hit at the next cocktail party, you
should sprinkle your discourse with tidbits about the fascinating history of the Internet
[Segaller 1998].
1.7.1 The Development of Packet Switching: 1961–1972
The field of computer networking and today’s Internet trace their beginnings
back to the early 1960s, when the telephone network was the world’s dominant
communication network. Recall from Section 1.3 that the telephone network uses
circuit switching to transmit information from a sender to a receiver—an appropriate
choice given that voice is transmitted at a constant rate between sender
and receiver. Given the increasing importance of computers in the early 1960s
and the advent of timeshared computers, it was perhaps natural to consider how
to hook computers together so that they could be shared among geographically
distributed users. The traffic generated by such users was likely to be bursty—
intervals of activity, such as the sending of a command to a remote computer, followed
by periods of inactivity while waiting for a reply or while contemplating
the received response.
Three research groups around the world, each unaware of the others’ work
[Leiner 1998], began inventing packet switching as an efficient and robust alternative
to circuit switching. The first published work on packet-switching techniques
was that of Leonard Kleinrock [Kleinrock 1961; Kleinrock 1964], then a graduate
student at MIT. Using queuing theory, Kleinrock’s work elegantly demonstrated the
effectiveness of the packet-switching approach for bursty traffic sources. In 1964,
Paul Baran [Baran 1964] at the Rand Institute had begun investigating the use of
packet switching for secure voice over military networks, and at the National Physical
Laboratory in England, Donald Davies and Roger Scantlebury were also developing
their ideas on packet switching.
The work at MIT, Rand, and the NPL laid the foundations for today’s Internet.
But the Internet also has a long history of a let’s-build-it-and-demonstrate-it
attitude that also dates back to the 1960s. J. C. R. Licklider [DEC 1990] and
Lawrence Roberts, both colleagues of Kleinrock’s at MIT, went on to lead the
computer science program at the Advanced Research Projects Agency (ARPA) in
the United States. Roberts published an overall plan for the ARPAnet [Roberts
1967], the first packet-switched computer network and a direct ancestor of today’s
public Internet. On Labor Day in 1969, the first packet switch was installed at
UCLA under Kleinrock’s supervision, and three additional packet switches were
installed shortly thereafter at the Stanford Research Institute (SRI), UC Santa Barbara,
and the University of Utah (Figure 1.26). The fledgling precursor to the
1.7 • HISTORY OF COMPUTER NETWORKING AND THE INTERNET 61
Figure 1.26  An early packet switch
62 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
Internet was four nodes large by the end of 1969. Kleinrock recalls the very first
use of the network to perform a remote login from UCLA to SRI, crashing the system
[Kleinrock 2004].
By 1972, ARPAnet had grown to approximately 15 nodes and was given its first
public demonstration by Robert Kahn. The first host-to-host protocol between
ARPAnet end systems, known as the network-control protocol (NCP), was completed
[RFC 001]. With an end-to-end protocol available, applications could now be
written. Ray Tomlinson wrote the first e-mail program in 1972.
1.7.2 Proprietary Networks and Internetworking: 1972–1980
The initial ARPAnet was a single, closed network. In order to communicate with an
ARPAnet host, one had to be actually attached to another ARPAnet IMP. In the early
to mid-1970s, additional stand-alone packet-switching networks besides ARPAnet
came into being: ALOHANet, a microwave network linking universities on the
Hawaiian islands [Abramson 1970], as well as DARPA’s packet-satellite [RFC 829]
and packet-radio networks [Kahn 1978]; Telenet, a BBN commercial packetswitching
network based on ARPAnet technology; Cyclades, a French packetswitching
network pioneered by Louis Pouzin [Think 2012]; Time-sharing networks
such as Tymnet and the GE Information Services network, among others, in the late
1960s and early 1970s [Schwartz 1977]; IBM’s SNA (1969–1974), which paralleled
the ARPAnet work [Schwartz 1977].
The number of networks was growing. With perfect hindsight we can see that
the time was ripe for developing an encompassing architecture for connecting networks
together. Pioneering work on interconnecting networks (under the sponsorship
of the Defense Advanced Research Projects Agency (DARPA)), in essence
creating a network of networks, was done by Vinton Cerf and Robert Kahn [Cerf
1974]; the term internetting was coined to describe this work.
These architectural principles were embodied in TCP. The early versions of
TCP, however, were quite different from today’s TCP. The early versions of TCP
combined a reliable in-sequence delivery of data via end-system retransmission
(still part of today’s TCP) with forwarding functions (which today are performed
by IP). Early experimentation with TCP, combined with the recognition of the
importance of an unreliable, non-flow-controlled, end-to-end transport service
for applications such as packetized voice, led to the separation of IP out of TCP
and the development of the UDP protocol. The three key Internet protocols that
we see today—TCP, UDP, and IP—were conceptually in place by the end of the
1970s.
In addition to the DARPA Internet-related research, many other important
networking activities were underway. In Hawaii, Norman Abramson was developing
ALOHAnet, a packet-based radio network that allowed multiple remote
sites on the Hawaiian Islands to communicate with each other. The ALOHA protocol
[Abramson 1970] was the first multiple-access protocol, allowing geographically
distributed users to share a single broadcast communication medium (a radio frequency).
Metcalfe and Boggs built on Abramson’s multiple-access protocol work
when they developed the Ethernet protocol [Metcalfe 1976] for wire-based
shared broadcast networks. Interestingly, Metcalfe and Boggs’ Ethernet protocol
was motivated by the need to connect multiple PCs, printers, and shared disks
[Perkins 1994]. Twenty-five years ago, well before the PC revolution and the
explosion of networks, Metcalfe and Boggs were laying the foundation for
today’s PC LANs.
1.7.3 A Proliferation of Networks: 1980–1990
By the end of the 1970s, approximately two hundred hosts were connected to the
ARPAnet. By the end of the 1980s the number of hosts connected to the public
Internet, a confederation of networks looking much like today’s Internet, would
reach a hundred thousand. The 1980s would be a time of tremendous growth.
Much of that growth resulted from several distinct efforts to create computer
networks linking universities together. BITNET provided e-mail and file transfers
among several universities in the Northeast. CSNET (computer science network)
was formed to link university researchers who did not have access to ARPAnet. In
1986, NSFNET was created to provide access to NSF-sponsored supercomputing
centers. Starting with an initial backbone speed of 56 kbps, NSFNET’s backbone
would be running at 1.5 Mbps by the end of the decade and would serve as a primary
backbone linking regional networks.
In the ARPAnet community, many of the final pieces of today’s Internet architecture
were falling into place. January 1, 1983 saw the official deployment of
TCP/IP as the new standard host protocol for ARPAnet (replacing the NCP protocol).
The transition [RFC 801] from NCP to TCP/IP was a flag day event—all
hosts were required to transfer over to TCP/IP as of that day. In the late 1980s,
important extensions were made to TCP to implement host-based congestion control
[Jacobson 1988]. The DNS, used to map between a human-readable Internet
name (for example, gaia.cs.umass.edu) and its 32-bit IP address, was also developed
[RFC 1034].
Paralleling this development of the ARPAnet (which was for the most part a
US effort), in the early 1980s the French launched the Minitel project, an ambitious
plan to bring data networking into everyone’s home. Sponsored by the
French government, the Minitel system consisted of a public packet-switched network
(based on the X.25 protocol suite), Minitel servers, and inexpensive terminals
with built-in low-speed modems. The Minitel became a huge success in 1984
when the French government gave away a free Minitel terminal to each French
household that wanted one. Minitel sites included free sites—such as a telephone
directory site—as well as private sites, which collected a usage-based fee from
1.7 • HISTORY OF COMPUTER NETWORKING AND THE INTERNET 63
each user. At its peak in the mid 1990s, it offered more than 20,000 services, ranging
from home banking to specialized research databases. The Minitel was in a
large proportion of French homes 10 years before most Americans had ever heard
of the Internet.
1.7.4 The Internet Explosion: The 1990s
The 1990s were ushered in with a number of events that symbolized the continued
evolution and the soon-to-arrive commercialization of the Internet. ARPAnet, the
progenitor of the Internet, ceased to exist. In 1991, NSFNET lifted its restrictions on
the use of NSFNET for commercial purposes. NSFNET itself would be decommissioned
in 1995, with Internet backbone traffic being carried by commercial Internet
Service Providers.
The main event of the 1990s was to be the emergence of the World Wide Web
application, which brought the Internet into the homes and businesses of millions of
people worldwide. The Web served as a platform for enabling and deploying hundreds
of new applications that we take for granted today, including search (e.g.,
Google and Bing) Internet commerce (e.g., Amazon and eBay) and social networks
(e.g., Facebook).
The Web was invented at CERN by Tim Berners-Lee between 1989 and 1991
[Berners-Lee 1989], based on ideas originating in earlier work on hypertext from
the 1940s by Vannevar Bush [Bush 1945] and since the 1960s by Ted Nelson
[Xanadu 2012]. Berners-Lee and his associates developed initial versions of HTML,
HTTP, a Web server, and a browser—the four key components of the Web. Around
the end of 1993 there were about two hundred Web servers in operation, this collection
of servers being just a harbinger of what was about to come. At about this time
several researchers were developing Web browsers with GUI interfaces, including
Marc Andreessen, who along with Jim Clark, formed Mosaic Communications,
which later became Netscape Communications Corporation [Cusumano 1998; Quittner
1998]. By 1995, university students were using Netscape browsers to surf the Web
on a daily basis. At about this time companies—big and small—began to operate
Web servers and transact commerce over the Web. In 1996, Microsoft started to
make browsers, which started the browser war between Netscape and Microsoft,
which Microsoft won a few years later [Cusumano 1998].
The second half of the 1990s was a period of tremendous growth and innovation
for the Internet, with major corporations and thousands of startups creating
Internet products and services. By the end of the millennium the Internet was supporting
hundreds of popular applications, including four killer applications:
• E-mail, including attachments and Web-accessible e-mail
• The Web, including Web browsing and Internet commerce
64 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
• Instant messaging, with contact lists
• Peer-to-peer file sharing of MP3s, pioneered by Napster
Interestingly, the first two killer applications came from the research community,
whereas the last two were created by a few young entrepreneurs.
The period from 1995 to 2001 was a roller-coaster ride for the Internet in the
financial markets. Before they were even profitable, hundreds of Internet startups
made initial public offerings and started to be traded in a stock market. Many companies
were valued in the billions of dollars without having any significant revenue
streams. The Internet stocks collapsed in 2000–2001, and many startups shut down.
Nevertheless, a number of companies emerged as big winners in the Internet space,
including Microsoft, Cisco, Yahoo, e-Bay, Google, and Amazon.
1.7.5 The New Millennium
Innovation in computer networking continues at a rapid pace. Advances are being
made on all fronts, including deployments of faster routers and higher transmission
speeds in both access networks and in network backbones. But the following developments
merit special attention:
• Since the beginning of the millennium, we have been seeing aggressive deployment
of broadband Internet access to homes—not only cable modems and DSL
but also fiber to the home, as discussed in Section 1.2. This high-speed Internet
access has set the stage for a wealth of video applications, including the distribution
of user-generated video (for example, YouTube), on-demand streaming of
movies and television shows (e.g., Netflix) , and multi-person video conference
(e.g., Skype).
• The increasing ubiquity of high-speed (54 Mbps and higher) public WiFi networks
and medium-speed (up to a few Mbps) Internet access via 3G and 4G
cellular telephony networks is not only making it possible to remain constantly
connected while on the move, but also enabling new location-specific
applications. The number of wireless devices connecting to the Internet surpassed
the number of wired devices in 2011. This high-speed wireless access
has set the stage for the rapid emergence of hand-held computers (iPhones,
Androids, iPads, and so on), which enjoy constant and untethered access to
the Internet.
• Online social networks, such as Facebook and Twitter, have created massive people
networks on top of the Internet. Many Internet users today “live” primarily
within Facebook. Through their APIs, the online social networks create platforms
for new networked applications and distributed games.
1.7 • HISTORY OF COMPUTER NETWORKING AND THE INTERNET 65
66 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
• As discussed in Section 1.3.3, online service providers, such as Google and
Microsoft, have deployed their own extensive private networks, which not only
connect together their globally distributed data centers, but are used to bypass
the Internet as much as possible by peering directly with lower-tier ISPs. As a
result, Google provides search results and email access almost instantaneously,
as if their data centers were running within one’s own computer.
• Many Internet commerce companies are now running their applications in the
“cloud”—such as in Amazon’s EC2, in Google’s Application Engine, or in
Microsoft’s Azure. Many companies and universities have also migrated their Internet
applications (e.g., email and Web hosting) to the cloud. Cloud companies not
only provide applications scalable computing and storage environments, but also
provide the applications implicit access to their high-performance private networks.
1.8 Summary
In this chapter we’ve covered a tremendous amount of material! We’ve looked at the
various pieces of hardware and software that make up the Internet in particular and
computer networks in general. We started at the edge of the network, looking at end
systems and applications, and at the transport service provided to the applications
running on the end systems. We also looked at the link-layer technologies and physical
media typically found in the access network. We then dove deeper inside the
network, into the network core, identifying packet switching and circuit switching
as the two basic approaches for transporting data through a telecommunication network,
and we examined the strengths and weaknesses of each approach. We also
examined the structure of the global Internet, learning that the Internet is a network
of networks. We saw that the Internet’s hierarchical structure, consisting of higherand
lower-tier ISPs, has allowed it to scale to include thousands of networks.
In the second part of this introductory chapter, we examined several topics central
to the field of computer networking. We first examined the causes of delay, throughput
and packet loss in a packet-switched network. We developed simple quantitative
models for transmission, propagation, and queuing delays as well as for throughput;
we’ll make extensive use of these delay models in the homework problems throughout
this book. Next we examined protocol layering and service models, key architectural
principles in networking that we will also refer back to throughout this book. We
also surveyed some of the more prevalent security attacks in the Internet day. We finished
our introduction to networking with a brief history of computer networking. The
first chapter in itself constitutes a mini-course in computer networking.
So, we have indeed covered a tremendous amount of ground in this first chapter!
If you’re a bit overwhelmed, don’t worry. In the following chapters we’ll revisit all of
these ideas, covering them in much more detail (that’s a promise, not a threat!). At this
point, we hope you leave this chapter with a still-developing intuition for the pieces
that make up a network, a still-developing command of the vocabulary of networking
(don’t be shy about referring back to this chapter), and an ever-growing desire to learn
more about networking. That’s the task ahead of us for the rest of this book.
Road-Mapping This Book
Before starting any trip, you should always glance at a road map in order to become
familiar with the major roads and junctures that lie ahead. For the trip we are about
to embark on, the ultimate destination is a deep understanding of the how, what, and
why of computer networks. Our road map is the sequence of chapters of this book:
1. Computer Networks and the Internet
2. Application Layer
3. Transport Layer
4. Network Layer
5. Link Layer and Local Area Networks
6. Wireless and Mobile Networks
7. Multimedia Networking
8. Security in Computer Networks
9. Network Management
Chapters 2 through 5 are the four core chapters of this book. You should notice
that these chapters are organized around the top four layers of the five-layer Internet
protocol stack, one chapter for each layer. Further note that our journey will begin at
the top of the Internet protocol stack, namely, the application layer, and will work
its way downward. The rationale behind this top-down journey is that once we
understand the applications, we can understand the network services needed to support
these applications. We can then, in turn, examine the various ways in which
such services might be implemented by a network architecture. Covering applications
early thus provides motivation for the remainder of the text.
The second half of the book—Chapters 6 through 9—zooms in on four enormously
important (and somewhat independent) topics in modern computer networking.
In Chapter 6, we examine wireless and mobile networks, including wireless
LANs (including WiFi and Bluetooth), Cellular telephony networks (including
GSM, 3G, and 4G), and mobility (in both IP and GSM networks). In Chapter 7
(Multimedia Networking) we examine audio and video applications such as Internet
phone, video conferencing, and streaming of stored media. We also look at how a
packet-switched network can be designed to provide consistent quality of service to
audio and video applications. In Chapter 8 (Security in Computer Networks), we
first look at the underpinnings of encryption and network security, and then we
examine how the basic theory is being applied in a broad range of Internet contexts.
The last chapter (Network Management) examines the key issues in network management
as well as the primary Internet protocols used for network management.
1.8 • SUMMARY 67
Homework Problems and Questions
Chapter 1 Review Questions
SECTION 1.1
R1. What is the difference between a host and an end system? List several different
types of end systems. Is a Web server an end system?
R2. The word protocol is often used to describe diplomatic relations. How does
Wikipedia describe diplomatic protocol?
R3. Why are standards important for protocols?
SECTION 1.2
R4. List six access technologies. Classify each one as home access, enterprise
access, or wide-area wireless access.
R5. Is HFC transmission rate dedicated or shared among users? Are collisions
possible in a downstream HFC channel? Why or why not?
R6. List the available residential access technologies in your city. For each type
of access, provide the advertised downstream rate, upstream rate, and
monthly price.
R7. What is the transmission rate of Ethernet LANs?
R8. What are some of the physical media that Ethernet can run over?
R9. Dial-up modems, HFC, DSL and FTTH are all used for residential access.
For each of these access technologies, provide a range of transmission rates
and comment on whether the transmission rate is shared or dedicated.
R10. Describe the most popular wireless Internet access technologies today. Compare
and contrast them.
SECTION 1.3
R11. Suppose there is exactly one packet switch between a sending host and a
receiving host. The transmission rates between the sending host and the
switch and between the switch and the receiving host are R1 and R2, respectively.
Assuming that the switch uses store-and-forward packet switching,
what is the total end-to-end delay to send a packet of length L? (Ignore
queuing, propagation delay, and processing delay.)
R12. What advantage does a circuit-switched network have over a packet-switched
network? What advantages does TDM have over FDM in a circuit-switched
network?
R13. Suppose users share a 2 Mbps link. Also suppose each user transmits continuously
at 1 Mbps when transmitting, but each user transmits only 20 percent of
the time. (See the discussion of statistical multiplexing in Section 1.3.)
68 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used. Why
will there be essentially no queuing delay before the link if two or fewer
users transmit at the same time? Why will there be a queuing delay if
three users transmit at the same time?
c. Find the probability that a given user is transmitting.
d. Suppose now there are three users. Find the probability that at any given
time, all three users are transmitting simultaneously. Find the fraction of
time during which the queue grows.
R14. Why will two ISPs at the same level of the hierarchy often peer with each
other? How does an IXP earn money?
R15. Some content providers have created their own networks. Describe Google’s
network. What motivates content providers to create these networks?
SECTION 1.4
R16. Consider sending a packet from a source host to a destination host over a
fixed route. List the delay components in the end-to-end delay. Which of
these delays are constant and which are variable?
R17. Visit the Transmission Versus Propagation Delay applet at the companion
Web site. Among the rates, propagation delay, and packet sizes available,
find a combination for which the sender finishes transmitting before the first
bit of the packet reaches the receiver. Find another combination for which
the first bit of the packet reaches the receiver before the sender finishes
transmitting.
R18. How long does it take a packet of length 1,000 bytes to propagate over a link
of distance 2,500 km, propagation speed 2.5 · 108 m/s, and transmission rate
2 Mbps? More generally, how long does it take a packet of length L to propagate
over a link of distance d, propagation speed s, and transmission rate R
bps? Does this delay depend on packet length? Does this delay depend on
transmission rate?
R19. Suppose Host A wants to send a large file to Host B. The path from Host A to
Host B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = 1 Mbps.
a. Assuming no other traffic in the network, what is the throughput for the
file transfer?
b. Suppose the file is 4 million bytes. Dividing the file size by the throughput,
roughly how long will it take to transfer the file to Host B?
c. Repeat (a) and (b), but now with R2 reduced to 100 kbps.
R20. Suppose end system A wants to send a large file to end system B. At a very
high level, describe how end system A creates packets from the file. When
HOMEWORK PROBLEMS AND QUESTIONS 69
one of these packets arrives to a packet switch, what information in the
packet does the switch use to determine the link onto which the packet is
forwarded? Why is packet switching in the Internet analogous to driving from
one city to another and asking directions along the way?
R21. Visit the Queuing and Loss applet at the companion Web site. What is the
maximum emission rate and the minimum transmission rate? With those
rates, what is the traffic intensity? Run the applet with these rates and determine
how long it takes for packet loss to occur. Then repeat the experiment a
second time and determine again how long it takes for packet loss to occur.
Are the values different? Why or why not?
SECTION 1.5
R22. List five tasks that a layer can perform. Is it possible that one (or more) of
these tasks could be performed by two (or more) layers?
R23. What are the five layers in the Internet protocol stack? What are the principal
responsibilities of each of these layers?
R24. What is an application-layer message? A transport-layer segment? A networklayer
datagram? A link-layer frame?
R25. Which layers in the Internet protocol stack does a router process? Which
layers does a link-layer switch process? Which layers does a host process?
SECTION 1.6
R26. What is the difference between a virus and a worm?
R27. Describe how a botnet can be created, and how it can be used for a DDoS
attack.
R28. Suppose Alice and Bob are sending packets to each other over a computer
network. Suppose Trudy positions herself in the network so that she can
capture all the packets sent by Alice and send whatever she wants to Bob;
she can also capture all the packets sent by Bob and send whatever she
wants to Alice. List some of the malicious things Trudy can do from this
position.
Problems
P1. Design and describe an application-level protocol to be used between an
automatic teller machine and a bank’s centralized computer. Your protocol
should allow a user’s card and password to be verified, the account balance
(which is maintained at the centralized computer) to be queried, and an
account withdrawal to be made (that is, money disbursed to the user). Your
70 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
protocol entities should be able to handle the all-too-common case in which
there is not enough money in the account to cover the withdrawal. Specify
your protocol by listing the messages exchanged and the action taken by the
automatic teller machine or the bank’s centralized computer on transmission
and receipt of messages. Sketch the operation of your protocol for the case of a
simple withdrawal with no errors, using a diagram similar to that in Figure 1.2.
Explicitly state the assumptions made by your protocol about the underlying
end-to-end transport service.
P2. Equation 1.1 gives a formula for the end-to-end delay of sending one packet
of length L over N links of transmission rate R. Generalize this formula for
sending P such packets back-to-back over the N links.
P3. Consider an application that transmits data at a steady rate (for example, the
sender generates an N-bit unit of data every k time units, where k is small and
fixed). Also, when such an application starts, it will continue running for a
relatively long period of time. Answer the following questions, briefly justifying
your answer:
a. Would a packet-switched network or a circuit-switched network be more
appropriate for this application? Why?
b. Suppose that a packet-switched network is used and the only traffic in
this network comes from such applications as described above. Furthermore,
assume that the sum of the application data rates is less than the
capacities of each and every link. Is some form of congestion control
needed? Why?
P4. Consider the circuit-switched network in Figure 1.13. Recall that there are 4
circuits on each link. Label the four switches A, B, C and D, going in the
clockwise direction.
a. What is the maximum number of simultaneous connections that can be in
progress at any one time in this network?
b. Suppose that all connections are between switches A and C. What is the
maximum number of simultaneous connections that can be in progress?
c. Suppose we want to make four connections between switches A and C,
and another four connections between switches B and D. Can we route
these calls through the four links to accommodate all eight connections?
P5. Review the car-caravan analogy in Section 1.4. Assume a propagation speed
of 100 km/hour.
a. Suppose the caravan travels 150 km, beginning in front of one tollbooth,
passing through a second tollbooth, and finishing just after a third tollbooth.
What is the end-to-end delay?
b. Repeat (a), now assuming that there are eight cars in the caravan instead
of ten.
PROBLEMS 71
P6. This elementary problem begins to explore propagation delay and transmission
delay, two central concepts in data networking. Consider two hosts, A
and B, connected by a single link of rate R bps. Suppose that the two hosts
are separated by m meters, and suppose the propagation speed along the link
is s meters/sec. Host A is to send a packet of size L bits to Host B.
a. Express the propagation delay, dprop, in terms of m and s.
b. Determine the transmission time of the packet, dtrans, in terms of L
and R.
c. Ignoring processing and queuing delays, obtain an expression for the endto-
end delay.
d. Suppose Host Abegins to transmit the packet at time t = 0. At time t = dtrans,
where is the last bit of the packet?
e. Suppose dprop is greater than dtrans. At time t = dtrans, where is the first bit of
the packet?
f. Suppose dprop is less than dtrans. At time t = dtrans, where is the first bit of
the packet?
g. Suppose s = 2.5 · 108, L = 120 bits, and R = 56 kbps. Find the distance m
so that dprop equals dtrans.
P7. In this problem, we consider sending real-time voice from Host A to Host B
over a packet-switched network (VoIP). Host A converts analog voice to a
digital 64 kbps bit stream on the fly. Host A then groups the bits into 56-byte
packets. There is one link between Hosts A and B; its transmission rate is 2
Mbps and its propagation delay is 10 msec. As soon as Host A gathers a
packet, it sends it to Host B. As soon as Host B receives an entire packet, it
converts the packet’s bits to an analog signal. How much time elapses from
the time a bit is created (from the original analog signal at Host A) until the
bit is decoded (as part of the analog signal at Host B)?
P8. Suppose users share a 3 Mbps link. Also suppose each user requires
150 kbps when transmitting, but each user transmits only 10 percent of the
time. (See the discussion of packet switching versus circuit switching in
Section 1.3.)
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used. Find
the probability that a given user is transmitting.
c. Suppose there are 120 users. Find the probability that at any given time,
exactly n users are transmitting simultaneously. (Hint: Use the binomial
distribution.)
d. Find the probability that there are 21 or more users transmitting
simultaneously.
72 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
VideoNote
Exploring propagation
delay and transmission
delay
P9. Consider the discussion in Section 1.3 of packet switching versus circuit
switching in which an example is provided with a 1 Mbps link. Users are
generating data at a rate of 100 kbps when busy, but are busy generating
data only with probability p = 0.1. Suppose that the 1 Mbps link is replaced
by a 1 Gbps link.
a. What is N, the maximum number of users that can be supported
simultaneously under circuit switching?
b. Now consider packet switching and a user population of M users. Give a
formula (in terms of p, M, N) for the probability that more than N users are
sending data.
P10. Consider a packet of length L which begins at end system A and travels over
three links to a destination end system. These three links are connected by
two packet switches. Let di, si, and Ri denote the length, propagation speed,
and the transmission rate of link i, for i = 1, 2, 3. The packet switch delays
each packet by dproc. Assuming no queuing delays, in terms of di, si, Ri,
(i = 1,2,3), and L, what is the total end-to-end delay for the packet? Suppose
now the packet is 1,500 bytes, the propagation speed on all three links is 2.5 ·
108 m/s, the transmission rates of all three links are 2 Mbps, the packet switch
processing delay is 3 msec, the length of the first link is 5,000 km, the length
of the second link is 4,000 km, and the length of the last link is 1,000 km. For
these values, what is the end-to-end delay?
P11. In the above problem, suppose R1 = R2 = R3 = R and dproc = 0. Further suppose
the packet switch does not store-and-forward packets but instead immediately
transmits each bit it receives before waiting for the entire packet to
arrive. What is the end-to-end delay?
P12. A packet switch receives a packet and determines the outbound link to which
the packet should be forwarded. When the packet arrives, one other packet is
halfway done being transmitted on this outbound link and four other packets
are waiting to be transmitted. Packets are transmitted in order of arrival.
Suppose all packets are 1,500 bytes and the link rate is 2 Mbps. What is the
queuing delay for the packet? More generally, what is the queuing delay
when all packets have length L, the transmission rate is R, x bits of the
currently-being-transmitted packet have been transmitted, and n packets are
already in the queue?
P13. (a) Suppose N packets arrive simultaneously to a link at which no packets
are currently being transmitted or queued. Each packet is of length L
and the link has transmission rate R. What is the average queuing delay
for the N packets?
(b) Now suppose that N such packets arrive to the link every LN/R seconds.
What is the average queuing delay of a packet?
PROBLEMS 73
P14. Consider the queuing delay in a router buffer. Let I denote traffic intensity;
that is, I = La/R. Suppose that the queuing delay takes the form IL/R (1 – I)
for I < 1.
a. Provide a formula for the total delay, that is, the queuing delay plus the
transmission delay.
b. Plot the total delay as a function of L/R.
P15. Let a denote the rate of packets arriving at a link in packets/sec, and let µ
denote the link’s transmission rate in packets/sec. Based on the formula for
the total delay (i.e., the queuing delay plus the transmission delay) derived in
the previous problem, derive a formula for the total delay in terms of a and µ.
P16. Consider a router buffer preceding an outbound link. In this problem, you will
use Little’s formula, a famous formula from queuing theory. Let N denote the
average number of packets in the buffer plus the packet being transmitted. Let
a denote the rate of packets arriving at the link. Let d denote the average total
delay (i.e., the queuing delay plus the transmission delay) experienced by a
packet. Little’s formula is N = a · d. Suppose that on average, the buffer contains
10 packets, and the average packet queuing delay is 10 msec. The link’s
transmission rate is 100 packets/sec. Using Little’s formula, what is the average
packet arrival rate, assuming there is no packet loss?
P17. a. Generalize Equation 1.2 in Section 1.4.3 for heterogeneous processing
rates, transmission rates, and propagation delays.
b. Repeat (a), but now also suppose that there is an average queuing delay of
dqueue at each node.
P18. Perform a Traceroute between source and destination on the same continent
at three different hours of the day.
a. Find the average and standard deviation of the round-trip delays at each of
the three hours.
b. Find the number of routers in the path at each of the three hours. Did the
paths change during any of the hours?
c. Try to identify the number of ISP networks that the Traceroute packets pass
through from source to destination. Routers with similar names and/or similar
IP addresses should be considered as part of the same ISP. In your experiments,
do the largest delays occur at the peering interfaces between adjacent ISPs?
d. Repeat the above for a source and destination on different continents.
Compare the intra-continent and inter-continent results.
P19. (a) Visit the site www.traceroute.org and perform traceroutes from two different
cities in France to the same destination host in the United States. How many
links are the same in the two traceroutes? Is the transatlantic link the same?
74 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
VideoNote
Using Traceroute to
discover network
paths and measure
network delay
(b) Repeat (a) but this time choose one city in France and another city in
Germany.
(c) Pick a city in the United States, and perform traceroutes to two hosts, each
in a different city in China. How many links are common in the two
traceroutes? Do the two traceroutes diverge before reaching China?
P20. Consider the throughput example corresponding to Figure 1.20(b). Now
suppose that there are M client-server pairs rather than 10. Denote Rs, Rc, and
R for the rates of the server links, client links, and network link. Assume all
other links have abundant capacity and that there is no other traffic in the
network besides the traffic generated by the M client-server pairs. Derive a
general expression for throughput in terms of Rs, Rc, R, and M.
P21. Consider Figure 1.19(b). Now suppose that there are M paths between the
server and the client. No two paths share any link. Path k (k = 1, . . ., M ) consists
of N links with transmission rates Rk
1, Rk
2, . . ., RkN
. If the server can only
use one path to send data to the client, what is the maximum throughput that
the server can achieve? If the server can use all M paths to send data, what is
the maximum throughput that the server can achieve?
P22. Consider Figure 1.19(b). Suppose that each link between the server and the
client has a packet loss probability p, and the packet loss probabilities for
these links are independent. What is the probability that a packet (sent by the
server) is successfully received by the receiver? If a packet is lost in the path
from the server to the client, then the server will re-transmit the packet. On
average, how many times will the server re-transmit the packet in order for
the client to successfully receive the packet?
P23. Consider Figure 1.19(a). Assume that we know the bottleneck link along the
path from the server to the client is the first link with rate Rs bits/sec. Suppose
we send a pair of packets back to back from the server to the client, and there
is no other traffic on this path. Assume each packet of size L bits, and both
links have the same propagation delay dprop.
a. What is the packet inter-arrival time at the destination? That is, how much
time elapses from when the last bit of the first packet arrives until the last
bit of the second packet arrives?
b. Now assume that the second link is the bottleneck link (i.e., Rc < Rs). Is it
possible that the second packet queues at the input queue of the second
link? Explain. Now suppose that the server sends the second packet T seconds
after sending the first packet. How large must T be to ensure no
queuing before the second link? Explain.
P24. Suppose you would like to urgently deliver 40 terabytes data from Boston to
Los Angeles. You have available a 100 Mbps dedicated link for data transfer.
Would you prefer to transmit the data via this link or instead use FedEx overnight
delivery? Explain.
PROBLEMS 75
P25. Suppose two hosts, A and B, are separated by 20,000 kilometers and are
connected by a direct link of R = 2 Mbps. Suppose the propagation speed
over the link is 2.5  108 meters/sec.
a. Calculate the bandwidth-delay product, R  dprop.
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose
the file is sent continuously as one large message. What is the maximum
number of bits that will be in the link at any given time?
c. Provide an interpretation of the bandwidth-delay product.
d. What is the width (in meters) of a bit in the link? Is it longer than a football
field?
e. Derive a general expression for the width of a bit in terms of the propagation
speed s, the transmission rate R, and the length of the link m.
P26. Referring to problem P25, suppose we can modify R. For what value of R is
the width of a bit as long as the length of the link?
P27. Consider problem P25 but now with a link of R = 1 Gbps.
a. Calculate the bandwidth-delay product, R  dprop.
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose
the file is sent continuously as one big message. What is the maximum
number of bits that will be in the link at any given time?
c. What is the width (in meters) of a bit in the link?
P28. Refer again to problem P25.
a. How long does it take to send the file, assuming it is sent continuously?
b. Suppose now the file is broken up into 20 packets with each packet containing
40,000 bits. Suppose that each packet is acknowledged by the
receiver and the transmission time of an acknowledgment packet is
negligible. Finally, assume that the sender cannot send a packet until the
preceding one is acknowledged. How long does it take to send the file?
c. Compare the results from (a) and (b).
P29. Suppose there is a 10 Mbps microwave link between a geostationary satellite
and its base station on Earth. Every minute the satellite takes a digital photo and
sends it to the base station. Assume a propagation speed of 2.4  108 meters/sec.
a. What is the propagation delay of the link?
b. What is the bandwidth-delay product, R · dprop?
c. Let x denote the size of the photo. What is the minimum value of x for the
microwave link to be continuously transmitting?
P30. Consider the airline travel analogy in our discussion of layering in Section
1.5, and the addition of headers to protocol data units as they flow down
76 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
the protocol stack. Is there an equivalent notion of header information that
is added to passengers and baggage as they move down the airline protocol
stack?
P31. In modern packet-switched networks, including the Internet, the source host
segments long, application-layer messages (for example, an image or a music
file) into smaller packets and sends the packets into the network. The receiver
then reassembles the packets back into the original message. We refer to this
process as message segmentation. Figure 1.27 illustrates the end-to-end
transport of a message with and without message segmentation. Consider a
message that is 8 · 106 bits long that is to be sent from source to destination in
Figure 1.27. Suppose each link in the figure is 2 Mbps. Ignore propagation,
queuing, and processing delays.
a. Consider sending the message from source to destination without message
segmentation. How long does it take to move the message from the source
host to the first packet switch? Keeping in mind that each switch uses
store-and-forward packet switching, what is the total time to move the
message from source host to destination host?
b. Now suppose that the message is segmented into 800 packets, with each
packet being 10,000 bits long. How long does it take to move the first
packet from source host to the first switch? When the first packet is being
sent from the first switch to the second switch, the second packet is being
sent from the source host to the first switch. At what time will the second
packet be fully received at the first switch?
c. How long does it take to move the file from source host to destination host
when message segmentation is used? Compare this result with your
answer in part (a) and comment.
PROBLEMS 77
a. Source Packet switch Packet switch Destination
Message
b. Source Packet switch
Packet
Packet switch Destination
Figure 1.27  End-to-end message transport: (a) without message
segmentation; (b) with message segmentation
d. In addition to reducing delay, what are reasons to use message segmentation?
e. Discuss the drawbacks of message segmentation.
P32. Experiment with the Message Segmentation applet at the book’s Web site. Do
the delays in the applet correspond to the delays in the previous problem?
How do link propagation delays affect the overall end-to-end delay for packet
switching (with message segmentation) and for message switching?
P33. Consider sending a large file of F bits from Host Ato Host B. There are three
links (and two switches) between Aand B, and the links are uncongested (that
is, no queuing delays). Host Asegments the file into segments of S bits each and
adds 80 bits of header to each segment, forming packets of L = 80 + S bits. Each
link has a transmission rate of R bps. Find the value of S that minimizes the
delay of moving the file from Host Ato Host B. Disregard propagation delay.
P34. Skype offers a service that allows you to make a phone call from a PC to an
ordinary phone. This means that the voice call must pass through both the
Internet and through a telephone network. Discuss how this might be done.
Wireshark Lab
“Tell me and I forget. Show me and I remember. Involve me and I understand.”
Chinese proverb
One’s understanding of network protocols can often be greatly deepened by seeing
them in action and by playing around with them—observing the sequence of messages
exchanged between two protocol entities, delving into the details of protocol
operation, causing protocols to perform certain actions, and observing these actions
and their consequences. This can be done in simulated scenarios or in a real network
environment such as the Internet. The Java applets at the textbook Web site take the
first approach. In the Wireshark labs, we’ll take the latter approach. You’ll run network
applications in various scenarios using a computer on your desk, at home, or
in a lab. You’ll observe the network protocols in your computer, interacting and
exchanging messages with protocol entities executing elsewhere in the Internet.
Thus, you and your computer will be an integral part of these live labs. You’ll
observe—and you’ll learn—by doing.
The basic tool for observing the messages exchanged between executing
protocol entities is called a packet sniffer. As the name suggests, a packet sniffer
passively copies (sniffs) messages being sent from and received by your computer;
it also displays the contents of the various protocol fields of these captured messages.
A screenshot of the Wireshark packet sniffer is shown in Figure 1.28. Wireshark
is a free packet sniffer that runs on Windows, Linux/Unix, and Mac
78 CHAPTER 1 • COMPUTER NETWORKS AND THE INTERNET
WIRESHARK LAB 79
Command
menus
Listing of
captured
packets
Details of
selected
packet
header
Packet
contents in
hexadecimal
and ASCII
Figure 1.28  A Wireshark screen shot (Wireshark screenshot reprinted
by permission of the Wireshark Foundation.)
computers. Throughout the textbook, you will find Wireshark labs that allow you to
explore a number of the protocols studied in the chapter. In this first Wireshark lab,
you’ll obtain and install a copy of Wireshark, access a Web site, and capture and
examine the protocol messages being exchanged between your Web browser and the
Web server.
You can find full details about this first Wireshark lab (including instructions
about how to obtain and install Wireshark) at the Web site http://www.awl.com/
kurose-ross.
80
Leonard Kleinrock
Leonard Kleinrock is a professor of computer science at the University
of California, Los Angeles. In 1969, his computer at UCLA became
the first node of the Internet. His creation of packet-switching principles
in 1961 became the technology behind the Internet. He
received his B.E.E. from the City College of New York (CCNY) and
his masters and PhD in electrical engineering from MIT.
AN INTERVIEW WITH...
What made you decide to specialize in networking/Internet technology?
As a PhD student at MIT in 1959, I looked around and found that most of my classmates
were doing research in the area of information theory and coding theory. At MIT, there was
the great researcher, Claude Shannon, who had launched these fields and had solved most of
the important problems already. The research problems that were left were hard and of lesser
consequence. So I decided to launch out in a new area that no one else had yet conceived
of. Remember that at MIT I was surrounded by lots of computers, and it was clear to me
that soon these machines would need to communicate with each other. At the time, there
was no effective way for them to do so, so I decided to develop the technology that would
permit efficient and reliable data networks to be created.
What was your first job in the computer industry? What did it entail?
I went to the evening session at CCNY from 1951 to 1957 for my bachelor’s degree in
electrical engineering. During the day, I worked first as a technician and then as an engineer
at a small, industrial electronics firm called Photobell. While there, I introduced
digital technology to their product line. Essentially, we were using photoelectric devices
to detect the presence of certain items (boxes, people, etc.) and the use of a circuit
known then as a bistable multivibrator was just the kind of technology we needed to
bring digital processing into this field of detection. These circuits happen to be the building
blocks for computers, and have come to be known as flip-flops or switches in today’s
vernacular.
What was going through your mind when you sent the first host-to-host message (from
UCLA to the Stanford Research Institute)?
Frankly, we had no idea of the importance of that event. We had not prepared a special message
of historic significance, as did so many inventors of the past (Samuel Morse with “What
hath God wrought.” or Alexander Graham Bell with “Watson, come here! I want you.” or
Neal Amstrong with “That’s one small step for a man, one giant leap for mankind.”) Those
guys were smart! They understood media and public relations. All we wanted to do was to
login to the SRI computer. So we typed the “L”, which was correctly received, we typed the
“o” which was received, and then we typed the “g” which caused the SRI host computer to
crash! So, it turned out that our message was the shortest and perhaps the most prophetic
message ever, namely “Lo!” as in “Lo and behold!”
Earlier that year, I was quoted in a UCLA press release saying that once the network was
up and running, it would be possible to gain access to computer utilities from our homes and
offices as easily as we gain access to electricity and telephone connectivity. So my vision at
that time was that the Internet would be ubiquitous, always on, always available, anyone with
any device could connect from any location, and it would be invisible. However, I never
anticipated that my 99-year-old mother would use the Internet—and indeed she did!
What is your vision for the future of networking?
The easy part of the vision is to predict the infrastructure itself. I anticipate that we see considerable
deployment of nomadic computing, mobile devices, and smart spaces. Indeed, the
availability of lightweight, inexpensive, high-performance, portable computing, and communication
devices (plus the ubiquity of the Internet) has enabled us to become nomads.
Nomadic computing refers to the technology that enables end users who travel from place to
place to gain access to Internet services in a transparent fashion, no matter where they travel
and no matter what device they carry or gain access to. The harder part of the vision is to
predict the applications and services, which have consistently surprised us in dramatic ways
(email, search technologies, the world-wide-web, blogs, social networks, user generation, and
sharing of music, photos, and videos, etc.). We are on the verge of a new class of surprising
and innovative mobile applications delivered to our hand-held devices.
The next step will enable us to move out from the netherworld of cyberspace to the
physical world of smart spaces. Our environments (desks, walls, vehicles, watches, belts, and
so on) will come alive with technology, through actuators, sensors, logic, processing, storage,
cameras, microphones, speakers, displays, and communication. This embedded technology
will allow our environment to provide the IP services we want. When I walk into a room, the
room will know I entered. I will be able to communicate with my environment naturally, as
in spoken English; my requests will generate replies that present Web pages to me from wall
displays, through my eyeglasses, as speech, holograms, and so forth.
Looking a bit further out, I see a networking future that includes the following additional
key components. I see intelligent software agents deployed across the network
whose function it is to mine data, act on that data, observe trends, and carry out tasks
dynamically and adaptively. I see considerably more network traffic generated not so much
by humans, but by these embedded devices and these intelligent software agents. I see
large collections of self-organizing systems controlling this vast, fast network. I see huge
amounts of information flashing across this network instantaneously with this information
undergoing enormous processing and filtering. The Internet will essentially be a pervasive
global nervous system. I see all these things and more as we move headlong through the
twenty-first century.
81
What people have inspired you professionally?
By far, it was Claude Shannon from MIT, a brilliant researcher who had the ability to relate
his mathematical ideas to the physical world in highly intuitive ways. He was on my PhD
thesis committee.
Do you have any advice for students entering the networking/Internet field?
The Internet and all that it enables is a vast new frontier, full of amazing challenges. There
is room for great innovation. Don’t be constrained by today’s technology. Reach out and
imagine what could be and then make it happen.
82
CHAPTER 2
Application
Layer
83
Network applications are the raisons d’être of a computer network—if we couldn’t
conceive of any useful applications, there wouldn’t be any need for networking protocols
that support these applications. Since the Internet’s inception, numerous useful and
entertaining applications have indeed been created. These applications have been the
driving force behind the Internet’s success, motivating people in homes, schools, governments,
and businesses to make the Internet an integral part of their daily activities.
Internet applications include the classic text-based applications that became
popular in the 1970s and 1980s: text email, remote access to computers, file transfers,
and newsgroups. They include the killer application of the mid-1990s, the
World Wide Web, encompassing Web surfing, search, and electronic commerce.
They include instant messaging and P2P file sharing, the two killer applications
introduced at the end of the millennium. Since 2000, we have seen an explosion of
popular voice and video applications, including: voice-over-IP (VoIP) and video
conferencing over IP such as Skype; user-generated video distribution such as
YouTube; and movies on demand such as Netflix. During this same period we have
also seen the immergence of highly engaging multi-player online games, including
Second Life and World of Warcraft. And most recently, we have seen the emergence
of a new generation of social networking applications, such as Facebook and Twitter,
which have created engaging human networks on top of the Internet’s network of
routers and communication links. Clearly, there has been no slowing down of new
and exciting Internet applications. Perhaps some of the readers of this text will create
the next generation of killer Internet applications!
In this chapter we study the conceptual and implementation aspects of network
applications. We begin by defining key application-layer concepts, including network
services required by applications, clients and servers, processes, and transport-layer
interfaces. We examine several network applications in detail, including the Web,
e-mail, DNS, and peer-to-peer (P2P) file distribution (Chapter 8 focuses on multimedia
applications, including streaming video and VoIP). We then cover network application
development, over both TCP and UDP. In particular, we study the socket API
and walk through some simple client-server applications in Python. We also provide
several fun and interesting socket programming assignments at the end of the chapter.
The application layer is a particularly good place to start our study of protocols.
It’s familiar ground. We’re acquainted with many of the applications that rely on the
protocols we’ll study. It will give us a good feel for what protocols are all about and
will introduce us to many of the same issues that we’ll see again when we study transport,
network, and link layer protocols.
2.1 Principles of Network Applications
Suppose you have an idea for a new network application. Perhaps this application
will be a great service to humanity, or will please your professor, or will bring you
great wealth, or will simply be fun to develop. Whatever the motivation may be, let’s
now examine how you transform the idea into a real-world network application.
At the core of network application development is writing programs that run on
different end systems and communicate with each other over the network. For
example, in the Web application there are two distinct programs that communicate
with each other: the browser program running in the user’s host (desktop, laptop,
tablet, smartphone, and so on); and the Web server program running in the Web
server host. As another example, in a P2P file-sharing system there is a program in
each host that participates in the file-sharing community. In this case, the programs
in the various hosts may be similar or identical.
Thus, when developing your new application, you need to write software that
will run on multiple end systems. This software could be written, for example, in C,
Java, or Python. Importantly, you do not need to write software that runs on networkcore
devices, such as routers or link-layer switches. Even if you wanted to write
application software for these network-core devices, you wouldn’t be able to do so.
As we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core
devices do not function at the application layer but instead function at lower layers—
specifically at the network layer and below. This basic design—namely, confining
application software to the end systems—as shown in Figure 2.1, has facilitated the
rapid development and deployment of a vast array of network applications.
84 CHAPTER 2 • APPLICATION LAYER
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 85
Mobile Network
Transport
Network
Link
Physical
Application
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Home Network
Transport
Network
Link
Application
Physical
Transport
Network
Link
Physical
Application
Figure 2.1  Communication for a network application takes place
between end systems at the application layer
2.1.1 Network Application Architectures
Before diving into software coding, you should have a broad architectural plan for
your application. Keep in mind that an application’s architecture is distinctly different
from the network architecture (e.g., the five-layer Internet architecture discussed
in Chapter 1). From the application developer’s perspective, the network architecture
is fixed and provides a specific set of services to applications. The application
architecture, on the other hand, is designed by the application developer and dictates
how the application is structured over the various end systems. In choosing the
application architecture, an application developer will likely draw on one of the two
predominant architectural paradigms used in modern network applications: the
client-server architecture or the peer-to-peer (P2P) architecture
In a client-server architecture, there is an always-on host, called the server,
which services requests from many other hosts, called clients. Aclassic example is the
Web application for which an always-on Web server services requests from browsers
running on client hosts. When a Web server receives a request for an object from a
client host, it responds by sending the requested object to the client host. Note that
with the client-server architecture, clients do not directly communicate with each
other; for example, in the Web application, two browsers do not directly communicate.
Another characteristic of the client-server architecture is that the server has a
fixed, well-known address, called an IP address (which we’ll discuss soon). Because
the server has a fixed, well-known address, and because the server is always on, a
client can always contact the server by sending a packet to the server’s IP address.
Some of the better-known applications with a client-server architecture include the
Web, FTP, Telnet, and e-mail. The client-server architecture is shown in Figure 2.2(a).
Often in a client-server application, a single-server host is incapable of keeping up
with all the requests from clients. For example, a popular social-networking site can
quickly become overwhelmed if it has only one server handling all of its requests. For
this reason, a data center, housing a large number of hosts, is often used to create a
powerful virtual server. The most popular Internet services—such as search engines
(e.g., Google and Bing), Internet commerce (e.g., Amazon and e-Bay), Web-based
email (e.g., Gmail and Yahoo Mail), social networking (e.g., Facebook and Twitter)—
employ one or more data centers. As discussed in Section 1.3.3, Google has 30 to 50
data centers distributed around the world, which collectively handle search, YouTube,
Gmail, and other services. A data center can have hundreds of thousands of servers,
which must be powered and maintained. Additionally, the service providers must pay
recurring interconnection and bandwidth costs for sending data from their data centers.
In a P2P architecture, there is minimal (or no) reliance on dedicated servers in
data centers. Instead the application exploits direct communication between pairs of
intermittently connected hosts, called peers. The peers are not owned by the service
provider, but are instead desktops and laptops controlled by users, with most of the
peers residing in homes, universities, and offices. Because the peers communicate
without passing through a dedicated server, the architecture is called peer-to-peer.
Many of today’s most popular and traffic-intensive applications are based on P2P
architectures. These applications include file sharing (e.g., BitTorrent), peer-assisted
86 CHAPTER 2 • APPLICATION LAYER
download acceleration (e.g., Xunlei), Internet Telephony (e.g., Skype), and IPTV (e.g.,
Kankan and PPstream). The P2P architecture is illustrated in Figure 2.2(b). We mention
that some applications have hybrid architectures, combining both client-server
and P2P elements. For example, for many instant messaging applications, servers are
used to track the IP addresses of users, but user-to-user messages are sent directly
between user hosts (without passing through intermediate servers).
One of the most compelling features of P2P architectures is their self-scalability.
For example, in a P2P file-sharing application, although each peer generates
workload by requesting files, each peer also adds service capacity to the system
by distributing files to other peers. P2P architectures are also cost effective, since
they normally don’t require significant server infrastructure and server bandwidth
(in contrast with clients-server designs with datacenters). However, future P2P
applications face three major challenges:
1. ISP Friendly. Most residential ISPs (including DSL and cable ISPs) have been
dimensioned for “asymmetrical” bandwidth usage, that is, for much more
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 87
Figure 2.2  (a) Client-server architecture; (b) P2P architecture
a. Client-server architecture b. Peer-to-peer architecture
downstream than upstream traffic. But P2P video streaming and file distribution
applications shift upstream traffic from servers to residential ISPs, thereby
putting significant stress on the ISPs. Future P2P applications need to be
designed so that they are friendly to ISPs [Xie 2008].
2. Security. Because of their highly distributed and open nature, P2P applications
can be a challenge to secure [Doucer 2002; Yu 2006; Liang 2006; Naoumov
2006; Dhungel 2008; LeBlond 2011].
3. Incentives. The success of future P2P applications also depends on convincing
users to volunteer bandwidth, storage, and computation resources to the applications,
which is the challenge of incentive design [Feldman 2005; Piatek
2008; Aperjis 2008; Liu 2010].
2.1.2 Processes Communicating
Before building your network application, you also need a basic understanding of
how the programs, running in multiple end systems, communicate with each other.
In the jargon of operating systems, it is not actually programs but processes that
communicate. A process can be thought of as a program that is running within an
end system. When processes are running on the same end system, they can communicate
with each other with interprocess communication, using rules that are
governed by the end system’s operating system. But in this book we are not particularly
interested in how processes in the same host communicate, but instead in
how processes running on different hosts (with potentially different operating systems)
communicate.
Processes on two different end systems communicate with each other by exchanging
messages across the computer network. A sending process creates and sends messages
into the network; a receiving process receives these messages and possibly
responds by sending messages back. Figure 2.1 illustrates that processes communicating
with each other reside in the application layer of the five-layer protocol stack.
Client and Server Processes
A network application consists of pairs of processes that send messages to each
other over a network. For example, in the Web application a client browser
process exchanges messages with a Web server process. In a P2P file-sharing system,
a file is transferred from a process in one peer to a process in another peer.
For each pair of communicating processes, we typically label one of the two
processes as the client and the other process as the server. With the Web, a
browser is a client process and a Web server is a server process. With P2P file
sharing, the peer that is downloading the file is labeled as the client, and the peer
that is uploading the file is labeled as the server.
You may have observed that in some applications, such as in P2P file sharing, a
process can be both a client and a server. Indeed, a process in a P2P file-sharing system
can both upload and download files. Nevertheless, in the context of any given
88 CHAPTER 2 • APPLICATION LAYER
communication session between a pair of processes, we can still label one process
as the client and the other process as the server. We define the client and server
processes as follows:
In the context of a communication session between a pair of processes, the
process that initiates the communication (that is, initially contacts the other
process at the beginning of the session) is labeled as the client. The process
that waits to be contacted to begin the session is the server.
In the Web, a browser process initializes contact with a Web server process;
hence the browser process is the client and the Web server process is the server. In
P2P file sharing, when Peer A asks Peer B to send a specific file, Peer A is the client
and Peer B is the server in the context of this specific communication session. When
there’s no confusion, we’ll sometimes also use the terminology “client side and
server side of an application.” At the end of this chapter, we’ll step through simple
code for both the client and server sides of network applications.
The Interface Between the Process and the Computer Network
As noted above, most applications consist of pairs of communicating processes,
with the two processes in each pair sending messages to each other. Any message
sent from one process to another must go through the underlying network. A process
sends messages into, and receives messages from, the network through a software
interface called a socket. Let’s consider an analogy to help us understand processes
and sockets. Aprocess is analogous to a house and its socket is analogous to its door.
When a process wants to send a message to another process on another host, it
shoves the message out its door (socket). This sending process assumes that there is
a transportation infrastructure on the other side of its door that will transport the
message to the door of the destination process. Once the message arrives at the destination
host, the message passes through the receiving process’s door (socket), and
the receiving process then acts on the message
Figure 2.3 illustrates socket communication between two processes that communicate
over the Internet. (Figure 2.3 assumes that the underlying transport protocol
used by the processes is the Internet’s TCP protocol.) As shown in this
figure, a socket is the interface between the application layer and the transport
layer within a host. It is also referred to as the Application Programming Interface
(API) between the application and the network, since the socket is the programming
interface with which network applications are built. The application
developer has control of everything on the application-layer side of the socket but
has little control of the transport-layer side of the socket. The only control that the
application developer has on the transport-layer side is (1) the choice of transport
protocol and (2) perhaps the ability to fix a few transport-layer parameters such as
maximum buffer and maximum segment sizes (to be covered in Chapter 3). Once
the application developer chooses a transport protocol (if a choice is available),
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 89
the application is built using the transport-layer services provided by that protocol.
We’ll explore sockets in some detail in Section 2.7.
Addressing Processes
In order to send postal mail to a particular destination, the destination needs to have
an address. Similarly, in order for a process running on one host to send packets to a
process running on another host, the receiving process needs to have an address.
To identify the receiving process, two pieces of information need to be specified:
(1) the address of the host and (2) an identifier that specifies the receiving process
in the destination host.
In the Internet, the host is identified by its IP address. We’ll discuss IP
addresses in great detail in Chapter 4. For now, all we need to know is that an IP
address is a 32-bit quantity that we can think of as uniquely identifying the host.
In addition to knowing the address of the host to which a message is destined, the
sending process must also identify the receiving process (more specifically, the
receiving socket) running in the host. This information is needed because in general
a host could be running many network applications. A destination port number
serves this purpose. Popular applications have been assigned specific port
numbers. For example, a Web server is identified by port number 80. A mail
server process (using the SMTP protocol) is identified by port number 25. A list
of well-known port numbers for all Internet standard protocols can be found at
http://www.iana.org. We’ll examine port numbers in detail in Chapter 3.
90 CHAPTER 2 • APPLICATION LAYER
Process
Host or
server
Host or
server
Controlled
by application
developer
Controlled
by application
developer
Process
TCP with
buffers,
variables Internet
Controlled
by operating
system
Controlled
by operating
system
TCP with
buffers,
variables
Socket Socket
Figure 2.3  Application processes, sockets, and underlying transport protocol
2.1.3 Transport Services Available to Applications
Recall that a socket is the interface between the application process and the
transport-layer protocol. The application at the sending side pushes messages
through the socket. At the other side of the socket, the transport-layer protocol
has the responsibility of getting the messages to the socket of the receiving
process.
Many networks, including the Internet, provide more than one transport-layer
protocol. When you develop an application, you must choose one of the available
transport-layer protocols. How do you make this choice? Most likely, you would
study the services provided by the available transport-layer protocols, and then pick
the protocol with the services that best match your application’s needs. The situation
is similar to choosing either train or airplane transport for travel between two
cities. You have to choose one or the other, and each transportation mode offers different
services. (For example, the train offers downtown pickup and drop-off,
whereas the plane offers shorter travel time.)
What are the services that a transport-layer protocol can offer to applications
invoking it? We can broadly classify the possible services along four dimensions:
reliable data transfer, throughput, timing, and security.
Reliable Data Transfer
As discussed in Chapter 1, packets can get lost within a computer network. For
example, a packet can overflow a buffer in a router, or can be discarded by a host
or router after having some of its bits corrupted. For many applications—such as
electronic mail, file transfer, remote host access, Web document transfers, and
financial applications—data loss can have devastating consequences (in the latter
case, for either the bank or the customer!). Thus, to support these applications,
something has to be done to guarantee that the data sent by one end of the application
is delivered correctly and completely to the other end of the application. If
a protocol provides such a guaranteed data delivery service, it is said to provide
reliable data transfer. One important service that a transport-layer protocol can
potentially provide to an application is process-to-process reliable data transfer.
When a transport protocol provides this service, the sending process can just pass
its data into the socket and know with complete confidence that the data will
arrive without errors at the receiving process.
When a transport-layer protocol doesn’t provide reliable data transfer, some of
the data sent by the sending process may never arrive at the receiving process. This
may be acceptable for loss-tolerant applications, most notably multimedia applications
such as conversational audio/video that can tolerate some amount of data loss.
In these multimedia applications, lost data might result in a small glitch in the
audio/video—not a crucial impairment.
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 91
Throughput
In Chapter 1 we introduced the concept of available throughput, which, in the context
of a communication session between two processes along a network path, is
the rate at which the sending process can deliver bits to the receiving process.
Because other sessions will be sharing the bandwidth along the network path, and
because these other sessions will be coming and going, the available throughput
can fluctuate with time. These observations lead to another natural service that a
transport-layer protocol could provide, namely, guaranteed available throughput
at some specified rate. With such a service, the application could request a guaranteed
throughput of r bits/sec, and the transport protocol would then ensure that
the available throughput is always at least r bits/sec. Such a guaranteed throughput
service would appeal to many applications. For example, if an Internet telephony
application encodes voice at 32 kbps, it needs to send data into the network
and have data delivered to the receiving application at this rate. If the transport
protocol cannot provide this throughput, the application would need to encode at
a lower rate (and receive enough throughput to sustain this lower coding rate) or
may have to give up, since receiving, say, half of the needed throughput is of little
or no use to this Internet telephony application. Applications that have throughput
requirements are said to be bandwidth-sensitive applications. Many current
multimedia applications are bandwidth sensitive, although some multimedia
applications may use adaptive coding techniques to encode digitized voice or
video at a rate that matches the currently available throughput.
While bandwidth-sensitive applications have specific throughput requirements,
elastic applications can make use of as much, or as little, throughput
as happens to be available. Electronic mail, file transfer, and Web transfers are
all elastic applications. Of course, the more throughput, the better. There’s
an adage that says that one cannot be too rich, too thin, or have too much
throughput!
Timing
A transport-layer protocol can also provide timing guarantees. As with throughput
guarantees, timing guarantees can come in many shapes and forms. An example
guarantee might be that every bit that the sender pumps into the socket arrives at the
receiver’s socket no more than 100 msec later. Such a service would be appealing to
interactive real-time applications, such as Internet telephony, virtual environments,
teleconferencing, and multiplayer games, all of which require tight timing constraints
on data delivery in order to be effective. (See Chapter 7, [Gauthier 1999;
Ramjee 1994].) Long delays in Internet telephony, for example, tend to result in
unnatural pauses in the conversation; in a multiplayer game or virtual interactive
environment, a long delay between taking an action and seeing the response from
the environment (for example, from another player at the end of an end-to-end connection)
makes the application feel less realistic. For non-real-time applications,
92 CHAPTER 2 • APPLICATION LAYER
lower delay is always preferable to higher delay, but no tight constraint is placed on
the end-to-end delays.
Security
Finally, a transport protocol can provide an application with one or more security
services. For example, in the sending host, a transport protocol can encrypt all data
transmitted by the sending process, and in the receiving host, the transport-layer
protocol can decrypt the data before delivering the data to the receiving process.
Such a service would provide confidentiality between the two processes, even if the
data is somehow observed between sending and receiving processes. A transport
protocol can also provide other security services in addition to confidentiality,
including data integrity and end-point authentication, topics that we’ll cover in
detail in Chapter 8.
2.1.4 Transport Services Provided by the Internet
Up until this point, we have been considering transport services that a computer
network could provide in general. Let’s now get more specific and examine the
type of transport services provided by the Internet. The Internet (and, more generally,
TCP/IP networks) makes two transport protocols available to applications,
UDP and TCP. When you (as an application developer) create a new network
application for the Internet, one of the first decisions you have to make is
whether to use UDP or TCP. Each of these protocols offers a different set of services
to the invoking applications. Figure 2.4 shows the service requirements for
some selected applications.
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 93
Application Data Loss Throughput Time-Sensitive
File transfer/download No loss Elastic No
E-mail No loss Elastic No
Web documents No loss Elastic (few kbps) No
Figure 2.4  Requirements of selected network applications
Internet telephony/
Video conferencing
Loss-tolerant Audio: few kbps–1Mbps
Video: 10 kbps–5 Mbps
Yes: 100s of msec
Streaming stored Loss-tolerant Same as above Yes: few seconds
audio/video
Interactive games Loss-tolerant Few kbps–10 kbps Yes: 100s of msec
Instant messaging No loss Elastic Yes and no
TCP Services
The TCP service model includes a connection-oriented service and a reliable data
transfer service. When an application invokes TCP as its transport protocol, the
application receives both of these services from TCP.
• Connection-oriented service. TCP has the client and server exchange transportlayer
control information with each other before the application-level messages
begin to flow. This so-called handshaking procedure alerts the client and server,
allowing them to prepare for an onslaught of packets. After the handshaking phase,
a TCP connection is said to exist between the sockets of the two processes. The
connection is a full-duplex connection in that the two processes can send messages
to each other over the connection at the same time. When the application finishes
sending messages, it must tear down the connection. In Chapter 3 we’ll discuss
connection-oriented service in detail and examine how it is implemented.
94 CHAPTER 2 • APPLICATION LAYER
SECURING TCP
Neither TCP nor UDP provide any encryption—the data that the sending process passes
into its socket is the same data that travels over the network to the destination
process. So, for example, if the sending process sends a password in cleartext (i.e.,
unencrypted) into its socket, the cleartext password will travel over all the links between
sender and receiver, potentially getting sniffed and discovered at any of the intervening
links. Because privacy and other security issues have become critical for many applications,
the Internet community has developed an enhancement for TCP, called Secure
Sockets Layer (SSL). TCP-enhanced-with-SSL not only does everything that traditional
TCP does but also provides critical process-to-process security services, including
encryption, data integrity, and end-point authentication. We emphasize that SSL is not
a third Internet transport protocol, on the same level as TCP and UDP, but instead is an
enhancement of TCP, with the enhancements being implemented in the application
layer. In particular, if an application wants to use the services of SSL, it needs to
include SSL code (existing, highly optimized libraries and classes) in both the client and
server sides of the application. SSL has its own socket API that is similar to the traditional
TCP socket API. When an application uses SSL, the sending process passes cleartext
data to the SSL socket; SSL in the sending host then encrypts the data and passes the
encrypted data to the TCP socket. The encrypted data travels over the Internet to the
TCP socket in the receiving process. The receiving socket passes the encrypted data to
SSL, which decrypts the data. Finally, SSL passes the cleartext data through its SSL
socket to the receiving process. We’ll cover SSL in some detail in Chapter 8.
FOCUS ON SECURITY
• Reliable data transfer service. The communicating processes can rely on TCP
to deliver all data sent without error and in the proper order. When one side of
the application passes a stream of bytes into a socket, it can count on TCP to
deliver the same stream of bytes to the receiving socket, with no missing or
duplicate bytes.
TCP also includes a congestion-control mechanism, a service for the general
welfare of the Internet rather than for the direct benefit of the communicating
processes. The TCP congestion-control mechanism throttles a sending process (client
or server) when the network is congested between sender and receiver. As we will
see in Chapter 3, TCP congestion control also attempts to limit each TCP connection
to its fair share of network bandwidth.
UDP Services
UDP is a no-frills, lightweight transport protocol, providing minimal services. UDP
is connectionless, so there is no handshaking before the two processes start to
communicate. UDP provides an unreliable data transfer service—that is, when a process
sends a message into a UDP socket, UDP provides no guarantee that the message
will ever reach the receiving process. Furthermore, messages that do arrive at the
receiving process may arrive out of order.
UDP does not include a congestion-control mechanism, so the sending side of
UDP can pump data into the layer below (the network layer) at any rate it pleases.
(Note, however, that the actual end-to-end throughput may be less than this rate due
to the limited transmission capacity of intervening links or due to congestion).
Services Not Provided by Internet Transport Protocols
We have organized transport protocol services along four dimensions: reliable data
transfer, throughput, timing, and security. Which of these services are provided by
TCP and UDP? We have already noted that TCP provides reliable end-to-end data
transfer. And we also know that TCP can be easily enhanced at the application layer
with SSL to provide security services. But in our brief description of TCP and UDP,
conspicuously missing was any mention of throughput or timing guarantees—services
not provided by today’s Internet transport protocols. Does this mean that timesensitive
applications such as Internet telephony cannot run in today’s Internet? The
answer is clearly no—the Internet has been hosting time-sensitive applications for
many years. These applications often work fairly well because they have been
designed to cope, to the greatest extent possible, with this lack of guarantee. We’ll
investigate several of these design tricks in Chapter 7. Nevertheless, clever design
has its limitations when delay is excessive, or the end-to-end throughput is limited.
In summary, today’s Internet can often provide satisfactory service to time-sensitive
applications, but it cannot provide any timing or throughput guarantees.
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 95
Figure 2.5 indicates the transport protocols used by some popular Internet
applications. We see that e-mail, remote terminal access, the Web, and file transfer
all use TCP. These applications have chosen TCP primarily because TCP provides
reliable data transfer, guaranteeing that all data will eventually get to its
destination. Because Internet telephony applications (such as Skype) can often
tolerate some loss but require a minimal rate to be effective, developers of Internet
telephony applications usually prefer to run their applications over UDP,
thereby circumventing TCP’s congestion control mechanism and packet overheads.
But because many firewalls are configured to block (most types of) UDP
traffic, Internet telephony applications often are designed to use TCP as a backup
if UDP communication fails.
2.1.5 Application-Layer Protocols
We have just learned that network processes communicate with each other by sending
messages into sockets. But how are these messages structured? What are the
meanings of the various fields in the messages? When do the processes send the messages?
These questions bring us into the realm of application-layer protocols. An
application-layer protocol defines how an application’s processes, running on different
end systems, pass messages to each other. In particular, an application-layer
protocol defines:
• The types of messages exchanged, for example, request messages and response
messages
• The syntax of the various message types, such as the fields in the message and
how the fields are delineated
96 CHAPTER 2 • APPLICATION LAYER
Application Application-Layer Protocol Underlying Transport Protocol
Electronic mail SMTP [RFC 5321] TCP
Remote terminal access Telnet [RFC 854] TCP
Web HTTP [RFC 2616] TCP
File transfer FTP [RFC 959] TCP
Streaming multimedia HTTP (e.g., YouTube) TCP
Internet telephony SIP [RFC 3261], RTP [RFC 3550], or proprietary UDP or TCP
(e.g., Skype)
Figure 2.5  Popular Internet applications, their application-layer
protocols, and their underlying transport protocols
• The semantics of the fields, that is, the meaning of the information in the fields
• Rules for determining when and how a process sends messages and responds to
messages
Some application-layer protocols are specified in RFCs and are therefore in the public
domain. For example, the Web’s application-layer protocol, HTTP (the HyperText
Transfer Protocol [RFC 2616]), is available as an RFC. If a browser developer follows
the rules of the HTTP RFC, the browser will be able to retrieve Web pages from
any Web server that has also followed the rules of the HTTP RFC. Many other
application-layer protocols are proprietary and intentionally not available in the public
domain. For example, Skype uses proprietary application-layer protocols.
It is important to distinguish between network applications and application-layer
protocols. An application-layer protocol is only one piece of a network application
(albeit, a very important piece of the application from our point of view!). Let’s look at
a couple of examples. The Web is a client-server application that allows users to
obtain documents from Web servers on demand. The Web application consists of
many components, including a standard for document formats (that is, HTML), Web
browsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for
example, Apache and Microsoft servers), and an application-layer protocol. The
Web’s application-layer protocol, HTTP, defines the format and sequence of messages
exchanged between browser and Web server. Thus, HTTP is only one piece (albeit, an
important piece) of the Web application. As another example, an Internet e-mail application
also has many components, including mail servers that house user mailboxes;
mail clients (such as Microsoft Outlook) that allow users to read and create messages; a
standard for defining the structure of an e-mail message; and application-layer protocols
that define how messages are passed between servers, how messages are passed
between servers and mail clients, and how the contents of message headers are to be
interpreted. The principal application-layer protocol for electronic mail is SMTP
(Simple Mail Transfer Protocol) [RFC 5321]. Thus, e-mail’s principal application-layer
protocol, SMTP, is only one piece (albeit, an important piece) of the e-mail application.
2.1.6 Network Applications Covered in This Book
New public domain and proprietary Internet applications are being developed every
day. Rather than covering a large number of Internet applications in an encyclopedic
manner, we have chosen to focus on a small number of applications that are both
pervasive and important. In this chapter we discuss five important applications: the
Web, file transfer, electronic mail, directory service, and P2P applications. We first
discuss the Web, not only because it is an enormously popular application, but also
because its application-layer protocol, HTTP, is straightforward and easy to understand.
After covering the Web, we briefly examine FTP, because it provides a nice
contrast to HTTP. We then discuss electronic mail, the Internet’s first killer application.
E-mail is more complex than the Web in the sense that it makes use of not one
2.1 • PRINCIPLES OF NETWORK APPLICATIONS 97
but several application-layer protocols. After e-mail, we cover DNS, which provides
a directory service for the Internet. Most users do not interact with DNS directly;
instead, users invoke DNS indirectly through other applications (including the Web,
file transfer, and electronic mail). DNS illustrates nicely how a piece of core network
functionality (network-name to network-address translation) can be implemented
at the application layer in the Internet. Finally, we discuss in this chapter
several P2P applications, focusing on file sharing applications, and distributed
lookup services. In Chapter 7, we’ll cover multimedia applications, including
streaming video and voice-over-IP.
2.2 The Web and HTTP
Until the early 1990s the Internet was used primarily by researchers, academics, and
university students to log in to remote hosts, to transfer files from local hosts to remote
hosts and vice versa, to receive and send news, and to receive and send electronic
mail. Although these applications were (and continue to be) extremely useful, the
Internet was essentially unknown outside of the academic and research communities.
Then, in the early 1990s, a major new application arrived on the scene—the World
Wide Web [Berners-Lee 1994]. The Web was the first Internet application that caught
the general public’s eye. It dramatically changed, and continues to change, how people
interact inside and outside their work environments. It elevated the Internet from
just one of many data networks to essentially the one and only data network.
Perhaps what appeals the most to users is that the Web operates on demand.
Users receive what they want, when they want it. This is unlike traditional broadcast
radio and television, which force users to tune in when the content provider
makes the content available. In addition to being available on demand, the Web has
many other wonderful features that people love and cherish. It is enormously easy
for any individual to make information available over the Web—everyone can
become a publisher at extremely low cost. Hyperlinks and search engines help us
navigate through an ocean of Web sites. Graphics stimulate our senses. Forms,
JavaScript, Java applets, and many other devices enable us to interact with pages
and sites. And the Web serves as a platform for many killer applications emerging
after 2003, including YouTube, Gmail, and Facebook.
2.2.1 Overview of HTTP
The HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol,
is at the heart of the Web. It is defined in [RFC 1945] and [RFC 2616]. HTTP is
implemented in two programs: a client program and a server program. The client
program and server program, executing on different end systems, talk to each other
by exchanging HTTP messages. HTTP defines the structure of these messages and
how the client and server exchange the messages. Before explaining HTTP in detail,
we should review some Web terminology.
98 CHAPTER 2 • APPLICATION LAYER
AWeb page (also called a document) consists of objects. An object is simply a
file—such as an HTML file, a JPEG image, a Java applet, or a video clip—that is
addressable by a single URL. Most Web pages consist of a base HTML file and
several referenced objects. For example, if a Web page contains HTML text and five
JPEG images, then the Web page has six objects: the base HTML file plus the five
images. The base HTML file references the other objects in the page with the
objects’ URLs. Each URL has two components: the hostname of the server that
houses the object and the object’s path name. For example, the URL
http://www.someSchool.edu/someDepartment/picture.gif
has www.someSchool.edu for a hostname and /someDepartment/
picture.gif for a path name. Because Web browsers (such as Internet Explorer
and Firefox) implement the client side of HTTP, in the context of the Web, we will use
the words browser and client interchangeably. Web servers, which implement the
server side of HTTP, house Web objects, each addressable by a URL. Popular Web
servers include Apache and Microsoft Internet Information Server.
HTTP defines how Web clients request Web pages from Web servers and how
servers transfer Web pages to clients. We discuss the interaction between client and
server in detail later, but the general idea is illustrated in Figure 2.6. When a user
requests a Web page (for example, clicks on a hyperlink), the browser sends HTTP
request messages for the objects in the page to the server. The server receives the
requests and responds with HTTP response messages that contain the objects.
HTTP uses TCP as its underlying transport protocol (rather than running on top of
UDP). The HTTP client first initiates a TCP connection with the server. Once the connection
is established, the browser and the server processes access TCP through their
socket interfaces. As described in Section 2.1, on the client side the socket interface is
the door between the client process and the TCP connection; on the server side it is the
2.2 • THE WEB AND HTTP 99
HTTP request
HTTP response
HTTP response
HTTP request
PC running
Internet Explorer
Linux running
Firefox
Server running
Apache Web server
Figure 2.6  HTTP request-response behavior
door between the server process and the TCP connection. The client sends HTTP
request messages into its socket interface and receives HTTP response messages from
its socket interface. Similarly, the HTTP server receives request messages from its
socket interface and sends response messages into its socket interface. Once the client
sends a message into its socket interface, the message is out of the client’s hands and is
“in the hands” of TCP. Recall from Section 2.1 that TCP provides a reliable data transfer
service to HTTP. This implies that each HTTP request message sent by a client
process eventually arrives intact at the server; similarly, each HTTP response message
sent by the server process eventually arrives intact at the client. Here we see one of the
great advantages of a layered architecture—HTTP need not worry about lost data or the
details of how TCP recovers from loss or reordering of data within the network. That is
the job of TCP and the protocols in the lower layers of the protocol stack.
It is important to note that the server sends requested files to clients without storing
any state information about the client. If a particular client asks for the same object
twice in a period of a few seconds, the server does not respond by saying that it just
served the object to the client; instead, the server resends the object, as it has completely
forgotten what it did earlier. Because an HTTP server maintains no information
about the clients, HTTP is said to be a stateless protocol. We also remark that the
Web uses the client-server application architecture, as described in Section 2.1. AWeb
server is always on, with a fixed IP address, and it services requests from potentially
millions of different browsers.
2.2.2 Non-Persistent and Persistent Connections
In many Internet applications, the client and server communicate for an extended
period of time, with the client making a series of requests and the server responding to
each of the requests. Depending on the application and on how the application is being
used, the series of requests may be made back-to-back, periodically at regular intervals,
or intermittently. When this client-server interaction is taking place over TCP, the application
developer needs to make an important decision––should each request/response
pair be sent over a separate TCP connection, or should all of the requests and their corresponding
responses be sent over the same TCP connection? In the former approach,
the application is said to use non-persistent connections; and in the latter approach,
persistent connections. To gain a deep understanding of this design issue, let’s examine
the advantages and disadvantages of persistent connections in the context of a specific
application, namely, HTTP, which can use both non-persistent connections and
persistent connections. Although HTTP uses persistent connections in its default mode,
HTTP clients and servers can be configured to use non-persistent connections instead.
HTTP with Non-Persistent Connections
Let’s walk through the steps of transferring a Web page from server to client for the
case of non-persistent connections. Let’s suppose the page consists of a base HTML
100 CHAPTER 2 • APPLICATION LAYER
file and 10 JPEG images, and that all 11 of these objects reside on the same server.
Further suppose the URL for the base HTML file is
http://www.someSchool.edu/someDepartment/home.index
Here is what happens:
1. The HTTP client process initiates a TCP connection to the server
www.someSchool.edu on port number 80, which is the default port number
for HTTP. Associated with the TCP connection, there will be a socket at the
client and a socket at the server.
2. The HTTP client sends an HTTP request message to the server via its socket. The
request message includes the path name /someDepartment/home.index.
(We will discuss HTTP messages in some detail below.)
3. The HTTP server process receives the request message via its socket, retrieves
the object /someDepartment/home.index from its storage (RAM or
disk), encapsulates the object in an HTTP response message, and sends the
response message to the client via its socket.
4. The HTTP server process tells TCP to close the TCP connection. (But TCP
doesn’t actually terminate the connection until it knows for sure that the client
has received the response message intact.)
5. The HTTP client receives the response message. The TCP connection terminates.
The message indicates that the encapsulated object is an HTML file. The
client extracts the file from the response message, examines the HTML file,
and finds references to the 10 JPEG objects.
6. The first four steps are then repeated for each of the referenced JPEG objects.
As the browser receives the Web page, it displays the page to the user. Two different
browsers may interpret (that is, display to the user) a Web page in somewhat different
ways. HTTP has nothing to do with how a Web page is interpreted by a client. The
HTTP specifications ([RFC 1945] and [RFC 2616]) define only the communication
protocol between the client HTTP program and the server HTTP program.
The steps above illustrate the use of non-persistent connections, where each TCP
connection is closed after the server sends the object—the connection does not persist
for other objects. Note that each TCP connection transports exactly one request message
and one response message. Thus, in this example, when a user requests the Web
page, 11 TCP connections are generated.
In the steps described above, we were intentionally vague about whether the
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the
JPEGs are obtained over parallel TCP connections. Indeed, users can configure
modern browsers to control the degree of parallelism. In their default modes, most
browsers open 5 to 10 parallel TCP connections, and each of these connections handles
one request-response transaction. If the user prefers, the maximum number of
2.2 • THE WEB AND HTTP 101
parallel connections can be set to one, in which case the 10 connections are established
serially. As we’ll see in the next chapter, the use of parallel connections shortens
the response time.
Before continuing, let’s do a back-of-the-envelope calculation to estimate the
amount of time that elapses from when a client requests the base HTML file until
the entire file is received by the client. To this end, we define the round-trip time
(RTT), which is the time it takes for a small packet to travel from client to server
and then back to the client. The RTT includes packet-propagation delays, packetqueuing
delays in intermediate routers and switches, and packet-processing
delays. (These delays were discussed in Section 1.4.) Now consider what happens
when a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser
to initiate a TCP connection between the browser and the Web server; this
involves a “three-way handshake”—the client sends a small TCP segment to the
server, the server acknowledges and responds with a small TCP segment, and,
finally, the client acknowledges back to the server. The first two parts of the threeway
handshake take one RTT. After completing the first two parts of the handshake,
the client sends the HTTP request message combined with the third part of
102 CHAPTER 2 • APPLICATION LAYER
Time
at client
Time
at server
Initiate TCP
connection
RTT
Request file
RTT
Entire file received
Time to transmit file
Figure 2.7  Back-of-the-envelope calculation for the time needed to
request and receive an HTML file
the three-way handshake (the acknowledgment) into the TCP connection. Once
the request message arrives at the server, the server sends the HTML file into the
TCP connection. This HTTP request/response eats up another RTT. Thus, roughly,
the total response time is two RTTs plus the transmission time at the server of the
HTML file.
HTTP with Persistent Connections
Non-persistent connections have some shortcomings. First, a brand-new connection
must be established and maintained for each requested object. For each of
these connections, TCP buffers must be allocated and TCP variables must be kept
in both the client and server. This can place a significant burden on the Web server,
which may be serving requests from hundreds of different clients simultaneously.
Second, as we just described, each object suffers a delivery delay of two RTTs—
one RTT to establish the TCP connection and one RTT to request and receive an
object.
With persistent connections, the server leaves the TCP connection open after
sending a response. Subsequent requests and responses between the same client and
server can be sent over the same connection. In particular, an entire Web page (in
the example above, the base HTML file and the 10 images) can be sent over a single
persistent TCP connection. Moreover, multiple Web pages residing on the same
server can be sent from the server to the same client over a single persistent TCP
connection. These requests for objects can be made back-to-back, without waiting
for replies to pending requests (pipelining). Typically, the HTTP server closes a connection
when it isn’t used for a certain time (a configurable timeout interval). When
the server receives the back-to-back requests, it sends the objects back-to-back. The
default mode of HTTP uses persistent connections with pipelining. We’ll quantitatively
compare the performance of non-persistent and persistent connections in the
homework problems of Chapters 2 and 3. You are also encouraged to see [Heidemann
1997; Nielsen 1997].
2.2.3 HTTP Message Format
The HTTP specifications [RFC 1945; RFC 2616] include the definitions of the
HTTP message formats. There are two types of HTTP messages, request messages
and response messages, both of which are discussed below.
HTTP Request Message
Below we provide a typical HTTP request message:
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
2.2 • THE WEB AND HTTP 103
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
We can learn a lot by taking a close look at this simple request message. First of
all, we see that the message is written in ordinary ASCII text, so that your ordinary
computer-literate human being can read it. Second, we see that the message consists
of five lines, each followed by a carriage return and a line feed. The last line is followed
by an additional carriage return and line feed. Although this particular request
message has five lines, a request message can have many more lines or as few as
one line. The first line of an HTTP request message is called the request line; the
subsequent lines are called the header lines. The request line has three fields: the
method field, the URL field, and the HTTP version field. The method field can take
on several different values, including GET, POST, HEAD, PUT, and DELETE.
The great majority of HTTP request messages use the GET method. The GET
method is used when the browser requests an object, with the requested object identified
in the URL field. In this example, the browser is requesting the object
/somedir/page.html. The version is self-explanatory; in this example, the
browser implements version HTTP/1.1.
Now let’s look at the header lines in the example. The header line Host:
www.someschool.edu specifies the host on which the object resides. You might
think that this header line is unnecessary, as there is already a TCP connection in
place to the host. But, as we’ll see in Section 2.2.5, the information provided by the
host header line is required by Web proxy caches. By including the Connection:
close header line, the browser is telling the server that it doesn’t want to bother
with persistent connections; it wants the server to close the connection after sending
the requested object. The User-agent: header line specifies the user agent, that
is, the browser type that is making the request to the server. Here the user agent is
Mozilla/5.0, a Firefox browser. This header line is useful because the server can
actually send different versions of the same object to different types of user agents.
(Each of the versions is addressed by the same URL.) Finally, the Acceptlanguage:
header indicates that the user prefers to receive a French version of
the object, if such an object exists on the server; otherwise, the server should send
its default version. The Accept-language: header is just one of many content
negotiation headers available in HTTP.
Having looked at an example, let’s now look at the general format of a request
message, as shown in Figure 2.8. We see that the general format closely follows our
earlier example. You may have noticed, however, that after the header lines (and the
additional carriage return and line feed) there is an “entity body.” The entity body is
empty with the GET method, but is used with the POST method. An HTTP client
often uses the POST method when the user fills out a form—for example, when a
user provides search words to a search engine. With a POST message, the user is still
requesting a Web page from the server, but the specific contents of the Web page
104 CHAPTER 2 • APPLICATION LAYER
depend on what the user entered into the form fields. If the value of the method field
is POST, then the entity body contains what the user entered into the form fields.
We would be remiss if we didn’t mention that a request generated with a form
does not necessarily use the POST method. Instead, HTML forms often use the GET
method and include the inputted data (in the form fields) in the requested URL. For
example, if a form uses the GET method, has two fields, and the inputs to the two
fields are monkeys and bananas, then the URL will have the structure
www.somesite.com/animalsearch?monkeys&bananas. In your day-today
Web surfing, you have probably noticed extended URLs of this sort.
The HEAD method is similar to the GET method. When a server receives a
request with the HEAD method, it responds with an HTTP message but it leaves out
the requested object. Application developers often use the HEAD method for debugging.
The PUT method is often used in conjunction with Web publishing tools. It
allows a user to upload an object to a specific path (directory) on a specific Web
server. The PUT method is also used by applications that need to upload objects to
Web servers. The DELETE method allows a user, or an application, to delete an
object on a Web server.
HTTP Response Message
Below we provide a typical HTTP response message. This response message could
be the response to the example request message just discussed.
HTTP/1.1 200 OK
Connection: close
2.2 • THE WEB AND HTTP 105
method sp sp cr lf
header field name: cr lf
Header lines
Blank line
Entity body
Request line
sp value
cr lf
cr lf
header field name: sp value
URL Version
Figure 2.8  General format of an HTTP request message
Date: Tue, 09 Aug 2011 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 09 Aug 2011 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html
(data data data data data ...)
Let’s take a careful look at this response message. It has three sections: an initial
status line, six header lines, and then the entity body. The entity body is the
meat of the message—it contains the requested object itself (represented by data
data data data data ...). The status line has three fields: the protocol version
field, a status code, and a corresponding status message. In this example, the
status line indicates that the server is using HTTP/1.1 and that everything is OK
(that is, the server has found, and is sending, the requested object).
Now let’s look at the header lines. The server uses the Connection: close
header line to tell the client that it is going to close the TCP connection after sending
the message. The Date: header line indicates the time and date when the HTTP
response was created and sent by the server. Note that this is not the time when the
object was created or last modified; it is the time when the server retrieves the
object from its file system, inserts the object into the response message, and sends
the response message. The Server: header line indicates that the message was generated
by an Apache Web server; it is analogous to the User-agent: header line
in the HTTP request message. The Last-Modified: header line indicates the
time and date when the object was created or last modified. The Last-Modified:
header, which we will soon cover in more detail, is critical for object caching, both in
the local client and in network cache servers (also known as proxy servers). The
Content-Length: header line indicates the number of bytes in the object being
sent. The Content-Type: header line indicates that the object in the entity body is
HTML text. (The object type is officially indicated by the Content-Type: header
and not by the file extension.)
Having looked at an example, let’s now examine the general format of a
response message, which is shown in Figure 2.9. This general format of the response
message matches the previous example of a response message. Let’s say a few additional
words about status codes and their phrases. The status code and associated
phrase indicate the result of the request. Some common status codes and associated
phrases include:
• 200 OK: Request succeeded and the information is returned in the response.
• 301 Moved Permanently: Requested object has been permanently moved;
the new URL is specified in Location: header of the response message. The
client software will automatically retrieve the new URL.
106 CHAPTER 2 • APPLICATION LAYER
• 400 Bad Request: This is a generic error code indicating that the request
could not be understood by the server.
• 404 Not Found: The requested document does not exist on this server.
• 505 HTTP Version Not Supported: The requested HTTP protocol
version is not supported by the server.
How would you like to see a real HTTP response message? This is highly recommended
and very easy to do! First Telnet into your favorite Web server. Then
type in a one-line request message for some object that is housed on the server. For
example, if you have access to a command prompt, type:
telnet cis.poly.edu 80
GET /~ross/ HTTP/1.1
Host: cis.poly.edu
(Press the carriage return twice after typing the last line.) This opens a TCP connection
to port 80 of the host cis.poly.edu and then sends the HTTP request message.
You should see a response message that includes the base HTML file of
Professor Ross’s homepage. If you’d rather just see the HTTP message lines and not
receive the object itself, replace GET with HEAD. Finally, replace /~ross/ with
/~banana/ and see what kind of response message you get.
In this section we discussed a number of header lines that can be used within
HTTP request and response messages. The HTTP specification defines many, many
2.2 • THE WEB AND HTTP 107
version sp sp cr lf
header field name: cr lf
Header lines
Blank line
Entity body
Status line
value
cr
sp
sp lf
cr lf
header field name: value
status code phrase
Figure 2.9  General format of an HTTP response message
VideoNote
Using Wireshark to
investigate the
HTTP protocol
more header lines that can be inserted by browsers, Web servers, and network cache
servers. We have covered only a small number of the totality of header lines. We’ll
cover a few more below and another small number when we discuss network Web
caching in Section 2.2.5. Ahighly readable and comprehensive discussion of the HTTP
protocol, including its headers and status codes, is given in [Krishnamurthy 2001].
How does a browser decide which header lines to include in a request message?
How does a Web server decide which header lines to include in a response
message? A browser will generate header lines as a function of the browser type
and version (for example, an HTTP/1.0 browser will not generate any 1.1 header
lines), the user configuration of the browser (for example, preferred language), and
whether the browser currently has a cached, but possibly out-of-date, version of the
object. Web servers behave similarly: There are different products, versions, and
configurations, all of which influence which header lines are included in response
messages.
2.2.4 User-Server Interaction: Cookies
We mentioned above that an HTTP server is stateless. This simplifies server design
and has permitted engineers to develop high-performance Web servers that can handle
thousands of simultaneous TCP connections. However, it is often desirable for a
Web site to identify users, either because the server wishes to restrict user access or
because it wants to serve content as a function of the user identity. For these purposes,
HTTP uses cookies. Cookies, defined in [RFC 6265], allow sites to keep
track of users. Most major commercial Web sites use cookies today.
As shown in Figure 2.10, cookie technology has four components: (1) a cookie
header line in the HTTP response message; (2) a cookie header line in the HTTP
request message; (3) a cookie file kept on the user’s end system and managed by the
user’s browser; and (4) a back-end database at the Web site. Using Figure 2.10, let’s
walk through an example of how cookies work. Suppose Susan, who always
accesses the Web using Internet Explorer from her home PC, contacts Amazon.com
for the first time. Let us suppose that in the past she has already visited the eBay site.
When the request comes into the Amazon Web server, the server creates a unique
identification number and creates an entry in its back-end database that is indexed
by the identification number. The Amazon Web server then responds to Susan’s
browser, including in the HTTP response a Set-cookie: header, which contains
the identification number. For example, the header line might be:
Set-cookie: 1678
When Susan’s browser receives the HTTP response message, it sees the Setcookie:
header. The browser then appends a line to the special cookie file that it
manages. This line includes the hostname of the server and the identification number
in the Set-cookie: header. Note that the cookie file already has an entry for
108 CHAPTER 2 • APPLICATION LAYER
eBay, since Susan has visited that site in the past. As Susan continues to browse the
Amazon site, each time she requests a Web page, her browser consults her cookie
file, extracts her identification number for this site, and puts a cookie header line
that includes the identification number in the HTTP request. Specifically, each of
her HTTP requests to the Amazon server includes the header line:
Cookie: 1678
2.2 • THE WEB AND HTTP 109
Client host Server host
usual http request msg
usual http response
Set-cookie: 1678
usual http request msg
cookie: 1678
usual http response msg
usual http request msg
cookie: 1678
usual http response msg
Time
One week later
ebay: 8734
Server creates
ID 1678 for user
Time
Cookie file
Key:
amazon: 1678
ebay: 8734
amazon: 1678
ebay: 8734
Cookie-specific
action
access
access
entry in backend
database
Cookie-specific
action
Figure 2.10  Keeping user state with cookies
In this manner, the Amazon server is able to track Susan’s activity at the Amazon
site. Although the Amazon Web site does not necessarily know Susan’s name, it
knows exactly which pages user 1678 visited, in which order, and at what times!
Amazon uses cookies to provide its shopping cart service—Amazon can maintain a
list of all of Susan’s intended purchases, so that she can pay for them collectively at
the end of the session.
If Susan returns to Amazon’s site, say, one week later, her browser will continue
to put the header line Cookie: 1678 in the request messages. Amazon also recommends
products to Susan based on Web pages she has visited at Amazon in the
past. If Susan also registers herself with Amazon—providing full name, e-mail
address, postal address, and credit card information—Amazon can then include this
information in its database, thereby associating Susan’s name with her identification
number (and all of the pages she has visited at the site in the past!). This is how
Amazon and other e-commerce sites provide “one-click shopping”—when Susan
chooses to purchase an item during a subsequent visit, she doesn’t need to re-enter
her name, credit card number, or address.
From this discussion we see that cookies can be used to identify a user. The first
time a user visits a site, the user can provide a user identification (possibly his or her
name). During the subsequent sessions, the browser passes a cookie header to the
server, thereby identifying the user to the server. Cookies can thus be used to create
a user session layer on top of stateless HTTP. For example, when a user logs in to a
Web-based e-mail application (such as Hotmail), the browser sends cookie information
to the server, permitting the server to identify the user throughout the user’s session
with the application.
Although cookies often simplify the Internet shopping experience for the user,
they are controversial because they can also be considered as an invasion of privacy.
As we just saw, using a combination of cookies and user-supplied account information,
a Web site can learn a lot about a user and potentially sell this information to a
third party. Cookie Central [Cookie Central 2012] includes extensive information
on the cookie controversy.
2.2.5 Web Caching
AWeb cache—also called a proxy server—is a network entity that satisfies HTTP
requests on the behalf of an origin Web server. The Web cache has its own disk storage
and keeps copies of recently requested objects in this storage. As shown in Figure 2.11, a
user’s browser can be configured so that all of the user’s HTTP requests are first directed
to the Web cache. Once a browser is configured, each browser request for an object is
first directed to the Web cache. As an example, suppose a browser is requesting the
object http://www.someschool.edu/campus.gif. Here is what happens:
1. The browser establishes a TCP connection to the Web cache and sends an
HTTP request for the object to the Web cache.
110 CHAPTER 2 • APPLICATION LAYER
2. The Web cache checks to see if it has a copy of the object stored locally. If it
does, the Web cache returns the object within an HTTP response message to
the client browser.
3. If the Web cache does not have the object, the Web cache opens a TCP connection
to the origin server, that is, to www.someschool.edu. The Web cache
then sends an HTTP request for the object into the cache-to-server TCP connection.
After receiving this request, the origin server sends the object within
an HTTP response to the Web cache.
4. When the Web cache receives the object, it stores a copy in its local storage and
sends a copy, within an HTTP response message, to the client browser (over the
existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time. When it receives
requests from and sends responses to a browser, it is a server. When it sends requests
to and receives responses from an origin server, it is a client.
Typically a Web cache is purchased and installed by an ISP. For example, a university
might install a cache on its campus network and configure all of the campus
browsers to point to the cache. Or a major residential ISP (such as AOL) might
install one or more caches in its network and preconfigure its shipped browsers to
point to the installed caches.
Web caching has seen deployment in the Internet for two reasons. First, a Web
cache can substantially reduce the response time for a client request, particularly if the
bottleneck bandwidth between the client and the origin server is much less than the bottleneck
bandwidth between the client and the cache. If there is a high-speed connection
between the client and the cache, as there often is, and if the cache has the requested
object, then the cache will be able to deliver the object rapidly to the client. Second, as
we will soon illustrate with an example, Web caches can substantially reduce traffic on
2.2 • THE WEB AND HTTP 111
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
Client Origin
server
Origin
server
Client
Proxy
server
Figure 2.11  Clients requesting objects through a Web cache
an institution’s access link to the Internet. By reducing traffic, the institution (for example,
a company or a university) does not have to upgrade bandwidth as quickly, thereby
reducing costs. Furthermore, Web caches can substantially reduce Web traffic in the
Internet as a whole, thereby improving performance for all applications.
To gain a deeper understanding of the benefits of caches, let’s consider an example
in the context of Figure 2.12. This figure shows two networks—the institutional
network and the rest of the public Internet. The institutional network is a high-speed
LAN. A router in the institutional network and a router in the Internet are connected
by a 15 Mbps link. The origin servers are attached to the Internet but are located all
over the globe. Suppose that the average object size is 1 Mbits and that the average
request rate from the institution’s browsers to the origin servers is 15 requests per
second. Suppose that the HTTP request messages are negligibly small and thus create
no traffic in the networks or in the access link (from institutional router to Internet
router). Also suppose that the amount of time it takes from when the router on the
Internet side of the access link in Figure 2.12 forwards an HTTP request (within an
IP datagram) until it receives the response (typically within many IP datagrams) is
two seconds on average. Informally, we refer to this last delay as the “Internet delay.”
112 CHAPTER 2 • APPLICATION LAYER
Public Internet
Institutional network
15 Mbps access link
100 Mbps LAN
Origin servers
Figure 2.12  Bottleneck between an institutional network and the Internet
The total response time—that is, the time from the browser’s request of an object
until its receipt of the object—is the sum of the LAN delay, the access delay (that is,
the delay between the two routers), and the Internet delay. Let’s now do a very crude
calculation to estimate this delay. The traffic intensity on the LAN (see Section 1.4.2) is
(15 requests/sec)  (1 Mbits/request)/(100 Mbps) = 0.15
whereas the traffic intensity on the access link (from the Internet router to institution
router) is
(15 requests/sec)  (1 Mbits/request)/(15 Mbps) = 1
A traffic intensity of 0.15 on a LAN typically results in, at most, tens of milliseconds
of delay; hence, we can neglect the LAN delay. However, as discussed in
Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link
in Figure 2.12), the delay on a link becomes very large and grows without bound.
Thus, the average response time to satisfy requests is going to be on the order of
minutes, if not more, which is unacceptable for the institution’s users. Clearly something
must be done.
One possible solution is to increase the access rate from 15 Mbps to, say, 100
Mbps. This will lower the traffic intensity on the access link to 0.15, which translates
to negligible delays between the two routers. In this case, the total response
time will roughly be two seconds, that is, the Internet delay. But this solution also
means that the institution must upgrade its access link from 15 Mbps to 100 Mbps, a
costly proposition.
Now consider the alternative solution of not upgrading the access link but
instead installing a Web cache in the institutional network. This solution is illustrated
in Figure 2.13. Hit rates—the fraction of requests that are satisfied by a
cache—typically range from 0.2 to 0.7 in practice. For illustrative purposes, let’s
suppose that the cache provides a hit rate of 0.4 for this institution. Because the
clients and the cache are connected to the same high-speed LAN, 40 percent of
the requests will be satisfied almost immediately, say, within 10 milliseconds, by the
cache. Nevertheless, the remaining 60 percent of the requests still need to be satisfied
by the origin servers. But with only 60 percent of the requested objects passing
through the access link, the traffic intensity on the access link is reduced from 1.0 to
0.6. Typically, a traffic intensity less than 0.8 corresponds to a small delay, say, tens
of milliseconds, on a 15 Mbps link. This delay is negligible compared with the twosecond
Internet delay. Given these considerations, average delay therefore is
0.4  (0.01 seconds) + 0.6  (2.01 seconds)
which is just slightly greater than 1.2 seconds. Thus, this second solution provides an
even lower response time than the first solution, and it doesn’t require the institution
to upgrade its link to the Internet. The institution does, of course, have to purchase
2.2 • THE WEB AND HTTP 113
and install a Web cache. But this cost is low—many caches use public-domain software
that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are
increasingly playing an important role in the Internet. A CDN company installs many
geographically distributed caches throughout the Internet, thereby localizing much of
the traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs
(such as Google and Microsoft). We will discuss CDNs in more detail in Chapter 7.
2.2.6 The Conditional GET
Although caching can reduce user-perceived response times, it introduces a new problem—
the copy of an object residing in the cache may be stale. In other words, the
object housed in the Web server may have been modified since the copy was cached
at the client. Fortunately, HTTP has a mechanism that allows a cache to verify that its
objects are up to date. This mechanism is called the conditional GET. An HTTP
114 CHAPTER 2 • APPLICATION LAYER
Public Internet
Institutional network
15 Mbps access link
Institutional
cache
100 Mbps LAN
Origin servers
Figure 2.13  Adding a cache to the institutional network
request message is a so-called conditional GET message if (1) the request message
uses the GET method and (2) the request message includes an If-Modified-
Since: header line.
To illustrate how the conditional GET operates, let’s walk through an example.
First, on the behalf of a requesting browser, a proxy cache sends a request message
to a Web server:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
Second, the Web server sends a response message with the requested object to the
cache:
HTTP/1.1 200 OK
Date: Sat, 8 Oct 2011 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 7 Sep 2011 09:23:24
Content-Type: image/gif
(data data data data data ...)
The cache forwards the object to the requesting browser but also caches the object
locally. Importantly, the cache also stores the last-modified date along with the
object. Third, one week later, another browser requests the same object via the
cache, and the object is still in the cache. Since this object may have been modified
at the Web server in the past week, the cache performs an up-to-date check by issuing
a conditional GET. Specifically, the cache sends:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 7 Sep 2011 09:23:24
Note that the value of the If-modified-since: header line is exactly equal to
the value of the Last-Modified: header line that was sent by the server one
week ago. This conditional GET is telling the server to send the object only if the
object has been modified since the specified date. Suppose the object has not been
modified since 7 Sep 2011 09:23:24. Then, fourth, the Web server sends a response
message to the cache:
HTTP/1.1 304 Not Modified
Date: Sat, 15 Oct 2011 15:39:29
Server: Apache/1.3.0 (Unix)
(empty entity body)
2.2 • THE WEB AND HTTP 115
We see that in response to the conditional GET, the Web server still sends a response
message but does not include the requested object in the response message. Including
the requested object would only waste bandwidth and increase user-perceived response
time, particularly if the object is large. Note that this last response message has 304
Not Modified in the status line, which tells the cache that it can go ahead and forward
its (the proxy cache’s) cached copy of the object to the requesting browser.
This ends our discussion of HTTP, the first Internet protocol (an application-layer
protocol) that we’ve studied in detail. We’ve seen the format of HTTP messages and
the actions taken by the Web client and server as these messages are sent and received.
We’ve also studied a bit of the Web’s application infrastructure, including caches, cookies,
and back-end databases, all of which are tied in some way to the HTTP protocol.
2.3 File Transfer: FTP
In a typical FTP session, the user is sitting in front of one host (the local host)
and wants to transfer files to or from a remote host. In order for the user to
access the remote account, the user must provide a user identification and a password.
After providing this authorization information, the user can transfer files
from the local file system to the remote file system and vice versa. As shown in
Figure 2.14, the user interacts with FTP through an FTP user agent. The user first
provides the hostname of the remote host, causing the FTP client process in the
local host to establish a TCP connection with the FTP server process in the
remote host. The user then provides the user identification and password, which
are sent over the TCP connection as part of FTP commands. Once the server has
authorized the user, the user copies one or more files stored in the local file system
into the remote file system (or vice versa).
116 CHAPTER 2 • APPLICATION LAYER
FTP user
interface
Local file
system
User
or host
Remote file
system
FTP
client
FTP
server
File transfer
Figure 2.14  FTP moves files between local and remote file systems
HTTP and FTP are both file transfer protocols and have many common characteristics;
for example, they both run on top of TCP. However, the two application-layer
protocols have some important differences. The most striking difference is that FTP
uses two parallel TCP connections to transfer a file, a control connection and a data
connection. The control connection is used for sending control information between
the two hosts—information such as user identification, password, commands to
change remote directory, and commands to “put” and “get” files. The data connection
is used to actually send a file. Because FTP uses a separate control connection, FTP is
said to send its control information out-of-band. HTTP, as you recall, sends request
and response header lines into the same TCP connection that carries the transferred
file itself. For this reason, HTTP is said to send its control information in-band. In the
next section, we’ll see that SMTP, the main protocol for electronic mail, also sends
control information in-band. The FTP control and data connections are illustrated in
Figure 2.15.
When a user starts an FTP session with a remote host, the client side of FTP
(user) first initiates a control TCP connection with the server side (remote host) on
server port number 21. The client side of FTP sends the user identification and
password over this control connection. The client side of FTP also sends, over the
control connection, commands to change the remote directory. When the server
side receives a command for a file transfer over the control connection (either to,
or from, the remote host), the server side initiates a TCP data connection to the
client side. FTP sends exactly one file over the data connection and then closes the
data connection. If, during the same session, the user wants to transfer another file,
FTP opens another data connection. Thus, with FTP, the control connection
remains open throughout the duration of the user session, but a new data connection
is created for each file transferred within a session (that is, the data connections
are non-persistent).
Throughout a session, the FTP server must maintain state about the user. In particular,
the server must associate the control connection with a specific user account,
and the server must keep track of the user’s current directory as the user wanders
about the remote directory tree. Keeping track of this state information for each
ongoing user session significantly constrains the total number of sessions that FTP
can maintain simultaneously. Recall that HTTP, on the other hand, is stateless—it
does not have to keep track of any user state.
2.3 • FILE TRANSFER: FTP 117
TCP control connection port 21
TCP data connection port 20
FTP
client
FTP
server
Figure 2.15  Control and data connections
2.3.1 FTP Commands and Replies
We end this section with a brief discussion of some of the more common FTP commands
and replies. The commands, from client to server, and replies, from server to
client, are sent across the control connection in 7-bit ASCII format. Thus, like HTTP
commands, FTP commands are readable by people. In order to delineate successive
commands, a carriage return and line feed end each command. Each command consists
of four uppercase ASCII characters, some with optional arguments. Some of
the more common commands are given below:
• USER username: Used to send the user identification to the server.
• PASS password: Used to send the user password to the server.
• LIST: Used to ask the server to send back a list of all the files in the current
remote directory. The list of files is sent over a (new and non-persistent) data
connection rather than the control TCP connection.
• RETR filename: Used to retrieve (that is, get) a file from the current directory
of the remote host. This command causes the remote host to initiate a data
connection and to send the requested file over the data connection.
• STOR filename: Used to store (that is, put) a file into the current directory
of the remote host.
There is typically a one-to-one correspondence between the command that the
user issues and the FTP command sent across the control connection. Each command
is followed by a reply, sent from server to client. The replies are three-digit
numbers, with an optional message following the number. This is similar in structure
to the status code and phrase in the status line of the HTTP response message.
Some typical replies, along with their possible messages, are as follows:
• 331 Username OK, password required
• 125 Data connection already open; transfer starting
• 425 Can’t open data connection
• 452 Error writing file
Readers who are interested in learning about the other FTP commands and replies
are encouraged to read RFC 959.
2.4 Electronic Mail in the Internet
Electronic mail has been around since the beginning of the Internet. It was the most
popular application when the Internet was in its infancy [Segaller 1998], and has
118 CHAPTER 2 • APPLICATION LAYER
Outgoing
message queue
Key:
User mailbox
SMTP
User agent
User agent
User agent
User agent
User agent
User agent
Mail server
Mail server
Mail server
SMTP
SMTP
become more and more elaborate and powerful over the years. It remains one of the
Internet’s most important and utilized applications.
As with ordinary postal mail, e-mail is an asynchronous communication
medium—people send and read messages when it is convenient for them, without
having to coordinate with other people’s schedules. In contrast with postal mail, electronic
mail is fast, easy to distribute, and inexpensive. Modern e-mail has many powerful
features, including messages with attachments, hyperlinks, HTML-formatted
text, and embedded photos.
In this section, we examine the application-layer protocols that are at the heart
of Internet e-mail. But before we jump into an in-depth discussion of these protocols,
let’s take a high-level view of the Internet mail system and its key components.
Figure 2.16 presents a high-level view of the Internet mail system. We see from
this diagram that it has three major components: user agents, mail servers, and the
2.4 • ELECTRONIC MAIL IN THE INTERNET 119
Figure 2.16  A high-level view of the Internet e-mail system
Simple Mail Transfer Protocol (SMTP). We now describe each of these components
in the context of a sender, Alice, sending an e-mail message to a recipient,
Bob. User agents allow users to read, reply to, forward, save, and compose messages.
Microsoft Outlook and Apple Mail are examples of user agents for e-mail.
When Alice is finished composing her message, her user agent sends the message to
her mail server, where the message is placed in the mail server’s outgoing message
queue. When Bob wants to read a message, his user agent retrieves the message
from his mailbox in his mail server.
Mail servers form the core of the e-mail infrastructure. Each recipient, such as
Bob, has a mailbox located in one of the mail servers. Bob’s mailbox manages and
maintains the messages that have been sent to him. A typical message starts its journey
in the sender’s user agent, travels to the sender’s mail server, and
travels to the recipient’s mail server, where it is deposited in the recipient’s mailbox.
120 CHAPTER 2 • APPLICATION LAYER
WEB E-MAIL
In December 1995, just a few years after the Web was “invented,” Sabeer Bhatia
and Jack Smith visited the Internet venture capitalist Draper Fisher Jurvetson and
proposed developing a free Web-based e-mail system. The idea was to give a free
e-mail account to anyone who wanted one, and to make the accounts accessible
from the Web. In exchange for 15 percent of the company, Draper Fisher
Jurvetson financed Bhatia and Smith, who formed a company called Hotmail.
With three full-time people and 14 part-time people who worked for stock options,
they were able to develop and launch the service in July 1996. Within a month
after launch, they had 100,000 subscribers. In December 1997, less than 18
months after launching the service, Hotmail had over 12 million subscribers and
was acquired by Microsoft, reportedly for $400 million. The success of Hotmail is
often attributed to its “first-mover advantage” and to the intrinsic “viral marketing”
of e-mail. (Perhaps some of the students reading this book will be among the new
entrepreneurs who conceive and develop first-mover Internet services with inherent
viral marketing.)
Web e-mail continues to thrive, becoming more sophisticated and powerful every
year. One of the most popular services today is Google’s gmail, which offers gigabytes
of free storage, advanced spam filtering and virus detection, e-mail encryption
(using SSL), mail fetching from third-party e-mail services, and a search-oriented interface.
Asynchronous messaging within social networks, such as Facebook, has also
become popular in recent years.
CASE HISTORY
When Bob wants to access the messages in his mailbox, the mail server
containing his mailbox authenticates Bob (with usernames and passwords). Alice’s
mail server must also deal with failures in Bob’s mail server. If Alice’s server cannot
deliver mail to Bob’s server, Alice’s server holds the message in a message
queue and attempts to transfer the message later. Reattempts are often done every
30 minutes or so; if there is no success after several days, the server removes the
message and notifies the sender (Alice) with an e-mail message.
SMTP is the principal application-layer protocol for Internet electronic mail. It
uses the reliable data transfer service of TCP to transfer mail from the sender’s mail
server to the recipient’s mail server. As with most application-layer protocols,
SMTP has two sides: a client side, which executes on the sender’s mail server, and a
server side, which executes on the recipient’s mail server. Both the client and server
sides of SMTP run on every mail server. When a mail server sends mail to other
mail servers, it acts as an SMTP client. When a mail server receives mail from other
mail servers, it acts as an SMTP server.
2.4.1 SMTP
SMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As mentioned
above, SMTP transfers messages from senders’ mail servers to the recipients’
mail servers. SMTP is much older than HTTP. (The original SMTP RFC
dates back to 1982, and SMTP was around long before that.) Although SMTP has
numerous wonderful qualities, as evidenced by its ubiquity in the Internet, it is
nevertheless a legacy technology that possesses certain archaic characteristics.
For example, it restricts the body (not just the headers) of all mail messages to
simple 7-bit ASCII. This restriction made sense in the early 1980s when transmission
capacity was scarce and no one was e-mailing large attachments or large
image, audio, or video files. But today, in the multimedia era, the 7-bit ASCII
restriction is a bit of a pain—it requires binary multimedia data to be encoded to
ASCII before being sent over SMTP; and it requires the corresponding ASCII
message to be decoded back to binary after SMTP transport. Recall from Section
2.2 that HTTP does not require multimedia data to be ASCII encoded before
transfer.
To illustrate the basic operation of SMTP, let’s walk through a common scenario.
Suppose Alice wants to send Bob a simple ASCII message.
1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address (for
example, bob@someschool.edu), composes a message, and instructs the
user agent to send the message.
2. Alice’s user agent sends the message to her mail server, where it is placed in a
message queue.
2.4 • ELECTRONIC MAIL IN THE INTERNET 121
3. The client side of SMTP, running on Alice’s mail server, sees the message in
the message queue. It opens a TCP connection to an SMTP server, running on
Bob’s mail server.
4. After some initial SMTP handshaking, the SMTP client sends Alice’s message
into the TCP connection.
5. At Bob’s mail server, the server side of SMTP receives the message. Bob’s
mail server then places the message in Bob’s mailbox.
6. Bob invokes his user agent to read the message at his convenience.
The scenario is summarized in Figure 2.17.
It is important to observe that SMTP does not normally use intermediate mail
servers for sending mail, even when the two mail servers are located at opposite
ends of the world. If Alice’s server is in Hong Kong and Bob’s server is in St. Louis,
the TCP connection is a direct connection between the Hong Kong and St. Louis
servers. In particular, if Bob’s mail server is down, the message remains in Alice’s
mail server and waits for a new attempt—the message does not get placed in some
intermediate mail server.
Let’s now take a closer look at how SMTP transfers a message from a sending
mail server to a receiving mail server. We will see that the SMTP protocol has
many similarities with protocols that are used for face-to-face human interaction.
First, the client SMTP (running on the sending mail server host) has TCP establish
a connection to port 25 at the server SMTP (running on the receiving mail
server host). If the server is down, the client tries again later. Once this connection
is established, the server and client perform some application-layer
handshaking—just as humans often introduce themselves before transferring
information from one to another, SMTP clients and servers introduce themselves
before transferring information. During this SMTP handshaking phase, the
122 CHAPTER 2 • APPLICATION LAYER
SMTP
Alice’s
mail server
Bob’s
mail server
Alice’s
agent
Bob’s
agent
1
2 4 6
5
Message queue
Key:
User mailbox
3
Figure 2.17  Alice sends a message to Bob
SMTP client indicates the e-mail address of the sender (the person who generated
the message) and the e-mail address of the recipient. Once the SMTP client and
server have introduced themselves to each other, the client sends the message.
SMTP can count on the reliable data transfer service of TCP to get the message
to the server without errors. The client then repeats this process over the same
TCP connection if it has other messages to send to the server; otherwise, it
instructs TCP to close the connection.
Let’s next take a look at an example transcript of messages exchanged
between an SMTP client (C) and an SMTP server (S). The hostname of the client
is crepes.fr and the hostname of the server is hamburger.edu. The
ASCII text lines prefaced with C: are exactly the lines the client sends into its
TCP socket, and the ASCII text lines prefaced with S: are exactly the lines the
server sends into its TCP socket. The following transcript begins as soon as the
TCP connection is established.
S: 220 hamburger.edu
C: HELO crepes.fr
S: 250 Hello crepes.fr, pleased to meet you
C: MAIL FROM: <alice@crepes.fr>
S: 250 alice@crepes.fr ... Sender ok
C: RCPT TO: <bob@hamburger.edu>
S: 250 bob@hamburger.edu ... Recipient ok
C: DATA
S: 354 Enter mail, end with “.” on a line by itself
C: Do you like ketchup?
C: How about pickles?
C: .
S: 250 Message accepted for delivery
C: QUIT
S: 221 hamburger.edu closing connection
In the example above, the client sends a message (“Do you like ketchup?
How about pickles?”) from mail server crepes.fr to mail server hamburger.
edu. As part of the dialogue, the client issued five commands: HELO (an
abbreviation for HELLO), MAIL FROM, RCPT TO, DATA, and QUIT. These commands
are self-explanatory. The client also sends a line consisting of a single period,
which indicates the end of the message to the server. (In ASCII jargon, each message
ends with CRLF.CRLF, where CR and LF stand for carriage return and line
feed, respectively.) The server issues replies to each command, with each reply having
a reply code and some (optional) English-language explanation. We mention
here that SMTP uses persistent connections: If the sending mail server has several
messages to send to the same receiving mail server, it can send all of the messages
over the same TCP connection. For each message, the client begins the process with
2.4 • ELECTRONIC MAIL IN THE INTERNET 123
a new MAIL FROM: crepes.fr, designates the end of message with an isolated
period, and issues QUIT only after all messages have been sent.
It is highly recommended that you use Telnet to carry out a direct dialogue with
an SMTP server. To do this, issue
telnet serverName 25
where serverName is the name of a local mail server. When you do this, you are
simply establishing a TCP connection between your local host and the mail server.
After typing this line, you should immediately receive the 220 reply from the
server. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA,
CRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended
that you do Programming Assignment 3 at the end of this chapter. In that assignment,
you’ll build a simple user agent that implements the client side of SMTP.
It will allow you to send an e-mail message to an arbitrary recipient via a local
mail server.
2.4.2 Comparison with HTTP
Let’s now briefly compare SMTP with HTTP. Both protocols are used to transfer
files from one host to another: HTTP transfers files (also called objects) from a Web
server to a Web client (typically a browser); SMTP transfers files (that is, e-mail
messages) from one mail server to another mail server. When transferring the files,
both persistent HTTP and SMTP use persistent connections. Thus, the two protocols
have common characteristics. However, there are important differences. First,
HTTP is mainly a pull protocol—someone loads information on a Web server and
users use HTTP to pull the information from the server at their convenience. In particular,
the TCP connection is initiated by the machine that wants to receive the file.
On the other hand, SMTP is primarily a push protocol—the sending mail server
pushes the file to the receiving mail server. In particular, the TCP connection is initiated
by the machine that wants to send the file.
A second difference, which we alluded to earlier, is that SMTP requires
each message, including the body of each message, to be in 7-bit ASCII format.
If the message contains characters that are not 7-bit ASCII (for example, French
characters with accents) or contains binary data (such as an image file), then the
message has to be encoded into 7-bit ASCII. HTTP data does not impose this
restriction.
A third important difference concerns how a document consisting of text and
images (along with possibly other media types) is handled. As we learned in Section
2.2, HTTP encapsulates each object in its own HTTP response message. Internet
mail places all of the message’s objects into one message.
124 CHAPTER 2 • APPLICATION LAYER
2.4.3 Mail Message Formats
When Alice writes an ordinary snail-mail letter to Bob, she may include all kinds of
peripheral header information at the top of the letter, such as Bob’s address, her own
return address, and the date. Similarly, when an e-mail message is sent from one person
to another, a header containing peripheral information precedes the body of the
message itself. This peripheral information is contained in a series of header lines,
which are defined in RFC 5322. The header lines and the body of the message are
separated by a blank line (that is, by CRLF). RFC 5322 specifies the exact format for
mail header lines as well as their semantic interpretations. As with HTTP, each header
line contains readable text, consisting of a keyword followed by a colon followed by
a value. Some of the keywords are required and others are optional. Every header
must have a From: header line and a To: header line; a header may include a Subject:
header line as well as other optional header lines. It is important to note that
these header lines are different from the SMTP commands we studied in Section 2.4.1
(even though they contain some common words such as “from” and “to”). The commands
in that section were part of the SMTP handshaking protocol; the header lines
examined in this section are part of the mail message itself.
A typical message header looks like this:
From: alice@crepes.fr
To: bob@hamburger.edu
Subject: Searching for the meaning of life.
After the message header, a blank line follows; then the message body (in ASCII)
follows. You should use Telnet to send a message to a mail server that contains some
header lines, including the Subject: header line. To do this, issue telnet
serverName 25, as discussed in Section 2.4.1.
2.4.4 Mail Access Protocols
Once SMTP delivers the message from Alice’s mail server to Bob’s mail server, the
message is placed in Bob’s mailbox. Throughout this discussion we have tacitly
assumed that Bob reads his mail by logging onto the server host and then executing
a mail reader that runs on that host. Up until the early 1990s this was the standard
way of doing things. But today, mail access uses a client-server architecture—the
typical user reads e-mail with a client that executes on the user’s end system, for
example, on an office PC, a laptop, or a smartphone. By executing a mail client on a
local PC, users enjoy a rich set of features, including the ability to view multimedia
messages and attachments.
Given that Bob (the recipient) executes his user agent on his local PC, it is natural
to consider placing a mail server on his local PC as well. With this approach,
2.4 • ELECTRONIC MAIL IN THE INTERNET 125
Alice’s mail server would dialogue directly with Bob’s PC. There is a problem with
this approach, however. Recall that a mail server manages mailboxes and runs the
client and server sides of SMTP. If Bob’s mail server were to reside on his local PC,
then Bob’s PC would have to remain always on, and connected to the Internet, in
order to receive new mail, which can arrive at any time. This is impractical for many
Internet users. Instead, a typical user runs a user agent on the local PC but accesses
its mailbox stored on an always-on shared mail server. This mail server is shared
with other users and is typically maintained by the user’s ISP (for example, university
or company).
Now let’s consider the path an e-mail message takes when it is sent from Alice
to Bob. We just learned that at some point along the path the e-mail message needs
to be deposited in Bob’s mail server. This could be done simply by having Alice’s
user agent send the message directly to Bob’s mail server. And this could be done
with SMTP—indeed, SMTP has been designed for pushing e-mail from one host to
another. However, typically the sender’s user agent does not dialogue directly with
the recipient’s mail server. Instead, as shown in Figure 2.18, Alice’s user agent uses
SMTP to push the e-mail message into her mail server, then Alice’s mail server uses
SMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server. Why
the two-step procedure? Primarily because without relaying through Alice’s mail
server, Alice’s user agent doesn’t have any recourse to an unreachable destination
mail server. By having Alice first deposit the e-mail in her own mail server, Alice’s
mail server can repeatedly try to send the message to Bob’s mail server, say every
30 minutes, until Bob’s mail server becomes operational. (And if Alice’s mail server
is down, then she has the recourse of complaining to her system administrator!) The
SMTP RFC defines how the SMTP commands can be used to relay a message
across multiple SMTP servers.
But there is still one missing piece to the puzzle! How does a recipient like Bob,
running a user agent on his local PC, obtain his messages, which are sitting in a mail
server within Bob’s ISP? Note that Bob’s user agent can’t use SMTP to obtain the
messages because obtaining the messages is a pull operation, whereas SMTP is a
126 CHAPTER 2 • APPLICATION LAYER
SMTP
Alice’s
mail server
Bob’s
mail server
Alice’s
agent
Bob’s
SMTP POP3, agent
IMAP, or
HTTP
Figure 2.18  E-mail protocols and their communicating entities
push protocol. The puzzle is completed by introducing a special mail access protocol
that transfers messages from Bob’s mail server to his local PC. There are currently
a number of popular mail access protocols, including Post Office
Protocol—Version 3 (POP3), Internet Mail Access Protocol (IMAP), and HTTP.
Figure 2.18 provides a summary of the protocols that are used for Internet mail:
SMTP is used to transfer mail from the sender’s mail server to the recipient’s mail
server; SMTP is also used to transfer mail from the sender’s user agent to the
sender’s mail server. A mail access protocol, such as POP3, is used to transfer mail
from the recipient’s mail server to the recipient’s user agent.
POP3
POP3 is an extremely simple mail access protocol. It is defined in [RFC 1939], which
is short and quite readable. Because the protocol is so simple, its functionality is
rather limited. POP3 begins when the user agent (the client) opens a TCP connection
to the mail server (the server) on port 110. With the TCP connection established,
POP3 progresses through three phases: authorization, transaction, and update.
During the first phase, authorization, the user agent sends a username and a password
(in the clear) to authenticate the user. During the second phase, transaction, the user
agent retrieves messages; also during this phase, the user agent can mark messages
for deletion, remove deletion marks, and obtain mail statistics. The third phase,
update, occurs after the client has issued the quit command, ending the POP3
session; at this time, the mail server deletes the messages that were marked for
deletion.
In a POP3 transaction, the user agent issues commands, and the server responds
to each command with a reply. There are two possible responses: +OK (sometimes
followed by server-to-client data), used by the server to indicate that the previous
command was fine; and -ERR, used by the server to indicate that something was
wrong with the previous command.
The authorization phase has two principal commands: user <username> and
pass <password>. To illustrate these two commands, we suggest that you Telnet
directly into a POP3 server, using port 110, and issue these commands. Suppose that
mailServer is the name of your mail server. You will see something like:
telnet mailServer 110
+OK POP3 server ready
user bob
+OK
pass hungry
+OK user successfully logged on
If you misspell a command, the POP3 server will reply with an -ERR message.
2.4 • ELECTRONIC MAIL IN THE INTERNET 127
Now let’s take a look at the transaction phase. A user agent using POP3 can
often be configured (by the user) to “download and delete” or to “download and
keep.” The sequence of commands issued by a POP3 user agent depends on which
of these two modes the user agent is operating in. In the download-and-delete mode,
the user agent will issue the list, retr, and dele commands. As an example,
suppose the user has two messages in his or her mailbox. In the dialogue below, C:
(standing for client) is the user agent and S: (standing for server) is the mail server.
The transaction will look something like:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
S: (blah blah ...
S: .................
S: ..........blah)
S: .
C: dele 1
C: retr 2
S: (blah blah ...
S: .................
S: ..........blah)
S: .
C: dele 2
C: quit
S: +OK POP3 server signing off
The user agent first asks the mail server to list the size of each of the stored messages.
The user agent then retrieves and deletes each message from the server. Note
that after the authorization phase, the user agent employed only four commands:
list, retr, dele, and quit. The syntax for these commands is defined in RFC
1939. After processing the quit command, the POP3 server enters the update
phase and removes messages 1 and 2 from the mailbox.
A problem with this download-and-delete mode is that the recipient, Bob, may
be nomadic and may want to access his mail messages from multiple machines, for
example, his office PC, his home PC, and his portable computer. The downloadand-
delete mode partitions Bob’s mail messages over these three machines; in particular,
if Bob first reads a message on his office PC, he will not be able to reread
the message from his portable at home later in the evening. In the download-andkeep
mode, the user agent leaves the messages on the mail server after downloading
them. In this case, Bob can reread messages from different machines; he can access
a message from work and access it again later in the week from home.
128 CHAPTER 2 • APPLICATION LAYER
During a POP3 session between a user agent and the mail server, the POP3
server maintains some state information; in particular, it keeps track of which user
messages have been marked deleted. However, the POP3 server does not carry state
information across POP3 sessions. This lack of state information across sessions
greatly simplifies the implementation of a POP3 server.
IMAP
With POP3 access, once Bob has downloaded his messages to the local machine,
he can create mail folders and move the downloaded messages into the folders.
Bob can then delete messages, move messages across folders, and search for
messages (by sender name or subject). But this paradigm—namely, folders and
messages in the local machine—poses a problem for the nomadic user, who
would prefer to maintain a folder hierarchy on a remote server that can be
accessed from any computer. This is not possible with POP3—the POP3 protocol
does not provide any means for a user to create remote folders and assign messages
to folders.
To solve this and other problems, the IMAP protocol, defined in [RFC 3501],
was invented. Like POP3, IMAP is a mail access protocol. It has many more features
than POP3, but it is also significantly more complex. (And thus the client and
server side implementations are significantly more complex.)
An IMAP server will associate each message with a folder; when a message first
arrives at the server, it is associated with the recipient’s INBOX folder. The recipient
can then move the message into a new, user-created folder, read the message, delete
the message, and so on. The IMAP protocol provides commands to allow users to
create folders and move messages from one folder to another. IMAP also provides
commands that allow users to search remote folders for messages matching specific
criteria. Note that, unlike POP3, an IMAP server maintains user state information
across IMAP sessions—for example, the names of the folders and which messages
are associated with which folders.
Another important feature of IMAP is that it has commands that permit a user
agent to obtain components of messages. For example, a user agent can obtain just
the message header of a message or just one part of a multipart MIME message.
This feature is useful when there is a low-bandwidth connection (for example, a
slow-speed modem link) between the user agent and its mail server. With a lowbandwidth
connection, the user may not want to download all of the messages in
its mailbox, particularly avoiding long messages that might contain, for example,
an audio or video clip.
Web-Based E-Mail
More and more users today are sending and accessing their e-mail through their Web
browsers. Hotmail introduced Web-based access in the mid 1990s. Now Web-based
2.4 • ELECTRONIC MAIL IN THE INTERNET 129
e-mail is also provided by Google, Yahoo!, as well as just about every major university
and corporation. With this service, the user agent is an ordinary Web browser,
and the user communicates with its remote mailbox via HTTP. When a recipient,
such as Bob, wants to access a message in his mailbox, the e-mail message is sent
from Bob’s mail server to Bob’s browser using the HTTP protocol rather than the
POP3 or IMAP protocol. When a sender, such as Alice, wants to send an e-mail
message, the e-mail message is sent from her browser to her mail server over HTTP
rather than over SMTP. Alice’s mail server, however, still sends messages to, and
receives messages from, other mail servers using SMTP.
2.5 DNS—The Internet’s Directory Service
We human beings can be identified in many ways. For example, we can be identified
by the names that appear on our birth certificates. We can be identified by our
social security numbers. We can be identified by our driver’s license numbers.
Although each of these identifiers can be used to identify people, within a given
context one identifier may be more appropriate than another. For example, the computers
at the IRS (the infamous tax-collecting agency in the United States) prefer to
use fixed-length social security numbers rather than birth certificate names. On the
other hand, ordinary people prefer the more mnemonic birth certificate names rather
than social security numbers. (Indeed, can you imagine saying, “Hi. My name is
132-67-9875. Please meet my husband, 178-87-1146.”)
Just as humans can be identified in many ways, so too can Internet hosts. One identifier
for a host is its hostname. Hostnames—such as cnn.com, www.yahoo.
com, gaia.cs.umass.edu, and cis.poly.edu—are mnemonic and are therefore
appreciated by humans. However, hostnames provide little, if any, information about
the location within the Internet of the host. (A hostname such as www.eurecom.fr,
which ends with the country code .fr, tells us that the host is probably in France, but
doesn’t say much more.) Furthermore, because hostnames can consist of variablelength
alphanumeric characters, they would be difficult to process by routers. For these
reasons, hosts are also identified by so-called IP addresses.
We discuss IP addresses in some detail in Chapter 4, but it is useful to say a few
brief words about them now. An IP address consists of four bytes and has a rigid
hierarchical structure. An IP address looks like 121.7.106.83, where each
period separates one of the bytes expressed in decimal notation from 0 to 255. An IP
address is hierarchical because as we scan the address from left to right, we obtain
more and more specific information about where the host is located in the Internet
(that is, within which network, in the network of networks). Similarly, when we scan
a postal address from bottom to top, we obtain more and more specific information
about where the addressee is located.
130 CHAPTER 2 • APPLICATION LAYER
2.5.1 Services Provided by DNS
We have just seen that there are two ways to identify a host—by a hostname and by
an IP address. People prefer the more mnemonic hostname identifier, while routers
prefer fixed-length, hierarchically structured IP addresses. In order to reconcile
these preferences, we need a directory service that translates hostnames to IP
addresses. This is the main task of the Internet’s domain name system (DNS). The
DNS is (1) a distributed database implemented in a hierarchy of DNS servers, and
(2) an application-layer protocol that allows hosts to query the distributed database.
The DNS servers are often UNIX machines running the Berkeley Internet Name
Domain (BIND) software [BIND 2012]. The DNS protocol runs over UDP and uses
port 53.
DNS is commonly employed by other application-layer protocols—including
HTTP, SMTP, and FTP—to translate user-supplied hostnames to IP addresses. As
an example, consider what happens when a browser (that is, an HTTP client),
running on some user’s host, requests the URL www.someschool.edu/
index.html. In order for the user’s host to be able to send an HTTP request message
to the Web server www.someschool.edu, the user’s host must first obtain
the IP address of www.someschool.edu. This is done as follows.
1. The same user machine runs the client side of the DNS application.
2. The browser extracts the hostname, www.someschool.edu, from the URL
and passes the hostname to the client side of the DNS application.
3. The DNS client sends a query containing the hostname to a DNS server.
4. The DNS client eventually receives a reply, which includes the IP address for
the hostname.
5. Once the browser receives the IP address from DNS, it can initiate a TCP connection
to the HTTP server process located at port 80 at that IP address.
We see from this example that DNS adds an additional delay—sometimes substantial—
to the Internet applications that use it. Fortunately, as we discuss below, the
desired IP address is often cached in a “nearby” DNS server, which helps to reduce
DNS network traffic as well as the average DNS delay.
DNS provides a few other important services in addition to translating hostnames
to IP addresses:
• Host aliasing. A host with a complicated hostname can have one or more alias
names. For example, a hostname such as relay1.west-coast.enterprise.
com could have, say, two aliases such as enterprise.com and
www.enterprise.com. In this case, the hostname relay1.westcoast.
enterprise.com is said to be a canonical hostname. Alias hostnames,
when present, are typically more mnemonic than canonical hostnames.
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 131
DNS can be invoked by an application to obtain the canonical hostname for a
supplied alias hostname as well as the IP address of the host.
• Mail server aliasing. For obvious reasons, it is highly desirable that e-mail
addresses be mnemonic. For example, if Bob has an account with Hotmail, Bob’s
e-mail address might be as simple as bob@hotmail.com. However, the hostname
of the Hotmail mail server is more complicated and much less mnemonic
than simply hotmail.com (for example, the canonical hostname might be
something like relay1.west-coast.hotmail.com). DNS can be
invoked by a mail application to obtain the canonical hostname for a supplied
alias hostname as well as the IP address of the host. In fact, the MX record (see
below) permits a company’s mail server and Web server to have identical
(aliased) hostnames; for example, a company’s Web server and mail server can
both be called enterprise.com.
• Load distribution. DNS is also used to perform load distribution among replicated
servers, such as replicated Web servers. Busy sites, such as cnn.com, are
replicated over multiple servers, with each server running on a different end system
and each having a different IP address. For replicated Web servers, a set of
IP addresses is thus associated with one canonical hostname. The DNS database
contains this set of IP addresses. When clients make a DNS query for a name
mapped to a set of addresses, the server responds with the entire set of IP
addresses, but rotates the ordering of the addresses within each reply. Because a
client typically sends its HTTP request message to the IP address that is listed
first in the set, DNS rotation distributes the traffic among the replicated servers.
132 CHAPTER 2 • APPLICATION LAYER
DNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER
PARADIGM
Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it (1) runs
between communicating end systems using the client-server paradigm and (2) relies on an
underlying end-to-end transport protocol to transfer DNS messages between communicating
end systems. In another sense, however, the role of the DNS is quite different from Web,
file transfer, and e-mail applications. Unlike these applications, the DNS is not an application
with which a user directly interacts. Instead, the DNS provides a core Internet function—
namely, translating hostnames to their underlying IP addresses, for user applications
and other software in the Internet. We noted in Section 1.2 that much of the complexity in
the Internet architecture is located at the “edges” of the network. The DNS, which implements
the critical name-to-address translation process using clients and servers located at
the edge of the network, is yet another example of that design philosophy.
PRINCIPLES IN PRACTICE
DNS rotation is also used for e-mail so that multiple mail servers can have the
same alias name. Also, content distribution companies such as Akamai have used
DNS in more sophisticated ways [Dilley 2002] to provide Web content distribution
(see Chapter 7).
The DNS is specified in RFC 1034 and RFC 1035, and updated in several
additional RFCs. It is a complex system, and we only touch upon key aspects of
its operation here. The interested reader is referred to these RFCs and the book
by Albitz and Liu [Albitz 1993]; see also the retrospective paper [Mockapetris
1988], which provides a nice description of the what and why of DNS, and
[Mockapetris 2005].
2.5.2 Overview of How DNS Works
We now present a high-level overview of how DNS works. Our discussion will
focus on the hostname-to-IP-address translation service.
Suppose that some application (such as a Web browser or a mail reader) running
in a user’s host needs to translate a hostname to an IP address. The application
will invoke the client side of DNS, specifying the hostname that needs to be
translated. (On many UNIX-based machines, gethostbyname() is the function
call that an application calls in order to perform the translation.) DNS in the
user’s host then takes over, sending a query message into the network. All DNS
query and reply messages are sent within UDP datagrams to port 53. After a delay,
ranging from milliseconds to seconds, DNS in the user’s host receives a DNS
reply message that provides the desired mapping. This mapping is then passed to
the invoking application. Thus, from the perspective of the invoking application
in the user’s host, DNS is a black box providing a simple, straightforward translation
service. But in fact, the black box that implements the service is complex,
consisting of a large number of DNS servers distributed around the globe, as well
as an application-layer protocol that specifies how the DNS servers and querying
hosts communicate.
A simple design for DNS would have one DNS server that contains all the mappings.
In this centralized design, clients simply direct all queries to the single DNS
server, and the DNS server responds directly to the querying clients. Although the
simplicity of this design is attractive, it is inappropriate for today’s Internet, with its
vast (and growing) number of hosts. The problems with a centralized design
include:
• A single point of failure. If the DNS server crashes, so does the entire Internet!
• Traffic volume. A single DNS server would have to handle all DNS queries (for
all the HTTP requests and e-mail messages generated from hundreds of millions
of hosts).
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 133
• Distant centralized database. A single DNS server cannot be “close to” all the
querying clients. If we put the single DNS server in New York City, then all
queries from Australia must travel to the other side of the globe, perhaps over
slow and congested links. This can lead to significant delays.
• Maintenance. The single DNS server would have to keep records for all Internet
hosts. Not only would this centralized database be huge, but it would have to be
updated frequently to account for every new host.
In summary, a centralized database in a single DNS server simply doesn’t scale.
Consequently, the DNS is distributed by design. In fact, the DNS is a wonderful
example of how a distributed database can be implemented in the Internet.
A Distributed, Hierarchical Database
In order to deal with the issue of scale, the DNS uses a large number of servers,
organized in a hierarchical fashion and distributed around the world. No single DNS
server has all of the mappings for all of the hosts in the Internet. Instead, the mappings
are distributed across the DNS servers. To a first approximation, there are
three classes of DNS servers—root DNS servers, top-level domain (TLD) DNS
servers, and authoritative DNS servers—organized in a hierarchy as shown in Figure
2.19. To understand how these three classes of servers interact, suppose a DNS
client wants to determine the IP address for the hostname www.amazon.com. To
a first approximation, the following events will take place. The client first contacts
one of the root servers, which returns IP addresses for TLD servers for the top-level
domain com. The client then contacts one of these TLD servers, which returns the
IP address of an authoritative server for amazon.com. Finally, the client contacts
one of the authoritative servers for amazon.com, which returns the IP address
134 CHAPTER 2 • APPLICATION LAYER
com DNS servers org DNS servers edu DNS servers
poly.edu
DNS servers
yahoo.com
DNS servers
amazon.com
DNS servers
pbs.org
DNS servers
umass.edu
DNS servers
Root DNS servers
Figure 2.19  Portion of the hierarchy of DNS servers
for the hostname www.amazon.com. We’ll soon examine this DNS lookup
process in more detail. But let’s first take a closer look at these three classes of
DNS servers:
• Root DNS servers. In the Internet there are 13 root DNS servers (labeled A
through M), most of which are located in North America. An October 2006 map
of the root DNS servers is shown in Figure 2.20; a list of the current root DNS
servers is available via [Root-servers 2012]. Although we have referred to each
of the 13 root DNS servers as if it were a single server, each “server” is actually
a network of replicated servers, for both security and reliability purposes. All
together, there are 247 root servers as of fall 2011.
• Top-level domain (TLD) servers. These servers are responsible for top-level
domains such as com, org, net, edu, and gov, and all of the country top-level domains
such as uk, fr, ca, and jp. The company Verisign Global Registry Services
maintains the TLD servers for the com top-level domain, and the company
Educause maintains the TLD servers for the edu top-level domain. See [IANA
TLD 2012] for a list of all top-level domains.
• Authoritative DNS servers. Every organization with publicly accessible hosts
(such as Web servers and mail servers) on the Internet must provide publicly accessible
DNS records that map the names of those hosts to IP addresses. An organization’s
authoritative DNS server houses these DNS records. An organization can
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 135
c.
d.
h.
j.
Cogent, Herndon, VA (5 other sites)
U Maryland College Park, MD
ARL Aberdeen, MD
Verisign, Dulles VA (69 other sites )
i. Netnod, Stockholm
(37 other sites)
k. RIPE London
(17 other sites)
m. WIDE Tokyo
(5 other sites)
g. US DoD Columbus, OH
(5 other sites)
e.
f.
NASA Mt View, CA
Internet Software C.
Palo Alto, CA
(and 48 other sites)
a.
b.
l.
Verisign, Los Angeles CA
(5 other sites)
USC-ISI Marina del Rey, CA
ICANN Los Angeles, CA
(41 other sites)
Figure 2.20  DNS root servers in 2012 (name, organization, location)
choose to implement its own authoritative DNS server to hold these records; alternatively,
the organization can pay to have these records stored in an authoritative
DNS server of some service provider. Most universities and large companies
implement and maintain their own primary and secondary (backup) authoritative
DNS server.
The root, TLD, and authoritative DNS servers all belong to the hierarchy of
DNS servers, as shown in Figure 2.19. There is another important type of DNS
server called the local DNS server. A local DNS server does not strictly belong to
the hierarchy of servers but is nevertheless central to the DNS architecture. Each
ISP—such as a university, an academic department, an employee’s company, or a
residential ISP—has a local DNS server (also called a default name server). When a
host connects to an ISP, the ISP provides the host with the IP addresses of one or
more of its local DNS servers (typically through DHCP, which is discussed in Chapter
4). You can easily determine the IP address of your local DNS server by accessing
network status windows in Windows or UNIX. A host’s local DNS server is
typically “close to” the host. For an institutional ISP, the local DNS server may be
on the same LAN as the host; for a residential ISP, it is typically separated from the
host by no more than a few routers. When a host makes a DNS query, the query is
sent to the local DNS server, which acts a proxy, forwarding the query into the DNS
server hierarchy, as we’ll discuss in more detail below.
Let’s take a look at a simple example. Suppose the host cis.poly.edu
desires the IP address of gaia.cs.umass.edu. Also suppose that Polytechnic’s
local DNS server is called dns.poly.edu and that an authoritative DNS server
for gaia.cs.umass.edu is called dns.umass.edu. As shown in Figure
2.21, the host cis.poly.edu first sends a DNS query message to its local DNS
server, dns.poly.edu. The query message contains the hostname to be translated,
namely, gaia.cs.umass.edu. The local DNS server forwards the query
message to a root DNS server. The root DNS server takes note of the edu suffix and
returns to the local DNS server a list of IP addresses for TLD servers responsible for
edu. The local DNS server then resends the query message to one of these TLD
servers. The TLD server takes note of the umass.edu suffix and responds with the
IP address of the authoritative DNS server for the University of Massachusetts,
namely, dns.umass.edu. Finally, the local DNS server resends the query
message directly to dns.umass.edu, which responds with the IP address of
gaia.cs.umass.edu. Note that in this example, in order to obtain the mapping
for one hostname, eight DNS messages were sent: four query messages and four
reply messages! We’ll soon see how DNS caching reduces this query traffic.
Our previous example assumed that the TLD server knows the authoritative
DNS server for the hostname. In general this not always true. Instead, the TLD server
may know only of an intermediate DNS server, which in turn knows the authoritative
DNS server for the hostname. For example, suppose again that the University of
136 CHAPTER 2 • APPLICATION LAYER
Massachusetts has a DNS server for the university, called dns.umass.edu. Also
suppose that each of the departments at the University of Massachusetts has its own
DNS server, and that each departmental DNS server is authoritative for all hosts in
the department. In this case, when the intermediate DNS server, dns.umass.edu,
receives a query for a host with a hostname ending with cs.umass.edu, it returns
to dns.poly.edu the IP address of dns.cs.umass.edu, which is authoritative
for all hostnames ending with cs.umass.edu. The local DNS server
dns.poly.edu then sends the query to the authoritative DNS server, which
returns the desired mapping to the local DNS server, which in turn returns the mapping
to the requesting host. In this case, a total of 10 DNS messages are sent!
The example shown in Figure 2.21 makes use of both recursive queries and
iterative queries. The query sent from cis.poly.edu to dns.poly.edu is a
recursive query, since the query asks dns.poly.edu to obtain the mapping on its
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 137
Requesting host
cis.poly.edu
Local DNS server TLD DNS server
dns.poly.edu
Root DNS server
1
8
2
7
4
5
3
6
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
Figure 2.21  Interaction of the various DNS servers
behalf. But the subsequent three queries are iterative since all of the replies are
directly returned to dns.poly.edu. In theory, any DNS query can be iterative or
recursive. For example, Figure 2.22 shows a DNS query chain for which all of
the queries are recursive. In practice, the queries typically follow the pattern in
Figure 2.21: The query from the requesting host to the local DNS server is recursive,
and the remaining queries are iterative.
DNS Caching
Our discussion thus far has ignored DNS caching, a critically important feature of the
DNS system. In truth, DNS extensively exploits DNS caching in order to improve
the delay performance and to reduce the number of DNS messages ricocheting around
138 CHAPTER 2 • APPLICATION LAYER
Requesting host
cis.poly.edu
Local DNS server TLD DNS server
dns.poly.edu
Root DNS server
1
8
5
4
2
7
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
6 3
Figure 2.22  Recursive queries in DNS
the Internet. The idea behind DNS caching is very simple. In a query chain, when a
DNS server receives a DNS reply (containing, for example, a mapping from a hostname
to an IP address), it can cache the mapping in its local memory. For example,
in Figure 2.21, each time the local DNS server dns.poly.edu receives a reply
from some DNS server, it can cache any of the information contained in the reply. If a
hostname/IP address pair is cached in a DNS server and another query arrives to the
DNS server for the same hostname, the DNS server can provide the desired IP address,
even if it is not authoritative for the hostname. Because hosts and mappings between
hostnames and IP addresses are by no means permanent, DNS servers discard cached
information after a period of time (often set to two days).
As an example, suppose that a host apricot.poly.edu queries
dns.poly.edu for the IP address for the hostname cnn.com. Furthermore, suppose
that a few hours later, another Polytechnic University host, say, kiwi.poly.fr,
also queries dns.poly.edu with the same hostname. Because of caching, the local
DNS server will be able to immediately return the IP address of cnn.com to this second
requesting host without having to query any other DNS servers. A local DNS
server can also cache the IP addresses of TLD servers, thereby allowing the local DNS
server to bypass the root DNS servers in a query chain (this often happens).
2.5.3 DNS Records and Messages
The DNS servers that together implement the DNS distributed database store
resource records (RRs), including RRs that provide hostname-to-IP address mappings.
Each DNS reply message carries one or more resource records. In this and
the following subsection, we provide a brief overview of DNS resource records and
messages; more details can be found in [Abitz 1993] or in the DNS RFCs [RFC
1034; RFC 1035].
A resource record is a four-tuple that contains the following fields:
(Name, Value, Type, TTL)
TTL is the time to live of the resource record; it determines when a resource should
be removed from a cache. In the example records given below, we ignore the TTL
field. The meaning of Name and Value depend on Type:
• If Type=A, then Name is a hostname and Value is the IP address for the hostname.
Thus, a Type A record provides the standard hostname-to-IP address mapping.
As an example, (relay1.bar.foo.com, 145.37.93.126, A)
is a Type A record.
• If Type=NS, then Name is a domain (such as foo.com) and Value is the hostname
of an authoritative DNS server that knows how to obtain the IP addresses
for hosts in the domain. This record is used to route DNS queries further along in
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 139
the query chain. As an example, (foo.com, dns.foo.com, NS) is a Type
NS record.
• If Type=CNAME, then Value is a canonical hostname for the alias hostname
Name. This record can provide querying hosts the canonical name for a hostname.
As an example, (foo.com, relay1.bar.foo.com, CNAME) is a
CNAME record.
• If Type=MX, then Value is the canonical name of a mail server that has an alias
hostname Name. As an example, (foo.com, mail.bar.foo.com, MX)
is an MX record. MX records allow the hostnames of mail servers to have simple
aliases. Note that by using the MX record, a company can have the same
aliased name for its mail server and for one of its other servers (such as its Web
server). To obtain the canonical name for the mail server, a DNS client would
query for an MX record; to obtain the canonical name for the other server, the
DNS client would query for the CNAME record.
If a DNS server is authoritative for a particular hostname, then the DNS server will
contain a Type A record for the hostname. (Even if the DNS server is not authoritative,
it may contain a Type A record in its cache.) If a server is not authoritative for a hostname,
then the server will contain a Type NS record for the domain that includes the
hostname; it will also contain a Type A record that provides the IP address of the DNS
server in the Value field of the NS record. As an example, suppose an edu TLD server
is not authoritative for the host gaia.cs.umass.edu. Then this server will contain
a record for a domain that includes the host gaia.cs.umass.edu, for example,
(umass.edu, dns.umass.edu, NS). The edu TLD server would also contain
a Type Arecord, which maps the DNS server dns.umass.edu to an IP address, for
example, (dns.umass.edu, 128.119.40.111, A).
DNS Messages
Earlier in this section, we referred to DNS query and reply messages. These are the
only two kinds of DNS messages. Furthermore, both query and reply messages have
the same format, as shown in Figure 2.23.The semantics of the various fields in a
DNS message are as follows:
• The first 12 bytes is the header section, which has a number of fields. The first field
is a 16-bit number that identifies the query. This identifier is copied into the reply
message to a query, allowing the client to match received replies with sent queries.
There are a number of flags in the flag field. A 1-bit query/reply flag indicates
whether the message is a query (0) or a reply (1). A1-bit authoritative flag is set in a
reply message when a DNS server is an authoritative server for a queried name. A
1-bit recursion-desired flag is set when a client (host or DNS server) desires that the
DNS server perform recursion when it doesn’t have the record. A 1-bit recursionavailable
field is set in a reply if the DNS server supports recursion. In the header,
140 CHAPTER 2 • APPLICATION LAYER
there are also four number-of fields. These fields indicate the number of occurrences
of the four types of data sections that follow the header.
• The question section contains information about the query that is being made.
This section includes (1) a name field that contains the name that is being
queried, and (2) a type field that indicates the type of question being asked about
the name—for example, a host address associated with a name (Type A) or the
mail server for a name (Type MX).
• In a reply from a DNS server, the answer section contains the resource records
for the name that was originally queried. Recall that in each resource record there
is the Type (for example, A, NS, CNAME, and MX), the Value, and the TTL.
A reply can return multiple RRs in the answer, since a hostname can have multiple
IP addresses (for example, for replicated Web servers, as discussed earlier in
this section).
• The authority section contains records of other authoritative servers.
• The additional section contains other helpful records. For example, the answer
field in a reply to an MX query contains a resource record providing the canonical
hostname of a mail server. The additional section contains a Type A record
providing the IP address for the canonical hostname of the mail server.
How would you like to send a DNS query message directly from the host
you’re working on to some DNS server? This can easily be done with the nslookup
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 141
Identification
Number of questions
Number of authority RRs
Name, type fields for
a query
12 bytes
RRs in response to query
Records for
authoritative servers
Additional “helpful”
info that may be used
Flags
Number of answer RRs
Number of additional RRs
Authority
(variable number of resource records)
Additional information
(variable number of resource records)
Answers
(variable number of resource records)
Questions
(variable number of questions)
Figure 2.23  DNS message format
program, which is available from most Windows and UNIX platforms. For example,
from a Windows host, open the Command Prompt and invoke the nslookup program
by simply typing “nslookup.” After invoking nslookup, you can send a DNS
query to any DNS server (root, TLD, or authoritative). After receiving the reply
message from the DNS server, nslookup will display the records included in the
reply (in a human-readable format). As an alternative to running nslookup from your
own host, you can visit one of many Web sites that allow you to remotely employ
nslookup. (Just type “nslookup” into a search engine and you’ll be brought to one of
these sites.) The DNS Wireshark lab at the end of this chapter will allow you to
explore the DNS in much more detail.
Inserting Records into the DNS Database
The discussion above focused on how records are retrieved from the DNS database.
You might be wondering how records get into the database in the first place. Let’s look
at how this is done in the context of a specific example. Suppose you have just created
an exciting new startup company called Network Utopia. The first thing you’ll surely
want to do is register the domain name networkutopia.com at a registrar. A
registrar is a commercial entity that verifies the uniqueness of the domain name,
enters the domain name into the DNS database (as discussed below), and collects a
small fee from you for its services. Prior to 1999, a single registrar, Network Solutions,
had a monopoly on domain name registration for com, net, and org domains. But
now there are many registrars competing for customers, and the Internet Corporation
for Assigned Names and Numbers (ICANN) accredits the various registrars. A complete
list of accredited registrars is available at http://www.internic.net.
When you register the domain name networkutopia.com with some registrar,
you also need to provide the registrar with the names and IP addresses of your
primary and secondary authoritative DNS servers. Suppose the names and IP
addresses are dns1.networkutopia.com, dns2.networkutopia.com,
212.212.212.1, and 212.212.212.2. For each of these two authoritative
DNS servers, the registrar would then make sure that a Type NS and a Type Arecord
are entered into the TLD com servers. Specifically, for the primary authoritative
server for networkutopia.com, the registrar would insert the following two
resource records into the DNS system:
(networkutopia.com, dns1.networkutopia.com, NS)
(dns1.networkutopia.com, 212.212.212.1, A)
You’ll also have to make sure that the Type A resource record for your Web server
www.networkutopia.com and the Type MX resource record for your mail
server mail.networkutopia.com are entered into your authoritative DNS
servers. (Until recently, the contents of each DNS server were configured statically,
142 CHAPTER 2 • APPLICATION LAYER
2.5 • DNS—THE INTERNET’S DIRECTORY SERVICE 143
DNS VULNERABILITIES
We have seen that DNS is a critical component of the Internet infrastructure, with
many important services - including the Web and e-mail - simply incapable of functioning
without it. We therefore naturally ask, how can DNS be attacked? Is DNS a
sitting duck, waiting to be knocked out of service, while taking most Internet applications
down with it?
The first type of attack that comes to mind is a DDoS bandwidth-flooding attack (see
Section 1.6) against DNS servers. For example, an attacker could attempt to send to
each DNS root server a deluge of packets, so many that the majority of legitimate DNS
queries never get answered. Such a large-scale DDoS attack against DNS root servers
actually took place on October 21, 2002. In this attack, the attackers leveraged a botnet
to send truck loads of ICMP ping messages to each of the 13 DNS root servers.
(ICMP messages are discussed in Chapter 4. For now, it suffices to know that ICMP packets
are special types of IP datagrams.) Fortunately, this large-scale attack caused minimal
damage, having little or no impact on users’ Internet experience. The attackers did
succeed at directing a deluge of packets at the root servers. But many of the DNS root
servers were protected by packet filters, configured to always block all ICMP ping
messages directed at the root servers. These protected servers were thus spared and
functioned as normal. Furthermore, most local DNS servers cache the IP addresses of toplevel-
domain servers, allowing the query process to often bypass the DNS root servers.
A potentially more effective DDoS attack against DNS would be send a deluge of
DNS queries to top-level-domain servers, for example, to all the top-level-domain
servers that handle the .com domain. It would be harder to filter DNS queries directed
to DNS servers; and top-level-domain servers are not as easily bypassed as are
root servers. But the severity of such an attack would be partially mitigated by
caching in local DNS servers.
DNS could potentially be attacked in other ways. In a man-in-the-middle attack,
the attacker intercepts queries from hosts and returns bogus replies. In the DNS poisoning
attack, the attacker sends bogus replies to a DNS server, tricking the server
into accepting bogus records into its cache. Either of these attacks could be used, for
example, to redirect an unsuspecting Web user to the attacker’s Web site. These
attacks, however, are difficult to implement, as they require intercepting packets or
throttling servers [Skoudis 2006].
Another important DNS attack is not an attack on the DNS service per se, but
instead exploits the DNS infrastructure to launch a DDoS attack against a targeted host
(for example, your university’s mail server). In this attack, the attacker sends DNS
queries to many authoritative DNS servers, with each query having the spoofed source
address of the targeted host. The DNS servers then send their replies directly to the targeted
host. If the queries can be crafted in such a way that a response is much larger
FOCUS ON SECURITY
for example, from a configuration file created by a system manager. More recently,
an UPDATE option has been added to the DNS protocol to allow data to be dynamically
added or deleted from the database via DNS messages. [RFC 2136] and [RFC
3007] specify DNS dynamic updates.)
Once all of these steps are completed, people will be able to visit your Web site
and send e-mail to the employees at your company. Let’s conclude our discussion of
DNS by verifying that this statement is true. This verification also helps to solidify
what we have learned about DNS. Suppose Alice in Australia wants to view the Web
page www.networkutopia.com. As discussed earlier, her host will first send a
DNS query to her local DNS server. The local DNS server will then contact a TLD
com server. (The local DNS server will also have to contact a root DNS server if the
address of a TLD com server is not cached.) This TLD server contains the Type NS
and Type A resource records listed above, because the registrar had these resource
records inserted into all of the TLD com servers. The TLD com server sends a reply
to Alice’s local DNS server, with the reply containing the two resource records. The
local DNS server then sends a DNS query to 212.212.212.1, asking for the
Type A record corresponding to www.networkutopia.com. This record provides
the IP address of the desired Web server, say, 212.212.71.4, which the
local DNS server passes back to Alice’s host. Alice’s browser can now initiate a TCP
connection to the host 212.212.71.4 and send an HTTP request over the connection.
Whew! There’s a lot more going on than what meets the eye when one surfs
the Web!
2.6 Peer-to-Peer Applications
The applications described in this chapter thus far—including the Web, e-mail, and
DNS—all employ client-server architectures with significant reliance on always-on
infrastructure servers. Recall from Section 2.1.1 that with a P2P architecture, there
is minimal (or no) reliance on always-on infrastructure servers. Instead, pairs of
intermittently connected hosts, called peers, communicate directly with each other.
144 CHAPTER 2 • APPLICATION LAYER
(in bytes) than a query (so-called amplification), then the attacker can potentially overwhelm
the target without having to generate much of its own traffic. Such reflection
attacks exploiting DNS have had limited success to date [Mirkovic 2005].
In summary, DNS has demonstrated itself to be surprisingly robust against attacks.
To date, there hasn’t been an attack that has successfully impeded the DNS service.
There have been successful reflector attacks; however, these attacks can be (and are
being) addressed by appropriate configuration of DNS servers.
FOCUS ON SECURITY
The peers are not owned by a service provider, but are instead desktops and laptops
controlled by users.
In this section we’ll examine two different applications that are particularly
well-suited for P2P designs. The first is file distribution, where the application distributes
a file from a single source to a large number of peers. File distribution is a
nice place to start our investigation of P2P, as it clearly exposes the self-scalability
of P2P architectures. As a specific example for file distribution, we’ll describe
the popular BitTorrent system. The second P2P application we’ll examine is a
database distributed over a large community of peers. For this application, we’ll
explore the concept of a Distributed Hash Table (DHT).
2.6.1 P2P File Distribution
We begin our foray into P2P by considering a very natural application, namely,
distributing a large file from a single server to a large number of hosts (called
peers). The file might be a new version of the Linux operating system, a software
patch for an existing operating system or application, an MP3 music file, or an
MPEG video file. In client-server file distribution, the server must send a copy of
the file to each of the peers—placing an enormous burden on the server and consuming
a large amount of server bandwidth. In P2P file distribution, each peer can
redistribute any portion of the file it has received to any other peers, thereby
assisting the server in the distribution process. As of 2012, the most popular P2P
file distribution protocol is BitTorrent. Originally developed by Bram Cohen,
there are now many different independent BitTorrent clients conforming to the
BitTorrent protocol, just as there are a number of Web browser clients that
conform to the HTTP protocol. In this subsection, we first examine the selfscalability
of P2P architectures in the context of file distribution. We then describe
BitTorrent in some detail, highlighting its most important characteristics and
features.
Scalability of P2P Architectures
To compare client-server architectures with peer-to-peer architectures, and illustrate
the inherent self-scalability of P2P, we now consider a simple quantitative model for
distributing a file to a fixed set of peers for both architecture types. As shown in Figure
2.24, the server and the peers are connected to the Internet with access links.
Denote the upload rate of the server’s access link by us, the upload rate of the ith
peer’s access link by ui, and the download rate of the ith peer’s access link by di.
Also denote the size of the file to be distributed (in bits) by F and the number of
peers that want to obtain a copy of the file by N. The distribution time is the time it
takes to get a copy of the file to all N peers. In our analysis of the distribution time
below, for both client-server and P2P architectures, we make the simplifying (and
generally accurate [Akella 2003]) assumption that the Internet core has abundant
2.6 • PEER-TO-PEER APPLICATIONS 145
bandwidth, implying that all of the bottlenecks are in access networks. We also suppose
that the server and clients are not participating in any other network applications,
so that all of their upload and download access bandwidth can be fully
devoted to distributing this file.
Let’s first determine the distribution time for the client-server architecture,
which we denote by Dcs. In the client-server architecture, none of the peers aids in
distributing the file. We make the following observations:
• The server must transmit one copy of the file to each of the N peers. Thus the
server must transmit NF bits. Since the server’s upload rate is us, the time to distribute
the file must be at least NF/us.
• Let dmin denote the download rate of the peer with the lowest download rate, that
is, dmin = min{d1,dp,...,dN}. The peer with the lowest download rate cannot
obtain all F bits of the file in less than F/dmin seconds. Thus the minimum distribution
time is at least F/dmin.
Putting these two observations together, we obtain
Dcs Ú maxb .
NF
us
,
F
dmin
r
146 CHAPTER 2 • APPLICATION LAYER
Internet
File: F
Server
us
u1 u2
u3
d1
d2
d3
u4
u u5 6
d4
d5
d6
uN
dN
Figure 2.24  An illustrative file distribution problem
This provides a lower bound on the minimum distribution time for the client-server
architecture. In the homework problems you will be asked to show that the server
can schedule its transmissions so that the lower bound is actually achieved. So let’s
take this lower bound provided above as the actual distribution time, that is,
(2.1)
We see from Equation 2.1 that for N large enough, the client-server distribution time
is given by NF/us. Thus, the distribution time increases linearly with the number of
peers N. So, for example, if the number of peers from one week to the next increases
a thousand-fold from a thousand to a million, the time required to distribute the file
to all peers increases by 1,000.
Let’s now go through a similar analysis for the P2P architecture, where each
peer can assist the server in distributing the file. In particular, when a peer receives
some file data, it can use its own upload capacity to redistribute the data to other
peers. Calculating the distribution time for the P2P architecture is somewhat more
complicated than for the client-server architecture, since the distribution time
depends on how each peer distributes portions of the file to the other peers. Nevertheless,
a simple expression for the minimal distribution time can be obtained
[Kumar 2006]. To this end, we first make the following observations:
• At the beginning of the distribution, only the server has the file. To get this file
into the community of peers, the server must send each bit of the file at least once
into its access link. Thus, the minimum distribution time is at least F/us. (Unlike
the client-server scheme, a bit sent once by the server may not have to be sent by
the server again, as the peers may redistribute the bit among themselves.)
• As with the client-server architecture, the peer with the lowest download rate
cannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum
distribution time is at least F/dmin.
• Finally, observe that the total upload capacity of the system as a whole is equal
to the upload rate of the server plus the upload rates of each of the individual
peers, that is, utotal = us + u1 + … + uN. The system must deliver (upload) F bits
to each of the N peers, thus delivering a total of NF bits. This cannot be done at a
rate faster than utotal. Thus, the minimum distribution time is also at least
NF/(us + u1 + … + uN).
Putting these three observations together, we obtain the minimum distribution time
for P2P, denoted by DP2P.
DP2P Ú max c (2.2)
F
us
,
F
dmin
,
NF
us + a
N
i=1
ui
s
Dcs = maxb
NF
us
,
F
dmin
r
2.6 • PEER-TO-PEER APPLICATIONS 147
Equation 2.2 provides a lower bound for the minimum distribution time for the P2P
architecture. It turns out that if we imagine that each peer can redistribute a bit as
soon as it receives the bit, then there is a redistribution scheme that actually achieves
this lower bound [Kumar 2006]. (We will prove a special case of this result in the
homework.) In reality, where chunks of the file are redistributed rather than individual
bits, Equation 2.2 serves as a good approximation of the actual minimum distribution
time. Thus, let’s take the lower bound provided by Equation 2.2 as the actual
minimum distribution time, that is,
(2.3)
Figure 2.25 compares the minimum distribution time for the client-server and
P2P architectures assuming that all peers have the same upload rate u. In Figure
2.25, we have set F/u = 1 hour, us = 10u, and dmin = us. Thus, a peer can transmit the
entire file in one hour, the server transmission rate is 10 times the peer upload rate,
and (for simplicity) the peer download rates are set large enough so as not to have
an effect. We see from Figure 2.25 that for the client-server architecture, the distribution
time increases linearly and without bound as the number of peers
increases. However, for the P2P architecture, the minimal distribution time is not
only always less than the distribution time of the client-server architecture; it is also
less than one hour for any number of peers N. Thus, applications with the P2P
architecture can be self-scaling. This scalability is a direct consequence of peers
being redistributors as well as consumers of bits.
DP2P = max c
F
us
,
F
dmin
,
NF
us + a
N
i=1
ui
s
148 CHAPTER 2 • APPLICATION LAYER
0
0 5 10 15 20 25 30
N
Minimum distributioin tiime
35
0.5
1.5
2.5
1.0
3.0
2.0
3.5
Client-Server
P2P
Figure 2.25  Distribution time for P2P and client-server architectures
BitTorrent
BitTorrent is a popular P2P protocol for file distribution [Chao 2011]. In BitTorrent
lingo, the collection of all peers participating in the distribution of a particular
file is called a torrent. Peers in a torrent download equal-size chunks of the file
from one another, with a typical chunk size of 256 KBytes. When a peer first joins
a torrent, it has no chunks. Over time it accumulates more and more chunks. While
it downloads chunks it also uploads chunks to other peers. Once a peer has
acquired the entire file, it may (selfishly) leave the torrent, or (altruistically) remain
in the torrent and continue to upload chunks to other peers. Also, any peer may leave
the torrent at any time with only a subset of chunks, and later rejoin the torrent.
Let’s now take a closer look at how BitTorrent operates. Since BitTorrent is a
rather complicated protocol and system, we’ll only describe its most important
mechanisms, sweeping some of the details under the rug; this will allow us to see
the forest through the trees. Each torrent has an infrastructure node called a tracker.
When a peer joins a torrent, it registers itself with the tracker and periodically
informs the tracker that it is still in the torrent. In this manner, the tracker keeps
track of the peers that are participating in the torrent. A given torrent may have
fewer than ten or more than a thousand peers participating at any instant of time.
As shown in Figure 2.26, when a new peer, Alice, joins the torrent, the tracker
randomly selects a subset of peers (for concreteness, say 50) from the set of participating
peers, and sends the IP addresses of these 50 peers to Alice. Possessing this list of
peers, Alice attempts to establish concurrent TCP connections with all the peers on this
list. Let’s call all the peers with which Alice succeeds in establishing a TCP connection
“neighboring peers.” (In Figure 2.26, Alice is shown to have only three neighboring
peers. Normally, she would have many more.) As time evolves, some of these
peers may leave and other peers (outside the initial 50) may attempt to establish TCP
connections with Alice. So a peer’s neighboring peers will fluctuate over time.
At any given time, each peer will have a subset of chunks from the file, with different
peers having different subsets. Periodically, Alice will ask each of her neighboring
peers (over the TCP connections) for the list of the chunks they have. If Alice has L
different neighbors, she will obtain L lists of chunks. With this knowledge, Alice will
issue requests (again over the TCP connections) for chunks she currently does not have.
So at any given instant of time, Alice will have a subset of chunks and will
know which chunks her neighbors have. With this information, Alice will have two
important decisions to make. First, which chunks should she request first from her
neighbors? And second, to which of her neighbors should she send requested
chunks? In deciding which chunks to request, Alice uses a technique called rarest
first. The idea is to determine, from among the chunks she does not have, the
chunks that are the rarest among her neighbors (that is, the chunks that have the
fewest repeated copies among her neighbors) and then request those rarest chunks
first. In this manner, the rarest chunks get more quickly redistributed, aiming to
(roughly) equalize the numbers of copies of each chunk in the torrent.
2.6 • PEER-TO-PEER APPLICATIONS 149
To determine which requests she responds to, BitTorrent uses a clever trading
algorithm. The basic idea is that Alice gives priority to the neighbors that are currently
supplying her data at the highest rate. Specifically, for each of her neighbors,
Alice continually measures the rate at which she receives bits and determines the four
peers that are feeding her bits at the highest rate. She then reciprocates by sending
chunks to these same four peers. Every 10 seconds, she recalculates the rates and possibly
modifies the set of four peers. In BitTorrent lingo, these four peers are said to
be unchoked. Importantly, every 30 seconds, she also picks one additional neighbor
at random and sends it chunks. Let’s call the randomly chosen peer Bob. In BitTorrent
lingo, Bob is said to be optimistically unchoked. Because Alice is sending data
to Bob, she may become one of Bob’s top four uploaders, in which case Bob would
start to send data to Alice. If the rate at which Bob sends data to Alice is high enough,
Bob could then, in turn, become one of Alice’s top four uploaders. In other words,
every 30 seconds, Alice will randomly choose a new trading partner and initiate trading
with that partner. If the two peers are satisfied with the trading, they will put each
other in their top four lists and continue trading with each other until one of the peers
finds a better partner. The effect is that peers capable of uploading at compatible rates
tend to find each other. The random neighbor selection also allows new peers to get
150 CHAPTER 2 • APPLICATION LAYER
Tracker
Trading chunks
Peer
Obtain
list of
peers
Alice
Figure 2.26  File distribution with BitTorrent
chunks, so that they can have something to trade. All other neighboring peers besides
these five peers (four “top” peers and one probing peer) are “choked,” that is, they do
not receive any chunks from Alice. BitTorrent has a number of interesting mechanisms
that are not discussed here, including pieces (mini-chunks), pipelining, random
first selection, endgame mode, and anti-snubbing [Cohen 2003].
The incentive mechanism for trading just described is often referred to as tit-for-tat
[Cohen 2003]. It has been shown that this incentive scheme can be circumvented
[Liogkas 2006; Locher 2006; Piatek 2007]. Nevertheless, the BitTorrent ecosystem is
wildly successful, with millions of simultaneous peers actively sharing files in hundreds
of thousands of torrents. If BitTorrent had been designed without tit-for-tat (or a
variant), but otherwise exactly the same, BitTorrent would likely not even exist now, as
the majority of the users would have been freeriders [Saroiu 2002].
Interesting variants of the BitTorrent protocol are proposed [Guo 2005; Piatek
2007]. Also, many of the P2P live streaming applications, such as PPLive and
ppstream, have been inspired by BitTorrent [Hei 2007].
2.6.2 Distributed Hash Tables (DHTs)
In this section, we will consider how to implement a simple database in a P2P network.
Let’s begin by describing a centralized version of this simple database, which
will simply contain (key, value) pairs. For example, the keys could be social security
numbers and the values could be the corresponding human names; in this case,
an example key-value pair is (156-45-7081, Johnny Wu). Or the keys could be content
names (e.g., names of movies, albums, and software), and the value could be
the IP address at which the content is stored; in this case, an example key-value pair
is (Led Zeppelin IV, 128.17.123.38). We query the database with a key. If there are
one or more key-value pairs in the database that match the query key, the database
returns the corresponding values. So, for example, if the database stores social security
numbers and their corresponding human names, we can query with a specific
social security number, and the database returns the name of the human who has that
social security number. Or, if the database stores content names and their corresponding
IP addresses, we can query with a specific content name, and the database
returns the IP addresses that store the specific content.
Building such a database is straightforward with a client-server architecture that
stores all the (key, value) pairs in one central server. So in this section, we’ll instead
consider how to build a distributed, P2P version of this database that will store the
(key, value) pairs over millions of peers. In the P2P system, each peer will only hold a
small subset of the totality of the (key, value) pairs. We’ll allow any peer to query the
distributed database with a particular key. The distributed database will then locate the
peers that have the corresponding (key, value) pairs and return the key-value pairs to
the querying peer. Any peer will also be allowed to insert new key-value pairs into the
database. Such a distributed database is referred to as a distributed hash table
(DHT).
2.6 • PEER-TO-PEER APPLICATIONS 151
Before describing how we can create a DHT, let’s first describe a specific
example DHT service in the context of P2P file sharing. In this case, a key is the
content name and the value is the IP address of a peer that has a copy of the content.
So, if Bob and Charlie each have a copy of the latest Linux distribution, then the
DHT database will include the following two key-value pairs: (Linux, IPBob) and
(Linux, IPCharlie). More specifically, since the DHT database is distributed over the
peers, some peer, say Dave, will be responsible for the key “Linux” and will have
the corresponding key-value pairs. Now suppose Alice wants to obtain a copy of
Linux. Clearly, she first needs to know which peers have a copy of Linux before she
can begin to download it. To this end, she queries the DHT with “Linux” as the key.
The DHT then determines that the peer Dave is responsible for the key “Linux.” The
DHT then contacts peer Dave, obtains from Dave the key-value pairs (Linux, IPBob)
and (Linux, IPCharlie), and passes them on to Alice. Alice can then download the latest
Linux distribution from either IPBob or IPCharlie.
Now let’s return to the general problem of designing a DHT for general keyvalue
pairs. One naïve approach to building a DHT is to randomly scatter the (key,
value) pairs across all the peers and have each peer maintain a list of the IP
addresses of all participating peers. In this design, the querying peer sends its query
to all other peers, and the peers containing the (key, value) pairs that match the key
can respond with their matching pairs. Such an approach is completely unscalable,
of course, as it would require each peer to not only know about all other peers (possibly
millions of such peers!) but even worse, have each query sent to all peers.
We now describe an elegant approach to designing a DHT. To this end, let’s first
assign an identifier to each peer, where each identifier is an integer in the range [0,
2n 1] for some fixed n. Note that each such identifier can be expressed by an n-bit
representation. Let’s also require each key to be an integer in the same range. The
astute reader may have observed that the example keys described a little earlier (social
security numbers and content names) are not integers. To create integers out of such
keys, we will use a hash function that maps each key (e.g., social security number) to
an integer in the range [0, 2n1]. Ahash function is a many-to-one function for which
two different inputs can have the same output (same integer), but the likelihood of the
having the same output is extremely small. (Readers who are unfamiliar with hash
functions may want to visit Chapter 7, in which hash functions are discussed in some
detail.) The hash function is assumed to be available to all peers in the system. Henceforth,
when we refer to the “key,” we are referring to the hash of the original key. So,
for example, if the original key is “Led Zeppelin IV,” the key used in the DHT will be
the integer that equals the hash of “Led Zeppelin IV.” As you may have guessed, this
is why “Hash” is used in the term “Distributed Hash Function.”
Let’s now consider the problem of storing the (key, value) pairs in the DHT. The
central issue here is defining a rule for assigning keys to peers. Given that each peer
has an integer identifier and that each key is also an integer in the same range, a natural
approach is to assign each (key, value) pair to the peer whose identifier is the
closest to the key. To implement such a scheme, we’ll need to define what is meant by
“closest,” for which many conventions are possible. For convenience, let’s define the
152 CHAPTER 2 • APPLICATION LAYER
VideoNote
Walking through
distributed hash tables
closest peer as the closest successor of the key. To gain some insight here, let’s take a
look at a specific example. Suppose n  4 so that all the peer and key identifiers are in
the range [0, 15]. Further suppose that there are eight peers in the system with identifiers
1, 3, 4, 5, 8, 10, 12, and 15. Finally, suppose we want to store the (key, value) pair
(11, Johnny Wu) in one of the eight peers. But in which peer? Using our closest convention,
since peer 12 is the closest successor for key 11, we therefore store the pair
(11, Johnny Wu) in the peer 12. [To complete our definition of closest, if the key is
exactly equal to one of the peer identifiers, we store the (key, value) pair in that matching
peer; and if the key is larger than all the peer identifiers, we use a modulo-2n convention,
storing the (key, value) pair in the peer with the smallest identifier.]
Now suppose a peer, Alice, wants to insert a (key, value) pair into the DHT.
Conceptually, this is straightforward: She first determines the peer whose identifier
is closest to the key; she then sends a message to that peer, instructing it to store the
(key, value) pair. But how does Alice determine the peer that is closest to the key? If
Alice were to keep track of all the peers in the system (peer IDs and corresponding
IP addresses), she could locally determine the closest peer. But such an approach
requires each peer to keep track of all other peers in the DHT—which is completely
impractical for a large-scale system with millions of peers.
Circular DHT
To address this problem of scale, let’s now consider organizing the peers into a
circle. In this circular arrangement, each peer only keeps track of its immediate successor
and immediate predecessor (modulo 2n). An example of such a circle is
shown in Figure 2.27(a). In this example, n is again 4 and there are the same eight
2.6 • PEER-TO-PEER APPLICATIONS 153
1
3
Who is
responsible
for key 11?
4
5
8
a. b.
10
12
15
1
3
4
5
8
10
12
15
Figure 2.27  (a) A circular DHT. Peer 3 wants to determine who is
responsible for key 11. (b) A circular DHT with shortcuts
peers from the previous example. Each peer is only aware of its immediate successor
and predecessor; for example, peer 5 knows the IP address and identifier for
peers 8 and 4 but does not necessarily know anything about any other peers that may
be in the DHT. This circular arrangement of the peers is a special case of an overlay
network. In an overlay network, the peers form an abstract logical network which
resides above the “underlay” computer network consisting of physical links, routers,
and hosts. The links in an overlay network are not physical links, but are simply virtual
liaisons between pairs of peers. In the overlay in Figure 2.27(a), there are eight
peers and eight overlay links; in the overlay in Figure 2.27(b) there are eight peers
and 16 overlay links. A single overlay link typically uses many physical links and
physical routers in the underlay network.
Using the circular overlay in Figure 2.27(a), now suppose that peer 3 wants to
determine which peer in the DHT is responsible for key 11. Using the circular overlay,
the origin peer (peer 3) creates a message saying “Who is responsible for key 11?” and
sends this message clockwise around the circle. Whenever a peer receives such a message,
because it knows the identifier of its successor and predecessor, it can determine
whether it is responsible for (that is, closest to) the key in question. If a peer is not
responsible for the key, it simply sends the message to its successor. So, for example,
when peer 4 receives the message asking about key 11, it determines that it is not
responsible for the key (because its successor is closer to the key), so it just passes the
message along to peer 5. This process continues until the message arrives at peer 12,
who determines that it is the closest peer to key 11. At this point, peer 12 can send a
message back to the querying peer, peer 3, indicating that it is responsible for key 11.
The circular DHT provides a very elegant solution for reducing the amount of
overlay information each peer must manage. In particular, each peer needs only to
be aware of two peers, its immediate successor and its immediate predecessor. But
this solution introduces yet a new problem. Although each peer is only aware of two
neighboring peers, to find the node responsible for a key (in the worst case), all N
nodes in the DHT will have to forward a message around the circle; N/2 messages
are sent on average.
Thus, in designing a DHT, there is tradeoff between the number of neighbors each
peer has to track and the number of messages that the DHT needs to send to resolve a
single query. On one hand, if each peer tracks all other peers (mesh overlay), then only
one message is sent per query, but each peer has to keep track of N peers. On the other
hand, with a circular DHT, each peer is only aware of two peers, but N/2 messages are
sent on average for each query. Fortunately, we can refine our designs of DHTs so that
the number of neighbors per peer as well as the number of messages per query is kept
to an acceptable size. One such refinement is to use the circular overlay as a foundation,
but add “shortcuts” so that each peer not only keeps track of its immediate successor
and predecessor, but also of a relatively small number of shortcut peers
scattered about the circle. An example of such a circular DHT with some shortcuts is
shown in Figure 2.27(b). Shortcuts are used to expedite the routing of query messages.
Specifically, when a peer receives a message that is querying for a key, it forwards the
154 CHAPTER 2 • APPLICATION LAYER
message to the neighbor (successor neighbor or one of the shortcut neighbors) which
is the closet to the key. Thus, in Figure 2.27(b), when peer 4 receives the message asking
about key 11, it determines that the closet peer to the key (among its neighbors) is
its shortcut neighbor 10 and then forwards the message directly to peer 10. Clearly,
shortcuts can significantly reduce the number of messages used to process a query.
The next natural question is “How many shortcut neighbors should a peer have,
and which peers should be these shortcut neighbors? This question has received significant
attention in the research community [Balakrishnan 2003; Androutsellis-
Theotokis 2004]. Importantly, it has been shown that the DHT can be designed so that
both the number of neighbors per peer as well as the number of messages per query is
O(log N), where N is the number of peers. Such designs strike a satisfactory compromise
between the extreme solutions of using mesh and circular overlay topologies.
Peer Churn
In P2P systems, a peer can come or go without warning. Thus, when designing a
DHT, we also must be concerned about maintaining the DHT overlay in the presence
of such peer churn. To get a big-picture understanding of how this could be
accomplished, let’s once again consider the circular DHT in Figure 2.27(a). To handle
peer churn, we will now require each peer to track (that is, know the IP address
of) its first and second successors; for example, peer 4 now tracks both peer 5 and
peer 8. We also require each peer to periodically verify that its two successors are
alive (for example, by periodically sending ping messages to them and asking for
responses). Let’s now consider how the DHT is maintained when a peer abruptly
leaves. For example, suppose peer 5 in Figure 2.27(a) abruptly leaves. In this case,
the two peers preceding the departed peer (4 and 3) learn that 5 has departed, since
it no longer responds to ping messages. Peers 4 and 3 thus need to update their successor
state information. Let’s consider how peer 4 updates its state:
1. Peer 4 replaces its first successor (peer 5) with its second successor (peer 8).
2. Peer 4 then asks its new first successor (peer 8) for the identifier and IP address of
its immediate successor (peer 10). Peer 4 then makes peer 10 its second successor.
In the homework problems, you will be asked to determine how peer 3 updates its
overlay routing information.
Having briefly addressed what has to be done when a peer leaves, let’s now
consider what happens when a peer wants to join the DHT. Let’s say a peer with
identifier 13 wants to join the DHT, and at the time of joining, it only knows about
peer 1’s existence in the DHT. Peer 13 would first send peer 1 a message, saying
“what will be 13’s predecessor and successor?” This message gets forwarded
through the DHT until it reaches peer 12, who realizes that it will be 13’s predecessor
and that its current successor, peer 15, will become 13’s successor. Next, peer 12
sends this predecessor and successor information to peer 13. Peer 13 can now join
2.6 • PEER-TO-PEER APPLICATIONS 155
the DHT by making peer 15 its successor and by notifying peer 12 that it should
change its immediate successor to 13.
DHTs have been finding widespread use in practice. For example, BitTorrent
uses the Kademlia DHT to create a distributed tracker. In the BitTorrent, the key is
the torrent identifier and the value is the IP addresses of all the peers currently participating
in the torrent [Falkner 2007, Neglia 2007]. In this manner, by querying
the DHT with a torrent identifier, a newly arriving BitTorrent peer can determine the
peer that is responsible for the identifier (that is, for tracking the peers in the torrent).
After having found that peer, the arriving peer can query it for a list of other
peers in the torrent.
2.7 Socket Programming: Creating Network
Applications
Now that we’ve looked at a number of important network applications, let’s explore
how network application programs are actually created. Recall from Section 2.1 that
a typical network application consists of a pair of programs—a client program and a
server program—residing in two different end systems. When these two programs
are executed, a client process and a server process are created, and these processes
communicate with each other by reading from, and writing to, sockets. When creating
a network application, the developer’s main task is therefore to write the code
for both the client and server programs.
There are two types of network applications. One type is an implementation
whose operation is specified in a protocol standard, such as an RFC or some
other standards document; such an application is sometimes referred to as
“open,” since the rules specifying its operation are known to all. For such an
implementation, the client and server programs must conform to the rules dictated
by the RFC. For example, the client program could be an implementation
of the client side of the FTP protocol, described in Section 2.3 and explicitly
defined in RFC 959; similarly, the server program could be an implementation of
the FTP server protocol, also explicitly defined in RFC 959. If one developer
writes code for the client program and another developer writes code for the
server program, and both developers carefully follow the rules of the RFC, then
the two programs will be able to interoperate. Indeed, many of today’s network
applications involve communication between client and server programs that
have been created by independent developers—for example, a Firefox browser
communicating with an Apache Web server, or a BitTorrent client communicating
with BitTorrent tracker.
The other type of network application is a proprietary network application.
In this case the client and server programs employ an application-layer protocol that
has not been openly published in an RFC or elsewhere. A single developer (or
156 CHAPTER 2 • APPLICATION LAYER
development team) creates both the client and server programs, and the developer
has complete control over what goes in the code. But because the code does not
implement an open protocol, other independent developers will not be able to
develop code that interoperates with the application.
In this section, we’ll examine the key issues in developing a client-server application,
and we’ll “get our hands dirty” by looking at code that implements a very
simple client-server application. During the development phase, one of the first
decisions the developer must make is whether the application is to run over TCP
or over UDP. Recall that TCP is connection oriented and provides a reliable bytestream
channel through which data flows between two end systems. UDP is
connectionless and sends independent packets of data from one end system to the
other, without any guarantees about delivery. Recall also that when a client or server
program implements a protocol defined by an RFC, it should use the well-known
port number associated with the protocol; conversely, when developing a proprietary
application, the developer must be careful to avoid using such well-known
port numbers. (Port numbers were briefly discussed in Section 2.1. They are covered
in more detail in Chapter 3.)
We introduce UDP and TCP socket programming by way of a simple UDP application
and a simple TCP application. We present the simple UDP and TCP applications
in Python. We could have written the code in Java, C, or C++, but we chose
Python mostly because Python clearly exposes the key socket concepts. With Python
there are fewer lines of code, and each line can be explained to the novice programmer
without difficulty. But there’s no need to be frightened if you are not familiar with
Python. You should be able to easily follow the code if you have experience programming
in Java, C, or C++.
If you are interested in client-server programming with Java, you are encouraged
to see the companion Web site for this textbook; in fact, you can find there all
the examples in this section (and associated labs) in Java. For readers who are interested
in client-server programming in C, there are several good references available
[Donahoo 2001; Stevens 1997; Frost 1994; Kurose 1996]; our Python examples
below have a similar look and feel to C.
2.7.1 Socket Programming with UDP
In this subsection, we’ll write simple client-server programs that use UDP; in the
following section, we’ll write similar programs that use TCP.
Recall from Section 2.1 that processes running on different machines communicate
with each other by sending messages into sockets. We said that each process is
analogous to a house and the process’s socket is analogous to a door. The application
resides on one side of the door in the house; the transport-layer protocol resides on
the other side of the door in the outside world. The application developer has control
of everything on the application-layer side of the socket; however, it has little control
of the transport-layer side.
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 157
Now let’s take a closer look at the interaction between two communicating
processes that use UDP sockets. Before the sending process can push a packet of
data out the socket door, when using UDP, it must first attach a destination address
to the packet. After the packet passes through the sender’s socket, the Internet will
use this destination address to route the packet through the Internet to the socket in
the receiving process. When the packet arrives at the receiving socket, the receiving
process will retrieve the packet through the socket, and then inspect the packet’s
contents and take appropriate action.
So you may be now wondering, what goes into the destination address that is
attached to the packet? As you might expect, the destination host’s IP address is part of
the destination address. By including the destination IP address in the packet, the
routers in the Internet will be able to route the packet through the Internet to the destination
host. But because a host may be running many network application processes,
each with one or more sockets, it is also necessary to identify the particular socket in
the destination host. When a socket is created, an identifier, called a port number, is
assigned to it. So, as you might expect, the packet’s destination address also includes
the socket’s port number. In summary, the sending process attaches to the packet a destination
address which consists of the destination host’s IP address and the destination
socket’s port number. Moreover, as we shall soon see, the sender’s source address—
consisting of the IP address of the source host and the port number of the source
socket—are also attached to the packet. However, attaching the source address to the
packet is typically not done by the UDP application code; instead it is automatically
done by the underlying operating system.
We’ll use the following simple client-server application to demonstrate socket
programming for both UDP and TCP:
1. The client reads a line of characters (data) from its keyboard and sends the data
to the server.
2. The server receives the data and converts the characters to uppercase.
3. The server sends the modified data to the client.
4. The client receives the modified data and displays the line on its screen.
Figure 2.28 highlights the main socket-related activity of the client and server that
communicate over the UDP transport service.
Now let’s get our hands dirty and take a look at the client-server program pair
for a UDP implementation of this simple application. We also provide a detailed,
line-by-line analysis after each program. We’ll begin with the UDP client, which
will send a simple application-level message to the server. In order for the server to
be able to receive and reply to the client’s message, it must be ready and running—
that is, it must be running as a process before the client sends its message.
The client program is called UDPClient.py, and the server program is called
UDPServer.py. In order to emphasize the key issues, we intentionally provide code
that is minimal. “Good code” would certainly have a few more auxiliary lines, in
158 CHAPTER 2 • APPLICATION LAYER
particular for handling error cases. For this application, we have arbitrarily chosen
12000 for the server port number.
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 159
Create socket, port=x:
Server
serverSocket =
socket(AF_INET,SOCK_DGRAM)
(Running on serverIP)
Client
Read UDP segment from
serverSocket
Write reply to
specifying client address,
port number
serverSocket
Create datagram with serverIP
and port=x;
send datagram via
clientSocket
Create socket:
clientSocket =
socket(AF_INET,SOCK_DGRAM)
Read datagram from
clientSocket
Close
clientSocket
Figure 2.28  The client-server application using UDP
UDPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ‘hostname’
serverPort = 12000
clientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)
message = raw_input(’Input lowercase sentence:’)
clientSocket.sendto(message,(serverName, serverPort))
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
print modifiedMessage
clientSocket.close()
Now let’s take a look at the various lines of code in UDPClient.py.
from socket import *
The socket module forms the basis of all network communications in Python. By
including this line, we will be able to create sockets within our program.
serverName = ‘hostname’
serverPort = 12000
The first line sets the string serverName to hostname. Here, we provide a string
containing either the IP address of the server (e.g., “128.138.32.126”) or the hostname
of the server (e.g., “cis.poly.edu”). If we use the hostname, then a DNS lookup
will automatically be performed to get the IP address.) The second line sets the integer
variable serverPort to 12000.
clientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)
This line creates the client’s socket, called clientSocket. The first parameter
indicates the address family; in particular, AF_INET indicates that the underlying
network is using IPv4. (Do not worry about this now—we will discuss IPv4 in
Chapter 4.) The second parameter indicates that the socket is of type SOCK_DGRAM,
which means it is a UDP socket (rather than a TCP socket). Note that we are not
specifying the port number of the client socket when we create it; we are instead letting
the operating system do this for us. Now that the client process’s door has been
created, we will want to create a message to send through the door.
160 CHAPTER 2 • APPLICATION LAYER
message = raw_input(’Input lowercase sentence:’)
raw_input() is a built-in function in Python. When this command is executed,
the user at the client is prompted with the words “Input data:” The user then uses her
keyboard to input a line, which is put into the variable message. Now that we have
a socket and a message, we will want to send the message through the socket to the
destination host.
clientSocket.sendto(message,(serverName, serverPort))
In the above line, the method sendto() attaches the destination address
(serverName, serverPort) to the message and sends the resulting packet into
the process’s socket, clientSocket. (As mentioned earlier, the source address is
also attached to the packet, although this is done automatically rather than explicitly
by the code.) Sending a client-to-server message via a UDP socket is that simple!
After sending the packet, the client waits to receive data from the server.
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 161
With the above line, when a packet arrives from the Internet at the client’s socket,
the packet’s data is put into the variable modifiedMessage and the packet’s
source address is put into the variable serverAddress. The variable
serverAddress contains both the server’s IP address and the server’s port
number. The program UDPClient doesn’t actually need this server address information,
since it already knows the server address from the outset; but this line of
Python provides the server address nevertheless. The method recvfrom also
takes the buffer size 2048 as input. (This buffer size works for most purposes.)
print modifiedMessage
This line prints out modifiedMessage on the user’s display. It should be the original
line that the user typed, but now capitalized.
clientSocket.close()
This line closes the socket. The process then terminates.
UDPServer.py
Let’s now take a look at the server side of the application:
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET, SOCK_DGRAM)
serverSocket.bind((’’, serverPort))
print ”The server is ready to receive”
while 1:
message, clientAddress = serverSocket.recvfrom(2048)
modifiedMessage = message.upper()
serverSocket.sendto(modifiedMessage, clientAddress)
Note that the beginning of UDPServer is similar to UDPClient. It also imports the
socket module, also sets the integer variable serverPort to 12000, and also creates
a socket of type SOCK_DGRAM (a UDP socket). The first line of code that is
significantly different from UDPClient is:
serverSocket.bind((’’, serverPort))
The above line binds (that is, assigns) the port number 12000 to the server’s socket.
Thus in UDPServer, the code (written by the application developer) is explicitly
assigning a port number to the socket. In this manner, when anyone sends a packet
to port 12000 at the IP address of the server, that packet will be directed to this
socket. UDPServer then enters a while loop; the while loop will allow UDPServer
to receive and process packets from clients indefinitely. In the while loop,
UDPServer waits for a packet to arrive.
message, clientAddress = serverSocket.recvfrom(2048)
This line of code is similar to what we saw in UDPClient. When a packet arrives at
the server’s socket, the packet’s data is put into the variable message and the
packet’s source address is put into the variable clientAddress. The variable
clientAddress contains both the client’s IP address and the client’s port number.
Here, UDPServer will make use of this address information, as it provides a return
address, similar to the return address with ordinary postal mail. With this source
address information, the server now knows to where it should direct its reply.
modifiedMessage = message.upper()
This line is the heart of our simple application. It takes the line sent by the client and
uses the method upper() to capitalize it.
serverSocket.sendto(modifiedMessage, clientAddress)
This last line attaches the client’s address (IP address and port number) to the capitalized
message, and sends the resulting packet into the server’s socket. (As mentioned
earlier, the server address is also attached to the packet, although this is done
automatically rather than explicitly by the code.) The Internet will then deliver the
packet to this client address. After the server sends the packet, it remains in the
while loop, waiting for another UDP packet to arrive (from any client running on
any host).
To test the pair of programs, you install and compile UDPClient.py in one host
and UDPServer.py in another host. Be sure to include the proper hostname or IP
address of the server in UDPClient.py. Next, you execute UDPServer.py, the compiled
server program, in the server host. This creates a process in the server that
idles until it is contacted by some client. Then you execute UDPClient.py, the compiled
client program, in the client. This creates a process in the client. Finally, to use
the application at the client, you type a sentence followed by a carriage return.
To develop your own UDP client-server application, you can begin by
slightly modifying the client or server programs. For example, instead of converting
all the letters to uppercase, the server could count the number of times the
letter s appears and return this number. Or you can modify the client so that after
receiving a capitalized sentence, the user can continue to send more sentences to
the server.
162 CHAPTER 2 • APPLICATION LAYER
2.7.2 Socket Programming with TCP
Unlike UDP, TCP is a connection-oriented protocol. This means that before the client
and server can start to send data to each other, they first need to handshake and establish
a TCP connection. One end of the TCP connection is attached to the client socket
and the other end is attached to a server socket. When creating the TCP connection,
we associate with it the client socket address (IP address and port number) and the
server socket address (IP address and port number). With the TCP connection established,
when one side wants to send data to the other side, it just drops the data into
the TCP connection via its socket. This is different from UDP, for which the server
must attach a destination address to the packet before dropping it into the socket.
Now let’s take a closer look at the interaction of client and server programs in
TCP. The client has the job of initiating contact with the server. In order for the
server to be able to react to the client’s initial contact, the server has to be ready.
This implies two things. First, as in the case of UDP, the TCP server must be running
as a process before the client attempts to initiate contact. Second, the server
program must have a special door—more precisely, a special socket—that welcomes
some initial contact from a client process running on an arbitrary host. Using
our house/door analogy for a process/socket, we will sometimes refer to the client’s
initial contact as “knocking on the welcoming door.”
With the server process running, the client process can initiate a TCP connection
to the server. This is done in the client program by creating a TCP socket. When
the client creates its TCP socket, it specifies the address of the welcoming socket in
the server, namely, the IP address of the server host and the port number of the
socket. After creating its socket, the client initiates a three-way handshake and
establishes a TCP connection with the server. The three-way handshake, which takes
place within the transport layer, is completely invisible to the client and server programs.
During the three-way handshake, the client process knocks on the welcoming door
of the server process. When the server “hears” the knocking, it creates a new door—
more precisely, a new socket that is dedicated to that particular client. In our example
below, the welcoming door is a TCP socket object that we call serverSocket; the
newly created socket dedicated to the client making the connection is called connectionSocket.
Students who are encountering TCP sockets for the first time sometimes
confuse the welcoming socket (which is the initial point of contact for all clients
wanting to communicate with the server), and each newly created server-side connection
socket that is subsequently created for communicating with each client.
From the application’s perspective, the client’s socket and the server’s connection
socket are directly connected by a pipe. As shown in Figure 2.29, the client
process can send arbitrary bytes into its socket, and TCP guarantees that the server
process will receive (through the connection socket) each byte in the order sent. TCP
thus provides a reliable service between the client and server processes. Furthermore,
just as people can go in and out the same door, the client process not only sends bytes
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 163
into but also receives bytes from its socket; similarly, the server process not only
receives bytes from but also sends bytes into its connection socket.
We use the same simple client-server application to demonstrate socket programming
with TCP: The client sends one line of data to the server, the server capitalizes
the line and sends it back to the client. Figure 2.30 highlights the main socket-related
activity of the client and server that communicate over the TCP transport service.
TCPClient.py
Here is the code for the client side of the application:
164 CHAPTER 2 • APPLICATION LAYER
Client process Server process
Client
socket
Welcoming
socket
Three-way handshake
Connection
socket
bytes
bytes
Figure 2.29  The TCPServer process has two sockets
from socket import *
serverName = ’servername’
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_STREAM)
clientSocket.connect((serverName,serverPort))
sentence = raw_input(‘Input lowercase sentence:’)
clientSocket.send(sentence)
modifiedSentence = clientSocket.recv(1024)
print ‘From Server:’, modifiedSentence
clientSocket.close()
Let’s now take a look at the various lines in the code that differ significantly
from the UDP implementation. The first such line is the creation of the client
socket.
clientSocket = socket(AF_INET, SOCK_STREAM)
This line creates the client’s socket, called clientSocket. The first parameter
again indicates that the underlying network is using IPv4. The second parameter
indicates that the socket is of type SOCK_STREAM, which means it is a TCP socket
(rather than a UDP socket). Note that we are again not specifying the port number
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 165
Close
connectionSocket
Write reply to
connectionSocket
Read request from
connectionSocket
Create socket, port=x,
for incoming request:
Server
serverSocket =
socket()
Wait for incoming
connection request:
connectionSocket =
serverSocket.accept()
(Running on serverIP)
Client
TCP
connection setup Create socket, connect
to serverIP, port=x:
clientSocket =
socket()
Read reply from
clientSocket
Send request using
clientSocket
Close
clientSocket
Figure 2.30  The client-server application using TCP
of the client socket when we create it; we are instead letting the operating system
do this for us. Now the next line of code is very different from what we saw in
UDPClient:
clientSocket.connect((serverName,serverPort))
Recall that before the client can send data to the server (or vice versa) using a TCP
socket, a TCP connection must first be established between the client and server.
The above line initiates the TCP connection between the client and server. The
parameter of the connect() method is the address of the server side of the connection.
After this line of code is executed, the three-way handshake is performed
and a TCP connection is established between the client and server.
166 CHAPTER 2 • APPLICATION LAYER
sentence = raw_input(‘Input lowercase sentence:’)
As with UDPClient, the above obtains a sentence from the user. The string sentence
continues to gather characters until the user ends the line by typing a carriage return.
The next line of code is also very different from UDPClient:
clientSocket.send(sentence)
The above line sends the string sentence through the client’s socket and into the
TCP connection. Note that the program does not explicitly create a packet and attach
the destination address to the packet, as was the case with UDP sockets. Instead the
client program simply drops the bytes in the string sentence into the TCP connection.
The client then waits to receive bytes from the server.
modifiedSentence = clientSocket.recv(2048)
When characters arrive from the server, they get placed into the string modified-
Sentence. Characters continue to accumulate in modifiedSentence until the
line ends with a carriage return character. After printing the capitalized sentence, we
close the client’s socket:
clientSocket.close()
This last line closes the socket and, hence, closes the TCP connection between the
client and the server. It causes TCP in the client to send a TCP message to TCP in
the server (see Section 3.5).
TCPServer.py
Now let’s take a look at the server program.
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET,SOCK_STREAM)
serverSocket.bind((‘’,serverPort))
serverSocket.listen(1)
print ‘The server is ready to receive’
while 1:
connectionSocket, addr = serverSocket.accept()
sentence = connectionSocket.recv(1024)
capitalizedSentence = sentence.upper()
connectionSocket.send(capitalizedSentence)
connectionSocket.close()
Let’s now take a look at the lines that differ significantly from UDPServer and TCPClient.
As with TCPClient, the server creates a TCP socket with:
serverSocket=socket(AF_INET,SOCK_STREAM)
Similar to UDPServer, we associate the server port number, serverPort, with
this socket:
serverSocket.bind((‘’,serverPort))
But with TCP, serverSocket will be our welcoming socket. After establishing
this welcoming door, we will wait and listen for some client to knock on the
door:
serverSocket.listen(1)
This line has the server listen for TCP connection requests from the client. The
parameter specifies the maximum number of queued connections (at least 1).
connectionSocket, addr = serverSocket.accept()
When a client knocks on this door, the program invokes the accept() method for
serverSocket, which creates a new socket in the server, called connectionSocket,
dedicated to this particular client. The client and server then complete
the handshaking, creating a TCP connection between the client’s clientSocket
and the server’s connectionSocket. With the TCP connection established, the
client and server can now send bytes to each other over the connection. With TCP, all
bytes sent from one side not are not only guaranteed to arrive at the other side but also
guaranteed arrive in order.
connectionSocket.close()
2.7 • SOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS 167
In this program, after sending the modified sentence to the client, we close the connection
socket. But since serverSocket remains open, another client can now
knock on the door and send the server a sentence to modify.
This completes our discussion of socket programming in TCP. You are encouraged
to run the two programs in two separate hosts, and also to modify them to
achieve slightly different goals. You should compare the UDP program pair with the
TCP program pair and see how they differ. You should also do many of the socket
programming assignments described at the ends of Chapters 2, 4, and 7. Finally, we
hope someday, after mastering these and more advanced socket programs, you will
write your own popular network application, become very rich and famous, and
remember the authors of this textbook!
2.8 Summary
In this chapter, we’ve studied the conceptual and the implementation aspects of network
applications. We’ve learned about the ubiquitous client-server architecture
adopted by many Internet applications and seen its use in the HTTP, FTP, SMTP,
POP3, and DNS protocols. We’ve studied these important application-level protocols,
and their corresponding associated applications (the Web, file transfer, e-mail,
and DNS) in some detail. We’ve also learned about the increasingly prevalent P2P
architecture and how it is used in many applications. We’ve examined how the
socket API can be used to build network applications. We’ve walked through the use
of sockets for connection-oriented (TCP) and connectionless (UDP) end-to-end
transport services. The first step in our journey down the layered network architecture
is now complete!
At the very beginning of this book, in Section 1.1, we gave a rather vague, barebones
definition of a protocol: “the format and the order of messages exchanged
between two or more communicating entities, as well as the actions taken on the transmission
and/or receipt of a message or other event.” The material in this chapter, and
in particular our detailed study of the HTTP, FTP, SMTP, POP3, and DNS protocols,
has now added considerable substance to this definition. Protocols are a key concept
in networking; our study of application protocols has now given us the opportunity to
develop a more intuitive feel for what protocols are all about.
In Section 2.1, we described the service models that TCP and UDP offer to
applications that invoke them. We took an even closer look at these service models
when we developed simple applications that run over TCP and UDP in Section 2.7.
However, we have said little about how TCP and UDP provide these service models.
For example, we know that TCP provides a reliable data service, but we haven’t
said yet how it does so. In the next chapter we’ll take a careful look at not only the
what, but also the how and why of transport protocols.
168 CHAPTER 2 • APPLICATION LAYER
Equipped with knowledge about Internet application structure and applicationlevel
protocols, we’re now ready to head further down the protocol stack and examine
the transport layer in Chapter 3.
Homework Problems and Questions
Chapter 2 Review Questions
SECTION 2.1
R1. List five nonproprietary Internet applications and the application-layer
protocols that they use.
R2. What is the difference between network architecture and application
architecture?
R3. For a communication session between a pair of processes, which process is
the client and which is the server?
R4. For a P2P file-sharing application, do you agree with the statement, “There is
no notion of client and server sides of a communication session”? Why or
why not?
R5. What information is used by a process running on one host to identify a
process running on another host?
R6. Suppose you wanted to do a transaction from a remote client to a server as
fast as possible. Would you use UDP or TCP? Why?
R7. Referring to Figure 2.4, we see that none of the applications listed in Figure
2.4 requires both no data loss and timing. Can you conceive of an application
that requires no data loss and that is also highly time-sensitive?
R8. List the four broad classes of services that a transport protocol can provide.
For each of the service classes, indicate if either UDP or TCP (or both) provides
such a service.
R9. Recall that TCP can be enhanced with SSL to provide process-to-process
security services, including encryption. Does SSL operate at the transport
layer or the application layer? If the application developer wants TCP to be
enhanced with SSL, what does the developer have to do?
SECTIONS 2.2–2.5
R10. What is meant by a handshaking protocol?
R11. Why do HTTP, FTP, SMTP, and POP3 run on top of TCP rather than on UDP?
R12. Consider an e-commerce site that wants to keep a purchase record for each of
its customers. Describe how this can be done with cookies.
HOMEWORK PROBLEMS AND QUESTIONS 169
R13. Describe how Web caching can reduce the delay in receiving a requested
object. Will Web caching reduce the delay for all objects requested by a user
or for only some of the objects? Why?
R14. Telnet into a Web server and send a multiline request message. Include in the
request message the If-modified-since: header line to force a
response message with the 304 Not Modified status code.
R15. Why is it said that FTP sends control information “out-of-band”?
R16. Suppose Alice, with a Web-based e-mail account (such as Hotmail or gmail),
sends a message to Bob, who accesses his mail from his mail server using
POP3. Discuss how the message gets from Alice’s host to Bob’s host. Be sure
to list the series of application-layer protocols that are used to move the message
between the two hosts.
R17. Print out the header of an e-mail message you have recently received. How
many Received: header lines are there? Analyze each of the header lines
in the message.
R18. From a user’s perspective, what is the difference between the download-anddelete
mode and the download-and-keep mode in POP3?
R19. Is it possible for an organization’s Web server and mail server to have exactly
the same alias for a hostname (for example, foo.com)? What would be the
type for the RR that contains the hostname of the mail server?
R20. Look over your received emails, and examine the header of a message sent
from a user with an .edu email address. Is it possible to determine from the
header the IP address of the host from which the message was sent? Do the
same for a message sent from a gmail account.
SECTION 2.6
R21. In BitTorrent, suppose Alice provides chunks to Bob throughout a 30-second
interval. Will Bob necessarily return the favor and provide chunks to Alice in
this same interval? Why or why not?
R22. Consider a new peer Alice that joins BitTorrent without possessing any
chunks. Without any chunks, she cannot become a top-four uploader for any
of the other peers, since she has nothing to upload. How then will Alice get
her first chunk?
R23. What is an overlay network? Does it include routers? What are the edges in
the overlay network?
R24. Consider a DHT with a mesh overlay topology (that is, every peer tracks all
peers in the system). What are the advantages and disadvantages of such a
design? What are the advantages and disadvantages of a circular DHT (with
no shortcuts)?
170 CHAPTER 2 • APPLICATION LAYER
R25. List at least four different applications that are naturally suitable for P2P
architectures. (Hint: File distribution and instant messaging are two.)
SECTION 2.7
R26. In Section 2.7, the UDP server described needed only one socket, whereas the
TCP server needed two sockets. Why? If the TCP server were to support n
simultaneous connections, each from a different client host, how many sockets
would the TCP server need?
R27. For the client-server application over TCP described in Section 2.7, why must
the server program be executed before the client program? For the clientserver
application over UDP, why may the client program be executed before
the server program?
Problems
P1. True or false?
a. A user requests a Web page that consists of some text and three images.
For this page, the client will send one request message and receive four
response messages.
b. Two distinct Web pages (for example, www.mit.edu/research.html
and www.mit.edu/students.html) can be sent over the same persistent
connection.
c. With nonpersistent connections between browser and origin server, it is possible
for a single TCP segment to carry two distinct HTTP request messages.
d. The Date: header in the HTTP response message indicates when the
object in the response was last modified.
e. HTTP response messages never have an empty message body.
P2. Read RFC 959 for FTP. List all of the client commands that are supported by
the RFC.
P3. Consider an HTTP client that wants to retrieve a Web document at a given
URL. The IP address of the HTTP server is initially unknown. What transport
and application-layer protocols besides HTTP are needed in this scenario?
P4. Consider the following string of ASCII characters that were captured by
Wireshark when the browser sent an HTTP GET message (i.e., this is the actual
content of an HTTP GET message). The characters <cr><lf> are carriage
return and line-feed characters (that is, the italized character string <cr> in
the text below represents the single carriage-return character that was contained
at that point in the HTTP header). Answer the following questions,
indicating where in the HTTP GET message below you find the answer.
PROBLEMS 171
GET /cs453/index.html HTTP/1.1<cr><lf>Host: gai
a.cs.umass.edu<cr><lf>User-Agent: Mozilla/5.0 (
Windows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec
ko/20040804 Netscape/7.2 (ax) <cr><lf>Accept:ex
t/xml, application/xml, application/xhtml+xml, text
/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5
<cr><lf>Accept-Language: en-us,en;q=0.5<cr><lf>Accept-
Encoding: zip,deflate<cr><lf>Accept-Charset: ISO
-8859-1,utf-8;q=0.7,*;q=0.7<cr><lf>Keep-Alive: 300<cr>
<lf>Connection:keep-alive<cr><lf><cr><lf>
a. What is the URL of the document requested by the browser?
b. What version of HTTP is the browser running?
c. Does the browser request a non-persistent or a persistent connection?
d. What is the IP address of the host on which the browser is running?
e. What type of browser initiates this message? Why is the browser type
needed in an HTTP request message?
P5. The text below shows the reply sent from the server in response to the HTTP
GET message in the question above. Answer the following questions, indicating
where in the message below you find the answer.
172 CHAPTER 2 • APPLICATION LAYER
HTTP/1.1 200 OK<cr><lf>Date: Tue, 07 Mar 2008
12:39:45GMT<cr><lf>Server: Apache/2.0.52 (Fedora)
<cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46
GMT<cr><lf>ETag: “526c3-f22-a88a4c80”<cr><lf>Accept-
Ranges: bytes<cr><lf>Content-Length: 3874<cr><lf>
Keep-Alive: timeout=max=100<cr><lf>Connection:
Keep-Alive<cr><lf>Content-Type: text/html; charset=
ISO-8859-1<cr><lf><cr><lf><!doctype html public “-
//w3c//dtd html 4.0 transitional//en”><lf><html><lf>
<head><lf> <meta http-equiv=”Content-Type”
content=”text/html; charset=iso-8859-1”><lf> <meta
name=”GENERATOR” content=”Mozilla/4.79 [en] (Windows NT
5.0; U) Netscape]”><lf> <title>CMPSCI 453 / 591 /
NTU-ST550A Spring 2005 homepage</title><lf></head><lf>
<much more document text following here (not shown)>
a. Was the server able to successfully find the document or not? What time
was the document reply provided?
b. When was the document last modified?
c. How many bytes are there in the document being returned?
d. What are the first 5 bytes of the document being returned? Did the server
agree to a persistent connection?
P6. Obtain the HTTP/1.1 specification (RFC 2616). Answer the following questions:
a. Explain the mechanism used for signaling between the client and server
to indicate that a persistent connection is being closed. Can the client, the
server, or both signal the close of a connection?
b. What encryption services are provided by HTTP?
c. Can a client open three or more simultaneous connections with a given
server?
d. Either a server or a client may close a transport connection between them
if either one detects the connection has been idle for some time. Is it possible
that one side starts closing a connection while the other side is transmitting
data via this connection? Explain.
P7. Suppose within your Web browser you click on a link to obtain a Web page.
The IP address for the associated URL is not cached in your local host, so a
DNS lookup is necessary to obtain the IP address. Suppose that n DNS
servers are visited before your host receives the IP address from DNS; the
successive visits incur an RTT of RTT1, . . ., RTTn. Further suppose that the
Web page associated with the link contains exactly one object, consisting of a
small amount of HTML text. Let RTT0 denote the RTT between the local host
and the server containing the object. Assuming zero transmission time of the
object, how much time elapses from when the client clicks on the link until
the client receives the object?
P8. Referring to Problem P7, suppose the HTML file references eight very small
objects on the same server. Neglecting transmission times, how much time
elapses with
a. Non-persistent HTTP with no parallel TCP connections?
b. Non-persistent HTTP with the browser configured for 5 parallel connections?
c. Persistent HTTP?
P9. Consider Figure 2.12, for which there is an institutional network connected to
the Internet. Suppose that the average object size is 850,000 bits and that the
average request rate from the institution’s browsers to the origin servers is 16
requests per second. Also suppose that the amount of time it takes from when
the router on the Internet side of the access link forwards an HTTP request
until it receives the response is three seconds on average (see Section 2.2.5).
Model the total average response time as the sum of the average access delay
(that is, the delay from Internet router to institution router) and the average
Internet delay. For the average access delay, use ?/(1 – ?), where ? is the
average time required to send an object over the access link and  is the
arrival rate of objects to the access link.
a. Find the total average response time.
b. Now suppose a cache is installed in the institutional LAN. Suppose the
miss rate is 0.4. Find the total response time.
PROBLEMS 173
P10. Consider a short, 10-meter link, over which a sender can transmit at a rate of
150 bits/sec in both directions. Suppose that packets containing data are
100,000 bits long, and packets containing only control (e.g., ACK or handshaking)
are 200 bits long. Assume that N parallel connections each get 1/N
of the link bandwidth. Now consider the HTTP protocol, and suppose that
each downloaded object is 100 Kbits long, and that the initial downloaded
object contains 10 referenced objects from the same sender. Would parallel
downloads via parallel instances of non-persistent HTTP make sense in this
case? Now consider persistent HTTP. Do you expect significant gains over
the non-persistent case? Justify and explain your answer.
P11. Consider the scenario introduced in the previous problem. Now suppose that
the link is shared by Bob with four other users. Bob uses parallel instances of
non-persistent HTTP, and the other four users use non-persistent HTTP without
parallel downloads.
a. Do Bob’s parallel connections help him get Web pages more quickly?
Why or why not?
b. If all five users open five parallel instances of non-persistent HTTP, then
would Bob’s parallel connections still be beneficial? Why or why not?
P12. Write a simple TCP program for a server that accepts lines of input from a
client and prints the lines onto the server’s standard output. (You can do this by
modifying the TCPServer.py program in the text.) Compile and execute your
program. On any other machine that contains a Web browser, set the proxy
server in the browser to the host that is running your server program; also configure
the port number appropriately. Your browser should now send its GET
request messages to your server, and your server should display the messages
on its standard output. Use this platform to determine whether your browser
generates conditional GET messages for objects that are locally cached.
P13. What is the difference between MAIL FROM: in SMTP and From: in the
mail message itself?
P14. How does SMTP mark the end of a message body? How about HTTP? Can
HTTP use the same method as SMTP to mark the end of a message body?
Explain.
P15. Read RFC 5321 for SMTP. What does MTA stand for? Consider the following
received spam email (modified from a real spam email). Assuming only
the originator of this spam email is malacious and all other hosts are honest,
identify the malacious host that has generated this spam email.
From - Fri Nov 07 13:41:30 2008
Return-Path: <tennis5@pp33head.com>
Received: from barmail.cs.umass.edu
(barmail.cs.umass.edu [128.119.240.3]) by cs.umass.edu
(8.13.1/8.12.6) for <hg@cs.umass.edu>; Fri, 7 Nov 2008
13:27:10 -0500
174 CHAPTER 2 • APPLICATION LAYER
Received: from asusus-4b96 (localhost [127.0.0.1]) by
barmail.cs.umass.edu (Spam Firewall) for
<hg@cs.umass.edu>; Fri, 7 Nov 2008 13:27:07 -0500
(EST)
Received: from asusus-4b96 ([58.88.21.177]) by
barmail.cs.umass.edu for <hg@cs.umass.edu>; Fri,
07 Nov 2008 13:27:07 -0500 (EST)
Received: from [58.88.21.177] by
inbnd55.exchangeddd.com; Sat, 8 Nov 2008 01:27:07 +0700
From: "Jonny" <tennis5@pp33head.com>
To: <hg@cs.umass.edu>
Subject: How to secure your savings
P16. Read the POP3 RFC, RFC 1939. What is the purpose of the UIDL POP3
command?
P17. Consider accessing your e-mail with POP3.
a. Suppose you have configured your POP mail client to operate in the
download-and-delete mode. Complete the following transaction:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
S: blah blah ...
S: ..........blah
S: .
?
?
b. Suppose you have configured your POP mail client to operate in the
download-and-keep mode. Complete the following transaction:
C: list
S: 1 498
S: 2 912
S: .
C: retr 1
S: blah blah ...
S: ..........blah
S: .
?
?
PROBLEMS 175
176 CHAPTER 2 • APPLICATION LAYER
c. Suppose you have configured your POP mail client to operate in the
download-and-keep mode. Using your transcript in part (b), suppose you
retrieve messages 1 and 2, exit POP, and then five minutes later you again
access POP to retrieve new e-mail. Suppose that in the five-minute interval
no new messages have been sent to you. Provide a transcript of this
second POP session.
P18. a. What is a whois database?
b. Use various whois databases on the Internet to obtain the names of two
DNS servers. Indicate which whois databases you used.
c. Use nslookup on your local host to send DNS queries to three DNS servers:
your local DNS server and the two DNS servers you found in part (b). Try
querying for Type A, NS, and MX reports. Summarize your findings.
d. Use nslookup to find a Web server that has multiple IP addresses. Does
the Web server of your institution (school or company) have multiple IP
addresses?
e. Use the ARIN whois database to determine the IP address range used by
your university.
f. Describe how an attacker can use whois databases and the nslookup tool
to perform reconnaissance on an institution before launching an attack.
g. Discuss why whois databases should be publicly available.
P19. In this problem, we use the useful dig tool available on Unix and Linux hosts
to explore the hierarchy of DNS servers. Recall that in Figure 2.21, a DNS
server higher in the DNS hierarchy delegates a DNS query to a DNS server
lower in the hierarchy, by sending back to the DNS client the name of that
lower-level DNS server. First read the man page for dig, and then answer the
following questions.
a. Starting with a root DNS server (from one of the root servers [a-m].rootservers.
net), initiate a sequence of queries for the IP address for your
department’s Web server by using dig. Show the list of the names of DNS
servers in the delegation chain in answering your query.
b. Repeat part a) for several popular Web sites, such as google.com,
yahoo.com, or amazon.com.
P20. Suppose you can access the caches in the local DNS servers of your department.
Can you propose a way to roughly determine the Web servers (outside your
department) that are most popular among the users in your department? Explain.
P21. Suppose that your department has a local DNS server for all computers in the
department. You are an ordinary user (i.e., not a network/system administrator).
Can you determine if an external Web site was likely accessed from a
computer in your department a couple of seconds ago? Explain.
PROBLEMS 177
P22. Consider distributing a file of F = 15 Gbits to N peers. The server has an upload
rate of us = 30 Mbps, and each peer has a download rate of di = 2 Mbps and an
upload rate of u. For N = 10, 100, and 1,000 and u = 300 Kbps, 700 Kbps, and
2 Mbps, prepare a chart giving the minimum distribution time for each of
the combinations of N and u for both client-server distribution and P2P
distribution.
P23. Consider distributing a file of F bits to N peers using a client-server architecture.
Assume a fluid model where the server can simultaneously transmit to
multiple peers, transmitting to each peer at different rates, as long as the combined
rate does not exceed us.
a. Suppose that us/N = dmin. Specify a distribution scheme that has a distribution
time of NF/us.
b. Suppose that us/N = dmin. Specify a distribution scheme that has a distribution
time of F/ dmin.
c. Conclude that the minimum distribution time is in general given by
max{NF/us, F/ dmin}.
P24. Consider distributing a file of F bits to N peers using a P2P architecture.
Assume a fluid model. For simplicity assume that dmin is very large, so that
peer download bandwidth is never a bottleneck.
a. Suppose that us = (us + u1 + ... + uN)/N. Specify a distribution scheme
that has a distribution time of F/us.
b. Suppose that us = (us + u1 + ... + uN)/N. Specify a distribution scheme
that has a distribution time of NF/(us + u1 + ... + uN).
c. Conclude that the minimum distribution time is in general given by
max{F/us, NF/(us + u1 + ... + uN)}.
P25. Consider an overlay network with N active peers, with each pair of peers having
an active TCP connection. Additionally, suppose that the TCP connections
pass through a total of M routers. How many nodes and edges are there in the
corresponding overlay network?
P26. Suppose Bob joins a BitTorrent torrent, but he does not want to upload any
data to any other peers (so called free-riding).
a. Bob claims that he can receive a complete copy of the file that is shared
by the swarm. Is Bob’s claim possible? Why or why not?
b. Bob further claims that he can further make his “free-riding” more efficient
by using a collection of multiple computers (with distinct IP
addresses) in the computer lab in his department. How can he do that?
P27. In the circular DHT example in Section 2.6.2, suppose that peer 3 learns that
peer 5 has left. How does peer 3 update its successor state information?
Which peer is now its first successor? Its second successor?
VideoNote
Walking through
distributed hash tables
178 CHAPTER 2 • APPLICATION LAYER
P28. In the circular DHT example in Section 2.6.2, suppose that a new peer 6
wants to join the DHT and peer 6 initially only knows peer 15’s IP address.
What steps are taken?
P29. Because an integer in [0, 2n 1] can be expressed as an n-bit binary number in
a DHT, each key can be expressed as k = (k0, k1, . . . , kn–1), and each peer identifier
can be expressed p = (p0, p1, . . . , pn–1). Let’s now define the XOR distance
between a key k and peer p as
Describe how this metric can be used to assign (key, value) pairs to peers.
(To learn about how to build an efficient DHT using this natural metric, see
[Maymounkov 2002] in which the Kademlia DHT is described.)
P30. As DHTs are overlay networks, they may not necessarily match the underlay
physical network well in the sense that two neighboring peers might be
physically very far away; for example, one peer could be in Asia and its
neighbor could be in North America. If we randomly and uniformly assign
identifiers to newly joined peers, would this assignment scheme cause such
a mismatch? Explain. And how would such a mismatch affect the DHT’s
performance?
P31. Install and compile the Python programs TCPClient and UDPClient on one
host and TCPServer and UDPServer on another host.
a. Suppose you run TCPClient before you run TCPServer. What happens?
Why?
b. Suppose you run UDPClient before you run UDPServer. What happens?
Why?
c. What happens if you use different port numbers for the client and server
sides?
P32. Suppose that in UDPClient.py, after we create the socket, we add the line:
clientSocket.bind(('', 5432))
Will it become necessary to change UDPServer.py? What are the port numbers
for the sockets in UDPClient and UDPServer? What were they before
making this change?
P33. Can you configure your browser to open multiple simultaneous connections
to a Web site? What are the advantages and disadvantages of having a large
number of simultaneous TCP connections?
P34 We have seen that Internet TCP sockets treat the data being sent as a byte
stream but UDP sockets recognize message boundaries. What are one
d1k, p2 = a
n-1
j=0
 kj - pj 2j
advantage and one disadvantage of byte-oriented API versus having the API
explicitly recognize and preserve application-defined message boundaries?
P35. What is the Apache Web server? How much does it cost? What functionality
does it currently have? You may want to look at Wikipedia to answer this
question.
P36. Many BitTorrent clients use DHTs to create a distributed tracker. For these
DHTs, what is the “key” and what is the “value”?
Socket Programming Assignments
The companion Web site includes six socket programming assignments. The first
four assignments are summarized below. The fifth assignment makes use of the
ICMP protocol and is summarized at the end of Chapter 4. The sixth assignment
employs multimedia protocols and is summarized at the end of Chapter 7. It is
highly recommended that students complete several, if not all, of these assignments.
Students can find full details of these assignments, as well as important snippets of
the Python code, at the Web site http://www.awl.com/kurose-ross.
Assignment 1: Web Server
In this assignment, you will develop a simple Web server in Python that is capable
of processing only one request. Specifically, your Web server will (i) create a connection
socket when contacted by a client (browser); (ii) receive the HTTP request
from this connection; (iii) parse the request to determine the specific file being
requested; (iv) get the requested file from the server’s file system; (v) create an
HTTP response message consisting of the requested file preceded by header lines;
and (vi) send the response over the TCP connection to the requesting browser. If a
browser requests a file that is not present in your server, your server should return a
“404 Not Found” error message.
In the companion Web site, we provide the skeleton code for your server. Your
job is to complete the code, run your server, and then test your server by sending
requests from browsers running on different hosts. If you run your server on a host
that already has a Web server running on it, then you should use a different port than
port 80 for your Web server.
Assignment 2: UDP Pinger
In this programming assignment, you will write a client ping program in Python.
Your client will send a simple ping message to a server, receive a corresponding
pong message back from the server, and determine the delay between when the
client sent the ping message and received the pong message. This delay is called the
Round Trip Time (RTT). The functionality provided by the client and server is
SOCKET PROGRAMMING ASSIGNMENTS 179
similar to the functionality provided by standard ping program available in modern
operating systems. However, standard ping programs use the Internet Control Message
Protocol (ICMP) (which we will study in Chapter 4). Here we will create a
nonstandard (but simple!) UDP-based ping program.
Your ping program is to send 10 ping messages to the target server over UDP.
For each message, your client is to determine and print the RTT when the corresponding
pong message is returned. Because UDP is an unreliable protocol, a packet sent
by the client or server may be lost. For this reason, the client cannot wait indefinitely
for a reply to a ping message. You should have the client wait up to one second for a
reply from the server; if no reply is received, the client should assume that the
packet was lost and print a message accordingly.
In this assignment, you will be given the complete code for the server (available
in the companion Web site). Your job is to write the client code, which will be
very similar to the server code. It is recommended that you first study carefully the
server code. You can then write your client code, liberally cutting and pasting lines
from the server code.
Assignment 3: Mail Client
The goal of this programming assignment is to create a simple mail client that sends
email to any recipient. Your client will need to establish a TCP connection with a
mail server (e.g., a Google mail server), dialogue with the mail server using the
SMTP protocol, send an email message to a recipient (e.g., your friend) via the mail
server, and finally close the TCP connection with the mail server.
For this assignment, the companion Web site provides the skeleton code for
your client. Your job is to complete the code and test your client by sending
email to different user accounts. You may also try sending through different
servers (for example, through a Google mail server and through your university
mail server).
Assignment 4: Multi-Threaded Web Proxy
In this assignment, you will develop a Web proxy. When your proxy receives an
HTTP request for an object from a browser, it generates a new HTTP request for
the same object and sends it to the origin server. When the proxy receives the
corresponding HTTP response with the object from the origin server, it creates a
new HTTP response, including the object, and sends it to the client. This proxy
will be multi-threaded, so that it will be able to handle multiple requests at the
same time.
For this assignment, the companion Web site provides the skeleton code for the
proxy server. Your job is to complete the code, and then test it by having different
browsers request Web objects via your proxy.
180 CHAPTER 2 • APPLICATION LAYER
Wireshark Lab: HTTP
Having gotten our feet wet with the Wireshark packet sniffer in Lab 1, we’re now
ready to use Wireshark to investigate protocols in operation. In this lab, we’ll explore
several aspects of the HTTP protocol: the basic GET/reply interaction, HTTP message
formats, retrieving large HTML files, retrieving HTML files with embedded URLs,
persistent and non-persistent connections, and HTTP authentication and security.
As is the case with all Wireshark labs, the full description of this lab is available
at this book’s Web site, http://www.awl.com/kurose-ross.
Wireshark Lab: DNS
In this lab, we take a closer look at the client side of the DNS, the protocol that translates
Internet hostnames to IP addresses. Recall from Section 2.5 that the client’s role in
the DNS is relatively simple—a client sends a query to its local DNS server and
receives a response back. Much can go on under the covers, invisible to the DNS
clients, as the hierarchical DNS servers communicate with each other to either recursively
or iteratively resolve the client’s DNS query. From the DNS client’s standpoint,
however, the protocol is quite simple—a query is formulated to the local DNS server
and a response is received from that server. We observe DNS in action in this lab.
As is the case with all Wireshark labs, the full description of this lab is available at
this book’s Web site, http://www.awl.com/kurose-ross.
WIRESHARK LABS 181
VideoNote
Using Wireshark to
investigate the
HTTP protocol
182
How did you become interested in computing? Did you always know that you wanted to
work in information technology?
The video game and personal computing revolutions hit right when I was growing up—
personal computing was the new technology frontier in the late 70’s and early 80’s. And it
wasn’t just Apple and the IBM PC, but hundreds of new companies like Commodore and
Atari as well. I taught myself to program out of a book called “Instant Freeze-Dried BASIC”
at age 10, and got my first computer (a TRS-80 Color Computer—look it up!) at age 12.
Please describe one or two of the most exciting projects you have worked on during your
career. What were the biggest challenges?
Undoubtedly the most exciting project was the original Mosaic web browser in ’92–’93—
and the biggest challenge was getting anyone to take it seriously back then. At the time,
everyone thought the interactive future would be delivered as “interactive television” by
huge companies, not as the Internet by startups.
What excites you about the future of networking and the Internet? What are your biggest
concerns?
The most exciting thing is the huge unexplored frontier of applications and services that
programmers and entrepreneurs are able to explore—the Internet has unleashed creativity at
Marc Andreessen
Marc Andreessen is the co-creator of Mosaic, the Web browser that
popularized the World Wide Web in 1993. Mosaic had a clean,
easily understood interface and was the first browser to display
images in-line with text. In 1994, Marc Andreessen and Jim Clark
founded Netscape, whose browser was by far the most popular
browser through the mid-1990s. Netscape also developed the Secure
Sockets Layer (SSL) protocol and many Internet server products, including
mail servers and SSL-based Web servers. He is now a co-founder
and general partner of venture capital firm Andreessen Horowitz, overseeing
portfolio development with holdings that include Facebook,
Foursquare, Groupon, Jawbone, Twitter, and Zynga. He serves on
numerous boards, including Bump, eBay, Glam Media, Facebook,
and Hewlett-Packard. He holds a BS in Computer Science from the
University of Illinois at Urbana-Champaign.
AN INTERVIEW WITH...
183
a level that I don’t think we’ve ever seen before. My biggest concern is the principle of
unintended consequences—we don’t always know the implications of what we do, such as
the Internet being used by governments to run a new level of surveillance on citizens.
Is there anything in particular students should be aware of as Web technology advances?
The rate of change—the most important thing to learn is how to learn—how to flexibly
adapt to changes in the specific technologies, and how to keep an open mind on the new
opportunities and possibilities as you move through your career.
What people inspired you professionally?
Vannevar Bush, Ted Nelson, Doug Engelbart, Nolan Bushnell, Bill Hewlett and Dave
Packard, Ken Olsen, Steve Jobs, Steve Wozniak, Andy Grove, Grace Hopper, Hedy Lamarr,
Alan Turing, Richard Stallman.
What are your recommendations for students who want to pursue careers in computing
and information technology?
Go as deep as you possibly can on understanding how technology is created, and then complement
with learning how business works.
Can technology solve the world’s problems?
No, but we advance the standard of living of people through economic growth, and most
economic growth throughout history has come from technology—so that’s as good as it gets.
This page intentionally left blank
CHAPTER 3
Transport
Layer
Residing between the application and network layers, the transport layer is a central
piece of the layered network architecture. It has the critical role of providing communication
services directly to the application processes running on different hosts.
The pedagogic approach we take in this chapter is to alternate between discussions
of transport-layer principles and discussions of how these principles are implemented
in existing protocols; as usual, particular emphasis will be given to Internet
protocols, in particular the TCP and UDP transport-layer protocols.
We’ll begin by discussing the relationship between the transport and network
layers. This sets the stage for examining the first critical function of the transport
layer—extending the network layer’s delivery service between two end systems to a
delivery service between two application-layer processes running on the end systems.
We’ll illustrate this function in our coverage of the Internet’s connectionless
transport protocol, UDP.
We’ll then return to principles and confront one of the most fundamental problems
in computer networking—how two entities can communicate reliably over a
medium that may lose and corrupt data. Through a series of increasingly complicated
(and realistic!) scenarios, we’ll build up an array of techniques that transport
protocols use to solve this problem. We’ll then show how these principles are
embodied in TCP, the Internet’s connection-oriented transport protocol.
We’ll next move on to a second fundamentally important problem in networking—
controlling the transmission rate of transport-layer entities in order to avoid, or 185
recover from, congestion within the network. We’ll consider the causes and consequences
of congestion, as well as commonly used congestion-control techniques.
After obtaining a solid understanding of the issues behind congestion control, we’ll
study TCP’s approach to congestion control.
3.1 Introduction and Transport-Layer Services
In the previous two chapters we touched on the role of the transport layer and the
services that it provides. Let’s quickly review what we have already learned about
the transport layer.
A transport-layer protocol provides for logical communication between application
processes running on different hosts. By logical communication, we mean
that from an application’s perspective, it is as if the hosts running the processes were
directly connected; in reality, the hosts may be on opposite sides of the planet, connected
via numerous routers and a wide range of link types. Application processes
use the logical communication provided by the transport layer to send messages to
each other, free from the worry of the details of the physical infrastructure used to
carry these messages. Figure 3.1 illustrates the notion of logical communication.
As shown in Figure 3.1, transport-layer protocols are implemented in the end
systems but not in network routers. On the sending side, the transport layer converts
the application-layer messages it receives from a sending application process into
transport-layer packets, known as transport-layer segments in Internet terminology.
This is done by (possibly) breaking the application messages into smaller chunks and
adding a transport-layer header to each chunk to create the transport-layer segment.
The transport layer then passes the segment to the network layer at the sending end
system, where the segment is encapsulated within a network-layer packet (a datagram)
and sent to the destination. It’s important to note that network routers act only
on the network-layer fields of the datagram; that is, they do not examine the fields of
the transport-layer segment encapsulated with the datagram. On the receiving side,
the network layer extracts the transport-layer segment from the datagram and passes
the segment up to the transport layer. The transport layer then processes the received
segment, making the data in the segment available to the receiving application.
More than one transport-layer protocol may be available to network applications.
For example, the Internet has two protocols—TCP and UDP. Each of these protocols
provides a different set of transport-layer services to the invoking application.
3.1.1 Relationship Between Transport and Network Layers
Recall that the transport layer lies just above the network layer in the protocol stack.
Whereas a transport-layer protocol provides logical communication between
processes running on different hosts, a network-layer protocol provides logical
186 CHAPTER 3 • TRANSPORT LAYER
3.1 • INTRODUCTION AND TRANSPORT-LAYER SERVICES 187
Mobile Network
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Network Home Network
Data link
Physical
Application
Transport
Network
Data link
Physical
Application
Transport
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Logical end-to-end transport
Figure 3.1  The transport layer provides logical rather than physical
communication between application processes
communication between hosts. This distinction is subtle but important. Let’s examine
this distinction with the aid of a household analogy.
Consider two houses, one on the East Coast and the other on the West Coast, with
each house being home to a dozen kids. The kids in the East Coast household are
cousins of the kids in the West Coast household. The kids in the two households love
to write to each other—each kid writes each cousin every week, with each letter delivered
by the traditional postal service in a separate envelope. Thus, each household
sends 144 letters to the other household every week. (These kids would save a lot of
money if they had e-mail!) In each of the households there is one kid—Ann in the
West Coast house and Bill in the East Coast house—responsible for mail collection
and mail distribution. Each week Ann visits all her brothers and sisters, collects the
mail, and gives the mail to a postal-service mail carrier, who makes daily visits to the
house. When letters arrive at the West Coast house, Ann also has the job of distributing
the mail to her brothers and sisters. Bill has a similar job on the East Coast.
In this example, the postal service provides logical communication between the
two houses—the postal service moves mail from house to house, not from person to
person. On the other hand, Ann and Bill provide logical communication among the
cousins—Ann and Bill pick up mail from, and deliver mail to, their brothers and sisters.
Note that from the cousins’ perspective, Ann and Bill are the mail service, even
though Ann and Bill are only a part (the end-system part) of the end-to-end delivery
process. This household example serves as a nice analogy for explaining how the
transport layer relates to the network layer:
application messages = letters in envelopes
processes = cousins
hosts (also called end systems) = houses
transport-layer protocol = Ann and Bill
network-layer protocol = postal service (including mail carriers)
Continuing with this analogy, note that Ann and Bill do all their work within
their respective homes; they are not involved, for example, in sorting mail in any
intermediate mail center or in moving mail from one mail center to another. Similarly,
transport-layer protocols live in the end systems. Within an end system, a
transport protocol moves messages from application processes to the network edge
(that is, the network layer) and vice versa, but it doesn’t have any say about how the
messages are moved within the network core. In fact, as illustrated in Figure 3.1,
intermediate routers neither act on, nor recognize, any information that the transport
layer may have added to the application messages.
Continuing with our family saga, suppose now that when Ann and Bill go on
vacation, another cousin pair—say, Susan and Harvey—substitute for them and provide
the household-internal collection and delivery of mail. Unfortunately for the
two families, Susan and Harvey do not do the collection and delivery in exactly the
same way as Ann and Bill. Being younger kids, Susan and Harvey pick up and drop
off the mail less frequently and occasionally lose letters (which are sometimes
188 CHAPTER 3 • TRANSPORT LAYER
chewed up by the family dog). Thus, the cousin-pair Susan and Harvey do not provide
the same set of services (that is, the same service model) as Ann and Bill. In an
analogous manner, a computer network may make available multiple transport protocols,
with each protocol offering a different service model to applications.
The possible services that Ann and Bill can provide are clearly constrained by
the possible services that the postal service provides. For example, if the postal service
doesn’t provide a maximum bound on how long it can take to deliver mail
between the two houses (for example, three days), then there is no way that Ann and
Bill can guarantee a maximum delay for mail delivery between any of the cousin
pairs. In a similar manner, the services that a transport protocol can provide are often
constrained by the service model of the underlying network-layer protocol. If the
network-layer protocol cannot provide delay or bandwidth guarantees for transportlayer
segments sent between hosts, then the transport-layer protocol cannot provide
delay or bandwidth guarantees for application messages sent between processes.
Nevertheless, certain services can be offered by a transport protocol even when
the underlying network protocol doesn’t offer the corresponding service at the network
layer. For example, as we’ll see in this chapter, a transport protocol can offer
reliable data transfer service to an application even when the underlying network
protocol is unreliable, that is, even when the network protocol loses, garbles, or
duplicates packets. As another example (which we’ll explore in Chapter 8 when we
discuss network security), a transport protocol can use encryption to guarantee that
application messages are not read by intruders, even when the network layer cannot
guarantee the confidentiality of transport-layer segments.
3.1.2 Overview of the Transport Layer in the Internet
Recall that the Internet, and more generally a TCP/IP network, makes two distinct
transport-layer protocols available to the application layer. One of these protocols is
UDP (User Datagram Protocol), which provides an unreliable, connectionless service
to the invoking application. The second of these protocols is TCP (Transmission Control
Protocol), which provides a reliable, connection-oriented service to the invoking
application. When designing a network application, the application developer must
specify one of these two transport protocols. As we saw in Section 2.7, the application
developer selects between UDP and TCP when creating sockets.
To simplify terminology, when in an Internet context, we refer to the transportlayer
packet as a segment. We mention, however, that the Internet literature (for example,
the RFCs) also refers to the transport-layer packet for TCP as a segment but often
refers to the packet for UDP as a datagram. But this same Internet literature also uses
the term datagram for the network-layer packet! For an introductory book on computer
networking such as this, we believe that it is less confusing to refer to both TCP and
UDP packets as segments, and reserve the term datagram for the network-layer packet.
Before proceeding with our brief introduction of UDP and TCP, it will be useful
to say a few words about the Internet’s network layer. (We’ll learn about the network
layer in detail in Chapter 4.) The Internet’s network-layer protocol has a
3.1 • INTRODUCTION AND TRANSPORT-LAYER SERVICES 189
name—IP, for Internet Protocol. IP provides logical communication between hosts.
The IP service model is a best-effort delivery service. This means that IP makes its
“best effort” to deliver segments between communicating hosts, but it makes no
guarantees. In particular, it does not guarantee segment delivery, it does not guarantee
orderly delivery of segments, and it does not guarantee the integrity of the data
in the segments. For these reasons, IP is said to be an unreliable service. We also
mention here that every host has at least one network-layer address, a so-called IP
address. We’ll examine IP addressing in detail in Chapter 4; for this chapter we need
only keep in mind that each host has an IP address.
Having taken a glimpse at the IP service model, let’s now summarize the service
models provided by UDP and TCP. The most fundamental responsibility of UDP
and TCP is to extend IP’s delivery service between two end systems to a delivery
service between two processes running on the end systems. Extending host-to-host
delivery to process-to-process delivery is called transport-layer multiplexing and
demultiplexing. We’ll discuss transport-layer multiplexing and demultiplexing in
the next section. UDP and TCP also provide integrity checking by including errordetection
fields in their segments’ headers. These two minimal transport-layer services—
process-to-process data delivery and error checking—are the only two
services that UDP provides! In particular, like IP, UDP is an unreliable service—it
does not guarantee that data sent by one process will arrive intact (or at all!) to the
destination process. UDP is discussed in detail in Section 3.3.
TCP, on the other hand, offers several additional services to applications. First
and foremost, it provides reliable data transfer. Using flow control, sequence numbers,
acknowledgments, and timers (techniques we’ll explore in detail in this chapter),
TCP ensures that data is delivered from sending process to receiving process,
correctly and in order. TCP thus converts IP’s unreliable service between end systems
into a reliable data transport service between processes. TCP also provides
congestion control. Congestion control is not so much a service provided to the
invoking application as it is a service for the Internet as a whole, a service for the
general good. Loosely speaking, TCP congestion control prevents any one TCP connection
from swamping the links and routers between communicating hosts with an
excessive amount of traffic. TCP strives to give each connection traversing a congested
link an equal share of the link bandwidth. This is done by regulating the rate
at which the sending sides of TCP connections can send traffic into the network.
UDP traffic, on the other hand, is unregulated. An application using UDP transport
can send at any rate it pleases, for as long as it pleases.
A protocol that provides reliable data transfer and congestion control is necessarily
complex. We’ll need several sections to cover the principles of reliable data
transfer and congestion control, and additional sections to cover the TCP protocol
itself. These topics are investigated in Sections 3.4 through 3.8. The approach taken
in this chapter is to alternate between basic principles and the TCP protocol. For
example, we’ll first discuss reliable data transfer in a general setting and then discuss
how TCP specifically provides reliable data transfer. Similarly, we’ll first
190 CHAPTER 3 • TRANSPORT LAYER
discuss congestion control in a general setting and then discuss how TCP performs
congestion control. But before getting into all this good stuff, let’s first look at
transport-layer multiplexing and demultiplexing.
3.2 Multiplexing and Demultiplexing
In this section, we discuss transport-layer multiplexing and demultiplexing, that is,
extending the host-to-host delivery service provided by the network layer to a
process-to-process delivery service for applications running on the hosts. In order to
keep the discussion concrete, we’ll discuss this basic transport-layer service in the
context of the Internet. We emphasize, however, that a multiplexing/demultiplexing
service is needed for all computer networks.
At the destination host, the transport layer receives segments from the network
layer just below. The transport layer has the responsibility of delivering the data in
these segments to the appropriate application process running in the host. Let’s take
a look at an example. Suppose you are sitting in front of your computer, and you are
downloading Web pages while running one FTP session and two Telnet sessions.
You therefore have four network application processes running—two Telnet
processes, one FTP process, and one HTTP process. When the transport layer in
your computer receives data from the network layer below, it needs to direct the
received data to one of these four processes. Let’s now examine how this is done.
First recall from Section 2.7 that a process (as part of a network application) can
have one or more sockets, doors through which data passes from the network to the
process and through which data passes from the process to the network. Thus, as
shown in Figure 3.2, the transport layer in the receiving host does not actually
deliver data directly to a process, but instead to an intermediary socket. Because at
any given time there can be more than one socket in the receiving host, each socket
has a unique identifier. The format of the identifier depends on whether the socket is
a UDP or a TCP socket, as we’ll discuss shortly.
Now let’s consider how a receiving host directs an incoming transport-layer segment
to the appropriate socket. Each transport-layer segment has a set of fields in the
segment for this purpose. At the receiving end, the transport layer examines these
fields to identify the receiving socket and then directs the segment to that socket. This
job of delivering the data in a transport-layer segment to the correct socket is called
demultiplexing. The job of gathering data chunks at the source host from different
sockets, encapsulating each data chunk with header information (that will later be
used in demultiplexing) to create segments, and passing the segments to the network
layer is called multiplexing. Note that the transport layer in the middle host in Figure
3.2 must demultiplex segments arriving from the network layer below to either
process P1 or P2 above; this is done by directing the arriving segment’s data to the
corresponding process’s socket. The transport layer in the middle host must also
3.2 • MULTIPLEXING AND DEMULTIPLEXING 191
gather outgoing data from these sockets, form transport-layer segments, and pass
these segments down to the network layer. Although we have introduced multiplexing
and demultiplexing in the context of the Internet transport protocols, it’s important
to realize that they are concerns whenever a single protocol at one layer (at the
transport layer or elsewhere) is used by multiple protocols at the next higher layer.
To illustrate the demultiplexing job, recall the household analogy in the previous
section. Each of the kids is identified by his or her name. When Bill receives a
batch of mail from the mail carrier, he performs a demultiplexing operation by
observing to whom the letters are addressed and then hand delivering the mail to his
brothers and sisters. Ann performs a multiplexing operation when she collects letters
from her brothers and sisters and gives the collected mail to the mail person.
Now that we understand the roles of transport-layer multiplexing and demultiplexing,
let us examine how it is actually done in a host. From the discussion above,
we know that transport-layer multiplexing requires (1) that sockets have unique
identifiers, and (2) that each segment have special fields that indicate the socket to
which the segment is to be delivered. These special fields, illustrated in Figure 3.3,
are the source port number field and the destination port number field. (The
UDP and TCP segments have other fields as well, as discussed in the subsequent
sections of this chapter.) Each port number is a 16-bit number, ranging from 0 to
65535. The port numbers ranging from 0 to 1023 are called well-known port numbers
and are restricted, which means that they are reserved for use by well-known
application protocols such as HTTP (which uses port number 80) and FTP (which
uses port number 21). The list of well-known port numbers is given in RFC 1700
and is updated at http://www.iana.org [RFC 3232]. When we develop a new
192 CHAPTER 3 • TRANSPORT LAYER
Network
Key:
Process Socket
Data link
Physical
Transport
Application
Network
Application
Data link
Physical
Transport
Network
Data link
Physical
Transport
P3 P1 P2 P4 Application
Figure 3.2  Transport-layer multiplexing and demultiplexing
application (such as the simple application developed in Section 2.7), we must
assign the application a port number.
It should now be clear how the transport layer could implement the demultiplexing
service: Each socket in the host could be assigned a port number, and when a segment
arrives at the host, the transport layer examines the destination port number in
the segment and directs the segment to the corresponding socket. The segment’s data
then passes through the socket into the attached process. As we’ll see, this is basically
how UDP does it. However, we’ll also see that multiplexing/demultiplexing in
TCP is yet more subtle.
Connectionless Multiplexing and Demultiplexing
Recall from Section 2.7.1 that the Python program running in a host can create a
UDP socket with the line
clientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)
When a UDP socket is created in this manner, the transport layer automatically
assigns a port number to the socket. In particular, the transport layer assigns a port
number in the range 1024 to 65535 that is currently not being used by any other UDP
port in the host. Alternatively, we can add a line into our Python program after we
create the socket to associate a specific port number (say, 19157) to this UDP socket
via the socket bind() method:
clientSocket.bind((‘’, 19157))
If the application developer writing the code were implementing the server side of a
“well-known protocol,” then the developer would have to assign the corresponding
3.2 • MULTIPLEXING AND DEMULTIPLEXING 193
Source port #
32 bits
Dest. port #
Other header fields
Application
data
(message)
Figure 3.3  Source and destination port-number fields in a transport-layer
segment
well-known port number. Typically, the client side of the application lets the transport
layer automatically (and transparently) assign the port number, whereas the
server side of the application assigns a specific port number.
With port numbers assigned to UDP sockets, we can now precisely describe
UDP multiplexing/demultiplexing. Suppose a process in Host A, with UDP port
19157, wants to send a chunk of application data to a process with UDP port 46428
in Host B. The transport layer in Host A creates a transport-layer segment that
includes the application data, the source port number (19157), the destination port
number (46428), and two other values (which will be discussed later, but are unimportant
for the current discussion). The transport layer then passes the resulting segment
to the network layer. The network layer encapsulates the segment in an IP
datagram and makes a best-effort attempt to deliver the segment to the receiving host.
If the segment arrives at the receiving Host B, the transport layer at the receiving
host examines the destination port number in the segment (46428) and delivers the
segment to its socket identified by port 46428. Note that Host B could be running
multiple processes, each with its own UDP socket and associated port number. As
UDP segments arrive from the network, Host B directs (demultiplexes) each segment
to the appropriate socket by examining the segment’s destination port number.
It is important to note that a UDP socket is fully identified by a two-tuple consisting
of a destination IP address and a destination port number. As a consequence, if two
UDP segments have different source IP addresses and/or source port numbers, but have
the same destination IP address and destination port number, then the two segments
will be directed to the same destination process via the same destination socket.
You may be wondering now, what is the purpose of the source port number? As
shown in Figure 3.4, in the A-to-B segment the source port number serves as part of
a “return address”—when B wants to send a segment back to A, the destination port
in the B-to-A segment will take its value from the source port value of the A-to-B
segment. (The complete return address is A’s IP address and the source port number.)
As an example, recall the UDP server program studied in Section 2.7. In
UDPServer.py, the server uses the recvfrom() method to extract the clientside
(source) port number from the segment it receives from the client; it then sends
a new segment to the client, with the extracted source port number serving as the
destination port number in this new segment.
Connection-Oriented Multiplexing and Demultiplexing
In order to understand TCP demultiplexing, we have to take a close look at TCP
sockets and TCP connection establishment. One subtle difference between a TCP
socket and a UDP socket is that a TCP socket is identified by a four-tuple: (source
IP address, source port number, destination IP address, destination port number).
Thus, when a TCP segment arrives from the network to a host, the host uses all four
values to direct (demultiplex) the segment to the appropriate socket. In particular,
and in contrast with UDP, two arriving TCP segments with different source IP
194 CHAPTER 3 • TRANSPORT LAYER
addresses or source port numbers will (with the exception of a TCP segment carrying
the original connection-establishment request) be directed to two different sockets.
To gain further insight, let’s reconsider the TCP client-server programming
example in Section 2.7.2:
• The TCP server application has a “welcoming socket,” that waits for connectionestablishment
requests from TCP clients (see Figure 2.29) on port number 12000.
• The TCP client creates a socket and sends a connection establishment request
segment with the lines:
clientSocket = socket(AF_INET, SOCK_STREAM)
clientSocket.connect((serverName,12000))
• Aconnection-establishment request is nothing more than a TCP segment with destination
port number 12000 and a special connection-establishment bit set in the TCP
header (discussed in Section 3.5). The segment also includes a source port number
that was chosen by the client.
• When the host operating system of the computer running the server process
receives the incoming connection-request segment with destination port 12000,
it locates the server process that is waiting to accept a connection on port number
12000. The server process then creates a new socket:
connectionSocket, addr = serverSocket.accept()
3.2 • MULTIPLEXING AND DEMULTIPLEXING 195
Host A
Client process
Socket
Server B
source port:
19157
dest. port:
46428
source port:
46428
dest. port:
19157
Figure 3.4  The inversion of source and destination port numbers
• Also, the transport layer at the server notes the following four values in the connection-
request segment: (1) the source port number in the segment, (2) the IP
address of the source host, (3) the destination port number in the segment, and
(4) its own IP address. The newly created connection socket is identified by these
four values; all subsequently arriving segments whose source port, source IP
address, destination port, and destination IP address match these four values will
be demultiplexed to this socket. With the TCP connection now in place, the client
and server can now send data to each other.
The server host may support many simultaneous TCP connection sockets, with
each socket attached to a process, and with each socket identified by its own fourtuple.
When a TCP segment arrives at the host, all four fields (source IP address,
source port, destination IP address, destination port) are used to direct (demultiplex)
the segment to the appropriate socket.
196 CHAPTER 3 • TRANSPORT LAYER
PORT SCANNING
We’ve seen that a server process waits patiently on an open port for contact by a
remote client. Some ports are reserved for well-known applications (e.g., Web, FTP,
DNS, and SMTP servers); other ports are used by convention by popular applications
(e.g., the Microsoft 2000 SQL server listens for requests on UDP port 1434). Thus, if
we determine that a port is open on a host, we may be able to map that port to a
specific application running on the host. This is very useful for system administrators,
who are often interested in knowing which network applications are running on the
hosts in their networks. But attackers, in order to “case the joint,” also want to know
which ports are open on target hosts. If a host is found to be running an application
with a known security flaw (e.g., a SQL server listening on port 1434 was subject to
a buffer overflow, allowing a remote user to execute arbitrary code on the vulnerable
host, a flaw exploited by the Slammer worm [CERT 2003–04]), then that host is ripe
for attack.
Determining which applications are listening on which ports is a relatively easy
task. Indeed there are a number of public domain programs, called port scanners,
that do just that. Perhaps the most widely used of these is nmap, freely available at
http://nmap.org and included in most Linux distributions. For TCP, nmap sequentially
scans ports, looking for ports that are accepting TCP connections. For UDP, nmap
again sequentially scans ports, looking for UDP ports that respond to transmitted
UDP segments. In both cases, nmap returns a list of open, closed, or unreachable
ports. A host running nmap can attempt to scan any target host anywhere in the
Internet. We’ll revisit nmap in Section 3.5.6, when we discuss TCP connection
management.
FOCUS ON SECURITY
The situation is illustrated in Figure 3.5, in which Host C initiates two HTTP sessions
to server B, and Host Ainitiates one HTTP session to B. Hosts Aand C and server
B each have their own unique IP address—A, C, and B, respectively. Host C assigns
two different source port numbers (26145 and 7532) to its two HTTP connections.
Because Host A is choosing source port numbers independently of C, it might also
assign a source port of 26145 to its HTTP connection. But this is not a problem—server
B will still be able to correctly demultiplex the two connections having the same source
port number, since the two connections have different source IP addresses.
Web Servers and TCP
Before closing this discussion, it’s instructive to say a few additional words about
Web servers and how they use port numbers. Consider a host running a Web server,
such as an Apache Web server, on port 80. When clients (for example, browsers)
send segments to the server, all segments will have destination port 80. In particular,
both the initial connection-establishment segments and the segments carrying
HTTP request messages will have destination port 80. As we have just described,
3.2 • MULTIPLEXING AND DEMULTIPLEXING 197
source port:
7532
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
A
dest. IP:
B
Per-connection
HTTP
processes
Transportlayer
demultiplexing
Web
server B
Web client
host C
Web client
host A
Figure 3.5  Two clients, using the same destination port number (80) to
communicate with the same Web server application
the server distinguishes the segments from the different clients using source IP
addresses and source port numbers.
Figure 3.5 shows a Web server that spawns a new process for each connection.
As shown in Figure 3.5, each of these processes has its own connection socket
through which HTTP requests arrive and HTTP responses are sent. We mention,
however, that there is not always a one-to-one correspondence between connection
sockets and processes. In fact, today’s high-performing Web servers often use only
one process, and create a new thread with a new connection socket for each new
client connection. (A thread can be viewed as a lightweight subprocess.) If you did
the first programming assignment in Chapter 2, you built a Web server that does just
this. For such a server, at any given time there may be many connection sockets
(with different identifiers) attached to the same process.
If the client and server are using persistent HTTP, then throughout the duration
of the persistent connection the client and server exchange HTTP messages via the
same server socket. However, if the client and server use non-persistent HTTP, then
a new TCP connection is created and closed for every request/response, and hence
a new socket is created and later closed for every request/response. This frequent
creating and closing of sockets can severely impact the performance of a busy Web
server (although a number of operating system tricks can be used to mitigate
the problem). Readers interested in the operating system issues surrounding persistent
and non-persistent HTTP are encouraged to see [Nielsen 1997; Nahum
2002].
Now that we’ve discussed transport-layer multiplexing and demultiplexing,
let’s move on and discuss one of the Internet’s transport protocols, UDP. In the next
section we’ll see that UDP adds little more to the network-layer protocol than a multiplexing/
demultiplexing service.
3.3 Connectionless Transport: UDP
In this section, we’ll take a close look at UDP, how it works, and what it does.
We encourage you to refer back to Section 2.1, which includes an overview of
the UDP service model, and to Section 2.7.1, which discusses socket programming
using UDP.
To motivate our discussion about UDP, suppose you were interested in designing
a no-frills, bare-bones transport protocol. How might you go about doing this?
You might first consider using a vacuous transport protocol. In particular, on the
sending side, you might consider taking the messages from the application process
and passing them directly to the network layer; and on the receiving side, you might
consider taking the messages arriving from the network layer and passing them
directly to the application process. But as we learned in the previous section, we
have to do a little more than nothing! At the very least, the transport layer has to
198 CHAPTER 3 • TRANSPORT LAYER
provide a multiplexing/demultiplexing service in order to pass data between the
network layer and the correct application-level process.
UDP, defined in [RFC 768], does just about as little as a transport protocol can
do. Aside from the multiplexing/demultiplexing function and some light error
checking, it adds nothing to IP. In fact, if the application developer chooses UDP
instead of TCP, then the application is almost directly talking with IP. UDP takes
messages from the application process, attaches source and destination port number
fields for the multiplexing/demultiplexing service, adds two other small fields, and
passes the resulting segment to the network layer. The network layer encapsulates
the transport-layer segment into an IP datagram and then makes a best-effort attempt
to deliver the segment to the receiving host. If the segment arrives at the receiving
host, UDP uses the destination port number to deliver the segment’s data to the correct
application process. Note that with UDP there is no handshaking between sending
and receiving transport-layer entities before sending a segment. For this reason,
UDP is said to be connectionless.
DNS is an example of an application-layer protocol that typically uses UDP.
When the DNS application in a host wants to make a query, it constructs a DNS
query message and passes the message to UDP. Without performing any handshaking
with the UDP entity running on the destination end system, the host-side UDP
adds header fields to the message and passes the resulting segment to the network
layer. The network layer encapsulates the UDP segment into a datagram and sends
the datagram to a name server. The DNS application at the querying host then waits
for a reply to its query. If it doesn’t receive a reply (possibly because the underlying
network lost the query or the reply), either it tries sending the query to another name
server, or it informs the invoking application that it can’t get a reply.
Now you might be wondering why an application developer would ever choose
to build an application over UDP rather than over TCP. Isn’t TCP always preferable,
since TCP provides a reliable data transfer service, while UDP does not? The answer
is no, as many applications are better suited for UDP for the following reasons:
• Finer application-level control over what data is sent, and when. Under UDP, as
soon as an application process passes data to UDP, UDP will package the data
inside a UDP segment and immediately pass the segment to the network layer.
TCP, on the other hand, has a congestion-control mechanism that throttles the
transport-layer TCP sender when one or more links between the source and destination
hosts become excessively congested. TCP will also continue to resend a
segment until the receipt of the segment has been acknowledged by the destination,
regardless of how long reliable delivery takes. Since real-time applications
often require a minimum sending rate, do not want to overly delay segment
transmission, and can tolerate some data loss, TCP’s service model is not particularly
well matched to these applications’ needs. As discussed below, these applications
can use UDP and implement, as part of the application, any additional
functionality that is needed beyond UDP’s no-frills segment-delivery service.
3.3 • CONNECTIONLESS TRANSPORT: UDP 199
• No connection establishment. As we’ll discuss later, TCP uses a three-way handshake
before it starts to transfer data. UDP just blasts away without any formal preliminaries.
Thus UDP does not introduce any delay to establish a connection. This
is probably the principal reason why DNS runs over UDP rather than TCP—DNS
would be much slower if it ran over TCP. HTTP uses TCP rather than UDP, since
reliability is critical for Web pages with text. But, as we briefly discussed in Section
2.2, the TCP connection-establishment delay in HTTP is an important contributor
to the delays associated with downloading Web documents.
• No connection state. TCP maintains connection state in the end systems. This
connection state includes receive and send buffers, congestion-control parameters,
and sequence and acknowledgment number parameters. We will see in Section
3.5 that this state information is needed to implement TCP’s reliable data
transfer service and to provide congestion control. UDP, on the other hand, does
not maintain connection state and does not track any of these parameters. For this
reason, a server devoted to a particular application can typically support many
more active clients when the application runs over UDP rather than TCP.
• Small packet header overhead. The TCP segment has 20 bytes of header overhead
in every segment, whereas UDP has only 8 bytes of overhead.
Figure 3.6 lists popular Internet applications and the transport protocols that they
use. As we expect, e-mail, remote terminal access, the Web, and file transfer run over
TCP—all these applications need the reliable data transfer service of TCP. Nevertheless,
many important applications run over UDP rather than TCP. UDP is used for RIP
routing table updates (see Section 4.6.1). Since RIP updates are sent periodically (typically
every five minutes), lost updates will be replaced by more recent updates, thus
making the lost, out-of-date update useless. UDP is also used to carry network management
(SNMP; see Chapter 9) data. UDP is preferred to TCP in this case, since network
management applications must often run when the network is in a stressed state—precisely
when reliable, congestion-controlled data transfer is difficult to achieve. Also,
as we mentioned earlier, DNS runs over UDP, thereby avoiding TCP’s connectionestablishment
delays.
As shown in Figure 3.6, both UDP and TCP are used today with multimedia
applications, such as Internet phone, real-time video conferencing, and streaming of
stored audio and video. We’ll take a close look at these applications in Chapter 7. We
just mention now that all of these applications can tolerate a small amount of packet
loss, so that reliable data transfer is not absolutely critical for the application’s success.
Furthermore, real-time applications, like Internet phone and video conferencing,
react very poorly to TCP’s congestion control. For these reasons, developers of
multimedia applications may choose to run their applications over UDP instead of
TCP. However, TCP is increasingly being used for streaming media transport. For
example, [Sripanidkulchai 2004] found that nearly 75% of on-demand and live
streaming used TCP. When packet loss rates are low, and with some organizations
200 CHAPTER 3 • TRANSPORT LAYER
blocking UDP traffic for security reasons (see Chapter 8), TCP becomes an increasingly
attractive protocol for streaming media transport.
Although commonly done today, running multimedia applications over UDP is
controversial. As we mentioned above, UDP has no congestion control. But congestion
control is needed to prevent the network from entering a congested state in
which very little useful work is done. If everyone were to start streaming high-bitrate
video without using any congestion control, there would be so much packet
overflow at routers that very few UDP packets would successfully traverse the
source-to-destination path. Moreover, the high loss rates induced by the uncontrolled
UDP senders would cause the TCP senders (which, as we’ll see, do decrease
their sending rates in the face of congestion) to dramatically decrease their rates.
Thus, the lack of congestion control in UDP can result in high loss rates between a
UDP sender and receiver, and the crowding out of TCP sessions—a potentially serious
problem [Floyd 1999]. Many researchers have proposed new mechanisms to
force all sources, including UDP sources, to perform adaptive congestion control
[Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340].
Before discussing the UDP segment structure, we mention that it is possible for
an application to have reliable data transfer when using UDP. This can be done if reliability
is built into the application itself (for example, by adding acknowledgment
and retransmission mechanisms, such as those we’ll study in the next section). But
this is a nontrivial task that would keep an application developer busy debugging for
3.3 • CONNECTIONLESS TRANSPORT: UDP 201
Application-Layer Underlying Transport
Application Protocol Protocol
Electronic mail SMTP TCP
Remote terminal access Telnet TCP
Web HTTP TCP
File transfer FTP TCP
Remote file server NFS Typically UDP
Streaming multimedia typically proprietary UDP or TCP
Internet telephony typically proprietary UDP or TCP
Network management SNMP Typically UDP
Routing protocol RIP Typically UDP
Name translation DNS Typically UDP
Figure 3.6  Popular Internet applications and their underlying transport
protocols
a long time. Nevertheless, building reliability directly into the application allows the
application to “have its cake and eat it too.” That is, application processes can communicate
reliably without being subjected to the transmission-rate constraints
imposed by TCP’s congestion-control mechanism.
3.3.1 UDP Segment Structure
The UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The application
data occupies the data field of the UDP segment. For example, for DNS, the data
field contains either a query message or a response message. For a streaming audio
application, audio samples fill the data field. The UDP header has only four fields,
each consisting of two bytes. As discussed in the previous section, the port numbers
allow the destination host to pass the application data to the correct process running
on the destination end system (that is, to perform the demultiplexing function). The
length field specifies the number of bytes in the UDP segment (header plus data). An
explicit length value is needed since the size of the data field may differ from one UDP
segment to the next. The checksum is used by the receiving host to check whether
errors have been introduced into the segment. In truth, the checksum is also calculated
over a few of the fields in the IP header in addition to the UDP segment. But we ignore
this detail in order to see the forest through the trees. We’ll discuss the checksum calculation
below. Basic principles of error detection are described in Section 5.2. The
length field specifies the length of the UDP segment, including the header, in bytes.
3.3.2 UDP Checksum
The UDP checksum provides for error detection. That is, the checksum is used to
determine whether bits within the UDP segment have been altered (for example, by
noise in the links or while stored in a router) as it moved from source to destination.
UDP at the sender side performs the 1s complement of the sum of all the 16-bit
words in the segment, with any overflow encountered during the sum being
202 CHAPTER 3 • TRANSPORT LAYER
Source port #
32 bits
Dest. port #
Length Checksum
Application
data
(message)
Figure 3.7  UDP segment structure
wrapped around. This result is put in the checksum field of the UDP segment. Here
we give a simple example of the checksum calculation. You can find details about
efficient implementation of the calculation in RFC 1071 and performance over real
data in [Stone 1998; Stone 2000]. As an example, suppose that we have the following
three 16-bit words:
0110011001100000
0101010101010101
1000111100001100
The sum of first two of these 16-bit words is
0110011001100000
0101010101010101
1011101110110101
Adding the third word to the above sum gives
1011101110110101
1000111100001100
0100101011000010
Note that this last addition had overflow, which was wrapped around. The 1s complement
is obtained by converting all the 0s to 1s and converting all the 1s to 0s.
Thus the 1s complement of the sum 0100101011000010 is 1011010100111101,
which becomes the checksum. At the receiver, all four 16-bit words are added,
including the checksum. If no errors are introduced into the packet, then clearly the
sum at the receiver will be 1111111111111111. If one of the bits is a 0, then we know
that errors have been introduced into the packet.
You may wonder why UDP provides a checksum in the first place, as many linklayer
protocols (including the popular Ethernet protocol) also provide error checking.
The reason is that there is no guarantee that all the links between source and destination
provide error checking; that is, one of the links may use a link-layer protocol that does
not provide error checking. Furthermore, even if segments are correctly transferred
across a link, it’s possible that bit errors could be introduced when a segment is stored
in a router’s memory. Given that neither link-by-link reliability nor in-memory error
detection is guaranteed, UDP must provide error detection at the transport layer, on an
end-end basis, if the end-end data transfer service is to provide error detection. This is
an example of the celebrated end-end principle in system design [Saltzer 1984], which
states that since certain functionality (error detection, in this case) must be implemented
on an end-end basis: “functions placed at the lower levels may be redundant or of little
value when compared to the cost of providing them at the higher level.”
Because IP is supposed to run over just about any layer-2 protocol, it is useful
for the transport layer to provide error checking as a safety measure. Although UDP
3.3 • CONNECTIONLESS TRANSPORT: UDP 203
provides error checking, it does not do anything to recover from an error. Some
implementations of UDP simply discard the damaged segment; others pass the damaged
segment to the application with a warning.
That wraps up our discussion of UDP. We will soon see that TCP offers reliable
data transfer to its applications as well as other services that UDP doesn’t offer. Naturally,
TCP is also more complex than UDP. Before discussing TCP, however, it will be
useful to step back and first discuss the underlying principles of reliable data transfer.
3.4 Principles of Reliable Data Transfer
In this section, we consider the problem of reliable data transfer in a general context.
This is appropriate since the problem of implementing reliable data transfer
occurs not only at the transport layer, but also at the link layer and the application
layer as well. The general problem is thus of central importance to networking.
Indeed, if one had to identify a “top-ten” list of fundamentally important problems
in all of networking, this would be a candidate to lead the list. In the next section
we’ll examine TCP and show, in particular, that TCP exploits many of the principles
that we are about to describe.
Figure 3.8 illustrates the framework for our study of reliable data transfer. The
service abstraction provided to the upper-layer entities is that of a reliable channel
through which data can be transferred. With a reliable channel, no transferred data
bits are corrupted (flipped from 0 to 1, or vice versa) or lost, and all are delivered in
the order in which they were sent. This is precisely the service model offered by
TCP to the Internet applications that invoke it.
It is the responsibility of a reliable data transfer protocol to implement this
service abstraction. This task is made difficult by the fact that the layer below the
reliable data transfer protocol may be unreliable. For example, TCP is a reliable data
transfer protocol that is implemented on top of an unreliable (IP) end-to-end network
layer. More generally, the layer beneath the two reliably communicating end
points might consist of a single physical link (as in the case of a link-level data
transfer protocol) or a global internetwork (as in the case of a transport-level protocol).
For our purposes, however, we can view this lower layer simply as an unreliable
point-to-point channel.
In this section, we will incrementally develop the sender and receiver sides of a
reliable data transfer protocol, considering increasingly complex models of the underlying
channel. For example, we’ll consider what protocol mechanisms are needed when
the underlying channel can corrupt bits or lose entire packets. One assumption we’ll
adopt throughout our discussion here is that packets will be delivered in the order in
which they were sent, with some packets possibly being lost; that is, the underlying
channel will not reorder packets. Figure 3.8(b) illustrates the interfaces for our data
transfer protocol. The sending side of the data transfer protocol will be invoked from
above by a call to rdt_send(). It will pass the data to be delivered to the upper layer
at the receiving side. (Here rdt stands for reliable data transfer protocol and _send
204 CHAPTER 3 • TRANSPORT LAYER
indicates that the sending side of rdt is being called. The first step in developing any
protocol is to choose a good name!) On the receiving side, rdt_rcv() will be called
when a packet arrives from the receiving side of the channel. When the rdt protocol
wants to deliver data to the upper layer, it will do so by calling deliver_data(). In
the following we use the terminology “packet” rather than transport-layer “segment.”
Because the theory developed in this section applies to computer networks in general
and not just to the Internet transport layer, the generic term “packet” is perhaps more
appropriate here.
In this section we consider only the case of unidirectional data transfer, that is,
data transfer from the sending to the receiving side. The case of reliable bidirectional
(that is, full-duplex) data transfer is conceptually no more difficult but considerably
more tedious to explain. Although we consider only unidirectional data transfer, it is
important to note that the sending and receiving sides of our protocol will nonetheless
need to transmit packets in both directions, as indicated in Figure 3.8. We will see
shortly that, in addition to exchanging packets containing the data to be transferred, the
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 205
Reliable channel
Unreliable channel
rdt_send()
udt_send()
Sending
process
Receiver
process
deliver_data
Application
layer
Transport
layer
a. Provided service
Network
layer
Key:
Data Packet
b. Service implementation
Reliable data
transfer protocol
(sending side)
Reliable data
transfer protocol
(receiving side)
rdt_rcv()
Figure 3.8  Reliable data transfer: Service model and service
implementation
sending and receiving sides of rdt will also need to exchange control packets back and
forth. Both the send and receive sides of rdt send packets to the other side by a call to
udt_send() (where udt stands for unreliable data transfer).
3.4.1 Building a Reliable Data Transfer Protocol
We now step through a series of protocols, each one becoming more complex, arriving
at a flawless, reliable data transfer protocol.
Reliable Data Transfer over a Perfectly Reliable Channel: rdt1.0
We first consider the simplest case, in which the underlying channel is completely
reliable. The protocol itself, which we’ll call rdt1.0, is trivial. The finite-state
machine (FSM) definitions for the rdt1.0 sender and receiver are shown in
Figure 3.9. The FSM in Figure 3.9(a) defines the operation of the sender, while the
FSM in Figure 3.9(b) defines the operation of the receiver. It is important to note
that there are separate FSMs for the sender and for the receiver. The sender and
receiver FSMs in Figure 3.9 each have just one state. The arrows in the FSM
description indicate the transition of the protocol from one state to another. (Since
each FSM in Figure 3.9 has just one state, a transition is necessarily from the one
state back to itself; we’ll see more complicated state diagrams shortly.) The event
causing the transition is shown above the horizontal line labeling the transition, and
206 CHAPTER 3 • TRANSPORT LAYER
Wait for
call from
above
a. rdt1.0: sending side
rdt_send(data)
packet=make_pkt(data)
udt_send(packet)
Wait for
call from
below
b. rdt1.0: receiving side
rdt_rcv(packet)
extract(packet,data)
deliver_data(data)
Figure 3.9  rdt1.0 – A protocol for a completely reliable channel
the actions taken when the event occurs are shown below the horizontal line. When
no action is taken on an event, or no event occurs and an action is taken, we’ll use
the symbol  below or above the horizontal, respectively, to explicitly denote the
lack of an action or event. The initial state of the FSM is indicated by the dashed
arrow. Although the FSMs in Figure 3.9 have but one state, the FSMs we will see
shortly have multiple states, so it will be important to identify the initial state of
each FSM.
The sending side of rdt simply accepts data from the upper layer via the
rdt_send(data) event, creates a packet containing the data (via the action
make_pkt(data)) and sends the packet into the channel. In practice, the
rdt_send(data) event would result from a procedure call (for example, to
rdt_send()) by the upper-layer application.
On the receiving side, rdt receives a packet from the underlying channel via
the rdt_rcv(packet) event, removes the data from the packet (via the action
extract (packet, data)) and passes the data up to the upper layer (via the
action deliver_data(data)). In practice, the rdt_rcv(packet) event
would result from a procedure call (for example, to rdt_rcv()) from the lowerlayer
protocol.
In this simple protocol, there is no difference between a unit of data and a
packet. Also, all packet flow is from the sender to receiver; with a perfectly reliable
channel there is no need for the receiver side to provide any feedback to the sender
since nothing can go wrong! Note that we have also assumed that the receiver is able
to receive data as fast as the sender happens to send data. Thus, there is no need for
the receiver to ask the sender to slow down!
Reliable Data Transfer over a Channel with Bit Errors: rdt2.0
A more realistic model of the underlying channel is one in which bits in a packet
may be corrupted. Such bit errors typically occur in the physical components of a
network as a packet is transmitted, propagates, or is buffered. We’ll continue to
assume for the moment that all transmitted packets are received (although their bits
may be corrupted) in the order in which they were sent.
Before developing a protocol for reliably communicating over such a channel,
first consider how people might deal with such a situation. Consider how you yourself
might dictate a long message over the phone. In a typical scenario, the message
taker might say “OK” after each sentence has been heard, understood, and recorded.
If the message taker hears a garbled sentence, you’re asked to repeat the garbled
sentence. This message-dictation protocol uses both positive acknowledgments
(“OK”) and negative acknowledgments (“Please repeat that.”). These control messages
allow the receiver to let the sender know what has been received correctly, and
what has been received in error and thus requires repeating. In a computer network
setting, reliable data transfer protocols based on such retransmission are known as
ARQ (Automatic Repeat reQuest) protocols.
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 207
Fundamentally, three additional protocol capabilities are required in ARQ
protocols to handle the presence of bit errors:
• Error detection. First, a mechanism is needed to allow the receiver to detect
when bit errors have occurred. Recall from the previous section that UDP uses
the Internet checksum field for exactly this purpose. In Chapter 5 we’ll examine
error-detection and -correction techniques in greater detail; these techniques
allow the receiver to detect and possibly correct packet bit errors. For
now, we need only know that these techniques require that extra bits (beyond
the bits of original data to be transferred) be sent from the sender to the
receiver; these bits will be gathered into the packet checksum field of the
rdt2.0 data packet.
• Receiver feedback. Since the sender and receiver are typically executing on different
end systems, possibly separated by thousands of miles, the only way for the
sender to learn of the receiver’s view of the world (in this case, whether or not a
packet was received correctly) is for the receiver to provide explicit feedback to the
sender. The positive (ACK) and negative (NAK) acknowledgment replies in the
message-dictation scenario are examples of such feedback. Our rdt2.0 protocol
will similarly send ACK and NAK packets back from the receiver to the sender. In
principle, these packets need only be one bit long; for example, a 0 value could indicate
a NAK and a value of 1 could indicate an ACK.
• Retransmission. A packet that is received in error at the receiver will be retransmitted
by the sender.
Figure 3.10 shows the FSM representation of rdt2.0, a data transfer protocol
employing error detection, positive acknowledgments, and negative acknowledgments.
The send side of rdt2.0 has two states. In the leftmost state, the send-side protocol
is waiting for data to be passed down from the upper layer. When the
rdt_send(data) event occurs, the sender will create a packet (sndpkt)
containing the data to be sent, along with a packet checksum (for example, as discussed
in Section 3.3.2 for the case of a UDP segment), and then send the packet via the
udt_send(sndpkt) operation. In the rightmost state, the sender protocol is waiting
for an ACK or a NAK packet from the receiver. If an ACK packet is received (the
notation rdt_rcv(rcvpkt) && isACK (rcvpkt) in Figure 3.10 corresponds
to this event), the sender knows that the most recently transmitted packet has been
received correctly and thus the protocol returns to the state of waiting for data from the
upper layer. If a NAK is received, the protocol retransmits the last packet and waits for
an ACK or NAK to be returned by the receiver in response to the retransmitted data
packet. It is important to note that when the sender is in the wait-for-ACK-or-NAK
state, it cannot get more data from the upper layer; that is, the rdt_send() event can
not occur; that will happen only after the sender receives an ACK and leaves this state.
Thus, the sender will not send a new piece of data until it is sure that the receiver has
208 CHAPTER 3 • TRANSPORT LAYER
correctly received the current packet. Because of this behavior, protocols such as
rdt2.0 are known as stop-and-wait protocols.
The receiver-side FSM for rdt2.0 still has a single state. On packet arrival,
the receiver replies with either an ACK or a NAK, depending on whether or not the
received packet is corrupted. In Figure 3.10, the notation rdt_rcv(rcvpkt) &&
corrupt(rcvpkt) corresponds to the event in which a packet is received and is
found to be in error.
Protocol rdt2.0 may look as if it works but, unfortunately, it has a fatal
flaw. In particular, we haven’t accounted for the possibility that the ACK or NAK
packet could be corrupted! (Before proceeding on, you should think about how this
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 209
Wait for
call from
above
a. rdt2.0: sending side
b. rdt2.0: receiving side
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isNAK(rcvpkt)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isACK(rcvpkt)
?
rdt_send(data)
sndpkt=make_pkt(data,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK)
udt_send(sndpkt)
Wait for
call from
below
Wait for
ACK or
NAK
Figure 3.10  rdt2.0–A protocol for a channel with bit errors
problem may be fixed.) Unfortunately, our slight oversight is not as innocuous as it
may seem. Minimally, we will need to add checksum bits to ACK/NAK packets in
order to detect such errors. The more difficult question is how the protocol should
recover from errors in ACK or NAK packets. The difficulty here is that if an ACK
or NAK is corrupted, the sender has no way of knowing whether or not the receiver
has correctly received the last piece of transmitted data.
Consider three possibilities for handling corrupted ACKs or NAKs:
• For the first possibility, consider what a human might do in the messagedictation
scenario. If the speaker didn’t understand the “OK” or “Please repeat
that” reply from the receiver, the speaker would probably ask, “What did you
say?” (thus introducing a new type of sender-to-receiver packet to our protocol).
The receiver would then repeat the reply. But what if the speaker’s “What did
you say?” is corrupted? The receiver, having no idea whether the garbled sentence
was part of the dictation or a request to repeat the last reply, would probably
then respond with “What did you say?” And then, of course, that response
might be garbled. Clearly, we’re heading down a difficult path.
• A second alternative is to add enough checksum bits to allow the sender not only
to detect, but also to recover from, bit errors. This solves the immediate problem
for a channel that can corrupt packets but not lose them.
• A third approach is for the sender simply to resend the current data packet when
it receives a garbled ACK or NAK packet. This approach, however, introduces
duplicate packets into the sender-to-receiver channel. The fundamental difficulty
with duplicate packets is that the receiver doesn’t know whether the ACK
or NAK it last sent was received correctly at the sender. Thus, it cannot know a
priori whether an arriving packet contains new data or is a retransmission!
A simple solution to this new problem (and one adopted in almost all existing
data transfer protocols, including TCP) is to add a new field to the data packet and
have the sender number its data packets by putting a sequence number into this
field. The receiver then need only check this sequence number to determine whether
or not the received packet is a retransmission. For this simple case of a stop-andwait
protocol, a 1-bit sequence number will suffice, since it will allow the receiver
to know whether the sender is resending the previously transmitted packet (the
sequence number of the received packet has the same sequence number as the most
recently received packet) or a new packet (the sequence number changes, moving
“forward” in modulo-2 arithmetic). Since we are currently assuming a channel that
does not lose packets, ACK and NAK packets do not themselves need to indicate
the sequence number of the packet they are acknowledging. The sender knows that
a received ACK or NAK packet (whether garbled or not) was generated in response
to its most recently transmitted data packet.
210 CHAPTER 3 • TRANSPORT LAYER
Figures 3.11 and 3.12 show the FSM description for rdt2.1, our fixed version
of rdt2.0. The rdt2.1 sender and receiver FSMs each now have twice as many
states as before. This is because the protocol state must now reflect whether the packet
currently being sent (by the sender) or expected (at the receiver) should have a
sequence number of 0 or 1. Note that the actions in those states where a 0-numbered
packet is being sent or expected are mirror images of those where a 1-numbered
packet is being sent or expected; the only differences have to do with the handling of
the sequence number.
Protocol rdt2.1 uses both positive and negative acknowledgments from the
receiver to the sender. When an out-of-order packet is received, the receiver sends a
positive acknowledgment for the packet it has received. When a corrupted packet is
received, the receiver sends a negative acknowledgment. We can accomplish the
same effect as a NAK if, instead of sending a NAK, we send an ACK for the last
correctly received packet. Asender that receives two ACKs for the same packet (that
is, receives duplicate ACKs) knows that the receiver did not correctly receive the
packet following the packet that is being ACKed twice. Our NAK-free reliable data
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 211
Wait for
call 0 from
above
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)
? ?
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK or
NAK 0
Wait for
ACK or
NAK 1
Wait for
call 1 from
above
Figure 3.11  rdt2.1 sender
transfer protocol for a channel with bit errors is rdt2.2, shown in Figures 3.13 and
3.14. One subtle change between rtdt2.1 and rdt2.2 is that the receiver must
now include the sequence number of the packet being acknowledged by an ACK
message (this is done by including the ACK,0 or ACK,1 argument in make_pkt()
in the receiver FSM), and the sender must now check the sequence number of the
packet being acknowledged by a received ACK message (this is done by including
the 0 or 1 argument in isACK()in the sender FSM).
Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0
Suppose now that in addition to corrupting bits, the underlying channel can lose
packets as well, a not-uncommon event in today’s computer networks (including the
Internet). Two additional concerns must now be addressed by the protocol: how to
detect packet loss and what to do when packet loss occurs. The use of checksumming,
sequence numbers, ACK packets, and retransmissions—the techniques
already developed in rdt2.2—will allow us to answer the latter concern. Handling
the first concern will require adding a new protocol mechanism.
There are many possible approaches toward dealing with packet loss (several
more of which are explored in the exercises at the end of the chapter). Here, we’ll
put the burden of detecting and recovering from lost packets on the sender. Suppose
212 CHAPTER 3 • TRANSPORT LAYER
rdt_rcv(rcvpkt)&& notcorrupt
(rcvpkt)&&has_seq0(rcvpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)&& notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
Wait for
0 from
below
Wait for
1 from
rdt_rcv(rcvpkt)&& notcorrupt below
(rcvpkt)&&has_seq1(rcvpkt)
Figure 3.12  rdt2.1 receiver
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 213
that the sender transmits a data packet and either that packet, or the receiver’s ACK
of that packet, gets lost. In either case, no reply is forthcoming at the sender from
the receiver. If the sender is willing to wait long enough so that it is certain that a
packet has been lost, it can simply retransmit the data packet. You should convince
yourself that this protocol does indeed work.
But how long must the sender wait to be certain that something has been lost?
The sender must clearly wait at least as long as a round-trip delay between the
sender and receiver (which may include buffering at intermediate routers) plus
whatever amount of time is needed to process a packet at the receiver. In many networks,
this worst-case maximum delay is very difficult even to estimate, much less
know with certainty. Moreover, the protocol should ideally recover from packet
loss as soon as possible; waiting for a worst-case delay could mean a long wait
until error recovery is initiated. The approach thus adopted in practice is for the
sender to judiciously choose a time value such that packet loss is likely, although
not guaranteed, to have happened. If an ACK is not received within this time, the
packet is retransmitted. Note that if a packet experiences a particularly large delay,
the sender may retransmit the packet even though neither the data packet nor its
ACK have been lost. This introduces the possibility of duplicate data packets in
Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK 0
Wait for
ACK 1
? ?
Wait for
call 1 from
above
Figure 3.13  rdt2.2 sender
the sender-to-receiver channel. Happily, protocol rdt2.2 already has enough
functionality (that is, sequence numbers) to handle the case of duplicate packets.
From the sender’s viewpoint, retransmission is a panacea. The sender does not
know whether a data packet was lost, an ACK was lost, or if the packet or ACK was
simply overly delayed. In all cases, the action is the same: retransmit. Implementing
a time-based retransmission mechanism requires a countdown timer that can
interrupt the sender after a given amount of time has expired. The sender will thus
need to be able to (1) start the timer each time a packet (either a first-time packet or
a retransmission) is sent, (2) respond to a timer interrupt (taking appropriate
actions), and (3) stop the timer.
Figure 3.15 shows the sender FSM for rdt3.0, a protocol that reliably transfers
data over a channel that can corrupt or lose packets; in the homework problems, you’ll
be asked to provide the receiver FSM for rdt3.0. Figure 3.16 shows how the protocol
operates with no lost or delayed packets and how it handles lost data packets. In
Figure 3.16, time moves forward from the top of the diagram toward the bottom of the
diagram; note that a receive time for a packet is necessarily later than the send time
for a packet as a result of transmission and propagation delays. In Figures 3.16(b)–(d),
the send-side brackets indicate the times at which a timer is set and later times out.
Several of the more subtle aspects of this protocol are explored in the exercises at the
end of this chapter. Because packet sequence numbers alternate between 0 and 1, protocol
rdt3.0 is sometimes known as the alternating-bit protocol.
214 CHAPTER 3 • TRANSPORT LAYER
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt))
sndpkt=make_pkt(ACK,0,che
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt))
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,0,checksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.14  rdt2.2 receiver
We have now assembled the key elements of a data transfer protocol. Checksums,
sequence numbers, timers, and positive and negative acknowledgment packets
each play a crucial and necessary role in the operation of the protocol. We now
have a working reliable data transfer protocol!
3.4.2 Pipelined Reliable Data Transfer Protocols
Protocol rdt3.0 is a functionally correct protocol, but it is unlikely that anyone would
be happy with its performance, particularly in today’s high-speed networks. At the heart
of rdt3.0’s performance problem is the fact that it is a stop-and-wait protocol.
To appreciate the performance impact of this stop-and-wait behavior, consider
an idealized case of two hosts, one located on the West Coast of the United States
and the other located on the East Coast, as shown in Figure 3.17. The speed-of-light
round-trip propagation delay between these two end systems, RTT, is approximately
30 milliseconds. Suppose that they are connected by a channel with a transmission
rate, R, of 1 Gbps (109 bits per second). With a packet size, L, of 1,000 bytes
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 215
Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
timeout
udt_send(sndpkt)
start_timer
rdt_rcv(rcvpkt)
rdt_rcv(rcvpkt) && ?
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
stop_timer
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
stop_timer
timeout
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
start_timer
Wait for
ACK 0
Wait for
ACK 1
?
?
Wait for
call 1 from
above
rdt_rcv(rcvpkt)
?
Figure 3.15  rdt3.0 sender
VideoNote
Developing a protocol
and FSM representation
for a simple applicationlayer
protocol
216 CHAPTER 3 • TRANSPORT LAYER
rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt0
send ACK0
Sender Receiver
a. Operation with no loss
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
(loss) X
b. Lost packet
rcv pkt0
send ACK0
rcv pkt1
send ACK1
c. Lost ACK
send pkt0
rcv ACK0
send pkt1
rcv ACK1
send pkt0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
(detect
duplicate)
send ACK1
send pkt0
rcv ACK0
send pkt1
rcv pkt0
send ACK0
timeout
resend pkt1
rcv pkt1
send ACK1
d. Premature timeout
rcv ACK1
send pkt0
rcv ACK1
do nothing
rcv pkt0
send ACK0
rcv pkt 1
(detect duplicate)
send ACK1
Sender Receiver Sender Receiver
pkt0
ACK0
pkt1
ACK1
ACK1
ACK0
ACK1
ACK0
pkt1
pkt0
pkt0
pkt1
pkt1
pkt0
ACK1
ACK0
X (loss)
pkt1
rcv pkt0
send ACK0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
send ACK1
Sender Receiver
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
Figure 3.16  Operation of rdt3.0, the alternating-bit protocol
(8,000 bits) per packet, including both header fields and data, the time needed to
actually transmit the packet into the 1 Gbps link is
dtrans
Figure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins
sending the packet at t = 0, then at t = L/R = 8 microseconds, the last bit enters the
channel at the sender side. The packet then makes its 15-msec cross-country journey,
with the last bit of the packet emerging at the receiver at t = RTT/2 + L/R =
15.008 msec. Assuming for simplicity that ACK packets are extremely small (so that
we can ignore their transmission time) and that the receiver can send an ACK as
soon as the last bit of a data packet is received, the ACK emerges back at the sender
at t = RTT + L/R = 30.008 msec. At this point, the sender can now transmit the next
message. Thus, in 30.008 msec, the sender was sending for only 0.008 msec. If we
define the utilization of the sender (or the channel) as the fraction of time the sender
is actually busy sending bits into the channel, the analysis in Figure 3.18(a) shows
that the stop-and-wait protocol has a rather dismal sender utilization, Usender, of
That is, the sender was busy only 2.7 hundredths of one percent of the time!
Viewed another way, the sender was able to send only 1,000 bytes in 30.008 milliseconds,
an effective throughput of only 267 kbps—even though a 1 Gbps link was
available! Imagine the unhappy network manager who just paid a fortune for a gigabit
capacity link but manages to get a throughput of only 267 kilobits per second!
This is a graphic example of how network protocols can limit the capabilities
Usender =
L>R
RTT + L>R =
.008
30.008 = 0.00027
=
L
R =
8000 bits>packet
109 bits/sec
= 8 microseconds
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 217
Data packet Data packets
ACK packets
a. A stop-and-wait protocol in operation b. A pipelined protocol in operation
Figure 3.17  Stop-and-wait versus pipelined protocol
provided by the underlying network hardware. Also, we have neglected lower-layer
protocol-processing times at the sender and receiver, as well as the processing and
queuing delays that would occur at any intermediate routers between the sender
and receiver. Including these effects would serve only to further increase the delay
and further accentuate the poor performance.
The solution to this particular performance problem is simple: Rather than operate
in a stop-and-wait manner, the sender is allowed to send multiple packets without
waiting for acknowledgments, as illustrated in Figure 3.17(b). Figure 3.18(b)
shows that if the sender is allowed to transmit three packets before having to wait
for acknowledgments, the utilization of the sender is essentially tripled. Since the
many in-transit sender-to-receiver packets can be visualized as filling a pipeline, this
technique is known as pipelining. Pipelining has the following consequences for
reliable data transfer protocols:
• The range of sequence numbers must be increased, since each in-transit packet
(not counting retransmissions) must have a unique sequence number and there
may be multiple, in-transit, unacknowledged packets.
• The sender and receiver sides of the protocols may have to buffer more than one
packet. Minimally, the sender will have to buffer packets that have been transmitted
but not yet acknowledged. Buffering of correctly received packets may
also be needed at the receiver, as discussed below.
• The range of sequence numbers needed and the buffering requirements will
depend on the manner in which a data transfer protocol responds to lost, corrupted,
and overly delayed packets. Two basic approaches toward pipelined error
recovery can be identified: Go-Back-N and selective repeat.
3.4.3 Go-Back-N (GBN)
In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets
(when available) without waiting for an acknowledgment, but is constrained to have no
more than some maximum allowable number, N, of unacknowledged packets in the
pipeline. We describe the GBN protocol in some detail in this section. But before reading
on, you are encouraged to play with the GBN applet (an awesome applet!) at the
companion Web site.
Figure 3.19 shows the sender’s view of the range of sequence numbers in a GBN
protocol. If we define base to be the sequence number of the oldest unacknowledged
packet and nextseqnum to be the smallest unused sequence number (that is, the
sequence number of the next packet to be sent), then four intervals in the range of
sequence numbers can be identified. Sequence numbers in the interval [0,base-1]
correspond to packets that have already been transmitted and acknowledged. The interval
[base,nextseqnum-1] corresponds to packets that have been sent but not yet
acknowledged. Sequence numbers in the interval [nextseqnum,base+N-1] can
218 CHAPTER 3 • TRANSPORT LAYER
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 219
First bit of first packet
transmitted, t = 0
Last bit of first packet
transmitted, t = L/R
First bit of first packet
transmitted, t = 0
Last bit of first packet
transmitted, t = L/R
ACK arrives, send next packet,
t = RTT + L/R
a. Stop-and-wait operation
Sender Receiver
RTT
First bit of first packet arrives
Last bit of first packet arrives, send ACK
First bit of first packet arrives
Last bit of first packet arrives, send ACK
ACK arrives, send next packet,
t = RTT + L/R
b. Pipelined operation
Sender Receiver
RTT
Last bit of 2nd packet arrives, send ACK
Last bit of 3rd packet arrives, send ACK
Figure 3.18  Stop-and-wait and pipelined sending
be used for packets that can be sent immediately, should data arrive from the upper
layer. Finally, sequence numbers greater than or equal to base+N cannot be used until
an unacknowledged packet currently in the pipeline (specifically, the packet with
sequence number base) has been acknowledged.
As suggested by Figure 3.19, the range of permissible sequence numbers for
transmitted but not yet acknowledged packets can be viewed as a window of size N
over the range of sequence numbers. As the protocol operates, this window slides
forward over the sequence number space. For this reason, N is often referred to as
the window size and the GBN protocol itself as a sliding-window protocol. You
might be wondering why we would even limit the number of outstanding, unacknowledged
packets to a value of N in the first place. Why not allow an unlimited
number of such packets? We’ll see in Section 3.5 that flow control is one reason to
impose a limit on the sender. We’ll examine another reason to do so in Section 3.7,
when we study TCP congestion control.
In practice, a packet’s sequence number is carried in a fixed-length field in the
packet header. If k is the number of bits in the packet sequence number field, the range
of sequence numbers is thus [0,2k – 1]. With a finite range of sequence numbers, all
arithmetic involving sequence numbers must then be done using modulo 2k arithmetic.
(That is, the sequence number space can be thought of as a ring of size 2k, where
sequence number 2k– 1 is immediately followed by sequence number 0.) Recall that
rdt3.0 had a 1-bit sequence number and a range of sequence numbers of [0,1]. Several
of the problems at the end of this chapter explore the consequences of a finite range
of sequence numbers. We will see in Section 3.5 that TCP has a 32-bit sequence number
field, where TCP sequence numbers count bytes in the byte stream rather than packets.
Figures 3.20 and 3.21 give an extended FSM description of the sender and
receiver sides of an ACK-based, NAK-free, GBN protocol. We refer to this FSM
description as an extended FSM because we have added variables (similar to programming-
language variables) for base and nextseqnum, and added operations
on these variables and conditional actions involving these variables. Note that the
extended FSM specification is now beginning to look somewhat like a programminglanguage
specification. [Bochman 1984] provides an excellent survey of additional
extensions to FSM techniques as well as other programming-language-based techniques
for specifying protocols.
220 CHAPTER 3 • TRANSPORT LAYER
base nextseqnum
Window size
N
Key:
Already
ACK’d
Sent, not
yet ACK’d
Usable,
not yet sent
Not usable
Figure 3.19  Sender’s view of sequence numbers in Go-Back-N
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 221
rdt_send(data)
if(nextseqnum<base+N){
sndpkt[nextseqnum]=make_pkt(nextseqnum,data,checksum)
udt_send(sndpkt[nextseqnum])
if(base==nextseqnum)
start_timer
nextseqnum++
}
else
refuse_data(data)
?
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
base=getacknum(rcvpkt)+1
If(base==nextseqnum)
stop_timer
else
start_timer
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
?
base=1
nextseqnum=1
timeout
start_timer
udt_send(sndpkt[base])
udt_send(sndpkt[base+1])
...
udt_send(sndpkt[nextseqnum-1])
Wait
Figure 3.20  Extended FSM description of GBN sender
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& hasseqnum(rcvpkt,expectedseqnum)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(expectedseqnum,ACK,checksum)
udt_send(sndpkt)
expectedseqnum++
?
expectedseqnum=1
sndpkt=make_pkt(0,ACK,checksum)
default
udt_send(sndpkt)
Wait
Figure 3.21  Extended FSM description of GBN receiver
The GBN sender must respond to three types of events:
• Invocation from above. When rdt_send() is called from above, the sender
first checks to see if the window is full, that is, whether there are N outstanding,
unacknowledged packets. If the window is not full, a packet is created and sent,
and variables are appropriately updated. If the window is full, the sender simply
returns the data back to the upper layer, an implicit indication that the window is
full. The upper layer would presumably then have to try again later. In a real
implementation, the sender would more likely have either buffered (but not
immediately sent) this data, or would have a synchronization mechanism (for
example, a semaphore or a flag) that would allow the upper layer to call
rdt_send() only when the window is not full.
• Receipt of an ACK. In our GBN protocol, an acknowledgment for a packet with
sequence number n will be taken to be a cumulative acknowledgment, indicating
that all packets with a sequence number up to and including n have been correctly
received at the receiver. We’ll come back to this issue shortly when we
examine the receiver side of GBN.
• A timeout event. The protocol’s name, “Go-Back-N,” is derived from the sender’s
behavior in the presence of lost or overly delayed packets. As in the stop-and-wait
protocol, a timer will again be used to recover from lost data or acknowledgment
packets. If a timeout occurs, the sender resends all packets that have been previously
sent but that have not yet been acknowledged. Our sender in Figure 3.20 uses
only a single timer, which can be thought of as a timer for the oldest transmitted but
not yet acknowledged packet. If an ACK is received but there are still additional
transmitted but not yet acknowledged packets, the timer is restarted. If there are no
outstanding, unacknowledged packets, the timer is stopped.
The receiver’s actions in GBN are also simple. If a packet with sequence number
n is received correctly and is in order (that is, the data last delivered to the upper
layer came from a packet with sequence number n – 1), the receiver sends an ACK
for packet n and delivers the data portion of the packet to the upper layer. In all other
cases, the receiver discards the packet and resends an ACK for the most recently
received in-order packet. Note that since packets are delivered one at a time to the
upper layer, if packet k has been received and delivered, then all packets with a
sequence number lower than k have also been delivered. Thus, the use of cumulative
acknowledgments is a natural choice for GBN.
In our GBN protocol, the receiver discards out-of-order packets. Although it
may seem silly and wasteful to discard a correctly received (but out-of-order)
packet, there is some justification for doing so. Recall that the receiver must deliver
data in order to the upper layer. Suppose now that packet n is expected, but packet
n + 1 arrives. Because data must be delivered in order, the receiver could buffer
(save) packet n + 1 and then deliver this packet to the upper layer after it had later
222 CHAPTER 3 • TRANSPORT LAYER
received and delivered packet n. However, if packet n is lost, both it and packet
n + 1 will eventually be retransmitted as a result of the GBN retransmission rule at
the sender. Thus, the receiver can simply discard packet n + 1. The advantage of this
approach is the simplicity of receiver buffering—the receiver need not buffer any
out-of-order packets. Thus, while the sender must maintain the upper and lower
bounds of its window and the position of nextseqnum within this window, the
only piece of information the receiver need maintain is the sequence number of the
next in-order packet. This value is held in the variable expectedseqnum, shown
in the receiver FSM in Figure 3.21. Of course, the disadvantage of throwing away a
correctly received packet is that the subsequent retransmission of that packet might
be lost or garbled and thus even more retransmissions would be required.
Figure 3.22 shows the operation of the GBN protocol for the case of a window
size of four packets. Because of this window size limitation, the sender sends packets
0 through 3 but then must wait for one or more of these packets to be acknowledged
before proceeding. As each successive ACK (for example, ACK0 and ACK1)
is received, the window slides forward and the sender can transmit one new packet
(pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets
3, 4, and 5 are found to be out of order and are discarded.
Before closing our discussion of GBN, it is worth noting that an implementation
of this protocol in a protocol stack would likely have a structure similar to that
of the extended FSM in Figure 3.20. The implementation would also likely be in the
form of various procedures that implement the actions to be taken in response to the
various events that can occur. In such event-based programming, the various procedures
are called (invoked) either by other procedures in the protocol stack, or as
the result of an interrupt. In the sender, these events would be (1) a call from the
upper-layer entity to invoke rdt_send(), (2) a timer interrupt, and (3) a call from
the lower layer to invoke rdt_rcv() when a packet arrives. The programming
exercises at the end of this chapter will give you a chance to actually implement
these routines in a simulated, but realistic, network setting.
We note here that the GBN protocol incorporates almost all of the techniques
that we will encounter when we study the reliable data transfer components of TCP
in Section 3.5. These techniques include the use of sequence numbers, cumulative
acknowledgments, checksums, and a timeout/retransmit operation.
3.4.4 Selective Repeat (SR)
The GBN protocol allows the sender to potentially “fill the pipeline” in Figure 3.17
with packets, thus avoiding the channel utilization problems we noted with stopand-
wait protocols. There are, however, scenarios in which GBN itself suffers from
performance problems. In particular, when the window size and bandwidth-delay
product are both large, many packets can be in the pipeline. A single packet error
can thus cause GBN to retransmit a large number of packets, many unnecessarily.
As the probability of channel errors increases, the pipeline can become filled with
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 223
these unnecessary retransmissions. Imagine, in our message-dictation scenario, that
if every time a word was garbled, the surrounding 1,000 words (for example, a window
size of 1,000 words) had to be repeated. The dictation would be slowed by all
of the reiterated words.
As the name suggests, selective-repeat protocols avoid unnecessary retransmissions
by having the sender retransmit only those packets that it suspects were
received in error (that is, were lost or corrupted) at the receiver. This individual, asneeded,
retransmission will require that the receiver individually acknowledge correctly
received packets. A window size of N will again be used to limit the number
224 CHAPTER 3 • TRANSPORT LAYER
Sender Receiver
send pkt0
send pkt1
send pkt2
send pkt3
(wait)
rcv ACK0
send pkt4
rcv ACK1
send pkt5
send pkt2
send pkt3
send pkt4
send pkt5
pkt2 timeout
rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt3, discard
send ACK1
rcv pkt4, discard
send ACK1
rcv pkt5, discard
send ACK1
rcv pkt2, deliver
send ACK2
rcv pkt3, deliver
send ACK3
X
(loss)
Figure 3.22  Go-Back-N in operation
of outstanding, unacknowledged packets in the pipeline. However, unlike GBN, the
sender will have already received ACKs for some of the packets in the window.
Figure 3.23 shows the SR sender’s view of the sequence number space. Figure 3.24
details the various actions taken by the SR sender.
The SR receiver will acknowledge a correctly received packet whether or not it
is in order. Out-of-order packets are buffered until any missing packets (that is,
packets with lower sequence numbers) are received, at which point a batch of packets
can be delivered in order to the upper layer. Figure 3.25 itemizes the various
actions taken by the SR receiver. Figure 3.26 shows an example of SR operation in
the presence of lost packets. Note that in Figure 3.26, the receiver initially buffers
packets 3, 4, and 5, and delivers them together with packet 2 to the upper layer when
packet 2 is finally received.
It is important to note that in Step 2 in Figure 3.25, the receiver reacknowledges
(rather than ignores) already received packets with certain sequence numbers below
the current window base. You should convince yourself that this reacknowledgment
is indeed needed. Given the sender and receiver sequence number spaces in Figure
3.23, for example, if there is no ACK for packet send_base propagating from the
receiver to the sender, the sender will eventually retransmit packet send_base,
even though it is clear (to us, not the sender!) that the receiver has already received
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 225
send_base nextseqnum
Window size
N
Key:
Key:
Already
ACK’d
Sent, not
yet ACK’d
Usable,
not yet sent
Not usable
Out of order
(buffered) but
already ACK’d
Expected, not
yet received
Acceptable
(within
window)
Not usable
a. Sender view of sequence numbers
b. Receiver view of sequence numbers
rcv_base
Window size
N
Figure 3.23  Selective-repeat (SR) sender and receiver views of
sequence-number space
that packet. If the receiver were not to acknowledge this packet, the sender’s window
would never move forward! This example illustrates an important aspect of SR
protocols (and many other protocols as well). The sender and receiver will not
always have an identical view of what has been received correctly and what has not.
For SR protocols, this means that the sender and receiver windows will not always
coincide.
226 CHAPTER 3 • TRANSPORT LAYER
1. Packet with sequence number in [rcv_base, rcv_base+N-1] is correctly
received. In this case, the received packet falls within the receiver’s window
and a selective ACK packet is returned to the sender. If the packet was not
previously received, it is buffered. If this packet has a sequence number equal to
the base of the receive window (rcv_base in Figure 3.22), then this packet,
and any previously buffered and consecutively numbered (beginning with
rcv_base) packets are delivered to the upper layer. The receive window is
then moved forward by the number of packets delivered to the upper layer. As
an example, consider Figure 3.26. When a packet with a sequence number of
rcv_base=2 is received, it and packets 3, 4, and 5 can be delivered to the
upper layer.
2. Packet with sequence number in [rcv_base-N, rcv_base-1] is correctly
received. In this case, an ACK must be generated, even though this is a
packet that the receiver has previously acknowledged.
3. Otherwise. Ignore the packet.
Figure 3.25  SR receiver events and actions
1. Data received from above. When data is received from above, the SR sender
checks the next available sequence number for the packet. If the sequence
number is within the sender’s window, the data is packetized and sent; otherwise
it is either buffered or returned to the upper layer for later transmission,
as in GBN.
2. Timeout. Timers are again used to protect against lost packets. However, each
packet must now have its own logical timer, since only a single packet will
be transmitted on timeout. A single hardware timer can be used to mimic the
operation of multiple logical timers [Varghese 1997].
3. ACK received. If an ACK is received, the SR sender marks that packet as
having been received, provided it is in the window. If the packet’s sequence
number is equal to send_base, the window base is moved forward to the
unacknowledged packet with the smallest sequence number. If the window
moves and there are untransmitted packets with sequence numbers that now
fall within the window, these packets are transmitted.
Figure 3.24  SR sender events and actions
The lack of synchronization between sender and receiver windows has important
consequences when we are faced with the reality of a finite range of sequence
numbers. Consider what could happen, for example, with a finite range of four packet
sequence numbers, 0, 1, 2, 3, and a window size of three. Suppose packets 0 through
2 are transmitted and correctly received and acknowledged at the receiver. At this
point, the receiver’s window is over the fourth, fifth, and sixth packets, which have
sequence numbers 3, 0, and 1, respectively. Now consider two scenarios. In the first
scenario, shown in Figure 3.27(a), the ACKs for the first three packets are lost and
pkt0 rcvd, delivered, ACK0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 rcvd, delivered, ACK1 sent
0 1 2 3 4 5 6 7 8 9
pkt3 rcvd, buffered, ACK3 sent
0 1 2 3 4 5 6 7 8 9
pkt4 rcvd, buffered, ACK4 sent
0 1 2 3 4 5 6 7 8 9
pkt5 rcvd; buffered, ACK5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 rcvd, pkt2,pkt3,pkt4,pkt5
delivered, ACK2 sent
0 1 2 3 4 5 6 7 8 9
pkt0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 sent
0 1 2 3 4 5 6 7 8 9
pkt2 sent
0 1 2 3 4 5 6 7 8 9
pkt3 sent, window full
0 1 2 3 4 5 6 7 8 9
ACK0 rcvd, pkt4 sent
0 1 2 3 4 5 6 7 8 9
ACK1 rcvd, pkt5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 TIMEOUT, pkt2
resent
0 1 2 3 4 5 6 7 8 9
ACK3 rcvd, nothing sent
0 1 2 3 4 5 6 7 8 9
X
(loss)
Sender Receiver
Figure 3.26  SR operation
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 227
228 CHAPTER 3 • TRANSPORT LAYER
pkt0
timeout
retransmit pkt0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0 0 1 2 3 0 1 2
ACK1
x ACK2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
a.
b.
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
pkt3
pkt0
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0 0 1 2 3 0 1 2
ACK1
ACK2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
x
x
x
Figure 3.27  SR receiver dilemma with too-large windows: A new packet
or a retransmission?
the sender retransmits these packets. The receiver thus next receives a packet with
sequence number 0—a copy of the first packet sent.
In the second scenario, shown in Figure 3.27(b), the ACKs for the first three
packets are all delivered correctly. The sender thus moves its window forward and
sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and 1, respectively.
The packet with sequence number 3 is lost, but the packet with sequence
number 0 arrives—a packet containing new data.
Now consider the receiver’s viewpoint in Figure 3.27, which has a figurative
curtain between the sender and the receiver, since the receiver cannot “see” the
actions taken by the sender. All the receiver observes is the sequence of messages it
receives from the channel and sends into the channel. As far as it is concerned, the
two scenarios in Figure 3.27 are identical. There is no way of distinguishing the
retransmission of the first packet from an original transmission of the fifth packet.
Clearly, a window size that is 1 less than the size of the sequence number space
won’t work. But how small must the window size be? A problem at the end of the
chapter asks you to show that the window size must be less than or equal to half the
size of the sequence number space for SR protocols.
At the companion Web site, you will find an applet that animates the operation
of the SR protocol. Try performing the same experiments that you did with the GBN
applet. Do the results agree with what you expect?
This completes our discussion of reliable data transfer protocols. We’ve covered
a lot of ground and introduced numerous mechanisms that together provide for reliable
data transfer. Table 3.1 summarizes these mechanisms. Now that we have seen all
of these mechanisms in operation and can see the “big picture,” we encourage you to
review this section again to see how these mechanisms were incrementally added to
cover increasingly complex (and realistic) models of the channel connecting the
sender and receiver, or to improve the performance of the protocols.
Let’s conclude our discussion of reliable data transfer protocols by considering
one remaining assumption in our underlying channel model. Recall that we have
assumed that packets cannot be reordered within the channel between the sender and
receiver. This is generally a reasonable assumption when the sender and receiver are
connected by a single physical wire. However, when the “channel” connecting the two
is a network, packet reordering can occur. One manifestation of packet reordering is
that old copies of a packet with a sequence or acknowledgment number of x can
appear, even though neither the sender’s nor the receiver’s window contains x. With
packet reordering, the channel can be thought of as essentially buffering packets and
spontaneously emitting these packets at any point in the future. Because sequence
numbers may be reused, some care must be taken to guard against such duplicate
packets. The approach taken in practice is to ensure that a sequence number is not
reused until the sender is “sure” that any previously sent packets with sequence number
x are no longer in the network. This is done by assuming that a packet cannot
“live” in the network for longer than some fixed maximum amount of time. A maximum
packet lifetime of approximately three minutes is assumed in the TCP extensions
3.4 • PRINCIPLES OF RELIABLE DATA TRANSFER 229
230 CHAPTER 3 • TRANSPORT LAYER
Table 3.1  Summary of reliable data transfer mechanisms and their use
Mechanism Use, Comments
Checksum Used to detect bit errors in a transmitted packet.
Timer Used to timeout/retransmit a packet, possibly because the packet (or its ACK) was
lost within the channel. Because timeouts can occur when a packet is delayed but
not lost (premature timeout), or when a packet has been received by the receiver
but the receiver-to-sender ACK has been lost, duplicate copies of a packet may be
received by a receiver.
Sequence number Used for sequential numbering of packets of data flowing from sender to receiver.
Gaps in the sequence numbers of received packets allow the receiver to detect a
lost packet. Packets with duplicate sequence numbers allow the receiver to detect
duplicate copies of a packet.
Acknowledgment Used by the receiver to tell the sender that a packet or set of packets has been
received correctly. Acknowledgments will typically carry the sequence number of the
packet or packets being acknowledged. Acknowledgments may be individual or
cumulative, depending on the protocol.
Negative acknowledgment Used by the receiver to tell the sender that a packet has not been received correctly.
Negative acknowledgments will typically carry the sequence number of the packet
that was not received correctly.
Window, pipelining The sender may be restricted to sending only packets with sequence numbers that
fall within a given range. By allowing multiple packets to be transmitted but not yet
acknowledged, sender utilization can be increased over a stop-and-wait mode of
operation. We’ll see shortly that the window size may be set on the basis of the
receiver’s ability to receive and buffer messages, or the level of congestion in the
network, or both.
for high-speed networks [RFC 1323]. [Sunshine 1978] describes a method for using
sequence numbers such that reordering problems can be completely avoided.
3.5 Connection-Oriented Transport: TCP
Now that we have covered the underlying principles of reliable data transfer, let’s
turn to TCP—the Internet’s transport-layer, connection-oriented, reliable transport
protocol. In this section, we’ll see that in order to provide reliable data transfer, TCP
relies on many of the underlying principles discussed in the previous section,
including error detection, retransmissions, cumulative acknowledgments, timers,
and header fields for sequence and acknowledgment numbers. TCP is defined in
RFC 793, RFC 1122, RFC 1323, RFC 2018, and RFC 2581.
3.5.1 The TCP Connection
TCP is said to be connection-oriented because before one application process can
begin to send data to another, the two processes must first “handshake” with each
other—that is, they must send some preliminary segments to each other to establish the
parameters of the ensuing data transfer. As part of TCP connection establishment, both
sides of the connection will initialize many TCP state variables (many of which will be
discussed in this section and in Section 3.7) associated with the TCP connection.
The TCP “connection” is not an end-to-end TDM or FDM circuit as in a circuitswitched
network. Nor is it a virtual circuit (see Chapter 1), as the connection state
resides entirely in the two end systems. Because the TCP protocol runs only in the
end systems and not in the intermediate network elements (routers and link-layer
switches), the intermediate network elements do not maintain TCP connection state.
VINTON CERF, ROBERT KAHN, AND TCP/IP
In the early 1970s, packet-switched networks began to proliferate, with the
ARPAnet—the precursor of the Internet—being just one of many networks. Each of
these networks had its own protocol. Two researchers, Vinton Cerf and Robert Kahn,
recognized the importance of interconnecting these networks and invented a crossnetwork
protocol called TCP/IP, which stands for Transmission Control
Protocol/Internet Protocol. Although Cerf and Kahn began by seeing the protocol as
a single entity, it was later split into its two parts, TCP and IP, which operated separately.
Cerf and Kahn published a paper on TCP/IP in May 1974 in IEEE
Transactions on Communications Technology [Cerf 1974].
The TCP/IP protocol, which is the bread and butter of today’s Internet, was devised
before PCs, workstations, smartphones, and tablets, before the proliferation of Ethernet,
cable, and DSL, WiFi, and other access network technologies, and before the Web,
social media, and streaming video. Cerf and Kahn saw the need for a networking protocol
that, on the one hand, provides broad support for yet-to-be-defined applications
and, on the other hand, allows arbitrary hosts and link-layer protocols to interoperate.
In 2004, Cerf and Kahn received the ACM’s Turing Award, considered the
“Nobel Prize of Computing” for “pioneering work on internetworking, including the
design and implementation of the Internet’s basic communications protocols, TCP/IP,
and for inspired leadership in networking.”
CASE HISTORY
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 231
232 CHAPTER 3 • TRANSPORT LAYER
In fact, the intermediate routers are completely oblivious to TCP connections; they
see datagrams, not connections.
A TCP connection provides a full-duplex service: If there is a TCP connection
between Process A on one host and Process B on another host, then applicationlayer
data can flow from Process A to Process B at the same time as applicationlayer
data flows from Process B to Process A. A TCP connection is also always
point-to-point, that is, between a single sender and a single receiver. So-called
“multicasting” (see Section 4.7)—the transfer of data from one sender to many
receivers in a single send operation—is not possible with TCP. With TCP, two hosts
are company and three are a crowd!
Let’s now take a look at how a TCP connection is established. Suppose a
process running in one host wants to initiate a connection with another process in
another host. Recall that the process that is initiating the connection is called the
client process, while the other process is called the server process. The client application
process first informs the client transport layer that it wants to establish a
connection to a process in the server. Recall from Section 2.7.2, a Python client program
does this by issuing the command
clientSocket.connect((serverName,serverPort))
where serverName is the name of the server and serverPort identifies the
process on the server. TCP in the client then proceeds to establish a TCP connection
with TCP in the server. At the end of this section we discuss in some detail the connection-
establishment procedure. For now it suffices to know that the client first sends
a special TCP segment; the server responds with a second special TCP segment; and
finally the client responds again with a third special segment. The first two segments
carry no payload, that is, no application-layer data; the third of these segments may
carry a payload. Because three segments are sent between the two hosts, this connection-
establishment procedure is often referred to as a three-way handshake.
Once a TCP connection is established, the two application processes can send
data to each other. Let’s consider the sending of data from the client process to the
server process. The client process passes a stream of data through the socket (the
door of the process), as described in Section 2.7. Once the data passes through
the door, the data is in the hands of TCP running in the client. As shown in Figure
3.28, TCP directs this data to the connection’s send buffer, which is one of the
buffers that is set aside during the initial three-way handshake. From time to time,
TCP will grab chunks of data from the send buffer and pass the data to the network
layer. Interestingly, the TCP specification [RFC 793] is very laid back about specifying
when TCP should actually send buffered data, stating that TCP should “send
that data in segments at its own convenience.” The maximum amount of data that
can be grabbed and placed in a segment is limited by the maximum segment size
(MSS). The MSS is typically set by first determining the length of the largest
link-layer frame that can be sent by the local sending host (the so-called maximum
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 233
transmission unit, MTU), and then setting the MSS to ensure that a TCP segment
(when encapsulated in an IP datagram) plus the TCP/IP header length (typically 40
bytes) will fit into a single link-layer frame. Both Ethernet and PPP link-layer protocols
have an MSS of 1,500 bytes. Approaches have also been proposed for discovering
the path MTU—the largest link-layer frame that can be sent on all links from
source to destination [RFC 1191]—and setting the MSS based on the path MTU
value. Note that the MSS is the maximum amount of application-layer data in the
segment, not the maximum size of the TCP segment including headers. (This terminology
is confusing, but we have to live with it, as it is well entrenched.)
TCP pairs each chunk of client data with a TCP header, thereby forming TCP
segments. The segments are passed down to the network layer, where they are separately
encapsulated within network-layer IP datagrams. The IP datagrams are then
sent into the network. When TCP receives a segment at the other end, the segment’s
data is placed in the TCP connection’s receive buffer, as shown in Figure 3.28. The
application reads the stream of data from this buffer. Each side of the connection has
its own send buffer and its own receive buffer. (You can see the online flow-control
applet at http://www.awl.com/kurose-ross, which provides an animation of the send
and receive buffers.)
We see from this discussion that a TCP connection consists of buffers, variables,
and a socket connection to a process in one host, and another set of buffers,
variables, and a socket connection to a process in another host. As mentioned earlier,
no buffers or variables are allocated to the connection in the network elements
(routers, switches, and repeaters) between the hosts.
3.5.2 TCP Segment Structure
Having taken a brief look at the TCP connection, let’s examine the TCP segment
structure. The TCP segment consists of header fields and a data field. The data
field contains a chunk of application data. As mentioned above, the MSS limits the
Process
writes data
Process
reads data
TCP
send
buffer
Socket
TCP
receive
buffer
Socket
Segment Segment
Figure 3.28  TCP send and receive buffers
234 CHAPTER 3 • TRANSPORT LAYER
maximum size of a segment’s data field. When TCP sends a large file, such as an
image as part of a Web page, it typically breaks the file into chunks of size MSS
(except for the last chunk, which will often be less than the MSS). Interactive applications,
however, often transmit data chunks that are smaller than the MSS; for
example, with remote login applications like Telnet, the data field in the TCP segment
is often only one byte. Because the TCP header is typically 20 bytes (12 bytes
more than the UDP header), segments sent by Telnet may be only 21 bytes in length.
Figure 3.29 shows the structure of the TCP segment. As with UDP, the header
includes source and destination port numbers, which are used for
multiplexing/demultiplexing data from/to upper-layer applications. Also, as with
UDP, the header includes a checksum field. A TCP segment header also contains
the following fields:
• The 32-bit sequence number field and the 32-bit acknowledgment number
field are used by the TCP sender and receiver in implementing a reliable data
transfer service, as discussed below.
• The 16-bit receive window field is used for flow control. We will see shortly that
it is used to indicate the number of bytes that a receiver is willing to accept.
• The 4-bit header length field specifies the length of the TCP header in 32-bit
words. The TCP header can be of variable length due to the TCP options field.
Source port #
Internet checksum
Header
length Unused
URG
ACK
PSH
RST
SYN
FIN
32 bits
Dest port #
Receive window
Urgent data pointer
Sequence number
Acknowledgment number
Options
Data
Figure 3.29  TCP segment structure
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 235
(Typically, the options field is empty, so that the length of the typical TCP header
is 20 bytes.)
• The optional and variable-length options field is used when a sender and
receiver negotiate the maximum segment size (MSS) or as a window scaling factor
for use in high-speed networks. A time-stamping option is also defined. See
RFC 854 and RFC 1323 for additional details.
• The flag field contains 6 bits. The ACK bit is used to indicate that the value carried
in the acknowledgment field is valid; that is, the segment contains an
acknowledgment for a segment that has been successfully received. The RST,
SYN, and FIN bits are used for connection setup and teardown, as we will discuss
at the end of this section. Setting the PSH bit indicates that the receiver
should pass the data to the upper layer immediately. Finally, the URG bit is used
to indicate that there is data in this segment that the sending-side upper-layer
entity has marked as “urgent.” The location of the last byte of this urgent data is
indicated by the 16-bit urgent data pointer field. TCP must inform the receiving-
side upper-layer entity when urgent data exists and pass it a pointer to the
end of the urgent data. (In practice, the PSH, URG, and the urgent data pointer
are not used. However, we mention these fields for completeness.)
Sequence Numbers and Acknowledgment Numbers
Two of the most important fields in the TCP segment header are the sequence number
field and the acknowledgment number field. These fields are a critical part of TCP’s
reliable data transfer service. But before discussing how these fields are used to provide
reliable data transfer, let us first explain what exactly TCP puts in these fields.
TCP views data as an unstructured, but ordered, stream of bytes. TCP’s use of
sequence numbers reflects this view in that sequence numbers are over the stream of
transmitted bytes and not over the series of transmitted segments. The sequence
number for a segment is therefore the byte-stream number of the first byte in the
segment. Let’s look at an example. Suppose that a process in Host A wants to send a
stream of data to a process in Host B over a TCP connection. The TCP in Host A will
implicitly number each byte in the data stream. Suppose that the data stream consists
of a file consisting of 500,000 bytes, that the MSS is 1,000 bytes, and that the first
byte of the data stream is numbered 0. As shown in Figure 3.30, TCP constructs 500
segments out of the data stream. The first segment gets assigned sequence number 0,
the second segment gets assigned sequence number 1,000, the third segment gets
assigned sequence number 2,000, and so on. Each sequence number is inserted in the
sequence number field in the header of the appropriate TCP segment.
Now let’s consider acknowledgment numbers. These are a little trickier than
sequence numbers. Recall that TCP is full-duplex, so that Host A may be receiving
data from Host B while it sends data to Host B (as part of the same TCP connection).
Each of the segments that arrive from Host B has a sequence number for the data
236 CHAPTER 3 • TRANSPORT LAYER
flowing from B to A. The acknowledgment number that Host A puts in its segment
is the sequence number of the next byte Host A is expecting from Host B. It is good
to look at a few examples to understand what is going on here. Suppose that Host A
has received all bytes numbered 0 through 535 from B and suppose that it is about
to send a segment to Host B. Host A is waiting for byte 536 and all the subsequent
bytes in Host B’s data stream. So Host A puts 536 in the acknowledgment number
field of the segment it sends to B.
As another example, suppose that Host A has received one segment from Host
B containing bytes 0 through 535 and another segment containing bytes 900 through
1,000. For some reason Host A has not yet received bytes 536 through 899. In this
example, Host A is still waiting for byte 536 (and beyond) in order to re-create B’s
data stream. Thus, A’s next segment to B will contain 536 in the acknowledgment
number field. Because TCP only acknowledges bytes up to the first missing byte in
the stream, TCP is said to provide cumulative acknowledgments.
This last example also brings up an important but subtle issue. Host A received
the third segment (bytes 900 through 1,000) before receiving the second segment
(bytes 536 through 899). Thus, the third segment arrived out of order. The subtle
issue is: What does a host do when it receives out-of-order segments in a TCP connection?
Interestingly, the TCP RFCs do not impose any rules here and leave the
decision up to the people programming a TCP implementation. There are basically
two choices: either (1) the receiver immediately discards out-of-order segments
(which, as we discussed earlier, can simplify receiver design), or (2) the receiver
keeps the out-of-order bytes and waits for the missing bytes to fill in the gaps.
Clearly, the latter choice is more efficient in terms of network bandwidth, and is the
approach taken in practice.
In Figure 3.30, we assumed that the initial sequence number was zero. In truth,
both sides of a TCP connection randomly choose an initial sequence number. This is
done to minimize the possibility that a segment that is still present in the network
from an earlier, already-terminated connection between two hosts is mistaken for a
valid segment in a later connection between these same two hosts (which also happen
to be using the same port numbers as the old connection) [Sunshine 1978].
0 1 1,000 1,999 499,999
File
Data for 1st segment Data for 2nd segment
Figure 3.30  Dividing file data into TCP segments
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 237
Telnet: A Case Study for Sequence and Acknowledgment Numbers
Telnet, defined in RFC 854, is a popular application-layer protocol used for
remote login. It runs over TCP and is designed to work between any pair of hosts.
Unlike the bulk data transfer applications discussed in Chapter 2, Telnet is an
interactive application. We discuss a Telnet example here, as it nicely illustrates
TCP sequence and acknowledgment numbers. We note that many users now
prefer to use the SSH protocol rather than Telnet, since data sent in a Telnet connection
(including passwords!) is not encrypted, making Telnet vulnerable to
eavesdropping attacks (as discussed in Section 8.7).
Suppose Host A initiates a Telnet session with Host B. Because Host A initiates
the session, it is labeled the client, and Host B is labeled the server. Each character
typed by the user (at the client) will be sent to the remote host; the remote host will
send back a copy of each character, which will be displayed on the Telnet user’s
screen. This “echo back” is used to ensure that characters seen by the Telnet user
have already been received and processed at the remote site. Each character thus
traverses the network twice between the time the user hits the key and the time the
character is displayed on the user’s monitor.
Now suppose the user types a single letter, ‘C,’ and then grabs a coffee. Let’s examine
the TCP segments that are sent between the client and server. As shown in Figure
3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server,
respectively. Recall that the sequence number of a segment is the sequence number of
the first byte in the data field. Thus, the first segment sent from the client will have
sequence number 42; the first segment sent from the server will have sequence number
79. Recall that the acknowledgment number is the sequence number of the next byte of
data that the host is waiting for. After the TCP connection is established but before any
data is sent, the client is waiting for byte 79 and the server is waiting for byte 42.
As shown in Figure 3.31, three segments are sent. The first segment is sent from
the client to the server, containing the 1-byte ASCII representation of the letter ‘C’
in its data field. This first segment also has 42 in its sequence number field, as we
just described. Also, because the client has not yet received any data from the server,
this first segment will have 79 in its acknowledgment number field.
The second segment is sent from the server to the client. It serves a dual purpose.
First it provides an acknowledgment of the data the server has received. By
putting 43 in the acknowledgment field, the server is telling the client that it has successfully
received everything up through byte 42 and is now waiting for bytes 43
onward. The second purpose of this segment is to echo back the letter ‘C.’ Thus, the
second segment has the ASCII representation of ‘C’ in its data field. This second
segment has the sequence number 79, the initial sequence number of the server-toclient
data flow of this TCP connection, as this is the very first byte of data that the
server is sending. Note that the acknowledgment for client-to-server data is carried
in a segment carrying server-to-client data; this acknowledgment is said to be
piggybacked on the server-to-client data segment.
238 CHAPTER 3 • TRANSPORT LAYER
The third segment is sent from the client to the server. Its sole purpose is to
acknowledge the data it has received from the server. (Recall that the second segment
contained data—the letter ‘C’—from the server to the client.) This segment
has an empty data field (that is, the acknowledgment is not being piggybacked with
any client-to-server data). The segment has 80 in the acknowledgment number field
because the client has received the stream of bytes up through byte sequence number
79 and it is now waiting for bytes 80 onward. You might think it odd that this
segment also has a sequence number since the segment contains no data. But
because TCP has a sequence number field, the segment needs to have some
sequence number.
3.5.3 Round-Trip Time Estimation and Timeout
TCP, like our rdt protocol in Section 3.4, uses a timeout/retransmit mechanism to
recover from lost segments. Although this is conceptually simple, many subtle
issues arise when we implement a timeout/retransmit mechanism in an actual protocol
such as TCP. Perhaps the most obvious question is the length of the timeout
Time Time
Host A Host B
User types
'C'
Seq=42, ACK=79, data='C'
Seq=79, ACK=43, data='C'
Seq=43, ACK=80
Host ACKs
receipt of 'C',
echoes back 'C'
Host ACKs
receipt of
echoed 'C'
Figure 3.31  Sequence and acknowledgment numbers for a simple
Telnet application over TCP
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 239
intervals. Clearly, the timeout should be larger than the connection’s round-trip time
(RTT), that is, the time from when a segment is sent until it is acknowledged. Otherwise,
unnecessary retransmissions would be sent. But how much larger? How
should the RTT be estimated in the first place? Should a timer be associated with
each and every unacknowledged segment? So many questions! Our discussion in
this section is based on the TCP work in [Jacobson 1988] and the current IETF recommendations
for managing TCP timers [RFC 6298].
Estimating the Round-Trip Time
Let’s begin our study of TCP timer management by considering how TCP estimates
the round-trip time between sender and receiver. This is accomplished as follows.
The sample RTT, denoted SampleRTT, for a segment is the amount of time
between when the segment is sent (that is, passed to IP) and when an acknowledgment
for the segment is received. Instead of measuring a SampleRTT for every
transmitted segment, most TCP implementations take only one SampleRTT measurement
at a time. That is, at any point in time, the SampleRTT is being estimated
for only one of the transmitted but currently unacknowledged segments, leading to a
new value of SampleRTT approximately once every RTT. Also, TCP never computes
a SampleRTT for a segment that has been retransmitted; it only measures
SampleRTT for segments that have been transmitted once [Karn 1987]. (A problem
at the end of the chapter asks you to consider why.)
Obviously, the SampleRTT values will fluctuate from segment to segment due
to congestion in the routers and to the varying load on the end systems. Because of
this fluctuation, any given SampleRTT value may be atypical. In order to estimate
a typical RTT, it is therefore natural to take some sort of average of the SampleRTT
values. TCP maintains an average, called EstimatedRTT, of the SampleRTT
values. Upon obtaining a new SampleRTT, TCP updates
EstimatedRTT according to the following formula:
EstimatedRTT = (1 – ) • EstimatedRTT +  • SampleRTT
The formula above is written in the form of a programming-language statement—
the new value of EstimatedRTT is a weighted combination of the previous value
of EstimatedRTT and the new value for SampleRTT. The recommended value
of  is  = 0.125 (that is, 1/8) [RFC 6298], in which case the formula above
becomes:
EstimatedRTT = 0.875 • EstimatedRTT + 0.125 • SampleRTT
Note that EstimatedRTT is a weighted average of the SampleRTT values.
As discussed in a homework problem at the end of this chapter, this weighted average
puts more weight on recent samples than on old samples. This is natural, as the
240 CHAPTER 3 • TRANSPORT LAYER
more recent samples better reflect the current congestion in the network. In statistics,
such an average is called an exponential weighted moving average (EWMA).
The word “exponential” appears in EWMA because the weight of a given SampleRTT
decays exponentially fast as the updates proceed. In the homework problems
you will be asked to derive the exponential term in EstimatedRTT.
Figure 3.32 shows the SampleRTT values and EstimatedRTT for a value of 
= 1/8 for a TCP connection between gaia.cs.umass.edu (in Amherst, Massachusetts)
to fantasia.eurecom.fr (in the south of France). Clearly, the variations in
the SampleRTT are smoothed out in the computation of the EstimatedRTT.
In addition to having an estimate of the RTT, it is also valuable to have a
measure of the variability of the RTT. [RFC 6298] defines the RTT variation,
DevRTT, as an estimate of how much SampleRTT typically deviates from
EstimatedRTT:
DevRTT = (1 – ) • DevRTT + •| SampleRTT – EstimatedRTT |
Note that DevRTT is an EWMA of the difference between SampleRTT and
EstimatedRTT. If the SampleRTT values have little fluctuation, then DevRTT
will be small; on the other hand, if there is a lot of fluctuation, DevRTT will be
large. The recommended value of ß is 0.25.
TCP provides reliable data transfer by using positive acknowledgments and timers in much
the same way that we studied in Section 3.4. TCP acknowledges data that has been
received correctly, and it then retransmits segments when segments or their corresponding
acknowledgments are thought to be lost or corrupted. Certain versions of TCP also have an
implicit NAK mechanism—with TCP’s fast retransmit mechanism, the receipt of three duplicate
ACKs for a given segment serves as an implicit NAK for the following segment, triggering
retransmission of that segment before timeout. TCP uses sequences of numbers to
allow the receiver to identify lost or duplicate segments. Just as in the case of our reliable
data transfer protocol, rdt3.0, TCP cannot itself tell for certain if a segment, or its
ACK, is lost, corrupted, or overly delayed. At the sender, TCP’s response will be the same:
retransmit the segment in question.
TCP also uses pipelining, allowing the sender to have multiple transmitted but yet-to-beacknowledged
segments outstanding at any given time. We saw earlier that pipelining
can greatly improve a session’s throughput when the ratio of the segment size to roundtrip
delay is small. The specific number of outstanding, unacknowledged segments that a
sender can have is determined by TCP’s flow-control and congestion-control mechanisms.
TCP flow control is discussed at the end of this section; TCP congestion control is discussed
in Section 3.7. For the time being, we must simply be aware that the TCP sender
uses pipelining.
PRINCIPLES IN PRACTICE
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 241
Setting and Managing the Retransmission Timeout Interval
Given values of EstimatedRTT and DevRTT, what value should be used for
TCP’s timeout interval? Clearly, the interval should be greater than or equal to
EstimatedRTT, or unnecessary retransmissions would be sent. But the timeout
interval should not be too much larger than EstimatedRTT; otherwise, when a segment
is lost, TCP would not quickly retransmit the segment, leading to large data transfer
delays. It is therefore desirable to set the timeout equal to the EstimatedRTT plus
some margin. The margin should be large when there is a lot of fluctuation in the
SampleRTT values; it should be small when there is little fluctuation. The value of
DevRTT should thus come into play here. All of these considerations are taken into
account in TCP’s method for determining the retransmission timeout interval:
TimeoutInterval = EstimatedRTT + 4 • DevRTT
An initial TimeoutInterval value of 1 second is recommended [RFC 6298].
Also, when a timeout occurs, the value of TimeoutInterval is doubled to avoid
a premature timeout occurring for a subsequent segment that will soon be acknowledged.
However, as soon as a segment is received and EstimatedRTT is updated,
the TimeoutInterval is again computed using the formula above.
RTT (milliseconds)
150
200
250
300
350
100
1 8 15 22 29 36 43 50
Time (seconds)
Sample RTT
57 64 71 78 85 92 99 106
Estimated RTT
Figure 3.32  RTT samples and RTT estimates
242 CHAPTER 3 • TRANSPORT LAYER
3.5.4 Reliable Data Transfer
Recall that the Internet’s network-layer service (IP service) is unreliable. IP does
not guarantee datagram delivery, does not guarantee in-order delivery of datagrams,
and does not guarantee the integrity of the data in the datagrams. With IP
service, datagrams can overflow router buffers and never reach their destination,
datagrams can arrive out of order, and bits in the datagram can get corrupted
(flipped from 0 to 1 and vice versa). Because transport-layer segments are carried
across the network by IP datagrams, transport-layer segments can suffer from these
problems as well.
TCP creates a reliable data transfer service on top of IP’s unreliable besteffort
service. TCP’s reliable data transfer service ensures that the data stream that a
process reads out of its TCP receive buffer is uncorrupted, without gaps, without
duplication, and in sequence; that is, the byte stream is exactly the same byte stream
that was sent by the end system on the other side of the connection. How TCP provides
a reliable data transfer involves many of the principles that we studied in
Section 3.4.
In our earlier development of reliable data transfer techniques, it was conceptually
easiest to assume that an individual timer is associated with each transmitted
but not yet acknowledged segment. While this is great in theory, timer management
can require considerable overhead. Thus, the recommended TCP timer management
procedures [RFC 6298] use only a single retransmission timer, even if there are multiple
transmitted but not yet acknowledged segments. The TCP protocol described
in this section follows this single-timer recommendation.
We will discuss how TCP provides reliable data transfer in two incremental
steps. We first present a highly simplified description of a TCP sender that uses only
timeouts to recover from lost segments; we then present a more complete description
that uses duplicate acknowledgments in addition to timeouts. In the ensuing discussion,
we suppose that data is being sent in only one direction, from Host A to
Host B, and that Host A is sending a large file.
Figure 3.33 presents a highly simplified description of a TCP sender. We see
that there are three major events related to data transmission and retransmission in
the TCP sender: data received from application above; timer timeout; and ACK
receipt. Upon the occurrence of the first major event, TCP receives data from the
application, encapsulates the data in a segment, and passes the segment to IP. Note
that each segment includes a sequence number that is the byte-stream number of
the first data byte in the segment, as described in Section 3.5.2. Also note that if the
timer is already not running for some other segment, TCP starts the timer when the
segment is passed to IP. (It is helpful to think of the timer as being associated with
the oldest unacknowledged segment.) The expiration interval for this timer is the
TimeoutInterval, which is calculated from EstimatedRTT and DevRTT,
as described in Section 3.5.3.
Figure 3.33  Simplified TCP sender
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 243
The second major event is the timeout. TCP responds to the timeout event by
retransmitting the segment that caused the timeout. TCP then restarts the timer.
The third major event that must be handled by the TCP sender is the arrival of an
acknowledgment segment (ACK) from the receiver (more specifically, a segment containing
a valid ACK field value). On the occurrence of this event, TCP compares the
ACK value y with its variable SendBase. The TCP state variable SendBase is the
sequence number of the oldest unacknowledged byte. (Thus SendBase–1 is the
sequence number of the last byte that is known to have been received correctly and in
order at the receiver.) As indicated earlier, TCP uses cumulative acknowledgments, so
that y acknowledges the receipt of all bytes before byte number y. If y > SendBase,
/* Assume sender is not constrained by TCP flow or congestion control, that data from above is less
than MSS in size, and that data transfer is in one direction only. */
NextSeqNum=InitialSeqNumber
SendBase=InitialSeqNumber
loop (forever) {
switch(event)
event: data received from application above
create TCP segment with sequence number NextSeqNum
if (timer currently not running)
start timer
pass segment to IP
NextSeqNum=NextSeqNum+length(data)
break;
event: timer timeout
retransmit not-yet-acknowledged segment with
smallest sequence number
start timer
break;
event: ACK received, with ACK field value of y
if (y > SendBase) {
SendBase=y
if (there are currently any not-yet-acknowledged segments)
start timer
}
break;
} /* end of loop forever */
244 CHAPTER 3 • TRANSPORT LAYER
then the ACK is acknowledging one or more previously unacknowledged segments.
Thus the sender updates its SendBase variable; it also restarts the timer if there currently
are any not-yet-acknowledged segments.
A Few Interesting Scenarios
We have just described a highly simplified version of how TCP provides reliable
data transfer. But even this highly simplified version has many subtleties. To get a
good feeling for how this protocol works, let’s now walk through a few simple
scenarios. Figure 3.34 depicts the first scenario, in which Host A sends one segment
to Host B. Suppose that this segment has sequence number 92 and contains 8
bytes of data. After sending this segment, Host A waits for a segment from B with
acknowledgment number 100. Although the segment from A is received at B, the
acknowledgment from B to A gets lost. In this case, the timeout event occurs, and
Host A retransmits the same segment. Of course, when Host B receives the
retransmission, it observes from the sequence number that the segment contains
data that has already been received. Thus, TCP in Host B will discard the bytes in
the retransmitted segment.
Time Time
Host A Host B
Timeout
Seq=92, 8 bytes data
Seq=92, 8 bytes data
ACK=100
ACK=100
X
(loss)
Figure 3.34  Retransmission due to a lost acknowledgment
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 245
In a second scenario, shown in Figure 3.35, Host A sends two segments back to
back. The first segment has sequence number 92 and 8 bytes of data, and the second
segment has sequence number 100 and 20 bytes of data. Suppose that both segments
arrive intact at B, and B sends two separate acknowledgments for each of these segments.
The first of these acknowledgments has acknowledgment number 100; the
second has acknowledgment number 120. Suppose now that neither of the acknowledgments
arrives at Host A before the timeout. When the timeout event occurs, Host
A resends the first segment with sequence number 92 and restarts the timer. As long
as the ACK for the second segment arrives before the new timeout, the second segment
will not be retransmitted.
In a third and final scenario, suppose Host A sends the two segments, exactly as
in the second example. The acknowledgment of the first segment is lost in the
network, but just before the timeout event, Host Areceives an acknowledgment with
acknowledgment number 120. Host A therefore knows that Host B has received
everything up through byte 119; so Host A does not resend either of the two
segments. This scenario is illustrated in Figure 3.36.
Time Time
Host A Host B
seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100, 20 bytes data
ACK=100
ACK=120
ACK=120
seq=92 timeout interval
Seq=92, 8 bytes data
Figure 3.35  Segment 100 not retransmitted
246 CHAPTER 3 • TRANSPORT LAYER
Doubling the Timeout Interval
We now discuss a few modifications that most TCP implementations employ. The
first concerns the length of the timeout interval after a timer expiration. In this modification,
whenever the timeout event occurs, TCP retransmits the not-yetacknowledged
segment with the smallest sequence number, as described above. But
each time TCP retransmits, it sets the next timeout interval to twice the previous
value, rather than deriving it from the last EstimatedRTT and DevRTT (as
described in Section 3.5.3). For example, suppose TimeoutInterval associated
with the oldest not yet acknowledged segment is .75 sec when the timer first expires.
TCP will then retransmit this segment and set the new expiration time to 1.5 sec. If
the timer expires again 1.5 sec later, TCP will again retransmit this segment, now
setting the expiration time to 3.0 sec. Thus the intervals grow exponentially after
each retransmission. However, whenever the timer is started after either of the two
other events (that is, data received from application above, and ACK received), the
Time Time
Host A Host B
Seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100, 20 bytes data
ACK=100
ACK=120
X
(loss)
Figure 3.36  A cumulative acknowledgment avoids retransmission of the
first segment
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 247
TimeoutInterval is derived from the most recent values of EstimatedRTT
and DevRTT.
This modification provides a limited form of congestion control. (More comprehensive
forms of TCP congestion control will be studied in Section 3.7.) The
timer expiration is most likely caused by congestion in the network, that is, too
many packets arriving at one (or more) router queues in the path between the source
and destination, causing packets to be dropped and/or long queuing delays. In times
of congestion, if the sources continue to retransmit packets persistently, the congestion
may get worse. Instead, TCP acts more politely, with each sender retransmitting
after longer and longer intervals. We will see that a similar idea is used by Ethernet
when we study CSMA/CD in Chapter 5.
Fast Retransmit
One of the problems with timeout-triggered retransmissions is that the timeout
period can be relatively long. When a segment is lost, this long timeout period
forces the sender to delay resending the lost packet, thereby increasing the end-toend
delay. Fortunately, the sender can often detect packet loss well before the timeout
event occurs by noting so-called duplicate ACKs. A duplicate ACK is an ACK
that reacknowledges a segment for which the sender has already received an earlier
acknowledgment. To understand the sender’s response to a duplicate ACK, we must
look at why the receiver sends a duplicate ACK in the first place. Table 3.2 summarizes
the TCP receiver’s ACK generation policy [RFC 5681]. When a TCP receiver
receives a segment with a sequence number that is larger than the next, expected,
in-order sequence number, it detects a gap in the data stream—that is, a missing segment.
This gap could be the result of lost or reordered segments within the network.
Event TCP Receiver Action
Arrival of in-order segment with expected sequence number. All Delayed ACK. Wait up to 500 msec for arrival of another in-order segdata
up to expected sequence number already acknowledged. ment. If next in-order segment does not arrive in this interval, send an ACK.
Arrival of in-order segment with expected sequence number. One Immediately send single cumulative ACK, ACKing both in-order segments.
other in-order segment waiting for ACK transmission.
Arrival of out-of-order segment with higher-than-expected sequence Immediately send duplicate ACK, indicating sequence number of next
number. Gap detected. expected byte (which is the lower end of the gap).
Arrival of segment that partially or completely fills in gap in Immediately send ACK, provided that segment starts at the lower end
received data. of gap.
Table 3.2  TCP ACK Generation Recommendation [RFC 5681]
248 CHAPTER 3 • TRANSPORT LAYER
Since TCP does not use negative acknowledgments, the receiver cannot send an
explicit negative acknowledgment back to the sender. Instead, it simply reacknowledges
(that is, generates a duplicate ACK for) the last in-order byte of data it has
received. (Note that Table 3.2 allows for the case that the receiver does not discard
out-of-order segments.)
Because a sender often sends a large number of segments back to back, if one segment
is lost, there will likely be many back-to-back duplicate ACKs. If the TCP sender
receives three duplicate ACKs for the same data, it takes this as an indication that the
segment following the segment that has been ACKed three times has been lost. (In the
homework problems, we consider the question of why the sender waits for three duplicate
ACKs, rather than just a single duplicate ACK.) In the case that three duplicate
ACKs are received, the TCP sender performs a fast retransmit [RFC 5681], retransmitting
the missing segment before that segment’s timer expires. This is shown in
Figure 3.37, where the second segment is lost, then retransmitted before its timer
expires. For TCP with fast retransmit, the following code snippet replaces the ACK
received event in Figure 3.33:
event: ACK received, with ACK field value of y
if (y > SendBase) {
SendBase=y
if (there are currently any not yet
acknowledged segments)
start timer
}
else { /* a duplicate ACK for already ACKed
segment */
increment number of duplicate ACKs
received for y
if (number of duplicate ACKS received
for y==3)
/* TCP fast retransmit */
resend segment with sequence number y
}
break;
We noted earlier that many subtle issues arise when a timeout/retransmit mechanism
is implemented in an actual protocol such as TCP. The procedures above,
which have evolved as a result of more than 20 years of experience with TCP timers,
should convince you that this is indeed the case!
Go-Back-N or Selective Repeat?
Let us close our study of TCP’s error-recovery mechanism by considering the following
question: Is TCP a GBN or an SR protocol? Recall that TCP acknowledgments are
cumulative and correctly received but out-of-order segments are not individually
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 249
ACKed by the receiver. Consequently, as shown in Figure 3.33 (see also Figure 3.19),
the TCP sender need only maintain the smallest sequence number of a transmitted but
unacknowledged byte (SendBase) and the sequence number of the next byte to be
sent (NextSeqNum). In this sense, TCP looks a lot like a GBN-style protocol. But
there are some striking differences between TCP and Go-Back-N. Many TCP implementations
will buffer correctly received but out-of-order segments [Stevens 1994].
Consider also what happens when the sender sends a sequence of segments 1, 2, . . . ,
N, and all of the segments arrive in order without error at the receiver. Further suppose
that the acknowledgment for packet n < N gets lost, but the remaining N – 1 acknowledgments
arrive at the sender before their respective timeouts. In this example, GBN
would retransmit not only packet n, but also all of the subsequent packets n + 1, n + 2,
. . . , N. TCP, on the other hand, would retransmit at most one segment, namely, segment
n. Moreover, TCP would not even retransmit segment n if the acknowledgment
for segment n + 1 arrived before the timeout for segment n.
Host A Host B
seq=100, 20 bytes of data Timeout
Time Time
X
seq=100, 20 bytes of data
seq=92, 8 bytes of data
seq=120, 15 bytes of data
seq=135, 6 bytes of data
seq=141, 16 bytes of data
ack=100
ack=100
ack=100
ack=100
Figure 3.37  Fast retransmit: retransmitting the missing segment before
the segment’s timer expires
A proposed modification to TCP, the so-called selective acknowledgment
[RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively
rather than just cumulatively acknowledging the last correctly received, inorder
segment. When combined with selective retransmission—skipping the
retransmission of segments that have already been selectively acknowledged by the
receiver—TCP looks a lot like our generic SR protocol. Thus, TCP’s error-recovery
mechanism is probably best categorized as a hybrid of GBN and SR protocols.
3.5.5 Flow Control
Recall that the hosts on each side of a TCP connection set aside a receive buffer for
the connection. When the TCP connection receives bytes that are correct and in
sequence, it places the data in the receive buffer. The associated application process
will read data from this buffer, but not necessarily at the instant the data arrives.
Indeed, the receiving application may be busy with some other task and may not
even attempt to read the data until long after it has arrived. If the application is relatively
slow at reading the data, the sender can very easily overflow the connection’s
receive buffer by sending too much data too quickly.
TCP provides a flow-control service to its applications to eliminate the possibility
of the sender overflowing the receiver’s buffer. Flow control is thus a speed-matching
service—matching the rate at which the sender is sending against the rate at which the
receiving application is reading. As noted earlier, a TCP sender can also be throttled
due to congestion within the IP network; this form of sender control is referred to as
congestion control, a topic we will explore in detail in Sections 3.6 and 3.7. Even
though the actions taken by flow and congestion control are similar (the throttling of
the sender), they are obviously taken for very different reasons. Unfortunately, many
authors use the terms interchangeably, and the savvy reader would be wise to distinguish
between them. Let’s now discuss how TCP provides its flow-control service. In
order to see the forest for the trees, we suppose throughout this section that the TCP
implementation is such that the TCP receiver discards out-of-order segments.
TCP provides flow control by having the sender maintain a variable called the
receive window. Informally, the receive window is used to give the sender an idea of
how much free buffer space is available at the receiver. Because TCP is full-duplex, the
sender at each side of the connection maintains a distinct receive window. Let’s investigate
the receive window in the context of a file transfer. Suppose that Host Ais sending
a large file to Host B over a TCP connection. Host B allocates a receive buffer to this
connection; denote its size by RcvBuffer. From time to time, the application process
in Host B reads from the buffer. Define the following variables:
• LastByteRead: the number of the last byte in the data stream read from the
buffer by the application process in B
• LastByteRcvd: the number of the last byte in the data stream that has arrived
from the network and has been placed in the receive buffer at B
250 CHAPTER 3 • TRANSPORT LAYER
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 251
Because TCP is not permitted to overflow the allocated buffer, we must have
LastByteRcvd – LastByteRead  RcvBuffer
The receive window, denoted rwnd is set to the amount of spare room in the buffer:
rwnd = RcvBuffer – [LastByteRcvd – LastByteRead]
Because the spare room changes with time, rwnd is dynamic. The variable rwnd is
illustrated in Figure 3.38.
How does the connection use the variable rwnd to provide the flow-control
service? Host B tells Host A how much spare room it has in the connection buffer
by placing its current value of rwnd in the receive window field of every segment it
sends to A. Initially, Host B sets rwnd = RcvBuffer. Note that to pull this off,
Host B must keep track of several connection-specific variables.
Host A in turn keeps track of two variables, LastByteSent and Last-
ByteAcked, which have obvious meanings. Note that the difference between these
two variables, LastByteSent – LastByteAcked, is the amount of unacknowledged
data that A has sent into the connection. By keeping the amount of
unacknowledged data less than the value of rwnd, Host A is assured that it is not
overflowing the receive buffer at Host B. Thus, Host A makes sure throughout the
connection’s life that
LastByteSent – LastByteAcked  rwnd
Application
process
Data
from IP TCP data
in buffer
rwnd
RcvBuffer
Spare room
Figure 3.38  The receive window (rwnd) and the receive buffer
(RcvBuffer)
There is one minor technical problem with this scheme. To see this, suppose
Host B’s receive buffer becomes full so that rwnd = 0. After advertising rwnd = 0
to Host A, also suppose that B has nothing to send to A. Now consider what happens.
As the application process at B empties the buffer, TCP does not send new segments
with new rwnd values to Host A; indeed, TCP sends a segment to Host A
only if it has data to send or if it has an acknowledgment to send. Therefore, Host A
is never informed that some space has opened up in Host B’s receive buffer—Host
A is blocked and can transmit no more data! To solve this problem, the TCP specification
requires Host A to continue to send segments with one data byte when B’s
receive window is zero. These segments will be acknowledged by the receiver.
Eventually the buffer will begin to empty and the acknowledgments will contain a
nonzero rwnd value.
The online site at http://www.awl.com/kurose-ross for this book provides an
interactive Java applet that illustrates the operation of the TCP receive window.
Having described TCP’s flow-control service, we briefly mention here that UDP
does not provide flow control. To understand the issue, consider sending a series of
UDP segments from a process on Host A to a process on Host B. For a typical UDP
implementation, UDP will append the segments in a finite-sized buffer that “precedes”
the corresponding socket (that is, the door to the process). The process reads one entire
segment at a time from the buffer. If the process does not read the segments fast
enough from the buffer, the buffer will overflow and segments will get dropped.
3.5.6 TCP Connection Management
In this subsection we take a closer look at how a TCP connection is established and
torn down. Although this topic may not seem particularly thrilling, it is important
because TCP connection establishment can significantly add to perceived delays
(for example, when surfing the Web). Furthermore, many of the most common network
attacks—including the incredibly popular SYN flood attack—exploit vulnerabilities
in TCP connection management. Let’s first take a look at how a TCP
connection is established. Suppose a process running in one host (client) wants to
initiate a connection with another process in another host (server). The client application
process first informs the client TCP that it wants to establish a connection to
a process in the server. The TCP in the client then proceeds to establish a TCP connection
with the TCP in the server in the following manner:
• Step 1. The client-side TCP first sends a special TCP segment to the server-side
TCP. This special segment contains no application-layer data. But one of the flag
bits in the segment’s header (see Figure 3.29), the SYN bit, is set to 1. For this
reason, this special segment is referred to as a SYN segment. In addition, the
client randomly chooses an initial sequence number (client_isn) and puts
this number in the sequence number field of the initial TCP SYN segment. This
segment is encapsulated within an IP datagram and sent to the server. There has
252 CHAPTER 3 • TRANSPORT LAYER
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 253
been considerable interest in properly randomizing the choice of the
client_isn in order to avoid certain security attacks [CERT 2001–09].
• Step 2. Once the IP datagram containing the TCP SYN segment arrives at the
server host (assuming it does arrive!), the server extracts the TCP SYN segment
from the datagram, allocates the TCP buffers and variables to the connection, and
sends a connection-granted segment to the client TCP. (We’ll see in Chapter 8 that
the allocation of these buffers and variables before completing the third step of the
three-way handshake makes TCP vulnerable to a denial-of-service attack known
as SYN flooding.) This connection-granted segment also contains no applicationlayer
data. However, it does contain three important pieces of information in the
segment header. First, the SYN bit is set to 1. Second, the acknowledgment field
of the TCP segment header is set to client_isn+1. Finally, the server
chooses its own initial sequence number (server_isn) and puts this value in
the sequence number field of the TCP segment header. This connection-granted
segment is saying, in effect, “I received your SYN packet to start a connection
with your initial sequence number, client_isn. I agree to establish this connection.
My own initial sequence number is server_isn.” The connectiongranted
segment is referred to as a SYNACK segment.
• Step 3. Upon receiving the SYNACK segment, the client also allocates buffers
and variables to the connection. The client host then sends the server yet another
segment; this last segment acknowledges the server’s connection-granted segment
(the client does so by putting the value server_isn+1 in the acknowledgment
field of the TCP segment header). The SYN bit is set to zero, since the
connection is established. This third stage of the three-way handshake may carry
client-to-server data in the segment payload.
Once these three steps have been completed, the client and server hosts can send
segments containing data to each other. In each of these future segments, the SYN bit
will be set to zero. Note that in order to establish the connection, three packets are sent
between the two hosts, as illustrated in Figure 3.39. For this reason, this connectionestablishment
procedure is often referred to as a three-way handshake. Several
aspects of the TCP three-way handshake are explored in the homework problems
(Why are initial sequence numbers needed? Why is a three-way handshake, as
opposed to a two-way handshake, needed?). It’s interesting to note that a rock climber
and a belayer (who is stationed below the rock climber and whose job it is to handle
the climber’s safety rope) use a three-way-handshake communication protocol that is
identical to TCP’s to ensure that both sides are ready before the climber begins ascent.
All good things must come to an end, and the same is true with a TCP connection.
Either of the two processes participating in a TCP connection can end the connection.
When a connection ends, the “resources” (that is, the buffers and variables)
in the hosts are deallocated. As an example, suppose the client decides to close the
connection, as shown in Figure 3.40. The client application process issues a close
254 CHAPTER 3 • TRANSPORT LAYER
command. This causes the client TCP to send a special TCP segment to the server
process. This special segment has a flag bit in the segment’s header, the FIN bit
(see Figure 3.29), set to 1. When the server receives this segment, it sends the client
an acknowledgment segment in return. The server then sends its own shutdown
segment, which has the FIN bit set to 1. Finally, the client acknowledges the
server’s shutdown segment. At this point, all the resources in the two hosts are now
deallocated.
During the life of a TCP connection, the TCP protocol running in each host
makes transitions through various TCP states. Figure 3.41 illustrates a typical
sequence of TCP states that are visited by the client TCP. The client TCP begins in
the CLOSED state. The application on the client side initiates a new TCP connection
(by creating a Socket object in our Java examples as in the Python examples
from Chapter 2). This causes TCP in the client to send a SYN segment to TCP in the
server. After having sent the SYN segment, the client TCP enters the SYN_SENT
state. While in the SYN_SENT state, the client TCP waits for a segment from the
server TCP that includes an acknowledgment for the client’s previous segment and
has the SYN bit set to 1. Having received such a segment, the client TCP enters the
ESTABLISHED state. While in the ESTABLISHED state, the TCP client can send
and receive TCP segments containing payload (that is, application-generated) data.
Time Time
Client host
Connection
request
Connection
granted
Server host
SYN=1, seq=client_isn
SYN=1, seq=server_isn,
ack=client_isn+1
SYN=0, seq=client_isn+1,
ack=server_isn+1 ACK
Figure 3.39  TCP three-way handshake: segment exchange
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 255
Suppose that the client application decides it wants to close the connection.
(Note that the server could also choose to close the connection.) This causes the
client TCP to send a TCP segment with the FIN bit set to 1 and to enter the
FIN_WAIT_1 state. While in the FIN_WAIT_1 state, the client TCP waits for a TCP
segment from the server with an acknowledgment. When it receives this segment,
the client TCP enters the FIN_WAIT_2 state. While in the FIN_WAIT_2 state, the
client waits for another segment from the server with the FIN bit set to 1; after
receiving this segment, the client TCP acknowledges the server’s segment and
enters the TIME_WAIT state. The TIME_WAIT state lets the TCP client resend the
final acknowledgment in case the ACK is lost. The time spent in the TIME_WAIT
state is implementation-dependent, but typical values are 30 seconds, 1 minute, and
2 minutes. After the wait, the connection formally closes and all resources on the
client side (including port numbers) are released.
Figure 3.42 illustrates the series of states typically visited by the server-side
TCP, assuming the client begins connection teardown. The transitions are selfexplanatory.
In these two state-transition diagrams, we have only shown how a TCP
connection is normally established and shut down. We have not described what
Time Time
Client
Close
Close
Server
FIN
ACK
ACK
FIN
Closed
Timed wait
Figure 3.40  Closing a TCP connection
256 CHAPTER 3 • TRANSPORT LAYER
CLOSED
SYN_SENT
ESTABLISHED
FIN_WAIT_1
FIN_WAIT_2
TIME_WAIT
Send SYN
Send FIN
Receive ACK,
send nothing
Wait 30 seconds
Receive FIN,
send ACK
Receive SYN & ACK,
send ACK
Client application
initiates a TCP connection
Client application
initiates close connection
Figure 3.41  A typical sequence of TCP states visited by a client TCP
CLOSED
LISTEN
SYN_RCVD
ESTABLISHED
CLOSE_WAIT
LAST_ACK
Receive FIN,
send ACK
Receive ACK,
send nothing
Send FIN
Receive SYN
send SYN & ACK
Server application
creates a listen socket
Receive ACK,
send nothing
Figure 3.42  A typical sequence of TCP states visited by a server-side TCP
3.5 • CONNECTION-ORIENTED TRANSPORT: TCP 257
THE SYN FLOOD ATTACK
We’ve seen in our discussion of TCP’s three-way handshake that a server allocates
and initializes connection variables and buffers in response to a received SYN. The
server then sends a SYNACK in response, and awaits an ACK segment from the
client. If the client does not send an ACK to complete the third step of this 3-way
handshake, eventually (often after a minute or more) the server will terminate the halfopen
connection and reclaim the allocated resources.
This TCP connection management protocol sets the stage for a classic Denial of
Service (DoS) attack known as the SYN flood attack. In this attack, the attacker(s)
send a large number of TCP SYN segments, without completing the third handshake
step. With this deluge of SYN segments, the server’s connection resources become
exhausted as they are allocated (but never used!) for half-open connections; legitimate
clients are then denied service. Such SYN flooding attacks were among the first
documented DoS attacks [CERT SYN 1996]. Fortunately, an effective defense known
as SYN cookies [RFC 4987] are now deployed in most major operating systems.
SYN cookies work as follows:
o When the server receives a SYN segment, it does not know if the segment is coming
from a legitimate user or is part of a SYN flood attack. So, instead of creating
a half-open TCP connection for this SYN, the server creates an initial TCP
sequence number that is a complicated function (hash function) of source and destination
IP addresses and port numbers of the SYN segment, as well as a secret
number only known to the server. This carefully crafted initial sequence number is
the so-called “cookie.” The server then sends the client a SYNACK packet with this
special initial sequence number. Importantly, the server does not remember the
cookie or any other state information corresponding to the SYN.
o A legitimate client will return an ACK segment. When the server receives this
ACK, it must verify that the ACK corresponds to some SYN sent earlier. But how is
this done if the server maintains no memory about SYN segments? As you may
have guessed, it is done with the cookie. Recall that for a legitimate ACK, the
value in the acknowledgment field is equal to the initial sequence number in the
SYNACK (the cookie value in this case) plus one (see Figure 3.39). The server can
then run the same hash function using the source and destination IP address and
port numbers in the SYNACK (which are the same as in the original SYN) and the
secret number. If the result of the function plus one is the same as the acknowledgment
(cookie) value in the client’s SYNACK, the server concludes that the ACK corresponds
to an earlier SYN segment and is hence valid. The server then creates a
fully open connection along with a socket.
o On the other hand, if the client does not return an ACK segment, then the original
SYN has done no harm at the server, since the server hasn’t yet allocated any
resources in response to the original bogus SYN.
FOCUS ON SECURITY
258 CHAPTER 3 • TRANSPORT LAYER
happens in certain pathological scenarios, for example, when both sides of a connection
want to initiate or shut down at the same time. If you are interested in learning
about this and other advanced issues concerning TCP, you are encouraged to see
Stevens’ comprehensive book [Stevens 1994].
Our discussion above has assumed that both the client and server are prepared
to communicate, i.e., that the server is listening on the port to which the client sends
its SYN segment. Let’s consider what happens when a host receives a TCP segment
whose port numbers or source IP address do not match with any of the ongoing
sockets in the host. For example, suppose a host receives a TCP SYN packet with
destination port 80, but the host is not accepting connections on port 80 (that is, it is
not running a Web server on port 80). Then the host will send a special reset segment
to the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to
1. Thus, when a host sends a reset segment, it is telling the source “I don’t have a
socket for that segment. Please do not resend the segment.” When a host receives a
UDP packet whose destination port number doesn’t match with an ongoing UDP
socket, the host sends a special ICMP datagram, as discussed in Chapter 4.
Now that we have a good understanding of TCP connection management, let’s
revisit the nmap port-scanning tool and examine more closely how it works. To explore
a specific TCP port, say port 6789, on a target host, nmap will send a TCP SYN segment
with destination port 6789 to that host. There are three possible outcomes:
• The source host receives a TCP SYNACK segment from the target host. Since this
means that an application is running with TCP port 6789 on the target post, nmap
returns “open.”
• The source host receives a TCP RST segment from the target host. This means that
the SYN segment reached the target host, but the target host is not running an application
with TCP port 6789. But the attacker at least knows that the segments destined
to the host at port 6789 are not blocked by any firewall on the path between
source and target hosts. (Firewalls are discussed in Chapter 8.)
• The source receives nothing. This likely means that the SYN segment was blocked
by an intervening firewall and never reached the target host.
Nmap is a powerful tool, which can “case the joint” not only for open TCP ports,
but also for open UDP ports, for firewalls and their configurations, and even for the versions
of applications and operating systems. Most of this is done by manipulating TCP
connection-management segments [Skoudis 2006]. You can download nmap from
www.nmap.org.
This completes our introduction to error control and flow control in TCP. In
Section 3.7 we’ll return to TCP and look at TCP congestion control in some depth.
Before doing so, however, we first step back and examine congestion-control issues
in a broader context.
3.6 • PRINCIPLES OF CONGESTION CONTROL 259
3.6 Principles of Congestion Control
In the previous sections, we examined both the general principles and specific
TCP mechanisms used to provide for a reliable data transfer service in the face of
packet loss. We mentioned earlier that, in practice, such loss typically results from
the overflowing of router buffers as the network becomes congested. Packet
retransmission thus treats a symptom of network congestion (the loss of a specific
transport-layer segment) but does not treat the cause of network congestion—too
many sources attempting to send data at too high a rate. To treat the cause of network
congestion, mechanisms are needed to throttle senders in the face of network
congestion.
In this section, we consider the problem of congestion control in a general context,
seeking to understand why congestion is a bad thing, how network congestion
is manifested in the performance received by upper-layer applications, and various
approaches that can be taken to avoid, or react to, network congestion. This more
general study of congestion control is appropriate since, as with reliable data transfer,
it is high on our “top-ten” list of fundamentally important problems in networking.
We conclude this section with a discussion of congestion control in the
available bit-rate (ABR) service in asynchronous transfer mode (ATM)
networks. The following section contains a detailed study of TCP’s congestioncontrol
algorithm.
3.6.1 The Causes and the Costs of Congestion
Let’s begin our general study of congestion control by examining three increasingly
complex scenarios in which congestion occurs. In each case, we’ll look at why congestion
occurs in the first place and at the cost of congestion (in terms of resources
not fully utilized and poor performance received by the end systems). We’ll not (yet)
focus on how to react to, or avoid, congestion but rather focus on the simpler issue
of understanding what happens as hosts increase their transmission rate and the network
becomes congested.
Scenario 1: Two Senders, a Router with Infinite Buffers
We begin by considering perhaps the simplest congestion scenario possible: Two
hosts (A and B) each have a connection that shares a single hop between source and
destination, as shown in Figure 3.43.
Let’s assume that the application in Host A is sending data into the connection
(for example, passing data to the transport-level protocol via a socket) at an average
rate of in bytes/sec. These data are original in the sense that each unit of data
is sent into the socket only once. The underlying transport-level protocol is a
260 CHAPTER 3 • TRANSPORT LAYER
simple one. Data is encapsulated and sent; no error recovery (for example, retransmission),
flow control, or congestion control is performed. Ignoring the additional
overhead due to adding transport- and lower-layer header information, the rate at
which Host A offers traffic to the router in this first scenario is thus in bytes/sec.
Host B operates in a similar manner, and we assume for simplicity that it too is
sending at a rate of in bytes/sec. Packets from Hosts A and B pass through a
router and over a shared outgoing link of capacity R. The router has buffers that
allow it to store incoming packets when the packet-arrival rate exceeds the outgoing
link’s capacity. In this first scenario, we assume that the router has an infinite
amount of buffer space.
Figure 3.44 plots the performance of Host A’s connection under this first
scenario. The left graph plots the per-connection throughput (number of bytes per
second at the receiver) as a function of the connection-sending rate. For a sending
rate between 0 and R/2, the throughput at the receiver equals the sender’s sending
rate—everything sent by the sender is received at the receiver with a finite delay.
When the sending rate is above R/2, however, the throughput is only R/2. This upper
limit on throughput is a consequence of the sharing of link capacity between two
connections. The link simply cannot deliver packets to a receiver at a steady-state
rate that exceeds R/2. No matter how high Hosts A and B set their sending rates,
they will each never see a throughput higher than R/2.
Achieving a per-connection throughput of R/2 might actually appear to be a
good thing, because the link is fully utilized in delivering packets to their destinations.
The right-hand graph in Figure 3.44, however, shows the consequence of
operating near link capacity. As the sending rate approaches R/2 (from the left), the
average delay becomes larger and larger. When the sending rate exceeds R/2, the
Host B
Unlimited shared
output link buffers
?in: original data
Host A Host C Host D
?out
Figure 3.43  Congestion scenario 1: Two connections sharing a single
hop with infinite buffers
3.6 • PRINCIPLES OF CONGESTION CONTROL 261
average number of queued packets in the router is unbounded, and the average delay
between source and destination becomes infinite (assuming that the connections
operate at these sending rates for an infinite period of time and there is an infinite
amount of buffering available). Thus, while operating at an aggregate throughput of
near R may be ideal from a throughput standpoint, it is far from ideal from a delay
standpoint. Even in this (extremely) idealized scenario, we’ve already found one
cost of a congested network—large queuing delays are experienced as the packetarrival
rate nears the link capacity.
Scenario 2: Two Senders and a Router with Finite Buffers
Let us now slightly modify scenario 1 in the following two ways (see Figure 3.45).
First, the amount of router buffering is assumed to be finite. A consequence of this
real-world assumption is that packets will be dropped when arriving to an alreadyfull
buffer. Second, we assume that each connection is reliable. If a packet containing
a transport-level segment is dropped at the router, the sender will eventually
retransmit it. Because packets can be retransmitted, we must now be more careful
with our use of the term sending rate. Specifically, let us again denote the rate at
which the application sends original data into the socket by in bytes/sec. The rate at
which the transport layer sends segments (containing original data and retransmitted
data) into the network will be denoted in bytes/sec. in is sometimes referred to
as the offered load to the network.
The performance realized under scenario 2 will now depend strongly on
how retransmission is performed. First, consider the unrealistic case that Host A is
able to somehow (magically!) determine whether or not a buffer is free in the router
and thus sends a packet only when a buffer is free. In this case, no loss would occur,
R/2
R/2
Delay
R/2
?in ?in
?out
a. b.
Figure 3.44  Congestion scenario 1: Throughput and delay as a function
of host sending rate
262 CHAPTER 3 • TRANSPORT LAYER
in would be equal to in, and the throughput of the connection would be equal to
in. This case is shown in Figure 3.46(a). From a throughput standpoint, performance
is ideal—everything that is sent is received. Note that the average host sending
rate cannot exceed R/2 under this scenario, since packet loss is assumed never
to occur.
Consider next the slightly more realistic case that the sender retransmits only
when a packet is known for certain to be lost. (Again, this assumption is a bit of a
stretch. However, it is possible that the sending host might set its timeout large
enough to be virtually assured that a packet that has not been acknowledged has
been lost.) In this case, the performance might look something like that shown in
Figure 3.46(b). To appreciate what is happening here, consider the case that the
offered load, in (the rate of original data transmission plus retransmissions), equals
R/2. According to Figure 3.46(b), at this value of the offered load, the rate at which
data are delivered to the receiver application is R/3. Thus, out of the 0.5R units of
data transmitted, 0.333R bytes/sec (on average) are original data and 0.166R bytes/
sec (on average) are retransmitted data. We see here another cost of a congested network—
the sender must perform retransmissions in order to compensate for dropped
(lost) packets due to buffer overflow.
Finally, let us consider the case that the sender may time out prematurely and
retransmit a packet that has been delayed in the queue but not yet lost. In this case,
both the original data packet and the retransmission may reach the receiver. Of
Finite shared output
link buffers
Host A Host B Host C Host D
?out
?in: original data
?’in: original data, plus
retransmitted data
Figure 3.45  Scenario 2: Two hosts (with retransmissions) and a router
with finite buffers
3.6 • PRINCIPLES OF CONGESTION CONTROL 263
course, the receiver needs but one copy of this packet and will discard the retransmission.
In this case, the work done by the router in forwarding the retransmitted
copy of the original packet was wasted, as the receiver will have already received
the original copy of this packet. The router would have better used the link transmission
capacity to send a different packet instead. Here then is yet another cost of
a congested network—unneeded retransmissions by the sender in the face of large
delays may cause a router to use its link bandwidth to forward unneeded copies of a
packet. Figure 3.46 (c) shows the throughput versus offered load when each packet
is assumed to be forwarded (on average) twice by the router. Since each packet is
forwarded twice, the throughput will have an asymptotic value of R/4 as the offered
load approaches R/2.
Scenario 3: Four Senders, Routers with Finite Buffers, and
Multihop Paths
In our final congestion scenario, four hosts transmit packets, each over overlapping
two-hop paths, as shown in Figure 3.47. We again assume that each host
uses a timeout/retransmission mechanism to implement a reliable data transfer
service, that all hosts have the same value of in, and that all router links have
capacity R bytes/sec.
Let’s consider the connection from Host A to Host C, passing through routers
R1 and R2. The A–C connection shares router R1 with the D–B connection and
shares router R2 with the B–D connection. For extremely small values of in, buffer
overflows are rare (as in congestion scenarios 1 and 2), and the throughput approximately
equals the offered load. For slightly larger values of in, the corresponding
throughput is also larger, since more original data is being transmitted into the
R/2
R/2 R/2
?out
a. b.
R/2
?out
R/3
R/2
R/2
?out
R/4
c.
?’in ?’in ?’in
Figure 3.46  Scenario 2 performance with finite buffers
264 CHAPTER 3 • TRANSPORT LAYER
network and delivered to the destination, and overflows are still rare. Thus, for small
values of in, an increase in in results in an increase in out.
Having considered the case of extremely low traffic, let’s next examine the
case that in (and hence in) is extremely large. Consider router R2. The A–C
traffic arriving to router R2 (which arrives at R2 after being forwarded from R1)
can have an arrival rate at R2 that is at most R, the capacity of the link from R1
to R2, regardless of the value of in. If in is extremely large for all connections
(including the B–D connection), then the arrival rate of B–D traffic at R2 can be
much larger than that of the A–C traffic. Because the A–C and B–D traffic must
compete at router R2 for the limited amount of buffer space, the amount of A–C
traffic that successfully gets through R2 (that is, is not lost due to buffer overflow)
becomes smaller and smaller as the offered load from B–D gets larger and
larger. In the limit, as the offered load approaches infinity, an empty buffer at R2
Host A Host B
R1
R4 R2
R3
Host D Host C
Finite shared output
link buffers
?in: original data
?’in : original
data, plus
retransmitted
data
?out
Figure 3.47  Four senders, routers with finite buffers, and multihop paths
3.6 • PRINCIPLES OF CONGESTION CONTROL 265
is immediately filled by a B–D packet, and the throughput of the A–C connection
at R2 goes to zero. This, in turn, implies that the A–C end-to-end throughput goes
to zero in the limit of heavy traffic. These considerations give rise to the offered
load versus throughput tradeoff shown in Figure 3.48.
The reason for the eventual decrease in throughput with increasing offered
load is evident when one considers the amount of wasted work done by the network.
In the high-traffic scenario outlined above, whenever a packet is dropped
at a second-hop router, the work done by the first-hop router in forwarding a
packet to the second-hop router ends up being “wasted.” The network would
have been equally well off (more accurately, equally bad off) if the first router
had simply discarded that packet and remained idle. More to the point, the transmission
capacity used at the first router to forward the packet to the second router
could have been much more profitably used to transmit a different packet. (For
example, when selecting a packet for transmission, it might be better for a router
to give priority to packets that have already traversed some number of upstream
routers.) So here we see yet another cost of dropping a packet due to congestion—
when a packet is dropped along a path, the transmission capacity that was
used at each of the upstream links to forward that packet to the point at which it
is dropped ends up having been wasted.
3.6.2 Approaches to Congestion Control
In Section 3.7, we’ll examine TCP’s specific approach to congestion control in great
detail. Here, we identify the two broad approaches to congestion control that are
taken in practice and discuss specific network architectures and congestion-control
protocols embodying these approaches.
R/2
?out
?’
in
Figure 3.48  Scenario 3 performance with finite buffers and multihop
paths
266 CHAPTER 3 • TRANSPORT LAYER
At the broadest level, we can distinguish among congestion-control approaches
by whether the network layer provides any explicit assistance to the transport layer
for congestion-control purposes:
• End-to-end congestion control. In an end-to-end approach to congestion control,
the network layer provides no explicit support to the transport layer for congestioncontrol
purposes. Even the presence of congestion in the network must be inferred
by the end systems based only on observed network behavior (for example, packet
loss and delay). We will see in Section 3.7 that TCP must necessarily take this endto-
end approach toward congestion control, since the IP layer provides no feedback
to the end systems regarding network congestion. TCP segment loss (as indicated
by a timeout or a triple duplicate acknowledgment) is taken as an indication of network
congestion and TCP decreases its window size accordingly. We will also see
a more recent proposal for TCP congestion control that uses increasing round-trip
delay values as indicators of increased network congestion.
• Network-assisted congestion control. With network-assisted congestion control,
network-layer components (that is, routers) provide explicit feedback to the
sender regarding the congestion state in the network. This feedback may be as
simple as a single bit indicating congestion at a link. This approach was taken in
the early IBM SNA [Schwartz 1982] and DEC DECnet [Jain 1989; Ramakrishnan
1990] architectures, was recently proposed for TCP/IP networks [Floyd TCP
1994; RFC 3168], and is used in ATM available bit-rate (ABR) congestion control
as well, as discussed below. More sophisticated network feedback is also possible.
For example, one form of ATM ABR congestion control that we will study
shortly allows a router to inform the sender explicitly of the transmission rate it
(the router) can support on an outgoing link. The XCP protocol [Katabi 2002] provides
router-computed feedback to each source, carried in the packet header,
regarding how that source should increase or decrease its transmission rate.
For network-assisted congestion control, congestion information is typically fed
back from the network to the sender in one of two ways, as shown in Figure 3.49.
Direct feedback may be sent from a network router to the sender. This form of notification
typically takes the form of a choke packet (essentially saying, “I’m congested!”).
The second form of notification occurs when a router marks/updates a field
in a packet flowing from sender to receiver to indicate congestion. Upon receipt of a
marked packet, the receiver then notifies the sender of the congestion indication.
Note that this latter form of notification takes at least a full round-trip time.
3.6.3 Network-Assisted Congestion-Control Example:
ATM ABR Congestion Control
We conclude this section with a brief case study of the congestion-control algorithm
in ATM ABR—a protocol that takes a network-assisted approach toward congestion
control. We stress that our goal here is not to describe aspects of the ATM architecture
in great detail, but rather to illustrate a protocol that takes a markedly different
approach toward congestion control from that of the Internet’s TCP protocol. Indeed,
we only present below those few aspects of the ATM architecture that are needed to
understand ABR congestion control.
Fundamentally ATM takes a virtual-circuit (VC) oriented approach toward
packet switching. Recall from our discussion in Chapter 1, this means that each
switch on the source-to-destination path will maintain state about the source-todestination
VC. This per-VC state allows a switch to track the behavior of individual
senders (e.g., tracking their average transmission rate) and to take
source-specific congestion-control actions (such as explicitly signaling to the
sender to reduce its rate when the switch becomes congested). This per-VC state
at network switches makes ATM ideally suited to perform network-assisted congestion
control.
ABR has been designed as an elastic data transfer service in a manner reminiscent
of TCP. When the network is underloaded, ABR service should be able to take
advantage of the spare available bandwidth; when the network is congested, ABR
service should throttle its transmission rate to some predetermined minimum transmission
rate. A detailed tutorial on ATM ABR congestion control and traffic management
is provided in [Jain 1996].
Figure 3.50 shows the framework for ATM ABR congestion control. In our
discussion we adopt ATM terminology (for example, using the term switch rather
than router, and the term cell rather than packet). With ATM ABR service, data
cells are transmitted from a source to a destination through a series of intermediate
switches. Interspersed with the data cells are resource-management cells
3.6 • PRINCIPLES OF CONGESTION CONTROL 267
Host A
Network feedback via receiver
Direct network
feedback
Host B
Figure 3.49  Two feedback pathways for network-indicated congestion
information
268 CHAPTER 3 • TRANSPORT LAYER
(RM cells); these RM cells can be used to convey congestion-related information
among the hosts and switches. When an RM cell arrives at a destination, it will
be turned around and sent back to the sender (possibly after the destination has
modified the contents of the RM cell). It is also possible for a switch to generate
an RM cell itself and send this RM cell directly to a source. RM cells can thus be
used to provide both direct network feedback and network feedback via the
receiver, as shown in Figure 3.50.
ATM ABR congestion control is a rate-based approach. That is, the sender
explicitly computes a maximum rate at which it can send and regulates itself accordingly.
ABR provides three mechanisms for signaling congestion-related information
from the switches to the receiver:
• EFCI bit. Each data cell contains an explicit forward congestion indication
(EFCI) bit. A congested network switch can set the EFCI bit in a data cell to
1 to signal congestion to the destination host. The destination must check the
EFCI bit in all received data cells. When an RM cell arrives at the destination,
if the most recently received data cell had the EFCI bit set to 1, then the destination
sets the congestion indication bit (the CI bit) of the RM cell to 1 and
sends the RM cell back to the sender. Using the EFCI in data cells and the CI
bit in RM cells, a sender can thus be notified about congestion at a network
switch.
• CI and NI bits. As noted above, sender-to-receiver RM cells are interspersed
with data cells. The rate of RM cell interspersion is a tunable parameter, with
the default value being one RM cell every 32 data cells. These RM cells have a
congestion indication (CI) bit and a no increase (NI) bit that can be set by a
Source Destination
Switch Switch
Key:
RM cells Data cells
Figure 3.50  Congestion-control framework for ATM ABR service
congested network switch. Specifically, a switch can set the NI bit in a passing
RM cell to 1 under mild congestion and can set the CI bit to 1 under severe
congestion conditions. When a destination host receives an RM cell, it will
send the RM cell back to the sender with its CI and NI bits intact (except that
CI may be set to 1 by the destination as a result of the EFCI mechanism
described above).
• ER setting. Each RM cell also contains a 2-byte explicit rate (ER) field. A congested
switch may lower the value contained in the ER field in a passing RM
cell. In this manner, the ER field will be set to the minimum supportable rate of
all switches on the source-to-destination path.
An ATM ABR source adjusts the rate at which it can send cells as a function of
the CI, NI, and ER values in a returned RM cell. The rules for making this rate
adjustment are rather complicated and a bit tedious. The interested reader is referred
to [Jain 1996] for details.
3.7 TCP Congestion Control
In this section we return to our study of TCP. As we learned in Section 3.5, TCP provides
a reliable transport service between two processes running on different hosts.
Another key component of TCP is its congestion-control mechanism. As indicated
in the previous section, TCP must use end-to-end congestion control rather than network-
assisted congestion control, since the IP layer provides no explicit feedback to
the end systems regarding network congestion.
The approach taken by TCP is to have each sender limit the rate at which it
sends traffic into its connection as a function of perceived network congestion. If a
TCP sender perceives that there is little congestion on the path between itself and
the destination, then the TCP sender increases its send rate; if the sender perceives
that there is congestion along the path, then the sender reduces its send rate. But this
approach raises three questions. First, how does a TCP sender limit the rate at which
it sends traffic into its connection? Second, how does a TCP sender perceive that
there is congestion on the path between itself and the destination? And third, what
algorithm should the sender use to change its send rate as a function of perceived
end-to-end congestion?
Let’s first examine how a TCP sender limits the rate at which it sends traffic
into its connection. In Section 3.5 we saw that each side of a TCP connection consists
of a receive buffer, a send buffer, and several variables (LastByteRead, rwnd,
and so on). The TCP congestion-control mechanism operating at the sender keeps
track of an additional variable, the congestion window. The congestion window,
denoted cwnd, imposes a constraint on the rate at which a TCP sender can send traffic
3.7 • TCP CONGESTION CONTROL 269
270 CHAPTER 3 • TRANSPORT LAYER
into the network. Specifically, the amount of unacknowledged data at a sender may
not exceed the minimum of cwnd and rwnd, that is:
LastByteSent – LastByteAcked  min{cwnd, rwnd}
In order to focus on congestion control (as opposed to flow control), let us henceforth
assume that the TCP receive buffer is so large that the receive-window constraint
can be ignored; thus, the amount of unacknowledged data at the sender is
solely limited by cwnd. We will also assume that the sender always has data to
send, i.e., that all segments in the congestion window are sent.
The constraint above limits the amount of unacknowledged data at the sender
and therefore indirectly limits the sender’s send rate. To see this, consider a connection
for which loss and packet transmission delays are negligible. Then, roughly, at
the beginning of every RTT, the constraint permits the sender to send cwnd bytes of
data into the connection; at the end of the RTT the sender receives acknowledgments
for the data. Thus the sender’s send rate is roughly cwnd/RTT bytes/sec. By
adjusting the value of cwnd, the sender can therefore adjust the rate at which it
sends data into its connection.
Let’s next consider how a TCP sender perceives that there is congestion on
the path between itself and the destination. Let us define a “loss event” at a TCP
sender as the occurrence of either a timeout or the receipt of three duplicate
ACKs from the receiver. (Recall our discussion in Section 3.5.4 of the timeout
event in Figure 3.33 and the subsequent modification to include fast retransmit
on receipt of three duplicate ACKs.) When there is excessive congestion, then
one (or more) router buffers along the path overflows, causing a datagram (containing
a TCP segment) to be dropped. The dropped datagram, in turn, results in
a loss event at the sender—either a timeout or the receipt of three duplicate
ACKs—which is taken by the sender to be an indication of congestion on the
sender-to-receiver path.
Having considered how congestion is detected, let’s next consider the more
optimistic case when the network is congestion-free, that is, when a loss event
doesn’t occur. In this case, acknowledgments for previously unacknowledged
segments will be received at the TCP sender. As we’ll see, TCP will take the
arrival of these acknowledgments as an indication that all is well—that segments
being transmitted into the network are being successfully delivered to the
destination—and will use acknowledgments to increase its congestion window
size (and hence its transmission rate). Note that if acknowledgments arrive at a
relatively slow rate (e.g., if the end-end path has high delay or contains a
low-bandwidth link), then the congestion window will be increased at a relatively
slow rate. On the other hand, if acknowledgments arrive at a high rate, then the
congestion window will be increased more quickly. Because TCP uses
acknowledgments to trigger (or clock) its increase in congestion window size,
TCP is said to be self-clocking.
Given the mechanism of adjusting the value of cwnd to control the sending rate,
the critical question remains: How should a TCP sender determine the rate at which
it should send? If TCP senders collectively send too fast, they can congest the network,
leading to the type of congestion collapse that we saw in Figure 3.48. Indeed,
the version of TCP that we’ll study shortly was developed in response to observed
Internet congestion collapse [Jacobson 1988] under earlier versions of TCP. However,
if TCP senders are too cautious and send too slowly, they could under utilize
the bandwidth in the network; that is, the TCP senders could send at a higher rate
without congesting the network. How then do the TCP senders determine their sending
rates such that they don’t congest the network but at the same time make use of
all the available bandwidth? Are TCP senders explicitly coordinated, or is there a
distributed approach in which the TCP senders can set their sending rates based only
on local information? TCP answers these questions using the following guiding
principles:
• A lost segment implies congestion, and hence, the TCP sender’s rate should be
decreased when a segment is lost. Recall from our discussion in Section 3.5.4,
that a timeout event or the receipt of four acknowledgments for a given segment
(one original ACK and then three duplicate ACKs) is interpreted as an
implicit “loss event” indication of the segment following the quadruply ACKed
segment, triggering a retransmission of the lost segment. From a congestioncontrol
standpoint, the question is how the TCP sender should decrease its congestion
window size, and hence its sending rate, in response to this inferred
loss event.
• An acknowledged segment indicates that the network is delivering the sender’s
segments to the receiver, and hence, the sender’s rate can be increased when an
ACK arrives for a previously unacknowledged segment. The arrival of acknowledgments
is taken as an implicit indication that all is well—segments are being
successfully delivered from sender to receiver, and the network is thus not congested.
The congestion window size can thus be increased.
• Bandwidth probing. Given ACKs indicating a congestion-free source-to-destination
path and loss events indicating a congested path, TCP’s strategy for adjusting its
transmission rate is to increase its rate in response to arriving ACKs until a loss
event occurs, at which point, the transmission rate is decreased. The TCP sender
thus increases its transmission rate to probe for the rate that at which congestion
onset begins, backs off from that rate, and then to begins probing again to see if
the congestion onset rate has changed. The TCP sender’s behavior is perhaps analogous
to the child who requests (and gets) more and more goodies until finally
he/she is finally told “No!”, backs off a bit, but then begins making requests
3.7 • TCP CONGESTION CONTROL 271
272 CHAPTER 3 • TRANSPORT LAYER
again shortly afterwards. Note that there is no explicit signaling of congestion
state by the network—ACKs and loss events serve as implicit signals—and that
each TCP sender acts on local information asynchronously from other TCP
senders.
Given this overview of TCP congestion control, we’re now in a position to consider
the details of the celebrated TCP congestion-control algorithm, which was first
described in [Jacobson 1988] and is standardized in [RFC 5681]. The algorithm has
three major components: (1) slow start, (2) congestion avoidance, and (3) fast recovery.
Slow start and congestion avoidance are mandatory components of TCP, differing
in how they increase the size of cwnd in response to received ACKs. We’ll see
shortly that slow start increases the size of cwnd more rapidly (despite its name!)
than congestion avoidance. Fast recovery is recommended, but not required, for
TCP senders.
Slow Start
When a TCP connection begins, the value of cwnd is typically initialized to a
small value of 1 MSS [RFC 3390], resulting in an initial sending rate of roughly
MSS/RTT. For example, if MSS = 500 bytes and RTT = 200 msec, the resulting
initial sending rate is only about 20 kbps. Since the available bandwidth to the
TCP sender may be much larger than MSS/RTT, the TCP sender would like to
find the amount of available bandwidth quickly. Thus, in the slow-start state, the
value of cwnd begins at 1 MSS and increases by 1 MSS every time a transmitted
segment is first acknowledged. In the example of Figure 3.51, TCP sends the first
segment into the network and waits for an acknowledgment. When this acknowledgment
arrives, the TCP sender increases the congestion window by one MSS
and sends out two maximum-sized segments. These segments are then acknowledged,
with the sender increasing the congestion window by 1 MSS for each of
the acknowledged segments, giving a congestion window of 4 MSS, and so on.
This process results in a doubling of the sending rate every RTT. Thus, the TCP
send rate starts slow but grows exponentially during the slow start phase.
But when should this exponential growth end? Slow start provides several
answers to this question. First, if there is a loss event (i.e., congestion) indicated
by a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow
start process anew. It also sets the value of a second state variable, ssthresh
(shorthand for “slow start threshold”) to cwnd/2—half of the value of the congestion
window value when congestion was detected. The second way in which
slow start may end is directly tied to the value of ssthresh. Since ssthresh
is half the value of cwnd when congestion was last detected, it might be a bit
reckless to keep doubling cwnd when it reaches or surpasses the value of
ssthresh. Thus, when the value of cwnd equals ssthresh, slow start ends
and TCP transitions into congestion avoidance mode. As we’ll see, TCP increases
3.7 • TCP CONGESTION CONTROL 273
TCP SPLITTING: OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES
For cloud services such as search, e-mail, and social networks, it is desirable to provide a
high-level of responsiveness, ideally giving users the illusion that the services are running
within their own end systems (including their smartphones). This can be a major challenge,
as users are often located far away from the data centers that are responsible for serving
the dynamic content associated with the cloud services. Indeed, if the end system is far
from a data center, then the RTT will be large, potentially leading to poor response time
performance due to TCP slow start.
As a case study, consider the delay in receiving a response for a search query.
Typically, the server requires three TCP windows during slow start to deliver the
response [Pathak 2010]. Thus the time from when an end system initiates a TCP connection
until the time when it receives the last packet of the response is roughly 4 RTT
(one RTT to set up the TCP connection plus three RTTs for the three windows of data)
plus the processing time in the data center. These RTT delays can lead to a noticeable
delay in returning search results for a significant fraction of queries. Moreover, there
can be significant packet loss in access networks, leading to TCP retransmissions and
even larger delays.
One way to mitigate this problem and improve user-perceived performance is to (1)
deploy front-end servers closer to the users, and (2) utilize TCP splitting by breaking the
TCP connection at the front-end server. With TCP splitting, the client establishes a TCP connection
to the nearby front-end, and the front-end maintains a persistent TCP connection to
the data center with a very large TCP congestion window [Tariq 2008, Pathak 2010,
Chen 2011]. With this approach, the response time roughly becomes 4 RTTFE RTTBE
processing time, where RTTFE is the round-trip time between client and front-end server, and
RTTBE is the round-trip time between the front-end server and the data center (back-end server).
If the front-end server is close to client, then this response time approximately becomes
RTT plus processing time, since RTTFE is negligibly small and RTTBE is approximately RTT. In
summary, TCP splitting can reduce the networking delay roughly from 4 RTT to RTT, significantly
improving user-perceived performance, particularly for users who are far from the
nearest data center. TCP splitting also helps reduce TCP retransmission delays caused by
losses in access networks. Today, Google and Akamai make extensive use of their CDN
servers in access networks (see Section 7.2) to perform TCP splitting for the cloud services
they support [Chen 2011].

 + +

PRINCIPLES IN PRACTICE
cwnd more cautiously when in congestion-avoidance mode. The final way in
which slow start can end is if three duplicate ACKs are detected, in which case
TCP performs a fast retransmit (see Section 3.5.4) and enters the fast recovery
state, as discussed below. TCP’s behavior in slow start is summarized in the FSM
274 CHAPTER 3 • TRANSPORT LAYER
description of TCP congestion control in Figure 3.52. The slow-start algorithm
traces it roots to [Jacobson 1988]; an approach similar to slow start was also proposed
independently in [Jain 1986].
Congestion Avoidance
On entry to the congestion-avoidance state, the value of cwnd is approximately half
its value when congestion was last encountered—congestion could be just around
the corner! Thus, rather than doubling the value of cwnd every RTT, TCP adopts a
more conservative approach and increases the value of cwnd by just a single MSS
every RTT [RFC 5681]. This can be accomplished in several ways. A common
approach is for the TCP sender to increase cwnd by MSS bytes (MSS/cwnd) whenever
a new acknowledgment arrives. For example, if MSS is 1,460 bytes and cwnd
is 14,600 bytes, then 10 segments are being sent within an RTT. Each arriving ACK
(assuming one ACK per segment) increases the congestion window size by 1/10
Host A Host B
one segment
two segments
four segments
RTT
Time Time
Figure 3.51  TCP slow start
3.7 • TCP CONGESTION CONTROL 275
MSS, and thus, the value of the congestion window will have increased by one MSS
after ACKs when all 10 segments have been received.
But when should congestion avoidance’s linear increase (of 1 MSS per RTT)
end? TCP’s congestion-avoidance algorithm behaves the same when a timeout
occurs. As in the case of slow start: The value of cwnd is set to 1 MSS, and the
value of ssthresh is updated to half the value of cwnd when the loss event
occurred. Recall, however, that a loss event also can be triggered by a triple duplicate
ACK event. In this case, the network is continuing to deliver segments from
sender to receiver (as indicated by the receipt of duplicate ACKs). So TCP’s behavior
to this type of loss event should be less drastic than with a timeout-indicated loss:
TCP halves the value of cwnd (adding in 3 MSS for good measure to account for
Slow
start
duplicate ACK
dupACKcount++
duplicate ACK
dupACKcount++
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd=1 MSS
ssthresh=64 KB
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd = ssthresh
Congestion
avoidance
Fast
recovery
new ACK
cwnd=cwnd+MSS •(MSS/cwnd)
dupACKcount=0
transmit new segment(s), as allowed
new ACK
cwnd=cwnd+MSS
dupACKcount=0
transmit new segment(s), as allowed
retransmit missing segment
retransmit missing segment dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3•MSS
retransmit missing segment
duplicate ACK
cwnd=cwnd+MSS
transmit new segment(s), as allowed
dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3•MSS
retransmit missing segment
retransmit missing segment
new ACK
cwnd=ssthresh
dupACKcount=0
?
?
Figure 3.52  FSM description of TCP congestion control
276 CHAPTER 3 • TRANSPORT LAYER
the triple duplicate ACKs received) and records the value of ssthresh to be half
the value of cwnd when the triple duplicate ACKs were received. The fast-recovery
state is then entered.
Fast Recovery
In fast recovery, the value of cwnd is increased by 1 MSS for every duplicate ACK
received for the missing segment that caused TCP to enter the fast-recovery state.
Eventually, when an ACK arrives for the missing segment, TCP enters the
congestion-avoidance state after deflating cwnd. If a timeout event occurs, fast
recovery transitions to the slow-start state after performing the same actions as in
slow start and congestion avoidance: The value of cwnd is set to 1 MSS, and the
value of ssthresh is set to half the value of cwnd when the loss event occurred.
Fast recovery is a recommended, but not required, component of TCP [RFC
5681]. It is interesting that an early version of TCP, known as TCP Tahoe, unconditionally
cut its congestion window to 1 MSS and entered the slow-start phase after
either a timeout-indicated or triple-duplicate-ACK-indicated loss event. The newer
version of TCP, TCP Reno, incorporated fast recovery.
Figure 3.53 illustrates the evolution of TCP’s congestion window for both Reno
and Tahoe. In this figure, the threshold is initially equal to 8 MSS. For the first eight
transmission rounds, Tahoe and Reno take identical actions. The congestion window
climbs exponentially fast during slow start and hits the threshold at the fourth round
of transmission. The congestion window then climbs linearly until a triple duplicate-
ACK event occurs, just after transmission round 8. Note that the congestion window
is 12 • MSS when this loss event occurs. The value of ssthresh is then set to
0
0 1 2 3 4 5 6 7 8
Transmission round
TCP Tahoe
ssthresh
ssthresh
Congestion window
(in segments)
9 10 11 12 13 14 15
2
4
6
8
10
12
14
16
TCP Reno
Figure 3.53  Evolution of TCP’s congestion window (Tahoe and Reno)
VideoNote
Examining the
behavior of TCP
3.7 • TCP CONGESTION CONTROL 277
0.5 • cwnd = 6 • MSS. Under TCP Reno, the congestion window is set to cwnd =
6 • MSS and then grows linearly. Under TCP Tahoe, the congestion window is set to
1 MSS and grows exponentially until it reaches the value of ssthresh, at which
point it grows linearly.
Figure 3.52 presents the complete FSM description of TCP’s congestioncontrol
algorithms—slow start, congestion avoidance, and fast recovery. The figure
also indicates where transmission of new segments or retransmitted segments can
occur. Although it is important to distinguish between TCP error control/retransmission
and TCP congestion control, it’s also important to appreciate how these two
aspects of TCP are inextricably linked.
TCP Congestion Control: Retrospective
Having delved into the details of slow start, congestion avoidance, and fast recovery,
it’s worthwhile to now step back and view the forest from the trees. Ignoring the
initial slow-start period when a connection begins and assuming that losses are indicated
by triple duplicate ACKs rather than timeouts, TCP’s congestion control consists
of linear (additive) increase in cwnd of 1 MSS per RTT and then a halving
(multiplicative decrease) of cwnd on a triple duplicate-ACK event. For this reason,
TCP congestion control is often referred to as an additive-increase, multiplicativedecrease
(AIMD) form of congestion control. AIMD congestion control gives rise
to the “saw tooth” behavior shown in Figure 3.54, which also nicely illustrates our
earlier intuition of TCP “probing” for bandwidth—TCP linearly increases its congestion
window size (and hence its transmission rate) until a triple duplicate-ACK
event occurs. It then decreases its congestion window size by a factor of two but
then again begins increasing it linearly, probing to see if there is additional available
bandwidth.
24 K
16 K
8 K
Time
Congestion window
Figure 3.54  Additive-increase, multiplicative-decrease congestion control
278 CHAPTER 3 • TRANSPORT LAYER
As noted previously, many TCP implementations use the Reno algorithm [Padhye
2001]. Many variations of the Reno algorithm have been proposed [RFC 3782; RFC
2018]. The TCP Vegas algorithm [Brakmo 1995; Ahn 1995] attempts to avoid congestion
while maintaining good throughput. The basic idea of Vegas is to (1) detect congestion
in the routers between source and destination before packet loss occurs, and (2)
lower the rate linearly when this imminent packet loss is detected. Imminent packet loss
is predicted by observing the RTT. The longer the RTT of the packets, the greater the
congestion in the routers. Linux supports a number of congestion-control algorithms
(including TCP Reno and TCP Vegas) and allows a system administrator to configure
which version of TCP will be used. The default version of TCP in Linux version 2.6.18
was set to CUBIC [Ha 2008], a version of TCP developed for high-bandwidth applications.
For a recent survey of the many flavors of TCP, see [Afanasyev 2010].
TCP’s AIMD algorithm was developed based on a tremendous amount of engineering
insight and experimentation with congestion control in operational networks.
Ten years after TCP’s development, theoretical analyses showed that TCP’s
congestion-control algorithm serves as a distributed asynchronous-optimization
algorithm that results in several important aspects of user and network performance
being simultaneously optimized [Kelly 1998]. A rich theory of congestion control
has since been developed [Srikant 2004].
Macroscopic Description of TCP Throughput
Given the saw-toothed behavior of TCP, it’s natural to consider what the average
throughput (that is, the average rate) of a long-lived TCP connection might be. In this
analysis we’ll ignore the slow-start phases that occur after timeout events. (These
phases are typically very short, since the sender grows out of the phase exponentially
fast.) During a particular round-trip interval, the rate at which TCP sends data is a
function of the congestion window and the current RTT. When the window size is w
bytes and the current round-trip time is RTT seconds, then TCP’s transmission rate is
roughly w/RTT. TCP then probes for additional bandwidth by increasing w by 1 MSS
each RTT until a loss event occurs. Denote by W the value of w when a loss event
occurs. Assuming that RTT and W are approximately constant over the duration of
the connection, the TCP transmission rate ranges from W/(2 · RTT) to W/RTT.
These assumptions lead to a highly simplified macroscopic model for the
steady-state behavior of TCP. The network drops a packet from the connection when
the rate increases to W/RTT; the rate is then cut in half and then increases by
MSS/RTT every RTT until it again reaches W/RTT. This process repeats itself over
and over again. Because TCP’s throughput (that is, rate) increases linearly between
the two extreme values, we have
average throughput of a connection =
0.75  W
RTT
3.7 • TCP CONGESTION CONTROL 279
Using this highly idealized model for the steady-state dynamics of TCP, we can
also derive an interesting expression that relates a connection’s loss rate to its available
bandwidth [Mahdavi 1997]. This derivation is outlined in the homework problems.
A more sophisticated model that has been found empirically to agree with
measured data is [Padhye 2000].
TCP Over High-Bandwidth Paths
It is important to realize that TCP congestion control has evolved over the years and
indeed continues to evolve. For a summary of current TCP variants and discussion
of TCP evolution, see [Floyd 2001, RFC 5681, Afanasyev 2010]. What was good
for the Internet when the bulk of the TCP connections carried SMTP, FTP, and Telnet
traffic is not necessarily good for today’s HTTP-dominated Internet or for a
future Internet with services that are still undreamed of.
The need for continued evolution of TCP can be illustrated by considering the
high-speed TCP connections that are needed for grid- and cloud-computing applications.
For example, consider a TCP connection with 1,500-byte segments and a 100
ms RTT, and suppose we want to send data through this connection at 10 Gbps.
Following [RFC 3649], we note that using the TCP throughput formula above, in
order to achieve a 10 Gbps throughput, the average congestion window size would
need to be 83,333 segments. That’s a lot of segments, leading us to be rather concerned
that one of these 83,333 in-flight segments might be lost. What would happen
in the case of a loss? Or, put another way, what fraction of the transmitted segments
could be lost that would allow the TCP congestion-control algorithm specified in Figure
3.52 still to achieve the desired 10 Gbps rate? In the homework questions for this
chapter, you are led through the derivation of a formula relating the throughput of a
TCP connection as a function of the loss rate (L), the round-trip time (RTT), and the
maximum segment size (MSS):
Using this formula, we can see that in order to achieve a throughput of 10 Gbps,
today’s TCP congestion-control algorithm can only tolerate a segment loss probability
of 2 · 10–10 (or equivalently, one loss event for every 5,000,000,000 segments)—
a very low rate. This observation has led a number of researchers to investigate new
versions of TCP that are specifically designed for such high-speed environments;
see [Jin 2004; RFC 3649; Kelly 2003; Ha 2008] for discussions of these efforts.
3.7.1 Fairness
Consider K TCP connections, each with a different end-to-end path, but all passing
through a bottleneck link with transmission rate R bps. (By bottleneck link, we mean
average throughput of a connection =
1.22  MSS
RTT 2L
280 CHAPTER 3 • TRANSPORT LAYER
that for each connection, all the other links along the connection’s path are not congested
and have abundant transmission capacity as compared with the transmission
capacity of the bottleneck link.) Suppose each connection is transferring a large file
and there is no UDP traffic passing through the bottleneck link. A congestion-control
mechanism is said to be fair if the average transmission rate of each connection
is approximately R/K; that is, each connection gets an equal share of the link bandwidth.
Is TCP’s AIMD algorithm fair, particularly given that different TCP connections
may start at different times and thus may have different window sizes at a
given point in time? [Chiu 1989] provides an elegant and intuitive explanation of
why TCP congestion control converges to provide an equal share of a bottleneck
link’s bandwidth among competing TCP connections.
Let’s consider the simple case of two TCP connections sharing a single link
with transmission rate R, as shown in Figure 3.55. Assume that the two connections
have the same MSS and RTT (so that if they have the same congestion window size,
then they have the same throughput), that they have a large amount of data to send,
and that no other TCP connections or UDP datagrams traverse this shared link. Also,
ignore the slow-start phase of TCP and assume the TCP connections are operating
in CA mode (AIMD) at all times.
Figure 3.56 plots the throughput realized by the two TCP connections. If TCP is
to share the link bandwidth equally between the two connections, then the realized
throughput should fall along the 45-degree arrow (equal bandwidth share) emanating
from the origin. Ideally, the sum of the two throughputs should equal R. (Certainly,
each connection receiving an equal, but zero, share of the link capacity is not
a desirable situation!) So the goal should be to have the achieved throughputs fall
somewhere near the intersection of the equal bandwidth share line and the full bandwidth
utilization line in Figure 3.56.
Suppose that the TCP window sizes are such that at a given point in time, connections
1 and 2 realize throughputs indicated by point A in Figure 3.56. Because
the amount of link bandwidth jointly consumed by the two connections is less than
TCP connection 2
TCP connection 1
Bottleneck
router capacity R
Figure 3.55  Two TCP connections sharing a single bottleneck link
3.7 • TCP CONGESTION CONTROL 281
R, no loss will occur, and both connections will increase their window by 1 MSS
per RTT as a result of TCP’s congestion-avoidance algorithm. Thus, the joint
throughput of the two connections proceeds along a 45-degree line (equal increase
for both connections) starting from point A. Eventually, the link bandwidth jointly
consumed by the two connections will be greater than R, and eventually packet loss
will occur. Suppose that connections 1 and 2 experience packet loss when they
realize throughputs indicated by point B. Connections 1 and 2 then decrease their
windows by a factor of two. The resulting throughputs realized are thus at point C,
halfway along a vector starting at B and ending at the origin. Because the joint
bandwidth use is less than R at point C, the two connections again increase their
throughputs along a 45-degree line starting from C. Eventually, loss will again
occur, for example, at point D, and the two connections again decrease their window
sizes by a factor of two, and so on. You should convince yourself that the
bandwidth realized by the two connections eventually fluctuates along the equal
bandwidth share line. You should also convince yourself that the two connections
will converge to this behavior regardless of where they are in the two-dimensional
space! Although a number of idealized assumptions lie behind this scenario, it still
provides an intuitive feel for why TCP results in an equal sharing of bandwidth
among connections.
In our idealized scenario, we assumed that only TCP connections traverse
the bottleneck link, that the connections have the same RTT value, and that only a
R
R
Equal
bandwidth
share
Connection 1 throughput
Connection 2 throughput
D
B
C
A
Full bandwidth
utilization line
Figure 3.56  Throughput realized by TCP connections 1 and 2
282 CHAPTER 3 • TRANSPORT LAYER
single TCP connection is associated with a host-destination pair. In practice, these
conditions are typically not met, and client-server applications can thus obtain very
unequal portions of link bandwidth. In particular, it has been shown that when multiple
connections share a common bottleneck, those sessions with a smaller RTT are
able to grab the available bandwidth at that link more quickly as it becomes free
(that is, open their congestion windows faster) and thus will enjoy higher throughput
than those connections with larger RTTs [Lakshman 1997].
Fairness and UDP
We have just seen how TCP congestion control regulates an application’s transmission
rate via the congestion window mechanism. Many multimedia applications,
such as Internet phone and video conferencing, often do not run over TCP for this
very reason—they do not want their transmission rate throttled, even if the network
is very congested. Instead, these applications prefer to run over UDP, which does
not have built-in congestion control. When running over UDP, applications can
pump their audio and video into the network at a constant rate and occasionally lose
packets, rather than reduce their rates to “fair” levels at times of congestion and not
lose any packets. From the perspective of TCP, the multimedia applications running
over UDP are not being fair—they do not cooperate with the other connections nor
adjust their transmission rates appropriately. Because TCP congestion control will
decrease its transmission rate in the face of increasing congestion (loss), while UDP
sources need not, it is possible for UDP sources to crowd out TCP traffic. An area of
research today is thus the development of congestion-control mechanisms for the
Internet that prevent UDP traffic from bringing the Internet’s throughput to a grinding
halt [Floyd 1999; Floyd 2000; Kohler 2006].
Fairness and Parallel TCP Connections
But even if we could force UDP traffic to behave fairly, the fairness problem would
still not be completely solved. This is because there is nothing to stop a TCP-based
application from using multiple parallel connections. For example, Web browsers
often use multiple parallel TCP connections to transfer the multiple objects within
a Web page. (The exact number of multiple connections is configurable in most
browsers.) When an application uses multiple parallel connections, it gets a larger
fraction of the bandwidth in a congested link. As an example, consider a link of rate
R supporting nine ongoing client-server applications, with each of the applications
using one TCP connection. If a new application comes along and also uses one
TCP connection, then each application gets approximately the same transmission
rate of R/10. But if this new application instead uses 11 parallel TCP connections,
then the new application gets an unfair allocation of more than R/2. Because
Web traffic is so pervasive in the Internet, multiple parallel connections are not
uncommon.
3.8 • SUMMARY 283
3.8 Summary
We began this chapter by studying the services that a transport-layer protocol can
provide to network applications. At one extreme, the transport-layer protocol can be
very simple and offer a no-frills service to applications, providing only a multiplexing/
demultiplexing function for communicating processes. The Internet’s UDP protocol
is an example of such a no-frills transport-layer protocol. At the other extreme,
a transport-layer protocol can provide a variety of guarantees to applications, such
as reliable delivery of data, delay guarantees, and bandwidth guarantees. Nevertheless,
the services that a transport protocol can provide are often constrained by the
service model of the underlying network-layer protocol. If the network-layer protocol
cannot provide delay or bandwidth guarantees to transport-layer segments, then
the transport-layer protocol cannot provide delay or bandwidth guarantees for the
messages sent between processes.
We learned in Section 3.4 that a transport-layer protocol can provide reliable
data transfer even if the underlying network layer is unreliable. We saw that providing
reliable data transfer has many subtle points, but that the task can be accomplished
by carefully combining acknowledgments, timers, retransmissions, and
sequence numbers.
Although we covered reliable data transfer in this chapter, we should keep in
mind that reliable data transfer can be provided by link-, network-, transport-, or
application-layer protocols. Any of the upper four layers of the protocol stack can
implement acknowledgments, timers, retransmissions, and sequence numbers and
provide reliable data transfer to the layer above. In fact, over the years, engineers
and computer scientists have independently designed and implemented link-, network-,
transport-, and application-layer protocols that provide reliable data transfer
(although many of these protocols have quietly disappeared).
In Section 3.5, we took a close look at TCP, the Internet’s connection-oriented
and reliable transport-layer protocol. We learned that TCP is complex, involving
connection management, flow control, and round-trip time estimation, as well as
reliable data transfer. In fact, TCP is actually more complex than our description—
we intentionally did not discuss a variety of TCP patches, fixes, and improvements
that are widely implemented in various versions of TCP. All of this complexity,
however, is hidden from the network application. If a client on one host wants to
send data reliably to a server on another host, it simply opens a TCP socket to the
server and pumps data into that socket. The client-server application is blissfully
unaware of TCP’s complexity.
In Section 3.6, we examined congestion control from a broad perspective, and in
Section 3.7, we showed how TCP implements congestion control. We learned that
congestion control is imperative for the well-being of the network. Without congestion
control, a network can easily become gridlocked, with little or no data being transported
end-to-end. In Section 3.7 we learned that TCP implements an end-to-end
284 CHAPTER 3 • TRANSPORT LAYER
congestion-control mechanism that additively increases its transmission rate when the
TCP connection’s path is judged to be congestion-free, and multiplicatively decreases
its transmission rate when loss occurs. This mechanism also strives to give each TCP
connection passing through a congested link an equal share of the link bandwidth. We
also examined in some depth the impact of TCP connection establishment and slow
start on latency. We observed that in many important scenarios, connection establishment
and slow start significantly contribute to end-to-end delay. We emphasize once
more that while TCP congestion control has evolved over the years, it remains an area
of intensive research and will likely continue to evolve in the upcoming years.
Our discussion of specific Internet transport protocols in this chapter has
focused on UDP and TCP—the two “work horses” of the Internet transport layer.
However, two decades of experience with these two protocols has identified
circumstances in which neither is ideally suited. Researchers have thus been
busy developing additional transport-layer protocols, several of which are now
IETF proposed standards.
The Datagram Congestion Control Protocol (DCCP) [RFC 4340] provides a lowoverhead,
message-oriented, UDP-like unreliable service, but with an applicationselected
form of congestion control that is compatible with TCP. If reliable or
semi-reliable data transfer is needed by an application, then this would be performed
within the application itself, perhaps using the mechanisms we have studied in Section
3.4. DCCP is envisioned for use in applications such as streaming media (see Chapter 7)
that can exploit the tradeoff between timeliness and reliability of data delivery, but that
want to be responsive to network congestion.
The Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a
reliable, message-oriented protocol that allows several different application-level
“streams” to be multiplexed through a single SCTP connection (an approach known as
“multi-streaming”). From a reliability standpoint, the different streams within the connection
are handled separately, so that packet loss in one stream does not affect the
delivery of data in other streams. SCTP also allows data to be transferred over two outgoing
paths when a host is connected to two or more networks, optional delivery of outof-
order data, and a number of other features. SCTP’s flow- and congestion-control
algorithms are essentially the same as in TCP.
The TCP-Friendly Rate Control (TFRC) protocol [RFC 5348] is a congestioncontrol
protocol rather than a full-fledged transport-layer protocol. It specifies a
congestion-control mechanism that could be used in anther transport protocol such as
DCCP (indeed one of the two application-selectable protocols available in DCCP is
TFRC). The goal of TFRC is to smooth out the “saw tooth” behavior (see Figure 3.54)
in TCP congestion control, while maintaining a long-term sending rate that is “reasonably”
close to that of TCP. With a smoother sending rate than TCP, TFRC is well-suited
for multimedia applications such as IP telephony or streaming media where such a
smooth rate is important. TFRC is an “equation-based” protocol that uses the measured
packet loss rate as input to an equation [Padhye 2000] that estimates what TCP’s
throughput would be if a TCP session experiences that loss rate. This rate is then taken
as TFRC’s target sending rate.
HOMEWORK PROBLEMS AND QUESTIONS 285
Only the future will tell whether DCCP, SCTP, or TFRC will see widespread
deployment. While these protocols clearly provide enhanced capabilities over TCP and
UDP, TCP and UDP have proven themselves “good enough” over the years. Whether
“better” wins out over “good enough” will depend on a complex mix of technical,
social, and business considerations.
In Chapter 1, we said that a computer network can be partitioned into the
“network edge” and the “network core.” The network edge covers everything that
happens in the end systems. Having now covered the application layer and the
transport layer, our discussion of the network edge is complete. It is time to
explore the network core! This journey begins in the next chapter, where we’ll
study the network layer, and continues into Chapter 5, where we’ll study the
link layer.
Homework Problems and Questions
Chapter 3 Review Questions
SECTIONS 3.1–3.3
R1. Suppose the network layer provides the following service. The network layer
in the source host accepts a segment of maximum size 1,200 bytes and a destination
host address from the transport layer. The network layer then guarantees
to deliver the segment to the transport layer at the destination host.
Suppose many network application processes can be running at the
destination host.
a. Design the simplest possible transport-layer protocol that will get application
data to the desired process at the destination host. Assume the operating
system in the destination host has assigned a 4-byte port number to
each running application process.
b. Modify this protocol so that it provides a “return address” to the destination
process.
c. In your protocols, does the transport layer “have to do anything” in the
core of the computer network?
R2. Consider a planet where everyone belongs to a family of six, every family
lives in its own house, each house has a unique address, and each person in a
given house has a unique name. Suppose this planet has a mail service that
delivers letters from source house to destination house. The mail service
requires that (1) the letter be in an envelope, and that (2) the address of the
destination house (and nothing more) be clearly written on the envelope. Suppose
each family has a delegate family member who collects and distributes
letters for the other family members. The letters do not necessarily provide
any indication of the recipients of the letters.
286 CHAPTER 3 • TRANSPORT LAYER
a. Using the solution to Problem R1 above as inspiration, describe a protocol
that the delegates can use to deliver letters from a sending family member
to a receiving family member.
b. In your protocol, does the mail service ever have to open the envelope and
examine the letter in order to provide its service?
R3. Consider a TCP connection between Host A and Host B. Suppose that the
TCP segments traveling from Host A to Host B have source port number x
and destination port number y. What are the source and destination port numbers
for the segments traveling from Host B to Host A?
R4. Describe why an application developer might choose to run an application
over UDP rather than TCP.
R5. Why is it that voice and video traffic is often sent over TCP rather than UDP
in today’s Internet? (Hint: The answer we are looking for has nothing to do
with TCP’s congestion-control mechanism.)
R6. Is it possible for an application to enjoy reliable data transfer even when the
application runs over UDP? If so, how?
R7. Suppose a process in Host C has a UDP socket with port number 6789. Suppose
both Host A and Host B each send a UDP segment to Host C with destination
port number 6789. Will both of these segments be directed to the same
socket at Host C? If so, how will the process at Host C know that these two
segments originated from two different hosts?
R8. Suppose that a Web server runs in Host C on port 80. Suppose this Web
server uses persistent connections, and is currently receiving requests from
two different Hosts, A and B. Are all of the requests being sent through the
same socket at Host C? If they are being passed through different sockets, do
both of the sockets have port 80? Discuss and explain.
SECTION 3.4
R9. In our rdt protocols, why did we need to introduce sequence numbers?
R10. In our rdt protocols, why did we need to introduce timers?
R11. Suppose that the roundtrip delay between sender and receiver is constant and
known to the sender. Would a timer still be necessary in protocol rdt 3.0,
assuming that packets can be lost? Explain.
R12. Visit the Go-Back-N Java applet at the companion Web site.
a. Have the source send five packets, and then pause the animation before
any of the five packets reach the destination. Then kill the first packet and
resume the animation. Describe what happens.
b. Repeat the experiment, but now let the first packet reach the destination
and kill the first acknowledgment. Describe again what happens.
c. Finally, try sending six packets. What happens?
HOMEWORK PROBLEMS AND QUESTIONS 287
R13. Repeat R12, but now with the Selective Repeat Java applet. How are Selective
Repeat and Go-Back-N different?
SECTION 3.5
R14. True or false?
a. Host A is sending Host B a large file over a TCP connection. Assume
Host B has no data to send Host A. Host B will not send acknowledgments
to Host A because Host B cannot piggyback the acknowledgments
on data.
b. The size of the TCP rwnd never changes throughout the duration of the
connection.
c. Suppose Host A is sending Host B a large file over a TCP connection. The
number of unacknowledged bytes that A sends cannot exceed the size of
the receive buffer.
d. Suppose Host A is sending a large file to Host B over a TCP connection. If
the sequence number for a segment of this connection is m, then the
sequence number for the subsequent segment will necessarily be m + 1.
e. The TCP segment has a field in its header for rwnd.
f. Suppose that the last SampleRTT in a TCP connection is equal to 1 sec.
The current value of TimeoutInterval for the connection will necessarily
be = 1 sec.
g. Suppose Host A sends one segment with sequence number 38 and 4 bytes
of data over a TCP connection to Host B. In this same segment the
acknowledgment number is necessarily 42.
R15. Suppose Host A sends two TCP segments back to back to Host B over a TCP
connection. The first segment has sequence number 90; the second has
sequence number 110.
a. How much data is in the first segment?
b. Suppose that the first segment is lost but the second segment arrives at B.
In the acknowledgment that Host B sends to Host A, what will be the
acknowledgment number?
R16. Consider the Telnet example discussed in Section 3.5. A few seconds after the
user types the letter ‘C,’ the user types the letter ‘R.’ After typing the letter
‘R,’ how many segments are sent, and what is put in the sequence number
and acknowledgment fields of the segments?
SECTION 3.7
R17. Suppose two TCP connections are present over some bottleneck link of rate R
bps. Both connections have a huge file to send (in the same direction over the
288 CHAPTER 3 • TRANSPORT LAYER
bottleneck link). The transmissions of the files start at the same time. What
transmission rate would TCP like to give to each of the connections?
R18. True or false? Consider congestion control in TCP. When the timer expires at
the sender, the value of ssthresh is set to one half of its previous value.
R19. In the discussion of TCP splitting in the sidebar in Section 7.2, it was
claimed that the response time with TCP splitting is approximately
Justify this claim.
Problems
P1. Suppose Client A initiates a Telnet session with Server S. At about the same
time, Client B also initiates a Telnet session with Server S. Provide possible
source and destination port numbers for
a. The segments sent from A to S.
b. The segments sent from B to S.
c. The segments sent from S to A.
d. The segments sent from S to B.
e. If A and B are different hosts, is it possible that the source port number in
the segments from A to S is the same as that from B to S?
f. How about if they are the same host?
P2. Consider Figure 3.5. What are the source and destination port values in the segments
flowing from the server back to the clients’ processes? What are the IP
addresses in the network-layer datagrams carrying the transport-layer segments?
P3. UDP and TCP use 1s complement for their checksums. Suppose you have the
following three 8-bit bytes: 01010011, 01100110, 01110100. What is the 1s
complement of the sum of these 8-bit bytes? (Note that although UDP and
TCP use 16-bit words in computing the checksum, for this problem you are
being asked to consider 8-bit sums.) Show all work. Why is it that UDP takes
the 1s complement of the sum; that is, why not just use the sum? With the 1s
complement scheme, how does the receiver detect errors? Is it possible that a
1-bit error will go undetected? How about a 2-bit error?
P4. a. Suppose you have the following 2 bytes: 01011100 and 01100101. What is
the 1s complement of the sum of these 2 bytes?
b. Suppose you have the following 2 bytes: 11011010 and 01100101. What is
the 1s complement of the sum of these 2 bytes?
c. For the bytes in part (a), give an example where one bit is flipped in each
of the 2 bytes and yet the 1s complement doesn’t change.
4  RTTFE + RTTBE + processing time.
PROBLEMS 289
P5. Suppose that the UDP receiver computes the Internet checksum for the received
UDP segment and finds that it matches the value carried in the checksum field.
Can the receiver be absolutely certain that no bit errors have occurred? Explain.
P6. Consider our motivation for correcting protocol rdt2.1. Show that the
receiver, shown in Figure 3.57, when operating with the sender shown in Figure
3.11, can lead the sender and receiver to enter into a deadlock state, where
each is waiting for an event that will never occur.
P7. In protocol rdt3.0, the ACK packets flowing from the receiver to the
sender do not have sequence numbers (although they do have an ACK field
that contains the sequence number of the packet they are acknowledging).
Why is it that our ACK packets do not require sequence numbers?
P8. Draw the FSM for the receiver side of protocol rdt3.0.
P9. Give a trace of the operation of protocol rdt3.0 when data packets and
acknowledgment packets are garbled. Your trace should be similar to that
used in Figure 3.16.
P10. Consider a channel that can lose packets but has a maximum delay that is
known. Modify protocol rdt2.1 to include sender timeout and retransmit.
Informally argue why your protocol can communicate correctly over this
channel.
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt) rdt_rvc(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
rdt_rvc(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.57  An incorrect receiver for protocol rdt 2.1
290 CHAPTER 3 • TRANSPORT LAYER
P11. Consider the rdt2.2 receiver in Figure 3.14, and the creation of a new packet
in the self-transition (i.e., the transition from the state back to itself) in the Waitfor-
0-from-below and the Wait-for-1-from-below states: sndpkt=make_
pkt(ACK,0,checksum) and sndpkt=make_pkt(ACK,0,
checksum). Would the protocol work correctly if this action were removed
from the self-transition in the Wait-for-1-from-below state? Justify your
answer. What if this event were removed from the self-transition in the Waitfor-
0-from-below state? [Hint: In this latter case, consider what would happen
if the first sender-to-receiver packet were corrupted.]
P12. The sender side of rdt3.0 simply ignores (that is, takes no action on)
all received packets that are either in error or have the wrong value in the
acknum field of an acknowledgment packet. Suppose that in such circumstances,
rdt3.0 were simply to retransmit the current data packet. Would
the protocol still work? (Hint: Consider what would happen if there were
only bit errors; there are no packet losses but premature timeouts can occur.
Consider how many times the nth packet is sent, in the limit as n
approaches infinity.)
P13. Consider the rdt 3.0 protocol. Draw a diagram showing that if the network
connection between the sender and receiver can reorder messages
(that is, that two messages propagating in the medium between the sender
and receiver can be reordered), then the alternating-bit protocol will not
work correctly (make sure you clearly identify the sense in which it will
not work correctly). Your diagram should have the sender on the left and
the receiver on the right, with the time axis running down the page, showing
data (D) and acknowledgment (A) message exchange. Make sure you
indicate the sequence number associated with any data or acknowledgment
segment.
P14. Consider a reliable data transfer protocol that uses only negative acknowledgments.
Suppose the sender sends data only infrequently. Would a NAK-only
protocol be preferable to a protocol that uses ACKs? Why? Now suppose the
sender has a lot of data to send and the end-to-end connection experiences
few losses. In this second case, would a NAK-only protocol be preferable to a
protocol that uses ACKs? Why?
P15. Consider the cross-country example shown in Figure 3.17. How big would
the window size have to be for the channel utilization to be greater than 98
percent? Suppose that the size of a packet is 1,500 bytes, including both
header fields and data.
P16. Suppose an application uses rdt 3.0 as its transport layer protocol. As the
stop-and-wait protocol has very low channel utilization (shown in the crosscountry
example), the designers of this application let the receiver keep sending
back a number (more than two) of alternating ACK 0 and ACK 1 even if
PROBLEMS 291
the corresponding data have not arrived at the receiver. Would this application
design increase the channel utilization? Why? Are there any potential
problems with this approach? Explain.
P17. Consider two network entities, A and B, which are connected by a perfect bidirectional
channel (i.e., any message sent will be received correctly; the
channel will not corrupt, lose, or re-order packets). A and B are to deliver
data messages to each other in an alternating manner: First, A must deliver a
message to B, then B must deliver a message to A, then A must deliver a message
to B and so on. If an entity is in a state where it should not attempt to
deliver a message to the other side, and there is an event like
rdt_send(data) call from above that attempts to pass data down for
transmission to the other side, this call from above can simply be ignored
with a call to rdt_unable_to_send(data), which informs the higher
layer that it is currently not able to send data. [Note: This simplifying
assumption is made so you don’t have to worry about buffering data.]
Draw a FSM specification for this protocol (one FSM for A, and one FSM for
B!). Note that you do not have to worry about a reliability mechanism here;
the main point of this question is to create a FSM specification that reflects
the synchronized behavior of the two entities. You should use the following
events and actions that have the same meaning as protocol rdt1.0 in
Figure 3.9: rdt_send(data), packet = make_pkt(data),
udt_send(packet), rdt_rcv(packet), extract
(packet,data), deliver_data(data). Make sure your protocol
reflects the strict alternation of sending between A and B. Also, make sure to
indicate the initial states for A and B in your FSM descriptions.
P18. In the generic SR protocol that we studied in Section 3.4.4, the sender transmits
a message as soon as it is available (if it is in the window) without waiting
for an acknowledgment. Suppose now that we want an SR protocol that
sends messages two at a time. That is, the sender will send a pair of messages
and will send the next pair of messages only when it knows that both messages
in the first pair have been received correctly.
Suppose that the channel may lose messages but will not corrupt or reorder
messages. Design an error-control protocol for the unidirectional reliable
transfer of messages. Give an FSM description of the sender and receiver.
Describe the format of the packets sent between sender and receiver, and vice
versa. If you use any procedure calls other than those in Section 3.4 (for
example, udt_send(), start_timer(), rdt_rcv(), and so on),
clearly state their actions. Give an example (a timeline trace of sender and
receiver) showing how your protocol recovers from a lost packet.
P19. Consider a scenario in which Host A wants to simultaneously send packets to
Hosts B and C. A is connected to B and C via a broadcast channel—a packet
292 CHAPTER 3 • TRANSPORT LAYER
sent by A is carried by the channel to both B and C. Suppose that the broadcast
channel connecting A, B, and C can independently lose and corrupt
packets (and so, for example, a packet sent from A might be correctly
received by B, but not by C). Design a stop-and-wait-like error-control protocol
for reliably transferring packets from A to B and C, such that A will not
get new data from the upper layer until it knows that both B and C have correctly
received the current packet. Give FSM descriptions of A and C. (Hint:
The FSM for B should be essentially the same as for C.) Also, give a description
of the packet format(s) used.
P20. Consider a scenario in which Host A and Host B want to send messages to
Host C. Hosts A and C are connected by a channel that can lose and corrupt
(but not reorder) messages. Hosts B and C are connected by another channel
(independent of the channel connecting A and C) with the same properties.
The transport layer at Host C should alternate in delivering messages
from A and B to the layer above (that is, it should first deliver the data from
a packet from A, then the data from a packet from B, and so on). Design a
stop-and-wait-like error-control protocol for reliably transferring packets
from A and B to C, with alternating delivery at C as described above. Give
FSM descriptions of A and C. (Hint: The FSM for B should be essentially
the same as for A.) Also, give a description of the packet format(s) used.
P21. Suppose we have two network entities, A and B. B has a supply of data messages
that will be sent to A according to the following conventions. When A
gets a request from the layer above to get the next data (D) message from B,
A must send a request (R) message to B on the A-to-B channel. Only when B
receives an R message can it send a data (D) message back to A on the B-to-
A channel. A should deliver exactly one copy of each D message to the layer
above. R messages can be lost (but not corrupted) in the A-to-B channel; D
messages, once sent, are always delivered correctly. The delay along both
channels is unknown and variable.
Design (give an FSM description of) a protocol that incorporates the appropriate
mechanisms to compensate for the loss-prone A-to-B channel and
implements message passing to the layer above at entity A, as discussed
above. Use only those mechanisms that are absolutely necessary.
P22. Consider the GBN protocol with a sender window size of 4 and a sequence
number range of 1,024. Suppose that at time t, the next in-order packet that the
receiver is expecting has a sequence number of k. Assume that the medium
does not reorder messages. Answer the following questions:
a. What are the possible sets of sequence numbers inside the sender’s window
at time t? Justify your answer.
b. What are all possible values of the ACK field in all possible messages currently
propagating back to the sender at time t? Justify your answer.
PROBLEMS 293
P23. Consider the GBN and SR protocols. Suppose the sequence number space is
of size k. What is the largest allowable sender window that will avoid the
occurrence of problems such as that in Figure 3.27 for each of these protocols?
P24. Answer true or false to the following questions and briefly justify your
answer:
a. With the SR protocol, it is possible for the sender to receive an ACK for a
packet that falls outside of its current window.
b. With GBN, it is possible for the sender to receive an ACK for a packet that
falls outside of its current window.
c. The alternating-bit protocol is the same as the SR protocol with a sender
and receiver window size of 1.
d. The alternating-bit protocol is the same as the GBN protocol with a sender
and receiver window size of 1.
P25. We have said that an application may choose UDP for a transport protocol
because UDP offers finer application control (than TCP) of what data is sent
in a segment and when.
a. Why does an application have more control of what data is sent in a segment?
b. Why does an application have more control on when the segment is sent?
P26. Consider transferring an enormous file of L bytes from Host A to Host B.
Assume an MSS of 536 bytes.
a. What is the maximum value of L such that TCP sequence numbers are not
exhausted? Recall that the TCP sequence number field has 4 bytes.
b. For the L you obtain in (a), find how long it takes to transmit the file.
Assume that a total of 66 bytes of transport, network, and data-link header
are added to each segment before the resulting packet is sent out over a
155 Mbps link. Ignore flow control and congestion control so A can pump
out the segments back to back and continuously.
P27. Host A and B are communicating over a TCP connection, and Host B has
already received from A all bytes up through byte 126. Suppose Host A then
sends two segments to Host B back-to-back. The first and second segments
contain 80 and 40 bytes of data, respectively. In the first segment, the
sequence number is 127, the source port number is 302, and the destination
port number is 80. Host B sends an acknowledgment whenever it receives a
segment from Host A.
a. In the second segment sent from Host A to B, what are the sequence number,
source port number, and destination port number?
b. If the first segment arrives before the second segment, in the acknowledgment
of the first arriving segment, what is the acknowledgment number,
the source port number, and the destination port number?
294 CHAPTER 3 • TRANSPORT LAYER
c. If the second segment arrives before the first segment, in the acknowledgment
of the first arriving segment, what is the acknowledgment
number?
d. Suppose the two segments sent by A arrive in order at B. The first acknowledgment
is lost and the second acknowledgment arrives after the first timeout
interval. Draw a timing diagram, showing these segments and all other
segments and acknowledgments sent. (Assume there is no additional packet
loss.) For each segment in your figure, provide the sequence number and
the number of bytes of data; for each acknowledgment that you add, provide
the acknowledgment number.
P28. Host Aand B are directly connected with a 100 Mbps link. There is one TCP
connection between the two hosts, and Host Ais sending to Host B an enormous
file over this connection. Host Acan send its application data into its TCP
socket at a rate as high as 120 Mbps but Host B can read out of its TCP receive
buffer at a maximum rate of 50 Mbps. Describe the effect of TCP flow control.
P29. SYN cookies were discussed in Section 3.5.6.
a. Why is it necessary for the server to use a special initial sequence number
in the SYNACK?
b. Suppose an attacker knows that a target host uses SYN cookies. Can the
attacker create half-open or fully open connections by simply sending an
ACK packet to the target? Why or why not?
c. Suppose an attacker collects a large amount of initial sequence numbers sent
by the server. Can the attacker cause the server to create many fully open
connections by sending ACKs with those initial sequence numbers? Why?
P30. Consider the network shown in Scenario 2 in Section 3.6.1. Suppose both
sending hosts A and B have some fixed timeout values.
a. Argue that increasing the size of the finite buffer of the router might possibly
decrease the throughput (out).
b. Now suppose both hosts dynamically adjust their timeout values (like
what TCP does) based on the buffering delay at the router. Would increasing
the buffer size help to increase the throughput? Why?
P31. Suppose that the five measured SampleRTT values (see Section 3.5.3) are
106 ms, 120 ms, 140 ms, 90 ms, and 115 ms. Compute the EstimatedRTT
after each of these SampleRTT values is obtained, using a value of a = 0.125
and assuming that the value of EstimatedRTT was 100 ms just before the
first of these five samples were obtained. Compute also the DevRTT after
each sample is obtained, assuming a value of ß = 0.25 and assuming the
value of DevRTT was 5 ms just before the first of these five samples was
obtained. Last, compute the TCP TimeoutInterval after each of these
samples is obtained.
PROBLEMS 295
P32. Consider the TCP procedure for estimating RTT. Suppose that  = 0.1. Let
SampleRTT1 be the most recent sample RTT, let SampleRTT2 be the next
most recent sample RTT, and so on.
a. For a given TCP connection, suppose four acknowledgments have been
returned with corresponding sample RTTs: SampleRTT4, SampleRTT3,
SampleRTT2, and SampleRTT1. Express EstimatedRTT in terms of
the four sample RTTs.
b. Generalize your formula for n sample RTTs.
c. For the formula in part (b) let n approach infinity. Comment on why this
averaging procedure is called an exponential moving average.
P33. In Section 3.5.3, we discussed TCP’s estimation of RTT. Why do you think
TCP avoids measuring the SampleRTT for retransmitted segments?
P34. What is the relationship between the variable SendBase in Section 3.5.4
and the variable LastByteRcvd in Section 3.5.5?
P35. What is the relationship between the variable LastByteRcvd in Section
3.5.5 and the variable y in Section 3.5.4?
P36. In Section 3.5.4, we saw that TCP waits until it has received three
duplicate ACKs before performing a fast retransmit. Why do you think the
TCP designers chose not to perform a fast retransmit after the first duplicate
ACK for a segment is received?
P37. Compare GBN, SR, and TCP (no delayed ACK). Assume that the timeout
values for all three protocols are sufficiently long such that 5 consecutive data
segments and their corresponding ACKs can be received (if not lost in the
channel) by the receiving host (Host B) and the sending host (Host A) respectively.
Suppose Host A sends 5 data segments to Host B, and the 2nd segment
(sent from A) is lost. In the end, all 5 data segments have been correctly
received by Host B.
a. How many segments has Host A sent in total and how many ACKs has
Host B sent in total? What are their sequence numbers? Answer this question
for all three protocols.
b. If the timeout values for all three protocol are much longer than 5 RTT,
then which protocol successfully delivers all five data segments in shortest
time interval?
P38. In our description of TCP in Figure 3.53, the value of the threshold,
ssthresh, is set as ssthresh=cwnd/2 in several places and
ssthresh value is referred to as being set to half the window size when a
loss event occurred. Must the rate at which the sender is sending when the
loss event occurred be approximately equal to cwnd segments per RTT?
Explain your answer. If your answer is no, can you suggest a different
manner in which ssthresh should be set?
296 CHAPTER 3 • TRANSPORT LAYER
P39. Consider Figure 3.46(b). If in increases beyond R/2, can out increase
beyond R/3? Explain. Now consider Figure 3.46(c). If in increases
beyond R/2, can out increase beyond R/4 under the assumption that a
packet will be forwarded twice on average from the router to the receiver?
Explain.
P40. Consider Figure 3.58. Assuming TCP Reno is the protocol experiencing the
behavior shown above, answer the following questions. In all cases, you
should provide a short discussion justifying your answer.
a. Identify the intervals of time when TCP slow start is operating.
b. Identify the intervals of time when TCP congestion avoidance is
operating.
c. After the 16th transmission round, is segment loss detected by a triple
duplicate ACK or by a timeout?
d. After the 22nd transmission round, is segment loss detected by a triple
duplicate ACK or by a timeout?
e. What is the initial value of ssthresh at the first transmission round?
f. What is the value of ssthresh at the 18th transmission round?
g. What is the value of ssthresh at the 24th transmission round?
h. During what transmission round is the 70th segment sent?
i. Assuming a packet loss is detected after the 26th round by the receipt of a
triple duplicate ACK, what will be the values of the congestion window
size and of ssthresh?
0
0 2 4 6 8 10 12
Transmission round
14 16 18 20 22 24 26
5
10
15
20
25 Congestion window size (
segments)
30
35
40
45
Figure 3.58  TCP window size as a function of time
VideoNote
Examining the
behavior of TCP
PROBLEMS 297
j. Suppose TCP Tahoe is used (instead of TCP Reno), and assume that triple
duplicate ACKs are received at the 16th round. What are the ssthresh
and the congestion window size at the 19th round?
k. Again suppose TCP Tahoe is used, and there is a timeout event at 22nd
round. How many packets have been sent out from 17th round till 22nd
round, inclusive?
P41. Refer to Figure 3.56, which illustrates the convergence of TCP’s AIMD
algorithm. Suppose that instead of a multiplicative decrease, TCP decreased
the window size by a constant amount. Would the resulting AIAD algorithm
converge to an equal share algorithm? Justify your answer using a diagram
similar to Figure 3.56.
P42. In Section 3.5.4, we discussed the doubling of the timeout interval after a
timeout event. This mechanism is a form of congestion control. Why does
TCP need a window-based congestion-control mechanism (as studied in
Section 3.7) in addition to this doubling-timeout-interval mechanism?
P43. Host A is sending an enormous file to Host B over a TCP connection.
Over this connection there is never any packet loss and the timers never
expire. Denote the transmission rate of the link connecting Host A to the
Internet by R bps. Suppose that the process in Host A is capable of sending
data into its TCP socket at a rate S bps, where S = 10 · R. Further suppose
that the TCP receive buffer is large enough to hold the entire file, and the
send buffer can hold only one percent of the file. What would prevent the
process in Host A from continuously passing data to its TCP socket at rate S
bps? TCP flow control? TCP congestion control? Or something else?
Elaborate.
P44. Consider sending a large file from a host to another over a TCP connection
that has no loss.
a. Suppose TCP uses AIMD for its congestion control without slow start.
Assuming cwnd increases by 1 MSS every time a batch of ACKs is received
and assuming approximately constant round-trip times, how long does it take
for cwnd increase from 6 MSS to 12 MSS (assuming no loss events)?
b. What is the average throughout (in terms of MSS and RTT) for this connection
up through time = 6 RTT?
P45. Recall the macroscopic description of TCP throughput. In the period of time
from when the connection’s rate varies from W/(2 · RTT) to W/RTT, only one
packet is lost (at the very end of the period).
a. Show that the loss rate (fraction of packets lost) is equal to
L = loss rate =
1
3
8
W2 +
3
4
W
298 CHAPTER 3 • TRANSPORT LAYER
b. Use the result above to show that if a connection has loss rate L, then its
average rate is approximately given by
P46. Consider that only a single TCP (Reno) connection uses one 10Mbps link
which does not buffer any data. Suppose that this link is the only congested
link between the sending and receiving hosts. Assume that the TCP sender
has a huge file to send to the receiver, and the receiver’s receive buffer is
much larger than the congestion window. We also make the following
assumptions: each TCP segment size is 1,500 bytes; the two-way propagation
delay of this connection is 150 msec; and this TCP connection is always in
congestion avoidance phase, that is, ignore slow start.
a. What is the maximum window size (in segments) that this TCP connection
can achieve?
b. What is the average window size (in segments) and average throughput (in
bps) of this TCP connection?
c. How long would it take for this TCP connection to reach its maximum
window again after recovering from a packet loss?
P47. Consider the scenario described in the previous problem. Suppose that the
10Mbps link can buffer a finite number of segments. Argue that in order for
the link to always be busy sending data, we would like to choose a buffer size
that is at least the product of the link speed C and the two-way propagation
delay between the sender and the receiver.
P48. Repeat Problem 43, but replacing the 10 Mbps link with a 10 Gbps link. Note
that in your answer to part c, you will realize that it takes a very long time for
the congestion window size to reach its maximum window size after recovering
from a packet loss. Sketch a solution to solve this problem.
P49. Let T (measured by RTT) denote the time interval that a TCP connection
takes to increase its congestion window size from W/2 to W, where W is the
maximum congestion window size. Argue that T is a function of TCP’s average
throughput.
P50. Consider a simplified TCP’s AIMD algorithm where the congestion window
size is measured in number of segments, not in bytes. In additive increase, the
congestion window size increases by one segment in each RTT. In multiplicative
decrease, the congestion window size decreases by half (if the result is
not an integer, round down to the nearest integer). Suppose that two TCP
connections, C1 and C2, share a single congested link of speed 30 segments
per second. Assume that both C1 and C2 are in the congestion avoidance

1.22  MSS
RTT 2L
PROBLEMS 299
phase. Connection C1’s RTT is 50 msec and connection C2’s RTT is
100 msec. Assume that when the data rate in the link exceeds the link’s
speed, all TCP connections experience data segment loss.
a. If both C1 and C2 at time t0 have a congestion window of 10 segments,
what are their congestion window sizes after 1000 msec?
b. In the long run, will these two connections get the same share of the bandwidth
of the congested link? Explain.
P51. Consider the network described in the previous problem. Now suppose that
the two TCP connections, C1 and C2, have the same RTT of 100 msec. Suppose
that at time t0, C1’s congestion window size is 15 segments but C2’s
congestion window size is 10 segments.
a. What are their congestion window sizes after 2200msec?
b. In the long run, will these two connections get about the same share of the
bandwidth of the congested link?
c. We say that two connections are synchronized, if both connections reach
their maximum window sizes at the same time and reach their minimum
window sizes at the same time. In the long run, will these two connections
get synchronized eventually? If so, what are their maximum window sizes?
d. Will this synchronization help to improve the utilization of the shared
link? Why? Sketch some idea to break this synchronization.
P52. Consider a modification to TCP’s congestion control algorithm. Instead of
additive increase, we can use multiplicative increase. ATCP sender increases
its window size by a small positive constant a (0 < a < 1) whenever it
receives a valid ACK. Find the functional relationship between loss rate L
and maximum congestion window W. Argue that for this modified TCP,
regardless of TCP’s average throughput, a TCP connection always spends the
same amount of time to increase its congestion window size from W/2 to W.
P53. In our discussion of TCP futures in Section 3.7, we noted that to achieve a
throughput of 10 Gbps, TCP could only tolerate a segment loss probability of
2 · 10-10 (or equivalently, one loss event for every 5,000,000,000 segments).
Show the derivation for the values of 2 · 10-10 (1 out of 5,000,000) for the
RTT and MSS values given in Section 3.7. If TCP needed to support a 100
Gbps connection, what would the tolerable loss be?
P54. In our discussion of TCP congestion control in Section 3.7, we implicitly
assumed that the TCP sender always had data to send. Consider now the case
that the TCP sender sends a large amount of data and then goes idle (since it
has no more data to send) at t1. TCP remains idle for a relatively long period of
time and then wants to send more data at t2. What are the advantages and disadvantages
of having TCP use the cwnd and ssthresh values from t1 when
starting to send data at t2? What alternative would you recommend? Why?
300 CHAPTER 3 • TRANSPORT LAYER
P55. In this problem we investigate whether either UDP or TCP provides a degree
of end-point authentication.
a. Consider a server that receives a request within a UDP packet and
responds to that request within a UDP packet (for example, as done by a
DNS server). If a client with IP address X spoofs its address with address
Y, where will the server send its response?
b. Suppose a server receives a SYN with IP source address Y, and after
responding with a SYNACK, receives an ACK with IP source address Y
with the correct acknowledgment number. Assuming the server chooses a
random initial sequence number and there is no “man-in-the-middle,” can
the server be certain that the client is indeed at Y (and not at some other
address X that is spoofing Y)?
P56. In this problem, we consider the delay introduced by the TCP slow-start
phase. Consider a client and a Web server directly connected by one link of
rate R. Suppose the client wants to retrieve an object whose size is exactly
equal to 15 S, where S is the maximum segment size (MSS). Denote the
round-trip time between client and server as RTT (assumed to be constant).
Ignoring protocol headers, determine the time to retrieve the object (including
TCP connection establishment) when
a. 4 S/R > S/R + RTT > 2S/R
b. S/R + RTT > 4 S/R
c. S/R > RTT.
Programming Assignments
Implementing a Reliable Transport Protocol
In this laboratory programming assignment, you will be writing the sending and
receiving transport-level code for implementing a simple reliable data transfer protocol.
There are two versions of this lab, the alternating-bit-protocol version and the
GBN version. This lab should be fun—your implementation will differ very little
from what would be required in a real-world situation.
Since you probably don’t have standalone machines (with an OS that you can
modify), your code will have to execute in a simulated hardware/software environment.
However, the programming interface provided to your routines—the
code that would call your entities from above and from below—is very close to
what is done in an actual UNIX environment. (Indeed, the software interfaces
described in this programming assignment are much more realistic than the infinite
loop senders and receivers that many texts describe.) Stopping and starting
timers are also simulated, and timer interrupts will cause your timer handling routine
to be activated.
The full lab assignment, as well as code you will need to compile with your
own code, are available at this book’s Web site: http://www.awl.com/kurose-ross.
Wireshark Lab: Exploring TCP
In this lab, you’ll use your Web browser to access a file from a Web server. As in
earlier Wireshark labs, you’ll use Wireshark to capture the packets arriving at your
computer. Unlike earlier labs, you’ll also be able to download a Wireshark-readable
packet trace from the Web server from which you downloaded the file. In this server
trace, you’ll find the packets that were generated by your own access of the Web
server. You’ll analyze the client- and server-side traces to explore aspects of TCP. In
particular, you’ll evaluate the performance of the TCP connection between your
computer and the Web server. You’ll trace TCP’s window behavior, and infer packet
loss, retransmission, flow control and congestion control behavior, and estimated
roundtrip time.
As is the case with all Wireshark labs, the full description of this lab is available
at this book’s Web site, http://www.awl.com/kurose-ross.
Wireshark Lab: Exploring UDP
In this short lab, you’ll do a packet capture and analysis of your favorite application
that uses UDP (for example, DNS or a multimedia application such as Skype). As we
learned in Section 3.3, UDP is a simple, no-frills transport protocol. In this lab, you’ll
investigate the header fields in the UDP segment as well as the checksum calculation.
As is the case with all Wireshark labs, the full description of this lab is available
at this book’s Web site, http://www.awl.com/kurose-ross.
WIRESHARK LAB: EXPLORING UDP 301
302
Please describe one or two of the most exciting projects you have worked on during your
career. What were the biggest challenges?
School teaches us lots of ways to find answers. In every interesting problem I’ve worked
on, the challenge has been finding the right question. When Mike Karels and I started looking
at TCP congestion, we spent months staring at protocol and packet traces asking “Why
is it failing?”. One day in Mike’s office, one of us said “The reason I can’t figure out why it
fails is because I don’t understand how it ever worked to begin with.” That turned out to be
the right question and it forced us to figure out the “ack clocking” that makes TCP work.
After that, the rest was easy.
More generally, where do you see the future of networking and the Internet?
For most people, the Web is the Internet. Networking geeks smile politely since we know
the Web is an application running over the Internet but what if they’re right? The Internet is
about enabling conversations between pairs of hosts. The Web is about distributed information
production and consumption. “Information propagation” is a very general view of communication
of which “pairwise conversation” is a tiny subset. We need to move into the
larger tent. Networking today deals with broadcast media (radios, PONs, etc.) by pretending
it’s a point-to-point wire. That’s massively inefficient. Terabits-per-second of data are being
exchanged all over the World via thumb drives or smart phones but we don’t know how to
treat that as “networking”. ISPs are busily setting up caches and CDNs to scalably distribute
video and audio. Caching is a necessary part of the solution but there's no part of today's
networking—from Information, Queuing or Traffic Theory down to the Internet protocol
specs—that tells us how to engineer and deploy it. I think and hope that over the next few
years, networking will evolve to embrace the much larger vision of communication that
underlies the Web.
Van Jacobson
Van Jacobson is a Research Fellow at PARC. Prior to that, he was
co-founder and Chief Scientist of Packet Design. Before that, he was
Chief Scientist at Cisco. Before joining Cisco, he was head of the
Network Research Group at Lawrence Berkeley National Laboratory
and taught at UC Berkeley and Stanford. Van received the ACM
SIGCOMM Award in 2001 for outstanding lifetime contribution to
the field of communication networks and the IEEE Kobayashi Award
in 2002 for “contributing to the understanding of network congestion
and developing congestion control mechanisms that enabled the successful
scaling of the Internet”. He was elected to the U.S. National
Academy of Engineering in 2004.
AN INTERVIEW WITH...
303
What people inspired you professionally?
When I was in grad school, Richard Feynman visited and gave a colloquium. He talked
about a piece of Quantum theory that I’d been struggling with all semester and his explanation
was so simple and lucid that what had been incomprehensible gibberish to me became
obvious and inevitable. That ability to see and convey the simplicity that underlies our complex
world seems to me a rare and wonderful gift.
What are your recommendations for students who want careers in computer science and
networking?
It’s a wonderful field—computers and networking have probably had more impact on society
than any invention since the book. Networking is fundamentally about connecting stuff,
and studying it helps you make intellectual connections: Ant foraging & Bee dances demonstrate
protocol design better than RFCs, traffic jams or people leaving a packed stadium are
the essence of congestion, and students finding flights back to school in a post-Thanksgiving
blizzard are the core of dynamic routing. If you’re interested in lots of stuff and want to
have an impact, it’s hard to imagine a better field.
This page intentionally left blank
CHAPTER 4
The Network
Layer
305
We learned in the previous chapter that the transport layer provides various forms of
process-to-process communication by relying on the network layer’s host-to-host
communication service. We also learned that the transport layer does so without any
knowledge about how the network layer actually implements this service. So perhaps
you’re now wondering, what’s under the hood of the host-to-host communication
service, what makes it tick?
In this chapter, we’ll learn exactly how the network layer implements the hostto-
host communication service. We’ll see that unlike the transport and application
layers, there is a piece of the network layer in each and every host and router in the
network. Because of this, network-layer protocols are among the most challenging
(and therefore among the most interesting!) in the protocol stack.
The network layer is also one of the most complex layers in the protocol stack,
and so we’ll have a lot of ground to cover here. We’ll begin our study with an
overview of the network layer and the services it can provide. We’ll then examine
two broad approaches towards structuring network-layer packet delivery—the datagram
and the virtual-circuit model—and see the fundamental role that addressing
plays in delivering a packet to its destination host.
In this chapter, we’ll make an important distinction between the forwarding
and routing functions of the network layer. Forwarding involves the transfer of a
packet from an incoming link to an outgoing link within a single router. Routing
involves all of a network’s routers, whose collective interactions via routing protocols
determine the paths that packets take on their trips from source to destination
node. This will be an important distinction to keep in mind as you progress through
this chapter.
In order to deepen our understanding of packet forwarding, we’ll look “inside”
a router—at its hardware architecture and organization. We’ll then look at packet
forwarding in the Internet, along with the celebrated Internet Protocol (IP). We’ll
investigate network-layer addressing and the IPv4 datagram format. We’ll then
explore network address translation (NAT), datagram fragmentation, the Internet
Control Message Protocol (ICMP), and IPv6.
We’ll then turn our attention to the network layer’s routing function. We’ll see
that the job of a routing algorithm is to determine good paths (equivalently, routes)
from senders to receivers. We’ll first study the theory of routing algorithms, concentrating
on the two most prevalent classes of algorithms: link-state and distancevector
algorithms. Since the complexity of routing algorithms grows considerably
as the number of network routers increases, hierarchical routing approaches will
also be of interest. We’ll then see how theory is put into practice when we cover the
Internet’s intra-autonomous system routing protocols (RIP, OSPF, and IS-IS) and its
inter-autonomous system routing protocol, BGP. We’ll close this chapter with a discussion
of broadcast and multicast routing.
In summary, this chapter has three major parts. The first part, Sections 4.1 and
4.2, covers network-layer functions and services. The second part, Sections 4.3 and
4.4, covers forwarding. Finally, the third part, Sections 4.5 through 4.7, covers
routing.
4.1 Introduction
Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers
on the path between H1 and H2. Suppose that H1 is sending information to H2, and
consider the role of the network layer in these hosts and in the intervening routers.
The network layer in H1 takes segments from the transport layer in H1, encapsulates
each segment into a datagram (that is, a network-layer packet), and then sends
the datagrams to its nearby router, R1. At the receiving host, H2, the network layer
receives the datagrams from its nearby router R2, extracts the transport-layer segments,
and delivers the segments up to the transport layer at H2. The primary role of
the routers is to forward datagrams from input links to output links. Note that the
routers in Figure 4.1 are shown with a truncated protocol stack, that is, with no
upper layers above the network layer, because (except for control purposes) routers
do not run application- and transport-layer protocols such as those we examined in
Chapters 2 and 3.
306 CHAPTER 4 • THE NETWORK LAYER
4.1 • INTRODUCTION 307
Mobile Network
Router R1
Router R2
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Home Network
End system H1
Data link
Physical
Application
Transport
Network
End system H2
Data link
Physical
Application
Transport
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Data link
Physical
Network
Figure 4.1  The network layer
4.1.1 Forwarding and Routing
The role of the network layer is thus deceptively simple—to move packets from a
sending host to a receiving host. To do so, two important network-layer functions
can be identified:
• Forwarding. When a packet arrives at a router’s input link, the router must move
the packet to the appropriate output link. For example, a packet arriving from
Host H1 to Router R1 must be forwarded to the next router on a path to H2. In
Section 4.3, we’ll look inside a router and examine how a packet is actually forwarded
from an input link to an output link within a router.
• Routing. The network layer must determine the route or path taken by packets as
they flow from a sender to a receiver. The algorithms that calculate these paths
are referred to as routing algorithms. A routing algorithm would determine, for
example, the path along which packets flow from H1 to H2.
The terms forwarding and routing are often used interchangeably by authors discussing
the network layer. We’ll use these terms much more precisely in this book.
Forwarding refers to the router-local action of transferring a packet from an input link
interface to the appropriate output link interface. Routing refers to the network-wide
process that determines the end-to-end paths that packets take from source to destination.
Using a driving analogy, consider the trip from Pennsylvania to Florida undertaken
by our traveler back in Section 1.3.1. During this trip, our driver passes through
many interchanges en route to Florida. We can think of forwarding as the process of
getting through a single interchange: A car enters the interchange from one road and
determines which road it should take to leave the interchange. We can think of routing
as the process of planning the trip from Pennsylvania to Florida: Before embarking on
the trip, the driver has consulted a map and chosen one of many paths possible, with
each path consisting of a series of road segments connected at interchanges.
Every router has a forwarding table. A router forwards a packet by examining
the value of a field in the arriving packet’s header, and then using this header
value to index into the router’s forwarding table. The value stored in the forwarding
table entry for that header indicates the router’s outgoing link interface to
which that packet is to be forwarded. Depending on the network-layer protocol,
the header value could be the destination address of the packet or an indication of
the connection to which the packet belongs. Figure 4.2 provides an example. In
Figure 4.2, a packet with a header field value of 0111 arrives to a router. The
router indexes into its forwarding table and determines that the output link
interface for this packet is interface 2. The router then internally forwards the
packet to interface 2. In Section 4.3, we’ll look inside a router and examine the
forwarding function in much greater detail.
You might now be wondering how the forwarding tables in the routers are configured.
This is a crucial issue, one that exposes the important interplay between
308 CHAPTER 4 • THE NETWORK LAYER
routing and forwarding. As shown in Figure 4.2, the routing algorithm determines
the values that are inserted into the routers’ forwarding tables. The routing algorithm
may be centralized (e.g., with an algorithm executing on a central site and downloading
routing information to each of the routers) or decentralized (i.e., with a
piece of the distributed routing algorithm running in each router). In either case, a
router receives routing protocol messages, which are used to configure its forwarding
table. The distinct and different purposes of the forwarding and routing functions
can be further illustrated by considering the hypothetical (and unrealistic, but
technically feasible) case of a network in which all forwarding tables are configured
directly by human network operators physically present at the routers. In this case,
no routing protocols would be required! Of course, the human operators would need
to interact with each other to ensure that the forwarding tables were configured in
such a way that packets reached their intended destinations. It’s also likely that
human configuration would be more error-prone and much slower to respond to
changes in the network topology than a routing protocol. We’re thus fortunate that
all networks have both a forwarding and a routing function!
4.1 • INTRODUCTION 309
Value in arriving
packet’s header
1
2
3
Routing algorithm
Local forwarding table
header value
0100
0101
0111
1001
0111
3
2
2
1
output link
Figure 4.2  Routing algorithms determine values in forwarding tables
While we’re on the topic of terminology, it’s worth mentioning two other terms
that are often used interchangeably, but that we will use more carefully. We’ll reserve
the term packet switch to mean a general packet-switching device that transfers a
packet from input link interface to output link interface, according to the value in a field
in the header of the packet. Some packet switches, called link-layer switches (examined
in Chapter 5), base their forwarding decision on values in the fields of the linklayer
frame; switches are thus referred to as link-layer (layer 2) devices. Other packet
switches, called routers, base their forwarding decision on the value in the networklayer
field. Routers are thus network-layer (layer 3) devices, but must also implement
layer 2 protocols as well, since layer 3 devices require the services of layer 2 to implement
their (layer 3) functionality. (To fully appreciate this important distinction, you
might want to review Section 1.5.2, where we discuss network-layer datagrams and
link-layer frames and their relationship.) To confuse matters, marketing literature often
refers to “layer 3 switches” for routers with Ethernet interfaces, but these are really
layer 3 devices. Since our focus in this chapter is on the network layer, we use the term
router in place of packet switch. We’ll even use the term router when talking about
packet switches in virtual-circuit networks (soon to be discussed).
Connection Setup
We just said that the network layer has two important functions, forwarding and routing.
But we’ll soon see that in some computer networks there is actually a third important
network-layer function, namely, connection setup. Recall from our study of TCP
that a three-way handshake is required before data can flow from sender to receiver.
This allows the sender and receiver to set up the needed state information (for example,
sequence number and initial flow-control window size). In an analogous manner, some
network-layer architectures—for example, ATM, frame relay, and MPLS (which we
will study in Section 5.8)––require the routers along the chosen path from source to
destination to handshake with each other in order to set up state before network-layer
data packets within a given source-to-destination connection can begin to flow. In the
network layer, this process is referred to as connection setup. We’ll examine connection
setup in Section 4.2.
4.1.2 Network Service Models
Before delving into the network layer, let’s take the broader view and consider the different
types of service that might be offered by the network layer. When the transport
layer at a sending host transmits a packet into the network (that is, passes it down to
the network layer at the sending host), can the transport layer rely on the network layer
to deliver the packet to the destination? When multiple packets are sent, will they be
delivered to the transport layer in the receiving host in the order in which they were
sent? Will the amount of time between the sending of two sequential packet transmissions
be the same as the amount of time between their reception? Will the network
310 CHAPTER 4 • THE NETWORK LAYER
provide any feedback about congestion in the network? What is the abstract view
(properties) of the channel connecting the transport layer in the sending and receiving
hosts? The answers to these questions and others are determined by the service model
provided by the network layer. The network service model defines the characteristics
of end-to-end transport of packets between sending and receiving end systems.
Let’s now consider some possible services that the network layer could provide.
In the sending host, when the transport layer passes a packet to the network layer,
specific services that could be provided by the network layer include:
• Guaranteed delivery. This service guarantees that the packet will eventually
arrive at its destination.
• Guaranteed delivery with bounded delay. This service not only guarantees delivery
of the packet, but delivery within a specified host-to-host delay bound (for
example, within 100 msec).
Furthermore, the following services could be provided to a flow of packets between
a given source and destination:
• In-order packet delivery. This service guarantees that packets arrive at the destination
in the order that they were sent.
• Guaranteed minimal bandwidth. This network-layer service emulates the behavior
of a transmission link of a specified bit rate (for example, 1 Mbps) between sending
and receiving hosts. As long as the sending host transmits bits (as part of packets)
at a rate below the specified bit rate, then no packet is lost and each packet
arrives within a prespecified host-to-host delay (for example, within 40 msec).
• Guaranteed maximum jitter. This service guarantees that the amount of time
between the transmission of two successive packets at the sender is equal to the
amount of time between their receipt at the destination (or that this spacing
changes by no more than some specified value).
• Security services. Using a secret session key known only by a source and destination
host, the network layer in the source host could encrypt the payloads of
all datagrams being sent to the destination host. The network layer in the
destination host would then be responsible for decrypting the payloads. With
such a service, confidentiality would be provided to all transport-layer segments
(TCP and UDP) between the source and destination hosts. In addition to confidentiality,
the network layer could provide data integrity and source authentication
services.
This is only a partial list of services that a network layer could provide—there are
countless variations possible.
The Internet’s network layer provides a single service, known as best-effort
service. From Table 4.1, it might appear that best-effort service is a euphemism for
4.1 • INTRODUCTION 311
no service at all. With best-effort service, timing between packets is not guaranteed
to be preserved, packets are not guaranteed to be received in the order in which they
were sent, nor is the eventual delivery of transmitted packets guaranteed. Given this
definition, a network that delivered no packets to the destination would satisfy the
definition of best-effort delivery service. As we’ll discuss shortly, however, there
are sound reasons for such a minimalist network-layer service model.
Other network architectures have defined and implemented service models that
go beyond the Internet’s best-effort service. For example, the ATM network architecture
[MFAForum 2012, Black 1995] provides for multiple service models, meaning
that different connections can be provided with different classes of service
within the same network. A discussion of how an ATM network provides such services
is well beyond the scope of this book; our aim here is only to note that alternatives
do exist to the Internet’s best-effort model. Two of the more important ATM
service models are constant bit rate and available bit rate service:
• Constant bit rate (CBR) ATM network service. This was the first ATM service
model to be standardized, reflecting early interest by the telephone companies in
ATM and the suitability of CBR service for carrying real-time, constant bit rate
audio and video traffic. The goal of CBR service is conceptually simple—to provide
a flow of packets (known as cells in ATM terminology) with a virtual pipe
whose properties are the same as if a dedicated fixed-bandwidth transmission
link existed between sending and receiving hosts. With CBR service, a flow of
ATM cells is carried across the network in such a way that a cell’s end-to-end
delay, the variability in a cell’s end-to-end delay (that is, the jitter), and the fraction
of cells that are lost or delivered late are all guaranteed to be less than specified
values. These values are agreed upon by the sending host and the ATM network
when the CBR connection is first established.
312 CHAPTER 4 • THE NETWORK LAYER
Table 4.1  Internet, ATM CBR, and ATM ABR service models
Network
Architecture
Service
Model
Bandwidth
Guarantee
No-Loss
Guarantee Ordering Timing
Internet Best Effort None None Any order
possible
Not
maintained
ATM CBR Guaranteed
constant rate
Yes In order Maintained
ATM ABR Guaranteed
minimum
None In order Not
maintained
Congestion
Indication
None
Congestion
will not occur
Congestion
indication
provided
• Available bit rate (ABR) ATM network service. With the Internet offering socalled
best-effort service, ATM’s ABR might best be characterized as being a
slightly-better-than-best-effort service. As with the Internet service model,
cells may be lost under ABR service. Unlike in the Internet, however, cells
cannot be reordered (although they may be lost), and a minimum cell transmission
rate (MCR) is guaranteed to a connection using ABR service. If the network
has enough free resources at a given time, a sender may also be able to
send cells successfully at a higher rate than the MCR. Additionally, as we saw
in Section 3.6, ATM ABR service can provide feedback to the sender (in terms
of a congestion notification bit, or an explicit rate at which to send) that controls
how the sender adjusts its rate between the MCR and an allowable peak
cell rate.
4.2 Virtual Circuit and Datagram Networks
Recall from Chapter 3 that a transport layer can offer applications connectionless
service or connection-oriented service between two processes. For example, the Internet’s
transport layer provides each application a choice between two services: UDP, a
connectionless service; or TCP, a connection-oriented service. In a similar manner, a
network layer can provide connectionless service or connection service between two
hosts. Network-layer connection and connectionless services in many ways parallel
transport-layer connection-oriented and connectionless services. For example, a network-
layer connection service begins with handshaking between the source and destination
hosts; and a network-layer connectionless service does not have any
handshaking preliminaries.
Although the network-layer connection and connectionless services have some
parallels with transport-layer connection-oriented and connectionless services, there
are crucial differences:
• In the network layer, these services are host-to-host services provided by the network
layer for the transport layer. In the transport layer these services are processto-
process services provided by the transport layer for the application layer.
• In all major computer network architectures to date (Internet, ATM, frame relay,
and so on), the network layer provides either a host-to-host connectionless service
or a host-to-host connection service, but not both. Computer networks that
provide only a connection service at the network layer are called virtual-circuit
(VC) networks; computer networks that provide only a connectionless service
at the network layer are called datagram networks.
• The implementations of connection-oriented service in the transport layer and
the connection service in the network layer are fundamentally different. We saw
in the previous chapter that the transport-layer connection-oriented service is
4.2 • VIRTUAL CIRCUIT AND DATAGRAM NETWORKS 313
implemented at the edge of the network in the end systems; we’ll see shortly that
the network-layer connection service is implemented in the routers in the network
core as well as in the end systems.
Virtual-circuit and datagram networks are two fundamental classes of computer networks.
They use very different information in making their forwarding decisions.
Let’s now take a closer look at their implementations.
4.2.1 Virtual-Circuit Networks
While the Internet is a datagram network, many alternative network architectures—
including those of ATM and frame relay—are virtual-circuit networks and, therefore,
use connections at the network layer. These network-layer connections are
called virtual circuits (VCs). Let’s now consider how a VC service can be implemented
in a computer network.
A VC consists of (1) a path (that is, a series of links and routers) between the
source and destination hosts, (2) VC numbers, one number for each link along the
path, and (3) entries in the forwarding table in each router along the path. A packet
belonging to a virtual circuit will carry a VC number in its header. Because a virtual
circuit may have a different VC number on each link, each intervening router must
replace the VC number of each traversing packet with a new VC number. The new
VC number is obtained from the forwarding table.
To illustrate the concept, consider the network shown in Figure 4.3. The numbers
next to the links of R1 in Figure 4.3 are the link interface numbers. Suppose now that
Host A requests that the network establish a VC between itself and Host B. Suppose
also that the network chooses the path A-R1-R2-B and assigns VC numbers 12, 22,
and 32 to the three links in this path for this virtual circuit. In this case, when a packet
in this VC leaves Host A, the value in the VC number field in the packet header is 12;
when it leaves R1, the value is 22; and when it leaves R2, the value is 32.
How does the router determine the replacement VC number for a packet traversing
the router? For a VC network, each router’s forwarding table includes VC
314 CHAPTER 4 • THE NETWORK LAYER
A R1 R2 B
1 2
3
1 2
3
R3 R4
Figure 4.3  A simple virtual circuit network
4.2 • VIRTUAL CIRCUIT AND DATAGRAM NETWORKS 315
number translation; for example, the forwarding table in R1 might look something
like this:
Whenever a new VC is established across a router, an entry is added to the forwarding
table. Similarly, whenever a VC terminates, the appropriate entries in each table
along its path are removed.
You might be wondering why a packet doesn’t just keep the same VC number
on each of the links along its route. The answer is twofold. First, replacing the number
from link to link reduces the length of the VC field in the packet header. Second,
and more importantly, VC setup is considerably simplified by permitting a different
VC number at each link along the path of the VC. Specifically, with multiple VC
numbers, each link in the path can choose a VC number independently of the VC
numbers chosen at other links along the path. If a common VC number were required
for all links along the path, the routers would have to exchange and process a substantial
number of messages to agree on a common VC number (e.g., one that is not
being used by any other existing VC at these routers) to be used for a connection.
In a VC network, the network’s routers must maintain connection state information
for the ongoing connections. Specifically, each time a new connection is
established across a router, a new connection entry must be added to the router’s forwarding
table; and each time a connection is released, an entry must be removed
from the table. Note that even if there is no VC-number translation, it is still necessary
to maintain connection state information that associates VC numbers with output
interface numbers. The issue of whether or not a router maintains connection
state information for each ongoing connection is a crucial one—one that we’ll return
to repeatedly in this book.
There are three identifiable phases in a virtual circuit:
• VC setup. During the setup phase, the sending transport layer contacts the network
layer, specifies the receiver’s address, and waits for the network to set up
the VC. The network layer determines the path between sender and receiver, that
is, the series of links and routers through which all packets of the VC will travel.
The network layer also determines the VC number for each link along the path.
Finally, the network layer adds an entry in the forwarding table in each router
Incoming Interface Incoming VC # Outgoing Interface Outgoing VC #
1 12 2 22
2 63 1 18
3 7 2 17
1 97 3 87
... ... ... ...
along the path. During VC setup, the network layer may also reserve resources
(for example, bandwidth) along the path of the VC.
• Data transfer. As shown in Figure 4.4, once the VC has been established, packets
can begin to flow along the VC.
• VC teardown. This is initiated when the sender (or receiver) informs the network
layer of its desire to terminate the VC. The network layer will then typically
inform the end system on the other side of the network of the call termination
and update the forwarding tables in each of the packet routers on the path to indicate
that the VC no longer exists.
There is a subtle but important distinction between VC setup at the network
layer and connection setup at the transport layer (for example, the TCP three-way
handshake we studied in Chapter 3). Connection setup at the transport layer
involves only the two end systems. During transport-layer connection setup, the
two end systems alone determine the parameters (for example, initial sequence
number and flow-control window size) of their transport-layer connection.
Although the two end systems are aware of the transport-layer connection, the
routers within the network are completely oblivious to it. On the other hand, with
a VC network layer, routers along the path between the two end systems are
involved in VC setup, and each router is fully aware of all the VCs passing
through it.
The messages that the end systems send into the network to initiate or terminate a
VC, and the messages passed between the routers to set up the VC (that is, to modify
connection state in router tables) are known as signaling messages, and the protocols
316 CHAPTER 4 • THE NETWORK LAYER
Transport
Data link
Physical
Application
Network
Transport
Data link
Physical
Application
Network
1. Initiate call 2. Incoming call
5. Data flow
begins
6. Receive
data
4. Call connected 3. Accept call
Figure 4.4  Virtual-circuit setup
used to exchange these messages are often referred to as signaling protocols. VC setup
is shown pictorially in Figure 4.4. We’ll not cover VC signaling protocols in this book;
see [Black 1997] for a general discussion of signaling in connection-oriented networks
and [ITU-T Q.2931 1995] for the specification of ATM’s Q.2931 signaling protocol.
4.2.2 Datagram Networks
In a datagram network, each time an end system wants to send a packet, it stamps
the packet with the address of the destination end system and then pops the packet
into the network. As shown in Figure 4.5, there is no VC setup and routers do not
maintain any VC state information (because there are no VCs!).
As a packet is transmitted from source to destination, it passes through a series
of routers. Each of these routers uses the packet’s destination address to forward the
packet. Specifically, each router has a forwarding table that maps destination
addresses to link interfaces; when a packet arrives at the router, the router uses the
packet’s destination address to look up the appropriate output link interface in the
forwarding table. The router then intentionally forwards the packet to that output
link interface.
To get some further insight into the lookup operation, let’s look at a specific
example. Suppose that all destination addresses are 32 bits (which just happens to
be the length of the destination address in an IP datagram). A brute-force implementation
of the forwarding table would have one entry for every possible destination
address. Since there are more than 4 billion possible addresses, this option is totally
out of the question.
4.2 • VIRTUAL CIRCUIT AND DATAGRAM NETWORKS 317
Transport
1. Send
data
2. Receive
Data link data
Physical
Application
Network
Transport
Data link
Physical
Application
Network
Figure 4.5  Datagram network
Now let’s further suppose that our router has four links, numbered 0 through 3,
and that packets are to be forwarded to the link interfaces as follows:
Destination Address Range Link Interface
11001000 00010111 00010000 00000000
through 0
11001000 00010111 00010111 11111111
11001000 00010111 00011000 00000000
through 1
11001000 00010111 00011000 11111111
11001000 00010111 00011001 00000000
through 2
11001000 00010111 00011111 11111111
otherwise 3
Clearly, for this example, it is not necessary to have 4 billion entries in the router’s
forwarding table. We could, for example, have the following forwarding table with
just four entries:
Prefix Match Link Interface
11001000 00010111 00010 0
11001000 00010111 00011000 1
11001000 00010111 00011 2
otherwise 3
With this style of forwarding table, the router matches a prefix of the packet’s destination
address with the entries in the table; if there’s a match, the router forwards
the packet to a link associated with the match. For example, suppose the packet’s
destination address is 11001000 00010111 00010110 10100001; because the 21-bit
prefix of this address matches the first entry in the table, the router forwards the
packet to link interface 0. If a prefix doesn’t match any of the first three entries, then
the router forwards the packet to interface 3. Although this sounds simple enough,
there’s an important subtlety here. You may have noticed that it is possible for a destination
address to match more than one entry. For example, the first 24 bits of the
address 11001000 00010111 00011000 10101010 match the second entry in the
table, and the first 21 bits of the address match the third entry in the table. When
there are multiple matches, the router uses the longest prefix matching rule; that
is, it finds the longest matching entry in the table and forwards the packet to the link
318 CHAPTER 4 • THE NETWORK LAYER
interface associated with the longest prefix match. We’ll see exactly why this
longest prefix-matching rule is used when we study Internet addressing in more
detail in Section 4.4.
Although routers in datagram networks maintain no connection state information,
they nevertheless maintain forwarding state information in their forwarding
tables. However, the time scale at which this forwarding state information changes
is relatively slow. Indeed, in a datagram network the forwarding tables are modified
by the routing algorithms, which typically update a forwarding table every one-tofive
minutes or so. In a VC network, a forwarding table in a router is modified
whenever a new connection is set up through the router or whenever an existing
connection through the router is torn down. This could easily happen at a microsecond
timescale in a backbone, tier-1 router.
Because forwarding tables in datagram networks can be modified at any time, a
series of packets sent from one end system to another may follow different paths
through the network and may arrive out of order. [Paxson 1997] and [Jaiswal 2003]
present interesting measurement studies of packet reordering and other phenomena
in the public Internet.
4.2.3 Origins of VC and Datagram Networks
The evolution of datagram and VC networks reflects their origins. The notion of a
virtual circuit as a central organizing principle has its roots in the telephony world,
which uses real circuits. With call setup and per-call state being maintained at the
routers within the network, a VC network is arguably more complex than a datagram
network (although see [Molinero-Fernandez 2002] for an interesting comparison
of the complexity of circuit- versus packet-switched networks). This, too, is in
keeping with its telephony heritage. Telephone networks, by necessity, had their
complexity within the network, since they were connecting dumb end-system
devices such as rotary telephones. (For those too young to know, a rotary phone is
an analog telephone with no buttons—only a dial.)
The Internet as a datagram network, on the other hand, grew out of the need to
connect computers together. Given more sophisticated end-system devices, the
Internet architects chose to make the network-layer service model as simple as possible.
As we have already seen in Chapters 2 and 3, additional functionality (for
example, in-order delivery, reliable data transfer, congestion control, and DNS name
resolution) is then implemented at a higher layer, in the end systems. This inverts
the model of the telephone network, with some interesting consequences:
• Since the resulting Internet network-layer service model makes minimal (no!)
service guarantees, it imposes minimal requirements on the network layer. This
makes it easier to interconnect networks that use very different link-layer technologies
(for example, satellite, Ethernet, fiber, or radio) that have very different
transmission rates and loss characteristics. We will address the interconnection
of IP networks in detail in Section 4.4.
4.2 • VIRTUAL CIRCUIT AND DATAGRAM NETWORKS 319
• As we saw in Chapter 2, applications such as e-mail, the Web, and even some
network infrastructure services such as the DNS are implemented in hosts
(servers) at the network edge. The ability to add a new service simply by attaching
a host to the network and defining a new application-layer protocol (such as
HTTP) has allowed new Internet applications such as the Web to be deployed in
a remarkably short period of time.
4.3 What’s Inside a Router?
Now that we’ve overviewed the network layer’s services and functions, let’s turn
our attention to its forwarding function—the actual transfer of packets from a
router’s incoming links to the appropriate outgoing links at that router. We
already took a brief look at a few aspects of forwarding in Section 4.2, namely,
addressing and longest prefix matching. We mention here in passing that the terms
forwarding and switching are often used interchangeably by computer-networking
researchers and practitioners; we’ll use both terms interchangeably in this
textbook as well.
A high-level view of a generic router architecture is shown in Figure 4.6. Four
router components can be identified:
• Input ports. An input port performs several key functions. It performs the
physical layer function of terminating an incoming physical link at a router;
this is shown in the leftmost box of the input port and the rightmost box of the
output port in Figure 4.6. An input port also performs link-layer functions
needed to interoperate with the link layer at the other side of the incoming
link; this is represented by the middle boxes in the input and output ports. Perhaps
most crucially, the lookup function is also performed at the input port;
this will occur in the rightmost box of the input port. It is here that the forwarding
table is consulted to determine the router output port to which an
arriving packet will be forwarded via the switching fabric. Control packets
(for example, packets carrying routing protocol information) are forwarded
from an input port to the routing processor. Note that the term port here—
referring to the physical input and output router interfaces—is distinctly
different from the software ports associated with network applications and
sockets discussed in Chapters 2 and 3.
• Switching fabric. The switching fabric connects the router’s input ports to its
output ports. This switching fabric is completely contained within the router—
a network inside of a network router!
• Output ports. An output port stores packets received from the switching fabric
and transmits these packets on the outgoing link by performing the necessary
link-layer and physical-layer functions. When a link is bidirectional (that is,
320 CHAPTER 4 • THE NETWORK LAYER
carries traffic in both directions), an output port will typically be paired with the
input port for that link on the same line card (a printed circuit board containing
one or more input ports, which is connected to the switching fabric).
• Routing processor. The routing processor executes the routing protocols (which
we’ll study in Section 4.6), maintains routing tables and attached link state information,
and computes the forwarding table for the router. It also performs the
network management functions that we’ll study in Chapter 9.
Recall that in Section 4.1.1 we distinguished between a router’s forwarding and
routing functions. A router’s input ports, output ports, and switching fabric
together implement the forwarding function and are almost always implemented
in hardware, as shown in Figure 4.6. These forwarding functions are sometimes
collectively referred to as the router forwarding plane. To appreciate why a
hardware implementation is needed, consider that with a 10 Gbps input link and a
64-byte IP datagram, the input port has only 51.2 ns to process the datagram
before another datagram may arrive. If N ports are combined on a line card (as is
often done in practice), the datagram-processing pipeline must operate N times
faster—far too fast for software implementation. Forwarding plane hardware can
be implemented either using a router vendor’s own hardware designs, or constructed
using purchased merchant-silicon chips (e.g., as sold by companies such
as Intel and Broadcom).
While the forwarding plane operates at the nanosecond time scale, a router’s
control functions—executing the routing protocols, responding to attached links that
4.3 • WHAT’S INSIDE A ROUTER? 321
Input port Output port
Input port Output port
Routing
processor
Routing, management
control plane (software)
Forwarding
data plane (hardware)
Switch
fabric
Figure 4.6  Router architecture
go up or down, and performing management functions such as those we’ll study in
Chapter 9—operate at the millisecond or second timescale. These router control
plane functions are usually implemented in software and execute on the routing
processor (typically a traditional CPU).
Before delving into the details of a router’s control and data plane, let’s return to
our analogy of Section 4.1.1, where packet forwarding was compared to cars entering
and leaving an interchange. Let’s suppose that the interchange is a roundabout, and that
before a car enters the roundabout, a bit of processing is required—the car stops at an
entry station and indicates its final destination (not at the local roundabout, but the ultimate
destination of its journey). An attendant at the entry station looks up the final destination,
determines the roundabout exit that leads to that final destination, and tells the
driver which roundabout exit to take. The car enters the roundabout (which may be
filled with other cars entering from other input roads and heading to other roundabout
exits) and eventually leaves at the prescribed roundabout exit ramp, where it may
encounter other cars leaving the roundabout at that exit.
We can recognize the principal router components in Figure 4.6 in this analogy—
the entry road and entry station correspond to the input port (with a lookup
function to determine to local outgoing port); the roundabout corresponds to the
switch fabric; and the roundabout exit road corresponds to the output port. With
this analogy, it’s instructive to consider where bottlenecks might occur. What happens
if cars arrive blazingly fast (for example, the roundabout is in Germany or
Italy!) but the station attendant is slow? How fast must the attendant work to ensure
there’s no backup on an entry road? Even with a blazingly fast attendant, what happens
if cars traverse the roundabout slowly—can backups still occur? And what
happens if most of the entering cars all want to leave the roundabout at the same
exit ramp—can backups occur at the exit ramp or elsewhere? How should the
roundabout operate if we want to assign priorities to different cars, or block certain
cars from entering the roundabout in the first place? These are all analogous to critical
questions faced by router and switch designers.
In the following subsections, we’ll look at router functions in more detail. [Iyer
2008, Chao 2001; Chuang 2005; Turner 1988; McKeown 1997a; Partridge 1998]
provide a discussion of specific router architectures. For concreteness, the ensuing
discussion assumes a datagram network in which forwarding decisions are based
on the packet’s destination address (rather than a VC number in a virtual-circuit
network). However, the concepts and techniques are quite similar for a virtualcircuit
network.
4.3.1 Input Processing
A more detailed view of input processing is given in Figure 4.7. As discussed above,
the input port’s line termination function and link-layer processing implement the
physical and link layers for that individual input link. The lookup performed in the
input port is central to the router’s operation—it is here that the router uses the forwarding
table to look up the output port to which an arriving packet will be
322 CHAPTER 4 • THE NETWORK LAYER
forwarded via the switching fabric. The forwarding table is computed and updated
by the routing processor, with a shadow copy typically stored at each input port. The
forwarding table is copied from the routing processor to the line cards over a separate
bus (e.g., a PCI bus) indicated by the dashed line from the routing processor to
the input line cards in Figure 4.6. With a shadow copy, forwarding decisions can be
made locally, at each input port, without invoking the centralized routing processor
on a per-packet basis and thus avoiding a centralized processing bottleneck.
Given the existence of a forwarding table, lookup is conceptually simple—we just
search through the forwarding table looking for the longest prefix match, as described
4.3 • WHAT’S INSIDE A ROUTER? 323
Line
termination
Data link
processing
(protocol,
decapsulation)
Lookup, fowarding,
queuing Switch
fabric
Figure 4.7  Input port processing
CISCO SYSTEMS: DOMINATING THE NETWORK CORE
As of this writing 2012, Cisco employs more than 65,000 people. How did this
gorilla of a networking company come to be? It all started in 1984 in the living room
of a Silicon Valley apartment.
Len Bosak and his wife Sandy Lerner were working at Stanford University when they
had the idea to build and sell Internet routers to research and academic institutions, the
primary adopters of the Internet at that time. Sandy Lerner came up with the name Cisco
(an abbreviation for San Francisco), and she also designed the company’s bridge logo.
Corporate headquarters was their living room, and they financed the project with credit
cards and moonlighting consulting jobs. At the end of 1986, Cisco’s revenues reached
$250,000 a month. At the end of 1987, Cisco succeeded in attracting venture capital—
$2 million from Sequoia Capital in exchange for one-third of the company. Over the next
few years, Cisco continued to grow and grab more and more market share. At the same
time, relations between Bosak/Lerner and Cisco management became strained. Cisco
went public in 1990; in the same year Lerner and Bosak left the company.
Over the years, Cisco has expanded well beyond the router market, selling security,
wireless caching, Ethernet switch, datacenter infrastructure, video conferencing, and
voice-over IP products and services. However, Cisco is facing increased international
competition, including from Huawei, a rapidly growing Chinese network-gear company.
Other sources of competition for Cisco in the router and switched Ethernet space
include Alcatel-Lucent and Juniper.
CASE HISTORY
in Section 4.2.2. But at Gigabit transmission rates, this lookup must be performed in
nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram).
Thus, not only must lookup be performed in hardware, but techniques beyond a simple
linear search through a large table are needed; surveys of fast lookup algorithms can be
found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must also be paid to memory
access times, resulting in designs with embedded on-chip DRAM and faster SRAM
(used as a DRAM cache) memories. Ternary Content Address Memories (TCAMs) are
also often used for lookup. With a TCAM, a 32-bit IP address is presented to the memory,
which returns the content of the forwarding table entry for that address in essentially
constant time. The Cisco 8500 has a 64K CAM for each input port.
Once a packet’s output port has been determined via the lookup, the packet can
be sent into the switching fabric. In some designs, a packet may be temporarily
blocked from entering the switching fabric if packets from other input ports are currently
using the fabric. A blocked packet will be queued at the input port and then
scheduled to cross the fabric at a later point in time. We’ll take a closer look at the
blocking, queuing, and scheduling of packets (at both input ports and output ports)
in Section 4.3.4. Although “lookup” is arguably the most important action in input
port processing, many other actions must be taken: (1) physical- and link-layer processing
must occur, as discussed above; (2) the packet’s version number, checksum
and time-to-live field—all of which we’ll study in Section 4.4.1—must be checked
and the latter two fields rewritten; and (3) counters used for network management
(such as the number of IP datagrams received) must be updated.
Let’s close our discussion of input port processing by noting that the input port
steps of looking up an IP address (“match”) then sending the packet into the switching
fabric (“action”) is a specific case of a more general “match plus action” abstraction
that is performed in many networked devices, not just routers. In link-layer switches
(covered in Chapter 5), link-layer destination addresses are looked up and several
actions may be taken in addition to sending the frame into the switching fabric towards
the output port. In firewalls (covered in Chapter 8)—devices that filter out selected
incoming packets—an incoming packet whose header matches a given criteria (e.g., a
combination of source/destination IP addresses and transport-layer port numbers) may
be prevented from being forwarded (action). In a network address translator (NAT, covered
in Section 4.4), an incoming packet whose transport-layer port number matches a
given value will have its port number rewritten before forwarding (action). Thus, the
“match plus action” abstraction is both powerful and prevalent in network devices.
4.3.2 Switching
The switching fabric is at the very heart of a router, as it is through this fabric that
the packets are actually switched (that is, forwarded) from an input port to an output
port. Switching can be accomplished in a number of ways, as shown in Figure 4.8:
• Switching via memory. The simplest, earliest routers were traditional computers,
with switching between input and output ports being done under direct control of
324 CHAPTER 4 • THE NETWORK LAYER
the CPU (routing processor). Input and output ports functioned as traditional I/O
devices in a traditional operating system. An input port with an arriving packet
first signaled the routing processor via an interrupt. The packet was then copied
from the input port into processor memory. The routing processor then extracted
the destination address from the header, looked up the appropriate output port in
the forwarding table, and copied the packet to the output port’s buffers. In this
scenario, if the memory bandwidth is such that B packets per second can be written
into, or read from, memory, then the overall forwarding throughput (the total
rate at which packets are transferred from input ports to output ports) must be
less than B/2. Note also that two packets cannot be forwarded at the same time,
even if they have different destination ports, since only one memory read/write
over the shared system bus can be done at a time.
Many modern routers switch via memory. A major difference from early routers,
however, is that the lookup of the destination address and the storing of the packet
into the appropriate memory location are performed by processing on the input
line cards. In some ways, routers that switch via memory look very much like
shared-memory multiprocessors, with the processing on a line card switching
(writing) packets into the memory of the appropriate output port. Cisco’s Catalyst
8500 series switches [Cisco 8500 2012] forward packets via a shared memory.
4.3 • WHAT’S INSIDE A ROUTER? 325
Memory
A
B
C
X
Y
Z
Memory
Key:
Input port Output port
A
X Y Z
B
C
Crossbar
A
B
C
X
Y
Z
Bus
Figure 4.8  Three switching techniques
• Switching via a bus. In this approach, an input port transfers a packet directly to the
output port over a shared bus, without intervention by the routing processor. This is
typically done by having the input port pre-pend a switch-internal label (header) to
the packet indicating the local output port to which this packet is being transferred
and transmitting the packet onto the bus. The packet is received by all output ports,
but only the port that matches the label will keep the packet. The label is then
removed at the output port, as this label is only used within the switch to cross the
bus. If multiple packets arrive to the router at the same time, each at a different input
port, all but one must wait since only one packet can cross the bus at a time. Because
every packet must cross the single bus, the switching speed of the router is limited
to the bus speed; in our roundabout analogy, this is as if the roundabout could only
contain one car at a time. Nonetheless, switching via a bus is often sufficient for
routers that operate in small local area and enterprise networks. The Cisco 5600
[Cisco Switches 2012] switches packets over a 32 Gbps backplane bus.
• Switching via an interconnection network. One way to overcome the bandwidth
limitation of a single, shared bus is to use a more sophisticated interconnection network,
such as those that have been used in the past to interconnect processors in a
multiprocessor computer architecture. A crossbar switch is an interconnection network
consisting of 2N buses that connect N input ports to N output ports, as shown
in Figure 4.8. Each vertical bus intersects each horizontal bus at a crosspoint, which
can be opened or closed at any time by the switch fabric controller (whose logic is
part of the switching fabric itself). When a packet arrives from port A and needs to
be forwarded to port Y, the switch controller closes the crosspoint at the intersection
of busses Aand Y, and port Athen sends the packet onto its bus, which is picked up
(only) by bus Y. Note that a packet from port B can be forwarded to port X at the
same time, since the A-to-Y and B-to-X packets use different input and output
busses. Thus, unlike the previous two switching approaches, crossbar networks are
capable of forwarding multiple packets in parallel. However, if two packets from
two different input ports are destined to the same output port, then one will have to
wait at the input, since only one packet can be sent over any given bus at a time.
More sophisticated interconnection networks use multiple stages of switching
elements to allow packets from different input ports to proceed towards the same
output port at the same time through the switching fabric. See [Tobagi 1990] for
a survey of switch architectures. Cisco 12000 family switches [Cisco 12000
2012] use an interconnection network.
4.3.3 Output Processing
Output port processing, shown in Figure 4.9, takes packets that have been stored in
the output port’s memory and transmits them over the output link. This includes
selecting and de-queueing packets for transmission, and performing the needed linklayer
and physical-layer transmission functions.
326 CHAPTER 4 • THE NETWORK LAYER
4.3.4 Where Does Queueing Occur?
If we consider input and output port functionality and the configurations shown in
Figure 4.8, it’s clear that packet queues may form at both the input ports and the output
ports, just as we identified cases where cars may wait at the inputs and outputs of
the traffic intersection in our roundabout analogy. The location and extent of queueing
(either at the input port queues or the output port queues) will depend on the traffic
load, the relative speed of the switching fabric, and the line speed. Let’s now consider
these queues in a bit more detail, since as these queues grow large, the router’s memory
can eventually be exhausted and packet loss will occur when no memory is available
to store arriving packets. Recall that in our earlier discussions, we said that
packets were “lost within the network” or “dropped at a router.” It is here, at these
queues within a router, where such packets are actually dropped and lost.
Suppose that the input and output line speeds (transmission rates) all have an
identical transmission rate of Rline packets per second, and that there are N input
ports and N output ports. To further simplify the discussion, let’s assume that all
packets have the same fixed length, and the packets arrive to input ports in a synchronous
manner. That is, the time to send a packet on any link is equal to the time
to receive a packet on any link, and during such an interval of time, either zero or
one packet can arrive on an input link. Define the switching fabric transfer rate
Rswitch as the rate at which packets can be moved from input port to output port. If
Rswitch is N times faster than Rline, then only negligible queuing will occur at the
input ports. This is because even in the worst case, where all N input lines are
receiving packets, and all packets are to be forwarded to the same output port, each
batch of N packets (one packet per input port) can be cleared through the switch fabric
before the next batch arrives.
But what can happen at the output ports? Let’s suppose that Rswitch is still N
times faster than Rline. Once again, packets arriving at each of the N input ports
are destined to the same output port. In this case, in the time it takes to send a single
packet onto the outgoing link, N new packets will arrive at this output port. Since
the output port can transmit only a single packet in a unit of time (the packet transmission
time), the N arriving packets will have to queue (wait) for transmission over
the outgoing link. Then N more packets can possibly arrive in the time it takes to
4.3 • WHAT’S INSIDE A ROUTER? 327
Line
termination
Data link
processing
(protocol,
encapsulation)
Queuing (buffer
Switch management)
fabric
Figure 4.9  Output port processing
transmit just one of the N packets that had just previously been queued. And so on.
Eventually, the number of queued packets can grow large enough to exhaust available
memory at the output port, in which case packets are dropped.
Output port queuing is illustrated in Figure 4.10. At time t, a packet has arrived at
each of the incoming input ports, each destined for the uppermost outgoing port.
Assuming identical line speeds and a switch operating at three times the line speed,
one time unit later (that is, in the time needed to receive or send a packet), all three
original packets have been transferred to the outgoing port and are queued awaiting
transmission. In the next time unit, one of these three packets will have been transmitted
over the outgoing link. In our example, two new packets have arrived at the incoming
side of the switch; one of these packets is destined for this uppermost output port.
Given that router buffers are needed to absorb the fluctuations in traffic load, the
natural question to ask is how much buffering is required. For many years, the rule of
thumb [RFC 3439] for buffer sizing was that the amount of buffering (B) should be
equal to an average round-trip time (RTT, say 250 msec) times the link capacity (C).
This result is based on an analysis of the queueing dynamics of a relatively small number
of TCP flows [Villamizar 1994]. Thus, a 10 Gbps link with an RTT of 250 msec
would need an amount of buffering equal to B = RTT · C = 2.5 Gbits of buffers. Recent
328 CHAPTER 4 • THE NETWORK LAYER
Switch
fabric
Output port contention at time t
One packet time later
Switch
fabric
Figure 4.10  Output port queuing
theoretical and experimental efforts [Appenzeller 2004], however, suggest that when
there are a large number of TCP flows (N) passing through a link, the amount of buffering
needed is B = RTT  C/vN
—
. With a large number of flows typically passing through
large backbone router links (see, e.g., [Fraleigh 2003]), the value of N can be large, with
the decrease in needed buffer size becoming quite significant. [Appenzellar 2004; Wischik
2005; Beheshti 2008] provide very readable discussions of the buffer sizing problem
from a theoretical, implementation, and operational standpoint.
A consequence of output port queuing is that a packet scheduler at the output
port must choose one packet among those queued for transmission. This selection
might be done on a simple basis, such as first-come-first-served (FCFS) scheduling,
or a more sophisticated scheduling discipline such as weighted fair queuing (WFQ),
which shares the outgoing link fairly among the different end-to-end connections
that have packets queued for transmission. Packet scheduling plays a crucial role in
providing quality-of-service guarantees. We’ll thus cover packet scheduling extensively
in Chapter 7. A discussion of output port packet scheduling disciplines is
[Cisco Queue 2012].
Similarly, if there is not enough memory to buffer an incoming packet, a decision
must be made to either drop the arriving packet (a policy known as drop-tail) or
remove one or more already-queued packets to make room for the newly arrived
packet. In some cases, it may be advantageous to drop (or mark the header of) a packet
before the buffer is full in order to provide a congestion signal to the sender. Anumber
of packet-dropping and -marking policies (which collectively have become known as
active queue management (AQM) algorithms) have been proposed and analyzed
[Labrador 1999, Hollot 2002]. One of the most widely studied and implemented AQM
algorithms is the Random Early Detection (RED) algorithm. Under RED, a
weighted average is maintained for the length of the output queue. If the average
queue length is less than a minimum threshold, minth, when a packet arrives, the
packet is admitted to the queue. Conversely, if the queue is full or the average queue
length is greater than a maximum threshold, maxth, when a packet arrives, the packet
is marked or dropped. Finally, if the packet arrives to find an average queue length in
the interval [minth, maxth], the packet is marked or dropped with a probability that is
typically some function of the average queue length, minth, and maxth. A number of
probabilistic marking/dropping functions have been proposed, and various versions of
RED have been analytically modeled, simulated, and/or implemented. [Christiansen
2001] and [Floyd 2012] provide overviews and pointers to additional reading.
4.3 • WHAT’S INSIDE A ROUTER? 329
If the switch fabric is not fast enough (relative to the input line speeds) to transfer
all arriving packets through the fabric without delay, then packet queuing can also
occur at the input ports, as packets must join input port queues to wait their turn to be
transferred through the switching fabric to the output port. To illustrate an important
consequence of this queuing, consider a crossbar switching fabric and suppose that
(1) all link speeds are identical, (2) that one packet can be transferred from any one
input port to a given output port in the same amount of time it takes for a packet to be
received on an input link, and (3) packets are moved from a given input queue to their
desired output queue in an FCFS manner. Multiple packets can be transferred in parallel,
as long as their output ports are different. However, if two packets at the front of
two input queues are destined for the same output queue, then one of the packets will
be blocked and must wait at the input queue—the switching fabric can transfer only
one packet to a given output port at a time.
Figure 4.11 shows an example in which two packets (darkly shaded) at the front
of their input queues are destined for the same upper-right output port. Suppose that
the switch fabric chooses to transfer the packet from the front of the upper-left
queue. In this case, the darkly shaded packet in the lower-left queue must wait. But
not only must this darkly shaded packet wait, so too must the lightly shaded packet
that is queued behind that packet in the lower-left queue, even though there is no
contention for the middle-right output port (the destination for the lightly shaded
packet). This phenomenon is known as head-of-the-line (HOL) blocking in an
330 CHAPTER 4 • THE NETWORK LAYER
Switch
fabric
Output port contention at time t—
one dark packet can be transferred
Light blue packet experiences HOL blocking
Switch
fabric
Key:
destined for upper output
port
destined for middle output
port
destined for lower output
port
Figure 4.11  HOL blocking at an input queued switch
input-queued switch—a queued packet in an input queue must wait for transfer
through the fabric (even though its output port is free) because it is blocked by
another packet at the head of the line. [Karol 1987] shows that due to HOL blocking,
the input queue will grow to unbounded length (informally, this is equivalent to
saying that significant packet loss will occur) under certain assumptions as soon as
the packet arrival rate on the input links reaches only 58 percent of their capacity. A
number of solutions to HOL blocking are discussed in [McKeown 1997b].
4.3.5 The Routing Control Plane
In our discussion thus far and in Figure 4.6, we’ve implicitly assumed that the routing
control plane fully resides and executes in a routing processor within the router.
The network-wide routing control plane is thus decentralized—with different pieces
(e.g., of a routing algorithm) executing at different routers and interacting by sending
control messages to each other. Indeed, today’s Internet routers and the routing
algorithms we’ll study in Section 4.6 operate in exactly this manner. Additionally,
router and switch vendors bundle their hardware data plane and software control
plane together into closed (but inter-operable) platforms in a vertically integrated
product.
Recently, a number of researchers [Caesar 2005a, Casado 2009, McKeown
2008] have begun exploring new router control plane architectures in which part of
the control plane is implemented in the routers (e.g., local measurement/reporting of
link state, forwarding table installation and maintenance) along with the data plane,
and part of the control plane can be implemented externally to the router (e.g., in a
centralized server, which could perform route calculation). A well-defined API dictates
how these two parts interact and communicate with each other. These
researchers argue that separating the software control plane from the hardware data
plane (with a minimal router-resident control plane) can simplify routing by replacing
distributed routing calculation with centralized routing calculation, and enable
network innovation by allowing different customized control planes to operate over
fast hardware data planes.
4.4 The Internet Protocol (IP): Forwarding and
Addressing in the Internet
Our discussion of network-layer addressing and forwarding thus far has been
without reference to any specific computer network. In this section, we’ll turn our
attention to how addressing and forwarding are done in the Internet. We’ll see that
Internet addressing and forwarding are important components of the Internet
Protocol (IP). There are two versions of IP in use today. We’ll first examine the
widely deployed IP protocol version 4, which is usually referred to simply as IPv4
4.4 • THE INTERNET PROTOCOL (IP) 331
[RFC 791]. We’ll examine IP version 6 [RFC 2460; RFC 4291], which has been
proposed to replace IPv4, at the end of this section.
But before beginning our foray into IP, let’s take a step back and consider the
components that make up the Internet’s network layer. As shown in Figure 4.12,
the Internet’s network layer has three major components. The first component is
the IP protocol, the topic of this section. The second major component is the routing
component, which determines the path a datagram follows from source to destination.
We mentioned earlier that routing protocols compute the forwarding
tables that are used to forward packets through the network. We’ll study the
Internet’s routing protocols in Section 4.6. The final component of the network
layer is a facility to report errors in datagrams and respond to requests for certain
network-layer information. We’ll cover the Internet’s network-layer error- and
information-reporting protocol, the Internet Control Message Protocol (ICMP), in
Section 4.4.3.
4.4.1 Datagram Format
Recall that a network-layer packet is referred to as a datagram. We begin our study
of IP with an overview of the syntax and semantics of the IPv4 datagram. You
might be thinking that nothing could be drier than the syntax and semantics of a
packet’s bits. Nevertheless, the datagram plays a central role in the Internet—every
networking student and professional needs to see it, absorb it, and master it. The
332 CHAPTER 4 • THE NETWORK LAYER
Routing protocols
• path selection
• RIP, OSPF, BGP
IP protocol
• addressing conventions
• datagram format
• packet handling
conventions
ICMP protocol
• error reporting
• router “signaling”
Forwarding
table
Transport layer: TCP, UDP
Link layer
Physical layer
Network layer
Figure 4.12  A look inside the Internet’s network layer
IPv4 datagram format is shown in Figure 4.13. The key fields in the IPv4 datagram
are the following:
• Version number. These 4 bits specify the IP protocol version of the datagram.
By looking at the version number, the router can determine how to interpret
the remainder of the IP datagram. Different versions of IP use different datagram
formats. The datagram format for the current version of IP, IPv4, is
shown in Figure 4.13. The datagram format for the new version of IP (IPv6) is
discussed at the end of this section.
• Header length. Because an IPv4 datagram can contain a variable number of
options (which are included in the IPv4 datagram header), these 4 bits are needed
to determine where in the IP datagram the data actually begins. Most IP datagrams
do not contain options, so the typical IP datagram has a 20-byte header.
• Type of service. The type of service (TOS) bits were included in the IPv4 header
to allow different types of IP datagrams (for example, datagrams particularly
requiring low delay, high throughput, or reliability) to be distinguished from each
other. For example, it might be useful to distinguish real-time datagrams (such as
those used by an IP telephony application) from non-real-time traffic (for example,
FTP). The specific level of service to be provided is a policy issue determined
by the router’s administrator. We’ll explore the topic of differentiated
service in Chapter 7.
4.4 • THE INTERNET PROTOCOL (IP) 333
Version Header Type of service
length
Upper-layer
protocol
16-bit Identifier
Time-to-live
Flags 13-bit Fragmentation offset
Datagram length (bytes)
Header checksum
32 bits
32-bit Source IP address
32-bit Destination IP address
Options (if any)
Data
Figure 4.13  IPv4 datagram format
• Datagram length. This is the total length of the IP datagram (header plus data),
measured in bytes. Since this field is 16 bits long, the theoretical maximum size
of the IP datagram is 65,535 bytes. However, datagrams are rarely larger than
1,500 bytes.
• Identifier, flags, fragmentation offset. These three fields have to do with so-called
IP fragmentation, a topic we will consider in depth shortly. Interestingly, the new
version of IP, IPv6, does not allow for fragmentation at routers.
• Time-to-live. The time-to-live (TTL) field is included to ensure that datagrams
do not circulate forever (due to, for example, a long-lived routing loop) in the
network. This field is decremented by one each time the datagram is processed
by a router. If the TTL field reaches 0, the datagram must be dropped.
• Protocol. This field is used only when an IP datagram reaches its final destination.
The value of this field indicates the specific transport-layer protocol to
which the data portion of this IP datagram should be passed. For example, a
value of 6 indicates that the data portion is passed to TCP, while a value of 17
indicates that the data is passed to UDP. For a list of all possible values, see
[IANA Protocol Numbers 2012]. Note that the protocol number in the IP datagram
has a role that is analogous to the role of the port number field in the transportlayer
segment. The protocol number is the glue that binds the network and transport
layers together, whereas the port number is the glue that binds the transport and
application layers together. We’ll see in Chapter 5 that the link-layer frame also
has a special field that binds the link layer to the network layer.
• Header checksum. The header checksum aids a router in detecting bit errors in a
received IP datagram. The header checksum is computed by treating each 2 bytes
in the header as a number and summing these numbers using 1s complement
arithmetic. As discussed in Section 3.3, the 1s complement of this sum, known
as the Internet checksum, is stored in the checksum field. A router computes the
header checksum for each received IP datagram and detects an error condition if
the checksum carried in the datagram header does not equal the computed checksum.
Routers typically discard datagrams for which an error has been detected.
Note that the checksum must be recomputed and stored again at each router, as
the TTL field, and possibly the options field as well, may change. An interesting
discussion of fast algorithms for computing the Internet checksum is [RFC
1071]. A question often asked at this point is, why does TCP/IP perform error
checking at both the transport and network layers? There are several reasons for
this repetition. First, note that only the IP header is checksummed at the IP layer,
while the TCP/UDP checksum is computed over the entire TCP/UDP segment.
Second, TCP/UDP and IP do not necessarily both have to belong to the same protocol
stack. TCP can, in principle, run over a different protocol (for example,
ATM) and IP can carry data that will not be passed to TCP/UDP.
• Source and destination IP addresses. When a source creates a datagram, it inserts
its IP address into the source IP address field and inserts the address of the
334 CHAPTER 4 • THE NETWORK LAYER
ultimate destination into the destination IP address field. Often the source host
determines the destination address via a DNS lookup, as discussed in Chapter 2.
We’ll discuss IP addressing in detail in Section 4.4.2.
• Options. The options fields allow an IP header to be extended. Header options
were meant to be used rarely—hence the decision to save overhead by not
including the information in options fields in every datagram header. However,
the mere existence of options does complicate matters—since datagram headers
can be of variable length, one cannot determine a priori where the data field will
start. Also, since some datagrams may require options processing and others may
not, the amount of time needed to process an IP datagram at a router can vary
greatly. These considerations become particularly important for IP processing in
high-performance routers and hosts. For these reasons and others, IP options
were dropped in the IPv6 header, as discussed in Section 4.4.4.
• Data (payload). Finally, we come to the last and most important field—the raison
d’être for the datagram in the first place! In most circumstances, the data
field of the IP datagram contains the transport-layer segment (TCP or UDP) to
be delivered to the destination. However, the data field can carry other types of
data, such as ICMP messages (discussed in Section 4.4.3).
Note that an IP datagram has a total of 20 bytes of header (assuming no options). If
the datagram carries a TCP segment, then each (nonfragmented) datagram carries a
total of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) along
with the application-layer message.
IP Datagram Fragmentation
We’ll see in Chapter 5 that not all link-layer protocols can carry network-layer packets
of the same size. Some protocols can carry big datagrams, whereas other protocols
can carry only little packets. For example, Ethernet frames can carry up to 1,500
bytes of data, whereas frames for some wide-area links can carry no more than 576
bytes. The maximum amount of data that a link-layer frame can carry is called the
maximum transmission unit (MTU). Because each IP datagram is encapsulated
within the link-layer frame for transport from one router to the next router, the MTU
of the link-layer protocol places a hard limit on the length of an IP datagram. Having
a hard limit on the size of an IP datagram is not much of a problem. What is a problem
is that each of the links along the route between sender and destination can use
different link-layer protocols, and each of these protocols can have different MTUs.
To understand the forwarding issue better, imagine that you are a router that
interconnects several links, each running different link-layer protocols with different
MTUs. Suppose you receive an IP datagram from one link. You check your forwarding
table to determine the outgoing link, and this outgoing link has an MTU
that is smaller than the length of the IP datagram. Time to panic—how are you going
to squeeze this oversized IP datagram into the payload field of the link-layer frame?
4.4 • THE INTERNET PROTOCOL (IP) 335
The solution is to fragment the data in the IP datagram into two or more smaller IP
datagrams, encapsulate each of these smaller IP datagrams in a separate link-layer
frame; and send these frames over the outgoing link. Each of these smaller datagrams
is referred to as a fragment.
Fragments need to be reassembled before they reach the transport layer at the destination.
Indeed, both TCP and UDP are expecting to receive complete, unfragmented
segments from the network layer. The designers of IPv4 felt that reassembling datagrams
in the routers would introduce significant complication into the protocol and
put a damper on router performance. (If you were a router, would you want to be
reassembling fragments on top of everything else you had to do?) Sticking to the principle
of keeping the network core simple, the designers of IPv4 decided to put the job
of datagram reassembly in the end systems rather than in network routers.
When a destination host receives a series of datagrams from the same source, it
needs to determine whether any of these datagrams are fragments of some original,
larger datagram. If some datagrams are fragments, it must further determine when it
has received the last fragment and how the fragments it has received should be
pieced back together to form the original datagram. To allow the destination host to
perform these reassembly tasks, the designers of IP (version 4) put identification,
flag, and fragmentation offset fields in the IP datagram header. When a datagram is
created, the sending host stamps the datagram with an identification number as well
as source and destination addresses. Typically, the sending host increments the identification
number for each datagram it sends. When a router needs to fragment a
datagram, each resulting datagram (that is, fragment) is stamped with the source
address, destination address, and identification number of the original datagram.
When the destination receives a series of datagrams from the same sending host, it
can examine the identification numbers of the datagrams to determine which of the
datagrams are actually fragments of the same larger datagram. Because IP is an
unreliable service, one or more of the fragments may never arrive at the destination.
For this reason, in order for the destination host to be absolutely sure it has received
the last fragment of the original datagram, the last fragment has a flag bit set to 0,
whereas all the other fragments have this flag bit set to 1. Also, in order for the destination
host to determine whether a fragment is missing (and also to be able to
reassemble the fragments in their proper order), the offset field is used to specify
where the fragment fits within the original IP datagram.
Figure 4.14 illustrates an example. A datagram of 4,000 bytes (20 bytes of IP
header plus 3,980 bytes of IP payload) arrives at a router and must be forwarded to
a link with an MTU of 1,500 bytes. This implies that the 3,980 data bytes in the
original datagram must be allocated to three separate fragments (each of which is
also an IP datagram). Suppose that the original datagram is stamped with an identification
number of 777. The characteristics of the three fragments are shown in
Table 4.2. The values in Table 4.2 reflect the requirement that the amount of original
payload data in all but the last fragment be a multiple of 8 bytes, and that the offset
value be specified in units of 8-byte chunks.
336 CHAPTER 4 • THE NETWORK LAYER
4.4 • THE INTERNET PROTOCOL (IP) 337
Fragmentation:
In: one large datagram (4,000 bytes)
Out: 3 smaller datagrams
Reassembly:
In: 3 smaller datagrams
Out: one large datagram (4,000 bytes)
Link MTU: 1,500 bytes
Figure 4.14  IP fragmentation and reassembly
Table 4.2  IP fragments
Fragment Bytes ID Offset Flag
1st fragment 1,480 bytes in
the data field of
the IP datagram
identification  777 offset  0 (meaning the data
should be inserted beginning
at byte 0)
2nd fragment 1,480 bytes
of data
identification  777 offset  185 (meaning the data
should be inserted beginning at byte
1,480. Note that 185 · 8  1,480)
3rd fragment 1,020 bytes
( 3,980–1,480–1,480)
of data
identification  777 offset  370 (meaning the data
should be inserted beginning at byte
2,960. Note that 370 · 8  2,960)
flag  1 (meaning
there is more)
flag  1 (meaning
there is more)
flag  0 (meaning this
is the last fragment)
At the destination, the payload of the datagram is passed to the transport layer
only after the IP layer has fully reconstructed the original IP datagram. If one or
more of the fragments does not arrive at the destination, the incomplete datagram is
discarded and not passed to the transport layer. But, as we learned in the previous
chapter, if TCP is being used at the transport layer, then TCP will recover from this
loss by having the source retransmit the data in the original datagram.
We have just learned that IP fragmentation plays an important role in gluing
together the many disparate link-layer technologies. But fragmentation also has its
costs. First, it complicates routers and end systems, which need to be designed to
accommodate datagram fragmentation and reassembly. Second, fragmentation can
be used to create lethal DoS attacks, whereby the attacker sends a series of bizarre
and unexpected fragments. A classic example is the Jolt2 attack, where the attacker
sends a stream of small fragments to the target host, none of which has an offset of
zero. The target can collapse as it attempts to rebuild datagrams out of the degenerate
packets. Another class of exploits sends overlapping IP fragments, that is, fragments
whose offset values are set so that the fragments do not align properly.
Vulnerable operating systems, not knowing what to do with overlapping fragments,
can crash [Skoudis 2006]. As we’ll see at the end of this section, a new version of
the IP protocol, IPv6, does away with fragmentation altogether, thereby streamlining
IP packet processing and making IP less vulnerable to attack.
At this book’s Web site, we provide a Java applet that generates fragments. You
provide the incoming datagram size, the MTU, and the incoming datagram identification.
The applet automatically generates the fragments for you. See http://
www.awl.com/kurose-ross.
4.4.2 IPv4 Addressing
We now turn our attention to IPv4 addressing. Although you may be thinking that
addressing must be a straightforward topic, hopefully by the end of this chapter
you’ll be convinced that Internet addressing is not only a juicy, subtle, and interesting
topic but also one that is of central importance to the Internet. Excellent treatments
of IPv4 addressing are [3Com Addressing 2012] and the first chapter in
[Stewart 1999].
Before discussing IP addressing, however, we’ll need to say a few words about
how hosts and routers are connected into the network. A host typically has only a
single link into the network; when IP in the host wants to send a datagram, it does
so over this link. The boundary between the host and the physical link is called an
interface. Now consider a router and its interfaces. Because a router’s job is to
receive a datagram on one link and forward the datagram on some other link, a
router necessarily has two or more links to which it is connected. The boundary
between the router and any one of its links is also called an interface. A router thus
has multiple interfaces, one for each of its links. Because every host and router is
capable of sending and receiving IP datagrams, IP requires each host and router
interface to have its own IP address. Thus, an IP address is technically associated
with an interface, rather than with the host or router containing that interface.
Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total
of 232 possible IP addresses. By approximating 210 by 103, it is easy to see that there
338 CHAPTER 4 • THE NETWORK LAYER
are about 4 billion possible IP addresses. These addresses are typically written in
so-called dotted-decimal notation, in which each byte of the address is written in
its decimal form and is separated by a period (dot) from other bytes in the address.
For example, consider the IP address 193.32.216.9. The 193 is the decimal equivalent
of the first 8 bits of the address; the 32 is the decimal equivalent of the second
8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary notation is
11000001 00100000 11011000 00001001
Each interface on every host and router in the global Internet must have an IP
address that is globally unique (except for interfaces behind NATs, as discussed at
the end of this section). These addresses cannot be chosen in a willy-nilly manner,
however. A portion of an interface’s IP address will be determined by the subnet to
which it is connected.
Figure 4.15 provides an example of IP addressing and interfaces. In this figure,
one router (with three interfaces) is used to interconnect seven hosts. Take a close look
at the IP addresses assigned to the host and router interfaces, as there are several things
to notice. The three hosts in the upper-left portion of Figure 4.15, and the router interface
to which they are connected, all have an IP address of the form 223.1.1.xxx. That
is, they all have the same leftmost 24 bits in their IP address. The four interfaces are
also interconnected to each other by a network that contains no routers. This network
4.4 • THE INTERNET PROTOCOL (IP) 339
223.1.1.1
223.1.2.1
223.1.2.2
223.1.1.2
223.1.1.4 223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1 223.1.3.2
Figure 4.15  Interface addresses and subnets
could be interconnected by an Ethernet LAN, in which case the interfaces would be
interconnected by an Ethernet switch (as we’ll discuss in Chapter 5), or by a wireless
access point (as we’ll discuss in Chapter 6). We’ll represent this routerless network
connecting these hosts as a cloud for now, and dive into the internals of such networks
in Chapters 5 and 6.
In IP terms, this network interconnecting three host interfaces and one router
interface forms a subnet [RFC 950]. (A subnet is also called an IP network or
simply a network in the Internet literature.) IP addressing assigns an address to this
subnet: 223.1.1.0/24, where the /24 notation, sometimes known as a subnet mask,
indicates that the leftmost 24 bits of the 32-bit quantity define the subnet
address. The subnet 223.1.1.0/24 thus consists of the three host interfaces
(223.1.1.1, 223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4). Any additional
hosts attached to the 223.1.1.0/24 subnet would be required to have an
address of the form 223.1.1.xxx. There are two additional subnets shown in Figure
4.15: the 223.1.2.0/24 network and the 223.1.3.0/24 subnet. Figure 4.16 illustrates
the three IP subnets present in Figure 4.15.
The IP definition of a subnet is not restricted to Ethernet segments that connect
multiple hosts to a router interface. To get some insight here, consider Figure 4.17,
which shows three routers that are interconnected with each other by point-to-point
links. Each router has three interfaces, one for each point-to-point link and one for
the broadcast link that directly connects the router to a pair of hosts. What subnets
are present here? Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are
similar to the subnets we encountered in Figure 4.15. But note that there are three
340 CHAPTER 4 • THE NETWORK LAYER
223.1.1.0/23
223.1.2.0/23
223.1.3.0/23
Figure 4.16  Subnet addresses
additional subnets in this example as well: one subnet, 223.1.9.0/24, for the interfaces
that connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces that
connect routers R2 and R3; and a third subnet, 223.1.7.0/24, for the interfaces that
connect routers R3 and R1. For a general interconnected system of routers and hosts,
we can use the following recipe to define the subnets in the system:
To determine the subnets, detach each interface from its host or router, creating
islands of isolated networks, with interfaces terminating the end points of the
isolated networks. Each of these isolated networks is called a subnet.
If we apply this procedure to the interconnected system in Figure 4.17, we get six
islands or subnets.
From the discussion above, it’s clear that an organization (such as a company
or academic institution) with multiple Ethernet segments and point-to-point links
will have multiple subnets, with all of the devices on a given subnet having the same
subnet address. In principle, the different subnets could have quite different subnet
addresses. In practice, however, their subnet addresses often have much in common.
To understand why, let’s next turn our attention to how addressing is handled in the
global Internet.
4.4 • THE INTERNET PROTOCOL (IP) 341
223.1.8.1 223.1.8.0
223.1.9.1 223.1.7.1
223.1.2.6
223.1.2.1 223.1.2.2 223.1.3.1 223.1.3.2
223.1.1.3
223.1.9.2 223.1.7.0
223.1.3.27
223.1.1.1 223.1.1.4
R1
R2 R3
Figure 4.17  Three routers interconnecting six subnets
The Internet’s address assignment strategy is known as Classless Interdomain
Routing (CIDR—pronounced cider) [RFC 4632]. CIDR generalizes the notion of
subnet addressing. As with subnet addressing, the 32-bit IP address is divided into
two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the
number of bits in the first part of the address.
The x most significant bits of an address of the form a.b.c.d/x constitute the
network portion of the IP address, and are often referred to as the prefix (or network
prefix) of the address. An organization is typically assigned a block of contiguous
addresses, that is, a range of addresses with a common prefix (see the
Principles in Practice sidebar). In this case, the IP addresses of devices within the
organization will share the common prefix. When we cover the Internet’s BGP
342 CHAPTER 4 • THE NETWORK LAYER
This example of an ISP that connects eight organizations to the Internet nicely illustrates
how carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure
4.18, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it
should be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The
rest of the world need not know that within the address block 200.23.16.0/20 there are
in fact eight other organizations, each with its own subnets. This ability to use a single prefix
to advertise multiple networks is often referred to as address aggregation (also
route aggregation or route summarization).
Address aggregation works extremely well when addresses are allocated in blocks to
ISPs and then from ISPs to client organizations. But what happens when addresses are
not allocated in such a hierarchical manner? What would happen, for example, if Fly-By-
Night-ISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet through
its subsidiary ISPs-R-Us? As shown in Figure 4.18, the subsidiary ISPs-R-Us owns the
address block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately outside
of this address block. What should be done here? Certainly, Organization 1 could
renumber all of its routers and hosts to have addresses within the ISPs-R-Us address
block. But this is a costly solution, and Organization 1 might well be reassigned to
another subsidiary in the future. The solution typically adopted is for Organization 1
to keep its IP addresses in 200.23.18.0/23. In this case, as shown in Figure 4.19,
Fly-By-Night-ISP continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us
continues to advertise 199.31.0.0/16. However, ISPs-R-Us now also advertises the block
of addresses for Organization 1, 200.23.18.0/23. When other routers in the larger
Internet see the address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and
200.23.18.0/23 (from ISPs-R-Us) and want to route to an address in the block
200.23.18.0/23, they will use longest prefix matching (see Section 4.2.2), and route
toward ISPs-R-Us, as it advertises the longest (most specific) address prefix that matches
the destination address.
PRINCIPLES IN PRACTICE
4.4 • THE INTERNET PROTOCOL (IP) 343
Organization 0
200.23.16.0/23
Organization 1
Fly-By-Night-ISP
“Send me anything
with addresses
beginning
200.23.16.0/20”
ISPs-R-Us
200.23.18.0/23
Organization 2
200.23.20.0/23
Organization 7
200.23.30.0/23
Internet
“Send me anything
with addresses
beginning
199.31.0.0/16”
Figure 4.18  Hierarchical addressing and route aggregation
Organization 0
200.23.16.0/23
Organization 2
Fly-By-Night-ISP
“Send me anything
with addresses
beginning
200.23.16.0/20”
ISPs-R-Us
200.23.20.0/23
Organization 7
200.23.30.0/23
Organization 1
200.23.18.0/23
Internet
“Send me anything
with addresses
beginning
199.31.0.0/16 or
200.23.18.0/23”
Figure 4.19  ISPs-R-Us has a more specific route to Organization 1
routing protocol in Section 4.6, we’ll see that only these x leading prefix bits are
considered by routers outside the organization’s network. That is, when a router
outside the organization forwards a datagram whose destination address is inside
the organization, only the leading x bits of the address need be considered. This
considerably reduces the size of the forwarding table in these routers, since a single
entry of the form a.b.c.d/x will be sufficient to forward packets to any destination
within the organization.
The remaining 32-x bits of an address can be thought of as distinguishing
among the devices within the organization, all of which have the same network prefix.
These are the bits that will be considered when forwarding packets at routers
within the organization. These lower-order bits may (or may not) have an additional
subnetting structure, such as that discussed above. For example, suppose the first 21
bits of the CIDRized address a.b.c.d/21 specify the organization’s network prefix
and are common to the IP addresses of all devices in that organization. The remaining
11 bits then identify the specific hosts in the organization. The organization’s
internal structure might be such that these 11 rightmost bits are used for subnetting
within the organization, as discussed above. For example, a.b.c.d/24 might refer to a
specific subnet within the organization.
Before CIDR was adopted, the network portions of an IP address were constrained
to be 8, 16, or 24 bits in length, an addressing scheme known as classful
addressing, since subnets with 8-, 16-, and 24-bit subnet addresses were known as
class A, B, and C networks, respectively. The requirement that the subnet portion of
an IP address be exactly 1, 2, or 3 bytes long turned out to be problematic for supporting
the rapidly growing number of organizations with small and medium-sized
subnets. A class C (/24) subnet could accommodate only up to 28 – 2 = 254 hosts
(two of the 28 = 256 addresses are reserved for special use)—too small for many
organizations. However, a class B (/16) subnet, which supports up to 65,634 hosts,
was too large. Under classful addressing, an organization with, say, 2,000 hosts was
typically allocated a class B (/16) subnet address. This led to a rapid depletion of the
class B address space and poor utilization of the assigned address space. For example,
the organization that used a class B address for its 2,000 hosts was allocated
enough of the address space for up to 65,534 interfaces—leaving more than 63,000
addresses that could not be used by other organizations.
We would be remiss if we did not mention yet another type of IP address, the IP
broadcast address 255.255.255.255. When a host sends a datagram with destination
address 255.255.255.255, the message is delivered to all hosts on the same subnet.
Routers optionally forward the message into neighboring subnets as well (although
they usually don’t).
Having now studied IP addressing in detail, we need to know how hosts and
subnets get their addresses in the first place. Let’s begin by looking at how an
organization gets a block of addresses for its devices, and then look at how a device
(such as a host) is assigned an address from within the organization’s block of
addresses.
344 CHAPTER 4 • THE NETWORK LAYER
Obtaining a Block of Addresses
In order to obtain a block of IP addresses for use within an organization’s subnet, a
network administrator might first contact its ISP, which would provide addresses
from a larger block of addresses that had already been allocated to the ISP. For
example, the ISP may itself have been allocated the address block 200.23.16.0/20.
The ISP, in turn, could divide its address block into eight equal-sized contiguous
address blocks and give one of these address blocks out to each of up to eight organizations
that are supported by this ISP, as shown below. (We have underlined the
subnet part of these addresses for your convenience.)
ISP’s block 200.23.16.0/20 11001000 00010111 00010000 00000000
Organization 0 200.23.16.0/23 11001000 00010111 00010000 00000000
Organization 1 200.23.18.0/23 11001000 00010111 00010010 00000000
Organization 2 200.23.20.0/23 11001000 00010111 00010100 00000000
. . . . . . . . .
Organization 7 200.23.30.0/23 11001000 00010111 00011110 00000000
While obtaining a set of addresses from an ISP is one way to get a block of
addresses, it is not the only way. Clearly, there must also be a way for the ISP itself
to get a block of addresses. Is there a global authority that has ultimate responsibility
for managing the IP address space and allocating address blocks to ISPs and other
organizations? Indeed there is! IP addresses are managed under the authority of the
Internet Corporation for Assigned Names and Numbers (ICANN) [ICANN 2012],
based on guidelines set forth in [RFC 2050]. The role of the nonprofit ICANN organization
[NTIA 1998] is not only to allocate IP addresses, but also to manage the DNS
root servers. It also has the very contentious job of assigning domain names and
resolving domain name disputes. The ICANN allocates addresses to regional Internet
registries (for example, ARIN, RIPE, APNIC, and LACNIC, which together
form the Address Supporting Organization of ICANN [ASO-ICANN 2012]), and
handle the allocation/management of addresses within their regions.
Obtaining a Host Address: the Dynamic Host Configuration Protocol
Once an organization has obtained a block of addresses, it can assign individual IP
addresses to the host and router interfaces in its organization. A system administrator
will typically manually configure the IP addresses into the router (often
remotely, with a network management tool). Host addresses can also be configured
manually, but more often this task is now done using the Dynamic Host Configuration
Protocol (DHCP) [RFC 2131]. DHCP allows a host to obtain (be allocated)
an IP address automatically. A network administrator can configure DHCP so that a
4.4 • THE INTERNET PROTOCOL (IP) 345
given host receives the same IP address each time it connects to the network, or a
host may be assigned a temporary IP address that will be different each time the
host connects to the network. In addition to host IP address assignment, DHCP also
allows a host to learn additional information, such as its subnet mask, the address of
its first-hop router (often called the default gateway), and the address of its local
DNS server.
Because of DHCP’s ability to automate the network-related aspects of connecting
a host into a network, it is often referred to as a plug-and-play protocol. This
capability makes it very attractive to the network administrator who would otherwise
have to perform these tasks manually! DHCP is also enjoying widespread use
in residential Internet access networks and in wireless LANs, where hosts join and
leave the network frequently. Consider, for example, the student who carries a laptop
from a dormitory room to a library to a classroom. It is likely that in each location,
the student will be connecting into a new subnet and hence will need a new IP
address at each location. DHCP is ideally suited to this situation, as there are many
users coming and going, and addresses are needed for only a limited amount of time.
DHCP is similarly useful in residential ISP access networks. Consider, for example,
a residential ISP that has 2,000 customers, but no more than 400 customers are ever
online at the same time. In this case, rather than needing a block of 2,048 addresses,
a DHCP server that assigns addresses dynamically needs only a block of 512
addresses (for example, a block of the form a.b.c.d/23). As the hosts join and leave,
the DHCP server needs to update its list of available IP addresses. Each time a host
joins, the DHCP server allocates an arbitrary address from its current pool of available
addresses; each time a host leaves, its address is returned to the pool.
DHCP is a client-server protocol. A client is typically a newly arriving host
wanting to obtain network configuration information, including an IP address for
itself. In the simplest case, each subnet (in the addressing sense of Figure 4.17) will
have a DHCP server. If no server is present on the subnet, a DHCP relay agent (typically
a router) that knows the address of a DHCP server for that network is needed.
Figure 4.20 shows a DHCP server attached to subnet 223.1.2/24, with the router
serving as the relay agent for arriving clients attached to subnets 223.1.1/24 and
223.1.3/24. In our discussion below, we’ll assume that a DHCP server is available
on the subnet.
For a newly arriving host, the DHCP protocol is a four-step process, as shown
in Figure 4.21 for the network setting shown in Figure 4.20. In this figure, yiaddr
(as in “your Internet address”) indicates the address being allocated to the newly
arriving client. The four steps are:
• DHCP server discovery. The first task of a newly arriving host is to find a DHCP
server with which to interact. This is done using a DHCP discover message,
which a client sends within a UDP packet to port 67. The UDP packet is encapsulated
in an IP datagram. But to whom should this datagram be sent? The host
doesn’t even know the IP address of the network to which it is attaching, much
346 CHAPTER 4 • THE NETWORK LAYER
less the address of a DHCP server for this network. Given this, the DHCP client
creates an IP datagram containing its DHCP discover message along with the
broadcast destination IP address of 255.255.255.255 and a “this host” source IP
address of 0.0.0.0. The DHCP client passes the IP datagram to the link layer,
which then broadcasts this frame to all nodes attached to the subnet (we will
cover the details of link-layer broadcasting in Section 5.4).
• DHCP server offer(s). A DHCP server receiving a DHCP discover message
responds to the client with a DHCP offer message that is broadcast to all nodes
on the subnet, again using the IP broadcast address of 255.255.255.255. (You
might want to think about why this server reply must also be broadcast). Since
several DHCP servers can be present on the subnet, the client may find itself in
the enviable position of being able to choose from among several offers. Each
server offer message contains the transaction ID of the received discover message,
the proposed IP address for the client, the network mask, and an IP address
lease time—the amount of time for which the IP address will be valid. It is common
for the server to set the lease time to several hours or days [Droms 2002].
4.4 • THE INTERNET PROTOCOL (IP) 347
223.1.1.1
223.1.1.2
223.1.1.4 223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1 223.1.3.2
223.1.2.1
223.1.2.5
223.1.2.2
Arriving
DHCP
client
DHCP
server
Figure 4.20  DHCP client-server scenario
• DHCP request. The newly arriving client will choose from among one or more
server offers and respond to its selected offer with a DHCP request message,
echoing back the configuration parameters.
• DHCP ACK. The server responds to the DHCP request message with a DHCP
ACK message, confirming the requested parameters.
Once the client receives the DHCP ACK, the interaction is complete and the
client can use the DHCP-allocated IP address for the lease duration. Since a client
348 CHAPTER 4 • THE NETWORK LAYER
DHCP server:
223.1.2.5
Arriving client
DHCP discover
Time Time
src: 0.0.0.0, 68
dest: 255.255.255.255,67
DHCPDISCOVER
yiaddr: 0.0.0.0
transaction ID: 654
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPOFFER
yiaddrr: 223.1.2.4
transaction ID: 654
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP offer
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPACK
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP ACK
src: 0.0.0.0, 68
dest: 255.255.255.255, 67
DHCPREQUEST
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP request
Figure 4.21  DHCP client-server interaction
may want to use its address beyond the lease’s expiration, DHCP also provides a
mechanism that allows a client to renew its lease on an IP address.
The value of DHCP’s plug-and-play capability is clear, considering the fact that
the alternative is to manually configure a host’s IP address. Consider the student
who moves from classroom to library to dorm room with a laptop, joins a new subnet,
and thus obtains a new IP address at each location. It is unimaginable that a system
administrator would have to reconfigure laptops at each location, and few
students (except those taking a computer networking class!) would have the expertise
to configure their laptops manually. From a mobility aspect, however, DHCP
does have shortcomings. Since a new IP address is obtained from DHCP each time
a node connects to a new subnet, a TCP connection to a remote application cannot
be maintained as a mobile node moves between subnets. In Chapter 6, we will
examine mobile IP—a recent extension to the IP infrastructure that allows a mobile
node to use a single permanent address as it moves between subnets. Additional
details about DHCP can be found in [Droms 2002] and [dhc 2012]. An open source
reference implementation of DHCP is available from the Internet Systems Consortium
[ISC 2012].
Network Address Translation (NAT)
Given our discussion about Internet addresses and the IPv4 datagram format, we’re
now well aware that every IP-capable device needs an IP address. With the proliferation
of small office, home office (SOHO) subnets, this would seem to imply that
whenever a SOHO wants to install a LAN to connect multiple machines, a range of
addresses would need to be allocated by the ISP to cover all of the SOHO’s
machines. If the subnet grew bigger (for example, the kids at home have not only
their own computers, but have smartphones and networked Game Boys as well), a
larger block of addresses would have to be allocated. But what if the ISP had already
allocated the contiguous portions of the SOHO network’s current address range?
And what typical homeowner wants (or should need) to know how to manage IP
addresses in the first place? Fortunately, there is a simpler approach to address allocation
that has found increasingly widespread use in such scenarios: network
address translation (NAT) [RFC 2663; RFC 3022; Zhang 2007].
Figure 4.22 shows the operation of a NAT-enabled router. The NAT-enabled
router, residing in the home, has an interface that is part of the home network on the
right of Figure 4.22. Addressing within the home network is exactly as we have seen
above—all four interfaces in the home network have the same subnet address of
10.0.0/24. The address space 10.0.0.0/8 is one of three portions of the IP address
space that is reserved in [RFC 1918] for a private network or a realm with private
addresses, such as the home network in Figure 4.22. A realm with private addresses
refers to a network whose addresses only have meaning to devices within that
network. To see why this is important, consider the fact that there are hundreds of
4.4 • THE INTERNET PROTOCOL (IP) 349
thousands of home networks, many using the same address space, 10.0.0.0/24.
Devices within a given home network can send packets to each other using
10.0.0.0/24 addressing. However, packets forwarded beyond the home network into
the larger global Internet clearly cannot use these addresses (as either a source or a
destination address) because there are hundreds of thousands of networks using this
block of addresses. That is, the 10.0.0.0/24 addresses can only have meaning within
the given home network. But if private addresses only have meaning within a given
network, how is addressing handled when packets are sent to or received from the
global Internet, where addresses are necessarily unique? The answer lies in understanding
NAT.
The NAT-enabled router does not look like a router to the outside world. Instead
the NAT router behaves to the outside world as a single device with a single IP
address. In Figure 4.22, all traffic leaving the home router for the larger Internet has
a source IP address of 138.76.29.7, and all traffic entering the home router must
have a destination address of 138.76.29.7. In essence, the NAT-enabled router is hiding
the details of the home network from the outside world. (As an aside, you might
wonder where the home network computers get their addresses and where the router
gets its single IP address. Often, the answer is the same—DHCP! The router gets its
address from the ISP’s DHCP server, and the router runs a DHCP server to provide
addresses to computers within the NAT-DHCP-router-controlled home network’s
address space.)
350 CHAPTER 4 • THE NETWORK LAYER
3
2
10.0.0.1
138.76.29.7
10.0.0.4 10.0.0.2
10.0.0.3
NAT translation table
WAN side
138.76.29.7, 5001
LAN side
10.0.0.1, 3345
. . . . . .
S = 138.76.29.7, 5001
D = 128.119.40.186, 80
1
4
S = 128.119.40.186, 80
D = 138.76.29.7, 5001
S = 128.119.40.186, 80
D = 10.0.0.1, 3345
S = 10.0.0.1, 3345
D = 128.119.40.186, 80
Figure 4.22  Network address translation
If all datagrams arriving at the NAT router from the WAN have the same destination
IP address (specifically, that of the WAN-side interface of the NAT router),
then how does the router know the internal host to which it should forward a given
datagram? The trick is to use a NAT translation table at the NAT router, and to
include port numbers as well as IP addresses in the table entries.
Consider the example in Figure 4.22. Suppose a user sitting in a home network
behind host 10.0.0.1 requests a Web page on some Web server (port 80) with IP
address 128.119.40.186. The host 10.0.0.1 assigns the (arbitrary) source port number
3345 and sends the datagram into the LAN. The NAT router receives the datagram,
generates a new source port number 5001 for the datagram, replaces the
source IP address with its WAN-side IP address 138.76.29.7, and replaces the original
source port number 3345 with the new source port number 5001. When generating
a new source port number, the NAT router can select any source port number
that is not currently in the NAT translation table. (Note that because a port number
field is 16 bits long, the NAT protocol can support over 60,000 simultaneous connections
with a single WAN-side IP address for the router!) NAT in the router also
adds an entry to its NAT translation table. The Web server, blissfully unaware that
the arriving datagram containing the HTTP request has been manipulated by the
NAT router, responds with a datagram whose destination address is the IP address
of the NAT router, and whose destination port number is 5001. When this datagram
arrives at the NAT router, the router indexes the NAT translation table using the destination
IP address and destination port number to obtain the appropriate IP address
(10.0.0.1) and destination port number (3345) for the browser in the home network.
The router then rewrites the datagram’s destination address and destination port
number, and forwards the datagram into the home network.
NAT has enjoyed widespread deployment in recent years. But we should
mention that many purists in the IETF community loudly object to NAT. First,
they argue, port numbers are meant to be used for addressing processes, not for
addressing hosts. (This violation can indeed cause problems for servers running
on the home network, since, as we have seen in Chapter 2, server processes wait
for incoming requests at well-known port numbers.) Second, they argue, routers
are supposed to process packets only up to layer 3. Third, they argue, the NAT
protocol violates the so-called end-to-end argument; that is, hosts should be talking
directly with each other, without interfering nodes modifying IP addresses and
port numbers. And fourth, they argue, we should use IPv6 (see Section 4.4.4) to
solve the shortage of IP addresses, rather than recklessly patching up the problem
with a stopgap solution like NAT. But like it or not, NAT has become an important
component of the Internet.
Yet another major problem with NAT is that it interferes with P2P applications,
including P2P file-sharing applications and P2P Voice-over-IP applications. Recall
from Chapter 2 that in a P2P application, any participating Peer A should be able to
initiate a TCP connection to any other participating Peer B. The essence of the
problem is that if Peer B is behind a NAT, it cannot act as a server and accept TCP
4.4 • THE INTERNET PROTOCOL (IP) 351
connections. As we’ll see in the homework problems, this NAT problem can be circumvented
if Peer A is not behind a NAT. In this case, Peer A can first contact Peer
B through an intermediate Peer C, which is not behind a NAT and to which B has
established an ongoing TCP connection. Peer A can then ask Peer B, via Peer C, to
initiate a TCP connection directly back to Peer A. Once the direct P2P TCP connection
is established between Peers A and B, the two peers can exchange messages or
files. This hack, called connection reversal, is actually used by many P2P applications
for NAT traversal. If both Peer A and Peer B are behind their own NATs, the
situation is a bit trickier but can be handled using application relays, as we saw with
Skype relays in Chapter 2.
UPnP
NAT traversal is increasingly provided by Universal Plug and Play (UPnP), which is
a protocol that allows a host to discover and configure a nearby NAT [UPnP Forum
2012]. UPnP requires that both the host and the NAT be UPnP compatible. With
UPnP, an application running in a host can request a NAT mapping between its
( private IP address, private port number) and the ( public IP address, public port
number) for some requested public port number. If the NAT accepts the request and
creates the mapping, then nodes from the outside can initiate TCP connections to
( public IP address, public port number). Furthermore, UPnP lets the application
know the value of ( public IP address, public port number), so that the application
can advertise it to the outside world.
As an example, suppose your host, behind a UPnP-enabled NAT, has private
address 10.0.0.1 and is running BitTorrent on port 3345. Also suppose that the
public IP address of the NAT is 138.76.29.7. Your BitTorrent application naturally
wants to be able to accept connections from other hosts, so that it can trade chunks
with them. To this end, the BitTorrent application in your host asks the NAT to create
a “hole” that maps (10.0.0.1, 3345) to (138.76.29.7, 5001). (The public port
number 5001 is chosen by the application.) The BitTorrent application in your host
could also advertise to its tracker that it is available at (138.76.29.7, 5001). In this
manner, an external host running BitTorrent can contact the tracker and learn that
your BitTorrent application is running at (138.76.29.7, 5001). The external host
can send a TCP SYN packet to (138.76.29.7, 5001). When the NAT receives the
SYN packet, it will change the destination IP address and port number in the
packet to (10.0.0.1, 3345) and forward the packet through the NAT.
In summary, UPnP allows external hosts to initiate communication sessions
to NATed hosts, using either TCP or UDP. NATs have long been a nemesis
for P2P applications; UPnP, providing an effective and robust NAT traversal
solution, may be their savior. Our discussion of NAT and UPnP here has been
necessarily brief. For more detailed discussions of NAT see [Huston 2004, Cisco
NAT 2012].
352 CHAPTER 4 • THE NETWORK LAYER
4.4.3 Internet Control Message Protocol (ICMP)
Recall that the network layer of the Internet has three main components: the IP protocol,
discussed in the previous section; the Internet routing protocols (including
RIP, OSPF, and BGP), which are covered in Section 4.6; and ICMP, which is the
subject of this section.
ICMP, specified in [RFC 792], is used by hosts and routers to communicate network-
layer information to each other. The most typical use of ICMP is for error
reporting. For example, when running a Telnet, FTP, or HTTP session, you may
have encountered an error message such as “Destination network unreachable.” This
message had its origins in ICMP. At some point, an IP router was unable to find a
path to the host specified in your Telnet, FTP, or HTTP application. That router created
and sent a type-3 ICMP message to your host indicating the error.
ICMP is often considered part of IP but architecturally it lies just above IP, as
ICMP messages are carried inside IP datagrams. That is, ICMP messages are carried
as IP payload, just as TCP or UDP segments are carried as IP payload. Similarly,
when a host receives an IP datagram with ICMP specified as the upper-layer protocol,
it demultiplexes the datagram’s contents to ICMP, just as it would demultiplex a
datagram’s content to TCP or UDP.
ICMP messages have a type and a code field, and contain the header and the
first 8 bytes of the IP datagram that caused the ICMP message to be generated in the
first place (so that the sender can determine the datagram that caused the error).
Selected ICMP message types are shown in Figure 4.23. Note that ICMP messages
are used not only for signaling error conditions.
The well-known ping program sends an ICMP type 8 code 0 message to the
specified host. The destination host, seeing the echo request, sends back a type 0
code 0 ICMP echo reply. Most TCP/IP implementations support the ping server
directly in the operating system; that is, the server is not a process. Chapter 11 of
[Stevens 1990] provides the source code for the ping client program. Note that the
client program needs to be able to instruct the operating system to generate an ICMP
message of type 8 code 0.
Another interesting ICMP message is the source quench message. This message
is seldom used in practice. Its original purpose was to perform congestion control—
to allow a congested router to send an ICMP source quench message to a host to
force that host to reduce its transmission rate. We have seen in Chapter 3 that TCP
has its own congestion-control mechanism that operates at the transport layer, without
the use of network-layer feedback such as the ICMP source quench message.
In Chapter 1 we introduced the Traceroute program, which allows us to trace a
route from a host to any other host in the world. Interestingly, Traceroute is implemented
with ICMP messages. To determine the names and addresses of the routers
between source and destination, Traceroute in the source sends a series of ordinary
IP datagrams to the destination. Each of these datagrams carries a UDP segment
with an unlikely UDP port number. The first of these datagrams has a TTL of 1, the
4.4 • THE INTERNET PROTOCOL (IP) 353
second of 2, the third of 3, and so on. The source also starts timers for each of the
datagrams. When the nth datagram arrives at the nth router, the nth router observes
that the TTL of the datagram has just expired. According to the rules of the IP protocol,
the router discards the datagram and sends an ICMP warning message to the
source (type 11 code 0). This warning message includes the name of the router and
its IP address. When this ICMP message arrives back at the source, the source
obtains the round-trip time from the timer and the name and IP address of the nth
router from the ICMP message.
How does a Traceroute source know when to stop sending UDP segments?
Recall that the source increments the TTL field for each datagram it sends. Thus,
one of the datagrams will eventually make it all the way to the destination host.
Because this datagram contains a UDP segment with an unlikely port number, the
destination host sends a port unreachable ICMP message (type 3 code 3) back to the
source. When the source host receives this particular ICMP message, it knows it
does not need to send additional probe packets. (The standard Traceroute program
actually sends sets of three packets with the same TTL; thus the Traceroute output
provides three results for each TTL.)
354 CHAPTER 4 • THE NETWORK LAYER
ICMP Type Code Description
0 0 echo reply (to ping)
3 0 destination network unreachable
3 1 destination host unreachable
3 2 destination protocol unreachable
3 3 destination port unreachable
3 6 destination network unknown
3 7 destination host unknown
4 0 source quench (congestion control)
8 0 echo request
9 0 router advertisement
10 0 router discovery
11 0 TTL expired
12 0 IP header bad
Figure 4.23  ICMP message types
In this manner, the source host learns the number and the identities of routers
that lie between it and the destination host and the round-trip time between the two
hosts. Note that the Traceroute client program must be able to instruct the operating
system to generate UDP datagrams with specific TTL values and must also be able to
be notified by its operating system when ICMP messages arrive. Now that you understand
how Traceroute works, you may want to go back and play with it some more.
4.4 • THE INTERNET PROTOCOL (IP) 355
INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION
SYSTEMS
Suppose you are assigned the task of administering a home, departmental, university, or
corporate network. Attackers, knowing the IP address range of your network, can easily
send IP datagrams to addresses in your range. These datagrams can do all kinds of
devious things, including mapping your network with ping sweeps and port scans,
crashing vulnerable hosts with malformed packets, flooding servers with a deluge of
ICMP packets, and infecting hosts by including malware in the packets. As the network
administrator, what are you going to do about all those bad guys out there, each capable
of sending malicious packets into your network? Two popular defense mechanisms
to malicious packet attacks are firewalls and intrusion detection systems (IDSs).
As a network administrator, you may first try installing a firewall between your
network and the Internet. (Most access routers today have firewall capability.)
Firewalls inspect the datagram and segment header fields, denying suspicious datagrams
entry into the internal network. For example, a firewall may be configured to
block all ICMP echo request packets, thereby preventing an attacker from doing a
traditional ping sweep across your IP address range. Firewalls can also block packets
based on source and destination IP addresses and port numbers. Additionally,
firewalls can be configured to track TCP connections, granting entry only to datagrams
that belong to approved connections.
Additional protection can be provided with an IDS. An IDS, typically situated at the
network boundary, performs “deep packet inspection,” examining not only header
fields but also the payloads in the datagram (including application-layer data). An IDS
has a database of packet signatures that are known to be part of attacks. This database
is automatically updated as new attacks are discovered. As packets pass through
the IDS, the IDS attempts to match header fields and payloads to the signatures in its
signature database. If such a match is found, an alert is created. An intrusion prevention
system (IPS) is similar to an IDS, except that it actually blocks packets in addition to
creating alerts. In Chapter 8, we’ll explore firewalls and IDSs in more detail.
Can firewalls and IDSs fully shield your network from all attacks? The answer is
clearly no, as attackers continually find new attacks for which signatures are not yet
available. But firewalls and traditional signature-based IDSs are useful in protecting
your network from known attacks.
FOCUS ON SECURITY
4.4.4 IPv6
In the early 1990s, the Internet Engineering Task Force began an effort to develop a
successor to the IPv4 protocol. A prime motivation for this effort was the realization
that the 32-bit IP address space was beginning to be used up, with new subnets and
IP nodes being attached to the Internet (and being allocated unique IP addresses) at
a breathtaking rate. To respond to this need for a large IP address space, a new IP
protocol, IPv6, was developed. The designers of IPv6 also took this opportunity to
tweak and augment other aspects of IPv4, based on the accumulated operational
experience with IPv4.
The point in time when IPv4 addresses would be completely allocated (and
hence no new networks could attach to the Internet) was the subject of considerable
debate. The estimates of the two leaders of the IETF’s Address Lifetime Expectations
working group were that addresses would become exhausted in 2008 and 2018,
respectively [Solensky 1996]. In February 2011, IANA allocated out the last remaining
pool of unassigned IPv4 addresses to a regional registry. While these registries
still have available IPv4 addresses within their pool, once these addresses are
exhausted, there are no more available address blocks that can be allocated from a
central pool [Huston 2011a]. Although the mid-1990s estimates of IPv4 address
depletion suggested that a considerable amount of time might be left until the IPv4
address space was exhausted, it was realized that considerable time would be needed
to deploy a new technology on such an extensive scale, and so the Next Generation
IP (IPng) effort [Bradner 1996; RFC 1752] was begun. The result of this effort was
the specification of IP version 6 (IPv6) [RFC 2460] which we’ll discuss below. (An
often-asked question is what happened to IPv5? It was initially envisioned that the
ST-2 protocol would become IPv5, but ST-2 was later dropped.) Excellent sources of
information about IPv6 are [Huitema 1998, IPv6 2012].
IPv6 Datagram Format
The format of the IPv6 datagram is shown in Figure 4.24. The most important
changes introduced in IPv6 are evident in the datagram format:
• Expanded addressing capabilities. IPv6 increases the size of the IP address
from 32 to 128 bits. This ensures that the world won’t run out of IP addresses.
Now, every grain of sand on the planet can be IP-addressable. In addition to
unicast and multicast addresses, IPv6 has introduced a new type of address,
called an anycast address, which allows a datagram to be delivered to any
one of a group of hosts. (This feature could be used, for example, to send an
HTTP GET to the nearest of a number of mirror sites that contain a given
document.)
• A streamlined 40-byte header. As discussed below, a number of IPv4 fields have
been dropped or made optional. The resulting 40-byte fixed-length header allows
356 CHAPTER 4 • THE NETWORK LAYER
for faster processing of the IP datagram. A new encoding of options allows for
more flexible options processing.
• Flow labeling and priority. IPv6 has an elusive definition of a flow. RFC 1752
and RFC 2460 state that this allows “labeling of packets belonging to particular
flows for which the sender requests special handling, such as a nondefault quality
of service or real-time service.” For example, audio and video transmission might
likely be treated as a flow. On the other hand, the more traditional applications,
such as file transfer and e-mail, might not be treated as flows. It is possible that the
traffic carried by a high-priority user (for example, someone paying for better service
for their traffic) might also be treated as a flow. What is clear, however, is that
the designers of IPv6 foresee the eventual need to be able to differentiate among
the flows, even if the exact meaning of a flow has not yet been determined. The
IPv6 header also has an 8-bit traffic class field. This field, like the TOS field in
IPv4, can be used to give priority to certain datagrams within a flow, or it can be
used to give priority to datagrams from certain applications (for example, ICMP)
over datagrams from other applications (for example, network news).
As noted above, a comparison of Figure 4.24 with Figure 4.13 reveals the simpler,
more streamlined structure of the IPv6 datagram. The following fields are
defined in IPv6:
• Version. This 4-bit field identifies the IP version number. Not surprisingly, IPv6
carries a value of 6 in this field. Note that putting a 4 in this field does not create
a valid IPv4 datagram. (If it did, life would be a lot simpler—see the discussion
below regarding the transition from IPv4 to IPv6.)
4.4 • THE INTERNET PROTOCOL (IP) 357
Version Traffic class
Payload length Next hdr Hop limit
Flow label
32 bits
Source address
(128 bits)
Destination address
(128 bits)
Data
Figure 4.24  IPv6 datagram format
• Traffic class. This 8-bit field is similar in spirit to the TOS field we saw in IPv4.
• Flow label. As discussed above, this 20-bit field is used to identify a flow of
datagrams.
• Payload length. This 16-bit value is treated as an unsigned integer giving the
number of bytes in the IPv6 datagram following the fixed-length, 40-byte datagram
header.
• Next header. This field identifies the protocol to which the contents (data field)
of this datagram will be delivered (for example, to TCP or UDP). The field uses
the same values as the protocol field in the IPv4 header.
• Hop limit. The contents of this field are decremented by one by each router that
forwards the datagram. If the hop limit count reaches zero, the datagram is
discarded.
• Source and destination addresses. The various formats of the IPv6 128-bit
address are described in RFC 4291.
• Data. This is the payload portion of the IPv6 datagram. When the datagram
reaches its destination, the payload will be removed from the IP datagram and
passed on to the protocol specified in the next header field.
The discussion above identified the purpose of the fields that are included in the
IPv6 datagram. Comparing the IPv6 datagram format in Figure 4.24 with the IPv4
datagram format that we saw in Figure 4.13, we notice that several fields appearing
in the IPv4 datagram are no longer present in the IPv6 datagram:
• Fragmentation/Reassembly. IPv6 does not allow for fragmentation and reassembly
at intermediate routers; these operations can be performed only by the source
and destination. If an IPv6 datagram received by a router is too large to be forwarded
over the outgoing link, the router simply drops the datagram and sends a
“Packet Too Big” ICMP error message (see below) back to the sender. The
sender can then resend the data, using a smaller IP datagram size. Fragmentation
and reassembly is a time-consuming operation; removing this functionality from
the routers and placing it squarely in the end systems considerably speeds up IP
forwarding within the network.
• Header checksum. Because the transport-layer (for example, TCP and UDP) and
link-layer (for example, Ethernet) protocols in the Internet layers perform checksumming,
the designers of IP probably felt that this functionality was sufficiently
redundant in the network layer that it could be removed. Once again, fast processing
of IP packets was a central concern. Recall from our discussion of IPv4
in Section 4.4.1 that since the IPv4 header contains a TTL field (similar to the
hop limit field in IPv6), the IPv4 header checksum needed to be recomputed at
every router. As with fragmentation and reassembly, this too was a costly operation
in IPv4.
358 CHAPTER 4 • THE NETWORK LAYER
• Options. An options field is no longer a part of the standard IP header. However,
it has not gone away. Instead, the options field is one of the possible next
headers pointed to from within the IPv6 header. That is, just as TCP or UDP
protocol headers can be the next header within an IP packet, so too can an
options field. The removal of the options field results in a fixed-length, 40-
byte IP header.
Recall from our discussion in Section 4.4.3 that the ICMP protocol is used by IP
nodes to report error conditions and provide limited information (for example, the
echo reply to a ping message) to an end system. A new version of ICMP has been
defined for IPv6 in RFC 4443. In addition to reorganizing the existing ICMP type
and code definitions, ICMPv6 also added new types and codes required by the new
IPv6 functionality. These include the “Packet Too Big” type, and an “unrecognized
IPv6 options” error code. In addition, ICMPv6 subsumes the functionality of the
Internet Group Management Protocol (IGMP) that we’ll study in Section 4.7. IGMP,
which is used to manage a host’s joining and leaving of multicast groups, was previously
a separate protocol from ICMP in IPv4.
Transitioning from IPv4 to IPv6
Now that we have seen the technical details of IPv6, let us consider a very practical
matter: How will the public Internet, which is based on IPv4, be transitioned to
IPv6? The problem is that while new IPv6-capable systems can be made backwardcompatible,
that is, can send, route, and receive IPv4 datagrams, already deployed
IPv4-capable systems are not capable of handling IPv6 datagrams. Several options
are possible [Huston 2011b].
One option would be to declare a flag day—a given time and date when all
Internet machines would be turned off and upgraded from IPv4 to IPv6. The last
major technology transition (from using NCP to using TCP for reliable transport
service) occurred almost 25 years ago. Even back then [RFC 801], when the Internet
was tiny and still being administered by a small number of “wizards,” it was
realized that such a flag day was not possible. A flag day involving hundreds of millions
of machines and millions of network administrators and users is even more
unthinkable today. RFC 4213 describes two approaches (which can be used either
alone or together) for gradually integrating IPv6 hosts and routers into an IPv4
world (with the long-term goal, of course, of having all IPv4 nodes eventually transition
to IPv6).
Probably the most straightforward way to introduce IPv6-capable nodes is a
dual-stack approach, where IPv6 nodes also have a complete IPv4 implementation.
Such a node, referred to as an IPv6/IPv4 node in RFC 4213, has the ability to send
and receive both IPv4 and IPv6 datagrams. When interoperating with an IPv4 node,
an IPv6/IPv4 node can use IPv4 datagrams; when interoperating with an IPv6 node,
it can speak IPv6. IPv6/IPv4 nodes must have both IPv6 and IPv4 addresses. They
4.4 • THE INTERNET PROTOCOL (IP) 359
must furthermore be able to determine whether another node is IPv6-capable or
IPv4-only. This problem can be solved using the DNS (see Chapter 2), which can
return an IPv6 address if the node name being resolved is IPv6-capable, or otherwise
return an IPv4 address. Of course, if the node issuing the DNS request is only
IPv4-capable, the DNS returns only an IPv4 address.
In the dual-stack approach, if either the sender or the receiver is only IPv4-
capable, an IPv4 datagram must be used. As a result, it is possible that two IPv6-
capable nodes can end up, in essence, sending IPv4 datagrams to each other. This is
illustrated in Figure 4.25. Suppose Node A is IPv6-capable and wants to send an IP
datagram to Node F, which is also IPv6-capable. Nodes A and B can exchange an
IPv6 datagram. However, Node B must create an IPv4 datagram to send to C. Certainly,
the data field of the IPv6 datagram can be copied into the data field of the
IPv4 datagram and appropriate address mapping can be done. However, in performing
the conversion from IPv6 to IPv4, there will be IPv6-specific fields in the IPv6
datagram (for example, the flow identifier field) that have no counterpart in IPv4.
The information in these fields will be lost. Thus, even though E and F can exchange
IPv6 datagrams, the arriving IPv4 datagrams at E from D do not contain all of the
fields that were in the original IPv6 datagram sent from A.
An alternative to the dual-stack approach, also discussed in RFC 4213, is
known as tunneling. Tunneling can solve the problem noted above, allowing, for
example, E to receive the IPv6 datagram originated by A. The basic idea behind
tunneling is the following. Suppose two IPv6 nodes (for example, B and E in Figure
4.25) want to interoperate using IPv6 datagrams but are connected to each
other by intervening IPv4 routers. We refer to the intervening set of IPv4 routers
between two IPv6 routers as a tunnel, as illustrated in Figure 4.26. With tunneling,
the IPv6 node on the sending side of the tunnel (for example, B) takes the
entire IPv6 datagram and puts it in the data (payload) field of an IPv4 datagram.
360 CHAPTER 4 • THE NETWORK LAYER
A B C D E F
IPv6
A to B: IPv6 B to C: IPv4 D to E: IPv4 E to F: IPv6
IPv6 IPv4 IPv4 IPv6 IPv6
Flow: X
Source: A
Dest: F
data
Source: A
Dest: F
data
Source: A
Dest: F
data
Flow: ??
Source: A
Dest: F
data
Figure 4.25  A dual-stack approach
This IPv4 datagram is then addressed to the IPv6 node on the receiving side of
the tunnel (for example, E) and sent to the first node in the tunnel (for example,
C). The intervening IPv4 routers in the tunnel route this IPv4 datagram among
themselves, just as they would any other datagram, blissfully unaware that the
IPv4 datagram itself contains a complete IPv6 datagram. The IPv6 node on the
receiving side of the tunnel eventually receives the IPv4 datagram (it is the destination
of the IPv4 datagram!), determines that the IPv4 datagram contains an
IPv6 datagram, extracts the IPv6 datagram, and then routes the IPv6 datagram
exactly as it would if it had received the IPv6 datagram from a directly connected
IPv6 neighbor.
We end this section by noting that while the adoption of IPv6 was initially
slow to take off [Lawton 2001], momentum has been building recently. See [Huston
2008b] for discussion of IPv6 deployment as of 2008; see [NIST IPv6 2012]
for a snapshort of US IPv6 deployment. The proliferation of devices such as IPenabled
phones and other portable devices provides an additional push for more
4.4 • THE INTERNET PROTOCOL (IP) 361
A B C D E F
IPv6
A to B: IPv6
Physical view
B to C: IPv4
(encapsulating IPv6)
D to E: IPv4
(encapsulating IPv6)
E to F: IPv6
IPv6 IPv4 IPv4 IPv6 IPv6
Flow: X
Source: A
Dest: F
data
Source: B
Dest: E
Source: B
Dest: E
A B E F
IPv6
Logical view
IPv6
Tunnel
IPv6 IPv6
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Figure 4.26  Tunneling
widespread deployment of IPv6. Europe’s Third Generation Partnership Program
[3GPP 2012] has specified IPv6 as the standard addressing scheme for mobile
multimedia.
One important lesson that we can learn from the IPv6 experience is that it is enormously
difficult to change network-layer protocols. Since the early 1990s, numerous
new network-layer protocols have been trumpeted as the next major revolution for the
Internet, but most of these protocols have had limited penetration to date. These protocols
include IPv6, multicast protocols (Section 4.7), and resource reservation protocols
(Chapter 7). Indeed, introducing new protocols into the network layer is like
replacing the foundation of a house—it is difficult to do without tearing the whole
house down or at least temporarily relocating the house’s residents. On the other hand,
the Internet has witnessed rapid deployment of new protocols at the application layer.
The classic examples, of course, are the Web, instant messaging, and P2P file sharing.
Other examples include audio and video streaming and distributed games. Introducing
new application-layer protocols is like adding a new layer of paint to a house—it is
relatively easy to do, and if you choose an attractive color, others in the neighborhood
will copy you. In summary, in the future we can expect to see changes in the Internet’s
network layer, but these changes will likely occur on a time scale that is much slower
than the changes that will occur at the application layer.
4.4.5 A Brief Foray into IP Security
Section 4.4.3 covered IPv4 in some detail, including the services it provides and
how those services are implemented. While reading through that section, you may
have noticed that there was no mention of any security services. Indeed, IPv4 was
designed in an era (the 1970s) when the Internet was primarily used among mutually-
trusted networking researchers. Creating a computer network that integrated a
multitude of link-layer technologies was already challenging enough, without having
to worry about security.
But with security being a major concern today, Internet researchers have moved
on to design new network-layer protocols that provide a variety of security services.
One of these protocols is IPsec, one of the more popular secure network-layer protocols
and also widely deployed in Virtual Private Networks (VPNs). Although IPsec and
its cryptographic underpinnings are covered in some detail in Chapter 8, we provide a
brief, high-level introduction into IPsec services in this section.
IPsec has been designed to be backward compatible with IPv4 and IPv6. In particular,
in order to reap the benefits of IPsec, we don’t need to replace the protocol
stacks in all the routers and hosts in the Internet. For example, using the transport
mode (one of two IPsec “modes”), if two hosts want to securely communicate, IPsec
needs to be available only in those two hosts. All other routers and hosts can continue
to run vanilla IPv4.
For concreteness, we’ll focus on IPsec’s transport mode here. In this mode, two
hosts first establish an IPsec session between themselves. (Thus IPsec is connectionoriented!)
With the session in place, all TCP and UDP segments sent between the
362 CHAPTER 4 • THE NETWORK LAYER
two hosts enjoy the security services provided by IPsec. On the sending side, the
transport layer passes a segment to IPsec. IPsec then encrypts the segment, appends
additional security fields to the segment, and encapsulates the resulting payload in
an ordinary IP datagram. (It’s actually a little more complicated than this, as we’ll
see in Chapter 8.) The sending host then sends the datagram into the Internet, which
transports it to the destination host. There, IPsec decrypts the segment and passes
the unencrypted segment to the transport layer.
The services provided by an IPsec session include:
• Cryptographic agreement. Mechanisms that allow the two communicating hosts
to agree on cryptographic algorithms and keys.
• Encryption of IP datagram payloads. When the sending host receives a segment
from the transport layer, IPsec encrypts the payload. The payload can only be
decrypted by IPsec in the receiving host.
• Data integrity. IPsec allows the receiving host to verify that the datagram’s
header fields and encrypted payload were not modified while the datagram was
en route from source to destination.
• Origin authentication. When a host receives an IPsec datagram from a trusted
source (with a trusted key—see Chapter 8), the host is assured that the source IP
address in the datagram is the actual source of the datagram.
When two hosts have an IPsec session established between them, all TCP and
UDP segments sent between them will be encrypted and authenticated. IPsec therefore
provides blanket coverage, securing all communication between the two hosts
for all network applications.
A company can use IPsec to communicate securely in the nonsecure public Internet.
For illustrative purposes, we’ll just look at a simple example here. Consider a
company that has a large number of traveling salespeople, each possessing a company
laptop computer. Suppose the salespeople need to frequently consult sensitive company
information (for example, pricing and product information) that is stored on a
server in the company’s headquarters. Further suppose that the salespeople also need
to send sensitive documents to each other. How can this be done with IPsec? As you
might guess, we install IPsec in the server and in all of the salespeople’s laptops. With
IPsec installed in these hosts, whenever a salesperson needs to communicate with the
server or with another salesperson, the communication session will be secure.
4.5 Routing Algorithms
So far in this chapter, we’ve mostly explored the network layer’s forwarding function.
We learned that when a packet arrives to a router, the router indexes a forwarding
table and determines the link interface to which the packet is to be directed. We
also learned that routing algorithms, operating in network routers, exchange and
4.5 • ROUTING ALGORITHMS 363
compute the information that is used to configure these forwarding tables. The interplay
between routing algorithms and forwarding tables was shown in Figure 4.2.
Having explored forwarding in some depth we now turn our attention to the other
major topic of this chapter, namely, the network layer’s critical routing function.
Whether the network layer provides a datagram service (in which case different packets
between a given source-destination pair may take different routes) or a VC service
(in which case all packets between a given source and destination will take the
same path), the network layer must nonetheless determine the path that packets take
from senders to receivers. We’ll see that the job of routing is to determine good paths
(equivalently, routes), from senders to receivers, through the network of routers.
Typically a host is attached directly to one router, the default router for the
host (also called the first-hop router for the host). Whenever a host sends a packet,
the packet is transferred to its default router. We refer to the default router of the
source host as the source router and the default router of the destination host as the
destination router. The problem of routing a packet from source host to destination
host clearly boils down to the problem of routing the packet from source router to
destination router, which is the focus of this section.
The purpose of a routing algorithm is then simple: given a set of routers, with
links connecting the routers, a routing algorithm finds a “good” path from source
router to destination router. Typically, a good path is one that has the least cost.
We’ll see, however, that in practice, real-world concerns such as policy issues (for
example, a rule such as “router x, belonging to organization Y, should not forward
any packets originating from the network owned by organization Z”) also come into
play to complicate the conceptually simple and elegant algorithms whose theory
underlies the practice of routing in today’s networks.
A graph is used to formulate routing problems. Recall that a graph G = (N,E)
is a set N of nodes and a collection E of edges, where each edge is a pair of nodes
from N. In the context of network-layer routing, the nodes in the graph represent
routers—the points at which packet-forwarding decisions are made—and the edges
connecting these nodes represent the physical links between these routers. Such a
graph abstraction of a computer network is shown in Figure 4.27. To view some
graphs representing real network maps, see [Dodge 2012, Cheswick 2000]; for a
discussion of how well different graph-based models model the Internet, see
[Zegura 1997, Faloutsos 1999, Li 2004].
As shown in Figure 4.27, an edge also has a value representing its cost. Typically,
an edge’s cost may reflect the physical length of the corresponding link (for
example, a transoceanic link might have a higher cost than a short-haul terrestrial
link), the link speed, or the monetary cost associated with a link. For our purposes,
we’ll simply take the edge costs as a given and won’t worry about how they are
determined. For any edge (x,y) in E, we denote c(x,y) as the cost of the edge between
nodes x and y. If the pair (x,y) does not belong to E, we set c(x,y) = 8. Also, throughout
we consider only undirected graphs (i.e., graphs whose edges do not have a
direction), so that edge (x,y) is the same as edge (y,x) and that c(x,y) = c(y,x). Also, a
node y is said to be a neighbor of node x if (x,y) belongs to E.
364 CHAPTER 4 • THE NETWORK LAYER
Given that costs are assigned to the various edges in the graph abstraction, a natural
goal of a routing algorithm is to identify the least costly paths between sources and
destinations. To make this problem more precise, recall that a path in a graph G =
(N,E) is a sequence of nodes (x1, x2,..., xp) such that each of the pairs (x1,x2),
(x2,x3),...,(xp-1,xp) are edges in E. The cost of a path (x1,x2,..., xp) is simply the sum of
all the edge costs along the path, that is, c(x1,x2) + c(x2,x3) + ...+ c(xp-1,xp). Given any
two nodes x and y, there are typically many paths between the two nodes, with each
path having a cost. One or more of these paths is a least-cost path. The least-cost
problem is therefore clear: Find a path between the source and destination that has
least cost. In Figure 4.27, for example, the least-cost path between source node u and
destination node w is (u, x, y, w) with a path cost of 3. Note that if all edges in the
graph have the same cost, the least-cost path is also the shortest path (that is, the
path with the smallest number of links between the source and the destination).
As a simple exercise, try finding the least-cost path from node u to z in Figure
4.27 and reflect for a moment on how you calculated that path. If you are like most
people, you found the path from u to z by examining Figure 4.27, tracing a few routes
from u to z, and somehow convincing yourself that the path you had chosen had the
least cost among all possible paths. (Did you check all of the 17 possible paths
between u and z? Probably not!) Such a calculation is an example of a centralized
routing algorithm—the routing algorithm was run in one location, your brain, with
complete information about the network. Broadly, one way in which we can classify
routing algorithms is according to whether they are global or decentralized.
• A global routing algorithm computes the least-cost path between a source and
destination using complete, global knowledge about the network. That is, the
algorithm takes the connectivity between all nodes and all link costs as inputs.
This then requires that the algorithm somehow obtain this information before
actually performing the calculation. The calculation itself can be run at one site
4.5 • ROUTING ALGORITHMS 365
x y
v
3
5
2 5
2
3
1
1 2
1
u z
w
Figure 4.27  Abstract graph model of a computer network
(a centralized global routing algorithm) or replicated at multiple sites. The key
distinguishing feature here, however, is that a global algorithm has complete
information about connectivity and link costs. In practice, algorithms with global
state information are often referred to as link-state (LS) algorithms, since the
algorithm must be aware of the cost of each link in the network. We’ll study LS
algorithms in Section 4.5.1.
• In a decentralized routing algorithm, the calculation of the least-cost path is
carried out in an iterative, distributed manner. No node has complete information
about the costs of all network links. Instead, each node begins with only the
knowledge of the costs of its own directly attached links. Then, through an iterative
process of calculation and exchange of information with its neighboring
nodes (that is, nodes that are at the other end of links to which it itself is
attached), a node gradually calculates the least-cost path to a destination or set of
destinations. The decentralized routing algorithm we’ll study below in Section
4.5.2 is called a distance-vector (DV) algorithm, because each node maintains a
vector of estimates of the costs (distances) to all other nodes in the network.
A second broad way to classify routing algorithms is according to whether they
are static or dynamic. In static routing algorithms, routes change very slowly over
time, often as a result of human intervention (for example, a human manually editing
a router’s forwarding table). Dynamic routing algorithms change the routing
paths as the network traffic loads or topology change. A dynamic algorithm can be
run either periodically or in direct response to topology or link cost changes. While
dynamic algorithms are more responsive to network changes, they are also more
susceptible to problems such as routing loops and oscillation in routes.
Athird way to classify routing algorithms is according to whether they are loadsensitive
or load-insensitive. In a load-sensitive algorithm, link costs vary dynamically
to reflect the current level of congestion in the underlying link. If a high cost is
associated with a link that is currently congested, a routing algorithm will tend to
choose routes around such a congested link. While early ARPAnet routing algorithms
were load-sensitive [McQuillan 1980], a number of difficulties were encountered
[Huitema 1998]. Today’s Internet routing algorithms (such as RIP, OSPF, and
BGP) are load-insensitive, as a link’s cost does not explicitly reflect its current (or
recent past) level of congestion.
4.5.1 The Link-State (LS) Routing Algorithm
Recall that in a link-state algorithm, the network topology and all link costs are
known, that is, available as input to the LS algorithm. In practice this is accomplished
by having each node broadcast link-state packets to all other nodes in the
network, with each link-state packet containing the identities and costs of its
attached links. In practice (for example, with the Internet’s OSPF routing protocol,
discussed in Section 4.6.1) this is often accomplished by a link-state broadcast
366 CHAPTER 4 • THE NETWORK LAYER
algorithm [Perlman 1999]. We’ll cover broadcast algorithms in Section 4.7. The
result of the nodes’ broadcast is that all nodes have an identical and complete view
of the network. Each node can then run the LS algorithm and compute the same set
of least-cost paths as every other node.
The link-state routing algorithm we present below is known as Dijkstra’s algorithm,
named after its inventor. A closely related algorithm is Prim’s algorithm; see
[Cormen 2001] for a general discussion of graph algorithms. Dijkstra’s algorithm
computes the least-cost path from one node (the source, which we will refer to as u)
to all other nodes in the network. Dijkstra’s algorithm is iterative and has the property
that after the kth iteration of the algorithm, the least-cost paths are known to k
destination nodes, and among the least-cost paths to all destination nodes, these k
paths will have the k smallest costs. Let us define the following notation:
• D(v): cost of the least-cost path from the source node to destination v as of this
iteration of the algorithm.
• p(v): previous node (neighbor of v) along the current least-cost path from the
source to v.
• N : subset of nodes; v is in N if the least-cost path from the source to v is definitively
known.
The global routing algorithm consists of an initialization step followed by a
loop. The number of times the loop is executed is equal to the number of nodes in
the network. Upon termination, the algorithm will have calculated the shortest paths
from the source node u to every other node in the network.
Link-State (LS) Algorithm for Source Node u
1 Initialization:
2 N’ = {u}
3 for all nodes v
4 if v is a neighbor of u
5 then D(v) = c(u,v)
6 else D(v) = 8
7
8 Loop
9 find w not in N’ such that D(w) is a minimum
10 add w to N’
11 update D(v) for each neighbor v of w and not in N’:
12 D(v) = min( D(v), D(w) + c(w,v) )
13 /* new cost to v is either old cost to v or known
14 least path cost to w plus cost from w to v */
15 until N’= N
4.5 • ROUTING ALGORITHMS 367
As an example, let’s consider the network in Figure 4.27 and compute the
least-cost paths from u to all possible destinations. A tabular summary of the
algorithm’s computation is shown in Table 4.3, where each line in the table gives
the values of the algorithm’s variables at the end of the iteration. Let’s consider
the few first steps in detail.
• In the initialization step, the currently known least-cost paths from u to its
directly attached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively.
Note in particular that the cost to w is set to 5 (even though we will soon see that
a lesser-cost path does indeed exist) since this is the cost of the direct (one hop)
link from u to w. The costs to y and z are set to infinity because they are not
directly connected to u.
• In the first iteration, we look among those nodes not yet added to the set N and
find that node with the least cost as of the end of the previous iteration. That node
is x, with a cost of 1, and thus x is added to the set N. Line 12 of the LS algorithm
is then performed to update D(v) for all nodes v, yielding the results shown
in the second line (Step 1) in Table 4.3. The cost of the path to v is unchanged.
The cost of the path to w (which was 5 at the end of the initialization) through
node x is found to have a cost of 4. Hence this lower-cost path is selected and w’s
predecessor along the shortest path from u is set to x. Similarly, the cost to y
(through x) is computed to be 2, and the table is updated accordingly.
• In the second iteration, nodes v and y are found to have the least-cost paths (2),
and we break the tie arbitrarily and add y to the set N so that N now contains u,
x, and y. The cost to the remaining nodes not yet in N, that is, nodes v, w, and z,
are updated via line 12 of the LS algorithm, yielding the results shown in the
third row in the Table 4.3.
• And so on. . . .
When the LS algorithm terminates, we have, for each node, its predecessor
along the least-cost path from the source node. For each predecessor, we also
368 CHAPTER 4 • THE NETWORK LAYER
step N’ D(v),p(v) D(w),p(w) D(x),p(x) D(y),p(y) D(z),p(z)
0 u 2,u 5,u 1,u 8 8
1 ux 2,u 4,x 2,x 8
2 uxy 2,u 3,y 4,y
3 uxyv 3,y 4,y
4 uxyvw 4,y
5 uxyvwz
Table 4.3  Running the link-state algorithm on the network in Figure 4.27
VideoNote
Dijkstra’s algorithm:
discussion and example
have its predecessor, and so in this manner we can construct the entire path from
the source to all destinations. The forwarding table in a node, say node u, can
then be constructed from this information by storing, for each destination, the
next-hop node on the least-cost path from u to the destination. Figure 4.28
shows the resulting least-cost paths and forwarding table in u for the network in
Figure 4.27.
What is the computational complexity of this algorithm? That is, given n
nodes (not counting the source), how much computation must be done in the
worst case to find the least-cost paths from the source to all destinations? In the
first iteration, we need to search through all n nodes to determine the node, w, not
in N that has the minimum cost. In the second iteration, we need to check n – 1
nodes to determine the minimum cost; in the third iteration n – 2 nodes, and so
on. Overall, the total number of nodes we need to search through over all the iterations
is n(n + 1)/2, and thus we say that the preceding implementation of the LS
algorithm has worst-case complexity of order n squared: O(n2). (A more sophisticated
implementation of this algorithm, using a data structure known as a heap,
can find the minimum in line 9 in logarithmic rather than linear time, thus reducing
the complexity.)
Before completing our discussion of the LS algorithm, let us consider a pathology
that can arise. Figure 4.29 shows a simple network topology where link costs
are equal to the load carried on the link, for example, reflecting the delay that would
be experienced. In this example, link costs are not symmetric; that is, c(u,v) equals
c(v,u) only if the load carried on both directions on the link (u,v) is the same. In this
example, node z originates a unit of traffic destined for w, node x also originates a
unit of traffic destined for w, and node y injects an amount of traffic equal to e, also
destined for w. The initial routing is shown in Figure 4.29(a) with the link costs corresponding
to the amount of traffic carried.
When the LS algorithm is next run, node y determines (based on the link costs
shown in Figure 4.29(a)) that the clockwise path to w has a cost of 1, while the
counterclockwise path to w (which it had been using) has a cost of 1 + e. Hence y’s
4.5 • ROUTING ALGORITHMS 369
Destination Link
v
w
x
y
z
(u, v)
(u, x)
(u, x)
(u, x)
X Y (u, x)
V
U Z
W
Figure 4.28  Least cost path and forwarding table for nodule u
least-cost path to w is now clockwise. Similarly, x determines that its new least-cost
path to w is also clockwise, resulting in costs shown in Figure 4.29(b). When the
LS algorithm is run next, nodes x, y, and z all detect a zero-cost path to w in the
counterclockwise direction, and all route their traffic to the counterclockwise
routes. The next time the LS algorithm is run, x, y, and z all then route their traffic
to the clockwise routes.
What can be done to prevent such oscillations (which can occur in any algorithm,
not just an LS algorithm, that uses a congestion or delay-based link metric)?
One solution would be to mandate that link costs not depend on the amount
of traffic carried—an unacceptable solution since one goal of routing is to avoid
370 CHAPTER 4 • THE NETWORK LAYER
w
y
z x
1
0 0
0 e
1 + e
1
a. Initial routing
1
e
w
y
z x
2 + e
1 + e 1
0 0
0
b. x, y detect better path
to w, clockwise
w
y
z x
0
0 0
1 1 + e
2+ e
c. x, y, z detect better path
to w, counterclockwise
w
y
z x
2 + e
1 + e 1
0 0
0
d. x, y, z, detect better path
to w, clockwise
1 1
e
1 1
e
1 1
e
Figure 4.29  Oscillations with congestion-sensitive routing
highly congested (for example, high-delay) links. Another solution is to ensure
that not all routers run the LS algorithm at the same time. This seems a more
reasonable solution, since we would hope that even if routers ran the LS algorithm
with the same periodicity, the execution instance of the algorithm would not be
the same at each node. Interestingly, researchers have found that routers in the
Internet can self-synchronize among themselves [Floyd Synchronization 1994].
That is, even though they initially execute the algorithm with the same period
but at different instants of time, the algorithm execution instance can eventually
become, and remain, synchronized at the routers. One way to avoid such selfsynchronization
is for each router to randomize the time it sends out a link
advertisement.
Having studied the LS algorithm, let’s consider the other major routing algorithm
that is used in practice today—the distance-vector routing algorithm.
4.5.2 The Distance-Vector (DV) Routing Algorithm
Whereas the LS algorithm is an algorithm using global information, the distancevector
(DV) algorithm is iterative, asynchronous, and distributed. It is distributed
in that each node receives some information from one or more of its directly
attached neighbors, performs a calculation, and then distributes the results of its
calculation back to its neighbors. It is iterative in that this process continues
on until no more information is exchanged between neighbors. (Interestingly, the
algorithm is also self-terminating—there is no signal that the computation should
stop; it just stops.) The algorithm is asynchronous in that it does not require all of
the nodes to operate in lockstep with each other. We’ll see that an asynchronous,
iterative, self-terminating, distributed algorithm is much more interesting and fun
than a centralized algorithm!
Before we present the DV algorithm, it will prove beneficial to discuss an
important relationship that exists among the costs of the least-cost paths. Let dx(y)
be the cost of the least-cost path from node x to node y. Then the least costs are
related by the celebrated Bellman-Ford equation, namely,
dx(y) = minv{c(x,v) + dv(y)}, (4.1)
where the minv in the equation is taken over all of x’s neighbors. The Bellman-Ford
equation is rather intuitive. Indeed, after traveling from x to v, if we then take the
least-cost path from v to y, the path cost will be c(x,v) + dv(y). Since we must begin
by traveling to some neighbor v, the least cost from x to y is the minimum of c(x,v)
+ dv(y) taken over all neighbors v.
But for those who might be skeptical about the validity of the equation, let’s
check it for source node u and destination node z in Figure 4.27. The source node u
4.5 • ROUTING ALGORITHMS 371
has three neighbors: nodes v, x, and w. By walking along various paths in the graph,
it is easy to see that dv(z) = 5, dx(z) = 3, and dw(z) = 3. Plugging these values into
Equation 4.1, along with the costs c(u,v) = 2, c(u,x) = 1, and c(u,w) = 5, gives du(z) =
min{2 + 5, 5 + 3, 1 + 3} = 4, which is obviously true and which is exactly what the
Dijskstra algorithm gave us for the same network. This quick verification should
help relieve any skepticism you may have.
The Bellman-Ford equation is not just an intellectual curiosity. It actually has
significant practical importance. In particular, the solution to the Bellman-Ford
equation provides the entries in node x’s forwarding table. To see this, let v* be any
neighboring node that achieves the minimum in Equation 4.1. Then, if node x wants
to send a packet to node y along a least-cost path, it should first forward the packet
to node v*. Thus, node x’s forwarding table would specify node v* as the next-hop
router for the ultimate destination y. Another important practical contribution of the
Bellman-Ford equation is that it suggests the form of the neighbor-to-neighbor communication
that will take place in the DV algorithm.
The basic idea is as follows. Each node x begins with Dx(y), an estimate of the
cost of the least-cost path from itself to node y, for all nodes in N. Let Dx = [Dx(y): y
in N] be node x’s distance vector, which is the vector of cost estimates from x to all
other nodes, y, in N. With the DV algorithm, each node x maintains the following
routing information:
• For each neighbor v, the cost c(x,v) from x to directly attached neighbor, v
• Node x’s distance vector, that is, Dx = [Dx(y): y in N], containing x’s estimate of
its cost to all destinations, y, in N
• The distance vectors of each of its neighbors, that is, Dv = [Dv(y): y in N] for each
neighbor v of x
In the distributed, asynchronous algorithm, from time to time, each node sends
a copy of its distance vector to each of its neighbors. When a node x receives a
new distance vector from any of its neighbors v, it saves v’s distance vector, and
then uses the Bellman-Ford equation to update its own distance vector as follows:
Dx(y)  minv{c(x,v) + Dv(y)} for each node y in N
If node x’s distance vector has changed as a result of this update step, node x will
then send its updated distance vector to each of its neighbors, which can in turn
update their own distance vectors. Miraculously enough, as long as all the nodes
continue to exchange their distance vectors in an asynchronous fashion, each cost
estimate Dx(y) converges to dx(y), the actual cost of the least-cost path from node x
to node y [Bertsekas 1991]!
372 CHAPTER 4 • THE NETWORK LAYER
Distance-Vector (DV) Algorithm
At each node, x:
4.5 • ROUTING ALGORITHMS 373
1 Initialization:
2 for all destinations y in N:
3 Dx(y) = c(x,y) /* if y is not a neighbor then c(x,y) = 8 */
4 for each neighbor w
5 Dw(y) = ? for all destinations y in N
6 for each neighbor w
7 send distance vector Dx = [Dx(y): y in N] to w
8
9 loop
10 wait (until I see a link cost change to some neighbor w or
11 until I receive a distance vector from some neighbor w)
12
13 for each y in N:
14 Dx(y) = minv{c(x,v) + Dv(y)}
15
16 if Dx(y) changed for any destination y
17 send distance vector Dx = [Dx(y): y in N] to all neighbors
18
19 forever
In the DV algorithm, a node x updates its distance-vector estimate when it
either sees a cost change in one of its directly attached links or receives a distancevector
update from some neighbor. But to update its own forwarding table for a
given destination y, what node x really needs to know is not the shortest-path
distance to y but instead the neighboring node v*(y) that is the next-hop router along
the shortest path to y. As you might expect, the next-hop router v*(y) is the neighbor
v that achieves the minimum in Line 14 of the DV algorithm. (If there are multiple
neighbors v that achieve the minimum, then v*(y) can be any of the minimizing
neighbors.) Thus, in Lines 13–14, for each destination y, node x also determines
v*(y) and updates its forwarding table for destination y.
Recall that the LS algorithm is a global algorithm in the sense that it requires
each node to first obtain a complete map of the network before running the Dijkstra
algorithm. The DV algorithm is decentralized and does not use such global information.
Indeed, the only information a node will have is the costs of the links to its
directly attached neighbors and information it receives from these neighbors. Each
node waits for an update from any neighbor (Lines 10–11), calculates its new distance
vector when receiving an update (Line 14), and distributes its new distance
vector to its neighbors (Lines 16–17). DV-like algorithms are used in many routing
protocols in practice, including the Internet’s RIP and BGP, ISO IDRP, Novell IPX,
and the original ARPAnet.
Figure 4.30 illustrates the operation of the DV algorithm for the simple threenode
network shown at the top of the figure. The operation of the algorithm is illustrated
in a synchronous manner, where all nodes simultaneously receive distance
vectors from their neighbors, compute their new distance vectors, and inform their
neighbors if their distance vectors have changed. After studying this example, you
374 CHAPTER 4 • THE NETWORK LAYER
Node y table
Node x table
0 2 7
x y z
8 8 8
8 8 8
Time
7
2 1
y
x z
Node z table from
cost to
x
y
z
0 2 3
x y z
2 0 1
7 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
2 0 1
x y z
8 8 8
8 8 8
from
cost to
x
y
z
0 2 7
x y z
2 0 1
7 1 0
from cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
7 1 0
x y z
8 8 8
8 8 8
from
cost to
x
y
z
0 2 7
x y z
2 0 1
3 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
Figure 4.30  Distance-vector (DV) algorithm
should convince yourself that the algorithm operates correctly in an asynchronous
manner as well, with node computations and update generation/reception occurring
at any time.
The leftmost column of the figure displays three initial routing tables for each
of the three nodes. For example, the table in the upper-left corner is node x’s initial
routing table. Within a specific routing table, each row is a distance vector—specifically,
each node’s routing table includes its own distance vector and that of each of
its neighbors. Thus, the first row in node x’s initial routing table is Dx = [Dx(x),
Dx(y), Dx(z)] = [0, 2, 7]. The second and third rows in this table are the most recently
received distance vectors from nodes y and z, respectively. Because at initialization
node x has not received anything from node y or z, the entries in the second and third
rows are initialized to infinity.
After initialization, each node sends its distance vector to each of its two neighbors.
This is illustrated in Figure 4.30 by the arrows from the first column of tables
to the second column of tables. For example, node x sends its distance vector Dx =
[0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its
own distance vector. For example, node x computes
Dx(x) = 0
Dx(y) = min{c(x,y) + Dy(y), c(x,z) + Dz(y)} = min{2 + 0, 7 + 1} = 2
Dx(z) = min{c(x,y) + Dy(z), c(x,z) + Dz(z)} = min{2 + 1, 7 + 0} = 3
The second column therefore displays, for each node, the node’s new distance vector
along with distance vectors just received from its neighbors. Note, for example,
that node x’s estimate for the least cost to node z, Dx(z), has changed from 7 to 3.
Also note that for node x, neighboring node y achieves the minimum in line 14 of
the DV algorithm; thus at this stage of the algorithm, we have at node x that v*(y) =
y and v*(z) = y.
After the nodes recompute their distance vectors, they again send their updated
distance vectors to their neighbors (if there has been a change). This is illustrated in
Figure 4.30 by the arrows from the second column of tables to the third column of
tables. Note that only nodes x and z send updates: node y’s distance vector didn’t
change so node y doesn’t send an update. After receiving the updates, the nodes then
recompute their distance vectors and update their routing tables, which are shown in
the third column.
The process of receiving updated distance vectors from neighbors, recomputing
routing table entries, and informing neighbors of changed costs of the least-cost path
to a destination continues until no update messages are sent. At this point, since no
update messages are sent, no further routing table calculations will occur and the
algorithm will enter a quiescent state; that is, all nodes will be performing the wait
in Lines 10–11 of the DV algorithm. The algorithm remains in the quiescent state
until a link cost changes, as discussed next.
4.5 • ROUTING ALGORITHMS 375
Distance-Vector Algorithm: Link-Cost Changes and Link Failure
When a node running the DV algorithm detects a change in the link cost from itself to
a neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, if there’s a
change in the cost of the least-cost path, informs its neighbors (Lines 16–17) of its new
distance vector. Figure 4.31(a) illustrates a scenario where the link cost from y to x
changes from 4 to 1. We focus here only on y’ and z’s distance table entries to destination
x. The DV algorithm causes the following sequence of events to occur:
• At time t0, y detects the link-cost change (the cost has changed from 4 to 1),
updates its distance vector, and informs its neighbors of this change since its distance
vector has changed.
• At time t1, z receives the update from y and updates its table. It computes a new
least cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new
distance vector to its neighbors.
• At time t2, y receives z’s update and updates its distance table. y’s least costs do
not change and hence y does not send any message to z. The algorithm comes to
a quiescent state.
Thus, only two iterations are required for the DV algorithm to reach a quiescent
state. The good news about the decreased cost between x and y has propagated
quickly through the network.
Let’s now consider what can happen when a link cost increases. Suppose that
the link cost between x and y increases from 4 to 60, as shown in Figure 4.31(b).
1. Before the link cost changes, Dy(x) = 4, Dy(z) = 1, Dz(y) = 1, and Dz(x) = 5. At
time t0, y detects the link-cost change (the cost has changed from 4 to 60). y
computes its new minimum-cost path to x to have a cost of
Dy(x) = min{c(y,x) + Dx(x), c(y,z) + Dz(x)} = min{60 + 0, 1 + 5} = 6
376 CHAPTER 4 • THE NETWORK LAYER
50
4
1 60
1
y
x
a. b.
z 50
4 1
y
x z
Figure 4.31  Changes in link cost
Of course, with our global view of the network, we can see that this new cost
via z is wrong. But the only information node y has is that its direct cost to x is
60 and that z has last told y that z could get to x with a cost of 5. So in order to
get to x, y would now route through z, fully expecting that z will be able to get
to x with a cost of 5. As of t1 we have a routing loop—in order to get to x, y
routes through z, and z routes through y. A routing loop is like a black hole—a
packet destined for x arriving at y or z as of t1 will bounce back and forth
between these two nodes forever (or until the forwarding tables are changed).
2. Since node y has computed a new minimum cost to x, it informs z of its new
distance vector at time t1.
3. Sometime after t1, z receives y’s new distance vector, which indicates that y’s
minimum cost to x is 6. z knows it can get to y with a cost of 1 and hence
computes a new least cost to x of Dz(x) = min{50 + 0,1 + 6} = 7. Since z’s
least cost to x has increased, it then informs y of its new distance vector at t2.
4. In a similar manner, after receiving z’s new distance vector, y determines
Dy(x) = 8 and sends z its distance vector. z then determines Dz(x) = 9 and
sends y its distance vector, and so on.
How long will the process continue? You should convince yourself that the loop
will persist for 44 iterations (message exchanges between y and z)—until z eventually
computes the cost of its path via y to be greater than 50. At this point, z will
(finally!) determine that its least-cost path to x is via its direct connection to x. y
will then route to x via z. The result of the bad news about the increase in link
cost has indeed traveled slowly! What would have happened if the link cost c(y,
x) had changed from 4 to 10,000 and the cost c(z, x) had been 9,999? Because of
such scenarios, the problem we have seen is sometimes referred to as the countto-
infinity problem.
Distance-Vector Algorithm: Adding Poisoned Reverse
The specific looping scenario just described can be avoided using a technique
known as poisoned reverse. The idea is simple—if z routes through y to get to
destination x, then z will advertise to y that its distance to x is infinity, that is, z will
advertise to y that Dz(x) = 8 (even though z knows Dz(x) = 5 in truth). z will continue
telling this little white lie to y as long as it routes to x via y. Since y believes
that z has no path to x, y will never attempt to route to x via z, as long as z continues
to route to x via y (and lies about doing so).
Let’s now see how poisoned reverse solves the particular looping problem we
encountered before in Figure 4.31(b). As a result of the poisoned reverse, y’s distance
table indicates Dz(x) = 8. When the cost of the (x, y) link changes from 4 to 60
at time t0, y updates its table and continues to route directly to x, albeit at a higher
cost of 60, and informs z of its new cost to x, that is, Dy(x) = 60. After receiving the
4.5 • ROUTING ALGORITHMS 377
update at t1, z immediately shifts its route to x to be via the direct (z, x) link at a cost
of 50. Since this is a new least-cost path to x, and since the path no longer passes
through y, z now informs y that Dz(x) = 50 at t2. After receiving the update from z, y
updates its distance table with Dy(x) = 51. Also, since z is now on y’s least-cost path
to x, y poisons the reverse path from z to x by informing z at time t3 that Dy(x) = 8
(even though y knows that Dy(x) = 51 in truth).
Does poisoned reverse solve the general count-to-infinity problem? It does not.
You should convince yourself that loops involving three or more nodes (rather than
simply two immediately neighboring nodes) will not be detected by the poisoned
reverse technique.
A Comparison of LS and DV Routing Algorithms
The DV and LS algorithms take complementary approaches towards computing
routing. In the DV algorithm, each node talks to only its directly connected neighbors,
but it provides its neighbors with least-cost estimates from itself to all the
nodes (that it knows about) in the network. In the LS algorithm, each node talks with
all other nodes (via broadcast), but it tells them only the costs of its directly connected
links. Let’s conclude our study of LS and DV algorithms with a quick comparison
of some of their attributes. Recall that N is the set of nodes (routers) and E
is the set of edges (links).
• Message complexity. We have seen that LS requires each node to know the
cost of each link in the network. This requires O(|N| |E|) messages to be sent.
Also, whenever a link cost changes, the new link cost must be sent to all
nodes. The DV algorithm requires message exchanges between directly connected
neighbors at each iteration. We have seen that the time needed for the
algorithm to converge can depend on many factors. When link costs change,
the DV algorithm will propagate the results of the changed link cost only if
the new link cost results in a changed least-cost path for one of the nodes
attached to that link.
• Speed of convergence. We have seen that our implementation of LS is an O(|N|2)
algorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly
and can have routing loops while the algorithm is converging. DV also suffers
from the count-to-infinity problem.
• Robustness. What can happen if a router fails, misbehaves, or is sabotaged?
Under LS, a router could broadcast an incorrect cost for one of its attached
links (but no others). A node could also corrupt or drop any packets it received
as part of an LS broadcast. But an LS node is computing only its own forwarding
tables; other nodes are performing similar calculations for themselves. This
means route calculations are somewhat separated under LS, providing a degree
of robustness. Under DV, a node can advertise incorrect least-cost paths to any
or all destinations. (Indeed, in 1997, a malfunctioning router in a small ISP
378 CHAPTER 4 • THE NETWORK LAYER
provided national backbone routers with erroneous routing information. This
caused other routers to flood the malfunctioning router with traffic and caused
large portions of the Internet to become disconnected for up to several hours
[Neumann 1997].) More generally, we note that, at each iteration, a node’s calculation
in DV is passed on to its neighbor and then indirectly to its neighbor’s
neighbor on the next iteration. In this sense, an incorrect node calculation can
be diffused through the entire network under DV.
In the end, neither algorithm is an obvious winner over the other; indeed, both algorithms
are used in the Internet.
Other Routing Algorithms
The LS and DV algorithms we have studied are not only widely used in practice,
they are essentially the only routing algorithms used in practice today in the Internet.
Nonetheless, many routing algorithms have been proposed by researchers
over the past 30 years, ranging from the extremely simple to the very sophisticated
and complex. A broad class of routing algorithms is based on viewing packet traffic
as flows between sources and destinations in a network. In this approach, the
routing problem can be formulated mathematically as a constrained optimization
problem known as a network flow problem [Bertsekas 1991]. Yet another set of
routing algorithms we mention here are those derived from the telephony world.
These circuit-switched routing algorithms are of interest to packet-switched
data networking in cases where per-link resources (for example, buffers, or a fraction
of the link bandwidth) are to be reserved for each connection that is routed
over the link. While the formulation of the routing problem might appear quite
different from the least-cost routing formulation we have seen in this chapter,
there are a number of similarities, at least as far as the path-finding algorithm
(routing algorithm) is concerned. See [Ash 1998; Ross 1995; Girard 1990] for a
detailed discussion of this research area.
4.5.3 Hierarchical Routing
In our study of LS and DV algorithms, we’ve viewed the network simply as a collection
of interconnected routers. One router was indistinguishable from another in
the sense that all routers executed the same routing algorithm to compute routing
paths through the entire network. In practice, this model and its view of a homogenous
set of routers all executing the same routing algorithm is a bit simplistic for at
least two important reasons:
• Scale. As the number of routers becomes large, the overhead involved in
computing, storing, and communicating routing information (for example,
4.5 • ROUTING ALGORITHMS 379
LS updates or least-cost path changes) becomes prohibitive. Today’s public
Internet consists of hundreds of millions of hosts. Storing routing information at
each of these hosts would clearly require enormous amounts of memory. The
overhead required to broadcast LS updates among all of the routers in the public
Internet would leave no bandwidth left for sending data packets! A distance-vector
algorithm that iterated among such a large number of routers would surely
never converge. Clearly, something must be done to reduce the complexity of
route computation in networks as large as the public Internet.
• Administrative autonomy. Although researchers tend to ignore issues such as a
company’s desire to run its routers as it pleases (for example, to run whatever
routing algorithm it chooses) or to hide aspects of its network’s internal organization
from the outside, these are important considerations. Ideally, an organization
should be able to run and administer its network as it wishes, while still
being able to connect its network to other outside networks.
Both of these problems can be solved by organizing routers into autonomous systems
(ASs), with each AS consisting of a group of routers that are typically under
the same administrative control (e.g., operated by the same ISP or belonging to the
same company network). Routers within the same AS all run the same routing algorithm
(for example, an LS or DV algorithm) and have information about each
other—exactly as was the case in our idealized model in the preceding section. The
routing algorithm running within an autonomous system is called an intraautonomous
system routing protocol. It will be necessary, of course, to connect
ASs to each other, and thus one or more of the routers in an AS will have the added
task of being responsible for forwarding packets to destinations outside the AS;
these routers are called gateway routers.
Figure 4.32 provides a simple example with three ASs: AS1, AS2, and AS3.
In this figure, the heavy lines represent direct link connections between pairs of
routers. The thinner lines hanging from the routers represent subnets that are
directly connected to the routers. AS1 has four routers—1a, 1b, 1c, and 1d—
which run the intra-AS routing protocol used within AS1. Thus, each of these
four routers knows how to forward packets along the optimal path to any destination
within AS1. Similarly, autonomous systems AS2 and AS3 each have three
routers. Note that the intra-AS routing protocols running in AS1, AS2, and AS3
need not be the same. Also note that the routers 1b, 1c, 2a, and 3a are all gateway
routers.
It should now be clear how the routers in an AS determine routing paths for
source-destination pairs that are internal to the AS. But there is still a big missing
piece to the end-to-end routing puzzle. How does a router, within some AS, know
how to route a packet to a destination that is outside the AS? It’s easy to answer
this question if the AS has only one gateway router that connects to only one
other AS. In this case, because the AS’s intra-AS routing algorithm has determined
the least-cost path from each internal router to the gateway router, each
380 CHAPTER 4 • THE NETWORK LAYER
internal router knows how it should forward the packet. The gateway router, upon
receiving the packet, forwards the packet on the one link that leads outside the
AS. The AS on the other side of the link then takes over the responsibility
of routing the packet to its ultimate destination. As an example, suppose router
2b in Figure 4.32 receives a packet whose destination is outside of AS2. Router
2b will then forward the packet to either router 2a or 2c, as specified by router
2b’s forwarding table, which was configured by AS2’s intra-AS routing protocol.
The packet will eventually arrive to the gateway router 2a, which will forward
the packet to 1b. Once the packet has left 2a, AS2’s job is done with this one
packet.
So the problem is easy when the source AS has only one link that leads outside
the AS. But what if the source AS has two or more links (through two or more gateway
routers) that lead outside the AS? Then the problem of knowing where to forward
the packet becomes significantly more challenging. For example, consider a
router in AS1 and suppose it receives a packet whose destination is outside the AS.
The router should clearly forward the packet to one of its two gateway routers, 1b or
1c, but which one? To solve this problem, AS1 needs (1) to learn which destinations
are reachable via AS2 and which destinations are reachable via AS3, and (2) to
propagate this reachability information to all the routers within AS1, so that each
router can configure its forwarding table to handle external-AS destinations. These
4.5 • ROUTING ALGORITHMS 381
AS1
AS3
3b
3c
3a
1a
1c
1b
1d
AS2
2a
2c
2b
Intra-AS routing
algorithm
Forwarding
table
Inter-AS routing
algorithm
Figure 4.32  An example of interconnected autonomous systems
two tasks—obtaining reachability information from neighboring ASs and propagating
the reachability information to all routers internal to the AS—are handled by the
inter-AS routing protocol. Since the inter-AS routing protocol involves communication
between two ASs, the two communicating ASs must run the same inter-AS
routing protocol. In fact, in the Internet all ASs run the same inter-AS routing protocol,
called BGP4, which is discussed in the next section. As shown in Figure 4.32,
each router receives information from an intra-AS routing protocol and an inter-AS
routing protocol, and uses the information from both protocols to configure its forwarding
table.
As an example, consider a subnet x (identified by its CIDRized address), and
suppose that AS1 learns from the inter-AS routing protocol that subnet x is reachable
from AS3 but is not reachable from AS2. AS1 then propagates this information
to all of its routers. When router 1d learns that subnet x is reachable from AS3, and
hence from gateway 1c, it then determines, from the information provided by the
intra-AS routing protocol, the router interface that is on the least-cost path from
router 1d to gateway router 1c. Say this is interface I. The router 1d can then put the
entry (x, I) into its forwarding table. (This example, and others presented in this section,
gets the general ideas across but is a simplification of what really happens in
the Internet. In the next section we’ll provide a more detailed description, albeit
more complicated, when we discuss BGP.)
Following up on the previous example, now suppose that AS2 and AS3 connect
to other ASs, which are not shown in the diagram. Also suppose that AS1
learns from the inter-AS routing protocol that subnet x is reachable both from AS2,
via gateway 1b, and from AS3, via gateway 1c. AS1 would then propagate this
information to all its routers, including router 1d. In order to configure its forwarding
table, router 1d would have to determine to which gateway router, 1b or 1c, it
should direct packets that are destined for subnet x. One approach, which is often
employed in practice, is to use hot-potato routing. In hot-potato routing, the AS
gets rid of the packet (the hot potato) as quickly as possible (more precisely, as
inexpensively as possible). This is done by having a router send the packet to the
gateway router that has the smallest router-to-gateway cost among all gateways
with a path to the destination. In the context of the current example, hot-potato
routing, running in 1d, would use information from the intra-AS routing protocol
to determine the path costs to 1b and 1c, and then choose the path with the least
cost. Once this path is chosen, router 1d adds an entry for subnet x in its forwarding
table. Figure 4.33 summarizes the actions taken at router 1d for adding the new
entry for x to the forwarding table.
When an AS learns about a destination from a neighboring AS, the AS can
advertise this routing information to some of its other neighboring ASs. For example,
suppose AS1 learns from AS2 that subnet x is reachable via AS2. AS1 could then tell
AS3 that x is reachable via AS1. In this manner, if AS3 needs to route a packet
destined to x, AS3 would forward the packet to AS1, which would in turn forward the
packet to AS2. As we’ll see in our discussion of BGP, an AS has quite a bit of
382 CHAPTER 4 • THE NETWORK LAYER
flexibility in deciding which destinations it advertises to its neighboring ASs. This
is a policy decision, typically depending more on economic issues than on technical
issues.
Recall from Section 1.5 that the Internet consists of a hierarchy of interconnected
ISPs. So what is the relationship between ISPs and ASs? You might think that
the routers in an ISP, and the links that interconnect them, constitute a single AS.
Although this is often the case, many ISPs partition their network into multiple ASs.
For example, some tier-1 ISPs use one AS for their entire network; others break up
their ISP into tens of interconnected ASs.
In summary, the problems of scale and administrative authority are solved by
defining autonomous systems. Within an AS, all routers run the same intra-AS routing
protocol. Among themselves, the ASs run the same inter-AS routing protocol.
The problem of scale is solved because an intra-AS router need only know about
routers within its AS. The problem of administrative authority is solved since an
organization can run whatever intra-AS routing protocol it chooses; however, each
pair of connected ASs needs to run the same inter-AS routing protocol to exchange
reachability information.
In the following section, we’ll examine two intra-AS routing protocols (RIP and
OSPF) and the inter-AS routing protocol (BGP) that are used in today’s Internet.
These case studies will nicely round out our study of hierarchical routing.
4.6 Routing in the Internet
Having studied Internet addressing and the IP protocol, we now turn our attention to
the Internet’s routing protocols; their job is to determine the path taken by a datagram
between source and destination. We’ll see that the Internet’s routing protocols
embody many of the principles we learned earlier in this chapter. The link-state and
distance-vector approaches studied in Sections 4.5.1 and 4.5.2 and the notion of an
autonomous system considered in Section 4.5.3 are all central to how routing is
done in today’s Internet.
4.6 • ROUTING IN THE INTERNET 383
Learn from inter-AS
protocol that subnet
x is reachable via
multiple gateways.
Use routing info from
intra-AS protocol to
determine costs of
least-cost paths to
each of the gateways.
Hot potato routing:
Choose the gateway
that has the
smallest least cost.
Determine from
forwarding table the
interface I that leads
to least-cost gateway.
Enter (x,I) in
forwarding table.
Figure 4.33  Steps in adding an outside-AS destination in a router’s forwarding
table
Recall from Section 4.5.3 that an autonomous system (AS) is a collection of
routers under the same administrative and technical control, and that all run the
same routing protocol among themselves. Each AS, in turn, typically contains multiple
subnets (where we use the term subnet in the precise, addressing sense in Section
4.4.2).
4.6.1 Intra-AS Routing in the Internet: RIP
An intra-AS routing protocol is used to determine how routing is performed within
an autonomous system (AS). Intra-AS routing protocols are also known as interior
gateway protocols. Historically, two routing protocols have been used extensively
for routing within an autonomous system in the Internet: the Routing Information
Protocol (RIP) and Open Shortest Path First (OSPF). A routing protocol closely
related to OSPF is the IS-IS protocol [RFC 1142, Perlman 1999]. We first discuss
RIP and then consider OSPF.
RIP was one of the earliest intra-AS Internet routing protocols and is still in
widespread use today. It traces its origins and its name to the Xerox Network Systems
(XNS) architecture. The widespread deployment of RIP was due in great part
to its inclusion in 1982 in the Berkeley Software Distribution (BSD) version of
UNIX supporting TCP/IP. RIP version 1 is defined in [RFC 1058], with a backwardcompatible
version 2 defined in [RFC 2453].
RIP is a distance-vector protocol that operates in a manner very close to the
idealized DV protocol we examined in Section 4.5.2. The version of RIP specified
in RFC 1058 uses hop count as a cost metric; that is, each link has a cost of 1. In
the DV algorithm in Section 4.5.2, for simplicity, costs were defined between pairs
of routers. In RIP (and also in OSPF), costs are actually from source router to a destination
subnet. RIP uses the term hop, which is the number of subnets traversed
along the shortest path from source router to destination subnet, including the destination
subnet. Figure 4.34 illustrates an AS with six leaf subnets. The table in the
figure indicates the number of hops from the source A to each of the leaf subnets.
The maximum cost of a path is limited to 15, thus limiting the use of RIP to
autonomous systems that are fewer than 15 hops in diameter. Recall that in DV
protocols, neighboring routers exchange distance vectors with each other. The
distance vector for any one router is the current estimate of the shortest path
distances from that router to the subnets in the AS. In RIP, routing updates
are exchanged between neighbors approximately every 30 seconds using a
RIP response message. The response message sent by a router or host contains
a list of up to 25 destination subnets within the AS, as well as the sender’s
distance to each of those subnets. Response messages are also known as RIP
advertisements.
Let’s take a look at a simple example of how RIP advertisements work. Consider
the portion of an AS shown in Figure 4.35. In this figure, lines connecting the
routers denote subnets. Only selected routers (A, B, C, and D) and subnets (w, x, y,
384 CHAPTER 4 • THE NETWORK LAYER
and z) are labeled. Dotted lines indicate that the AS continues on; thus this
autonomous system has many more routers and links than are shown.
Each router maintains a RIP table known as a routing table. A router’s routing
table includes both the router’s distance vector and the router’s forwarding table.
Figure 4.36 shows the routing table for router D. Note that the routing table has
three columns. The first column is for the destination subnet, the second column
indicates the identity of the next router along the shortest path to the destination subnet,
and the third column indicates the number of hops (that is, the number of subnets
that have to be traversed, including the destination subnet) to get to the
destination subnet along the shortest path. For this example, the table indicates that
to send a datagram from router D to destination subnet w, the datagram should first
be forwarded to neighboring router A; the table also indicates that destination subnet
w is two hops away along the shortest path. Similarly, the table indicates that
subnet z is seven hops away via router B. In principle, a routing table will have one
row for each subnet in the AS, although RIP version 2 allows subnet entries to be
aggregated using route aggregation techniques similar to those we examined in
4.6 • ROUTING IN THE INTERNET 385
C D
A
u Destination Hops
u
v
w
x
y
z
1
2
2
3
3
2
v
w
x
z y
B
Figure 4.34  Number of hops from source router A to various subnets
A
C
D B
z
w x y
Figure 4.35  A portion of an autonomous system
Section 4.4. The table in Figure 4.36, and the subsequent tables to come, are only
partially complete.
Now suppose that 30 seconds later, router D receives from router A the advertisement
shown in Figure 4.37. Note that this advertisement is nothing other than
the routing table information from router A! This information indicates, in particular,
that subnet z is only four hops away from router A. Router D, upon receiving this
advertisement, merges the advertisement (Figure 4.37) with the old routing table
(Figure 4.36). In particular, router D learns that there is now a path through router A
to subnet z that is shorter than the path through router B. Thus, router D updates its
routing table to account for the shorter shortest path, as shown in Figure 4.38. How
is it, you might ask, that the shortest path to subnet z has become shorter? Possibly,
the decentralized distance-vector algorithm is still in the process of converging (see
Section 4.5.2), or perhaps new links and/or routers were added to the AS, thus
changing the shortest paths in the AS.
Let’s next consider a few of the implementation aspects of RIP. Recall that
RIP routers exchange advertisements approximately every 30 seconds. If a router
does not hear from its neighbor at least once every 180 seconds, that neighbor is
considered to be no longer reachable; that is, either the neighbor has died or the
386 CHAPTER 4 • THE NETWORK LAYER
Destination Subnet Next Router Number of Hops to Destination
w A 2
y B 2
z B 7
x — 1
. . . . . . . . . . . .
Figure 4.36  Routing table in router D before receiving advertisement
from router A
Destination Subnet Next Router Number of Hops to Destination
z C 4
w — 1
x — 1
. . . . . . . . . . . .
Figure 4.37  Advertisement from router A
connecting link has gone down. When this happens, RIP modifies the local routing
table and then propagates this information by sending advertisements to its neighboring
routers (the ones that are still reachable). A router can also request information
about its neighbor’s cost to a given destination using RIP’s request message.
Routers send RIP request and response messages to each other over UDP using port
number 520. The UDP segment is carried between routers in a standard IP datagram.
The fact that RIP uses a transport-layer protocol (UDP) on top of a networklayer
protocol (IP) to implement network-layer functionality (a routing algorithm)
may seem rather convoluted (it is!). Looking a little deeper at how RIP is implemented
will clear this up.
Figure 4.39 sketches how RIP is typically implemented in a UNIX system, for
example, a UNIX workstation serving as a router. Aprocess called routed (pronounced
“route dee”) executes RIP, that is, maintains routing information and exchanges
messages with routed processes running in neighboring routers. Because RIP is
implemented as an application-layer process (albeit a very special one that is able to
4.6 • ROUTING IN THE INTERNET 387
Destination Subnet Next Router Number of Hops to Destination
w A 2
y B 2
z A 5
. . . . . . . . . . . .
Figure 4.38  Routing table in router D after receiving advertisement from
router A
Network
(IP)
Transport
(UDP)
Data link
Physical
Forwarding
tables
Routed
Forwarding
tables
Routed
Network
(IP)
Transport
(UDP)
Data link
Physical
Figure 4.39  Implementation of RIP as the routed daemon
manipulate the routing tables within the UNIX kernel), it can send and receive messages
over a standard socket and use a standard transport protocol. As shown, RIP is
implemented as an application-layer protocol (see Chapter 2) running over UDP. If
you’re interested in looking at an implementation of RIP (or the OSPF and BGP protocols
that we will study shortly), see [Quagga 2012].
4.6.2 Intra-AS Routing in the Internet: OSPF
Like RIP, OSPF routing is widely used for intra-AS routing in the Internet. OSPF
and its closely related cousin, IS-IS, are typically deployed in upper-tier ISPs
whereas RIP is deployed in lower-tier ISPs and enterprise networks. The Open in
OSPF indicates that the routing protocol specification is publicly available (for
example, as opposed to Cisco’s EIGRP protocol). The most recent version of OSPF,
version 2, is defined in RFC 2328, a public document.
OSPF was conceived as the successor to RIP and as such has a number of
advanced features. At its heart, however, OSPF is a link-state protocol that uses
flooding of link-state information and a Dijkstra least-cost path algorithm. With
OSPF, a router constructs a complete topological map (that is, a graph) of the entire
autonomous system. The router then locally runs Dijkstra’s shortest-path algorithm
to determine a shortest-path tree to all subnets, with itself as the root node. Individual
link costs are configured by the network administrator (see Principles and Practice:
Setting OSPF Weights). The administrator might choose to set all link costs to
1, thus achieving minimum-hop routing, or might choose to set the link weights to
be inversely proportional to link capacity in order to discourage traffic from using
low-bandwidth links. OSPF does not mandate a policy for how link weights are set
(that is the job of the network administrator), but instead provides the mechanisms
(protocol) for determining least-cost path routing for the given set of link weights.
With OSPF, a router broadcasts routing information to all other routers in the
autonomous system, not just to its neighboring routers. A router broadcasts linkstate
information whenever there is a change in a link’s state (for example, a change
in cost or a change in up/down status). It also broadcasts a link’s state periodically
(at least once every 30 minutes), even if the link’s state has not changed. RFC 2328
notes that “this periodic updating of link state advertisements adds robustness to the
link state algorithm.” OSPF advertisements are contained in OSPF messages that
are carried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus, the
OSPF protocol must itself implement functionality such as reliable message transfer
and link-state broadcast. The OSPF protocol also checks that links are operational
(via a HELLO message that is sent to an attached neighbor) and allows an OSPF
router to obtain a neighboring router’s database of network-wide link state.
Some of the advances embodied in OSPF include the following:
• Security. Exchanges between OSPF routers (for example, link-state updates)
can be authenticated. With authentication, only trusted routers can participate
388 CHAPTER 4 • THE NETWORK LAYER
in the OSPF protocol within an AS, thus preventing malicious intruders (or networking
students taking their newfound knowledge out for a joyride) from
injecting incorrect information into router tables. By default, OSPF packets
between routers are not authenticated and could be forged. Two types of
authentication can be configured—simple and MD5 (see Chapter 8 for a discussion
on MD5 and authentication in general). With simple authentication, the
same password is configured on each router. When a router sends an OSPF
packet, it includes the password in plaintext. Clearly, simple authentication is
not very secure. MD5 authentication is based on shared secret keys that are
configured in all the routers. For each OSPF packet that it sends, the router
computes the MD5 hash of the content of the OSPF packet appended with the
secret key. (See the discussion of message authentication codes in Chapter 7.)
Then the router includes the resulting hash value in the OSPF packet. The
receiving router, using the preconfigured secret key, will compute an MD5
hash of the packet and compare it with the hash value that the packet carries,
thus verifying the packet’s authenticity. Sequence numbers are also used with
MD5 authentication to protect against replay attacks.
• Multiple same-cost paths. When multiple paths to a destination have the same
cost, OSPF allows multiple paths to be used (that is, a single path need not be
chosen for carrying all traffic when multiple equal-cost paths exist).
• Integrated support for unicast and multicast routing. Multicast OSPF (MOSPF)
[RFC 1584] provides simple extensions to OSPF to provide for multicast routing
(a topic we cover in more depth in Section 4.7.2). MOSPF uses the existing
OSPF link database and adds a new type of link-state advertisement to the existing
OSPF link-state broadcast mechanism.
• Support for hierarchy within a single routing domain. Perhaps the most significant
advance in OSPF is the ability to structure an autonomous system hierarchically.
Section 4.5.3 has already looked at the many advantages of hierarchical
routing structures. We cover the implementation of OSPF hierarchical routing in
the remainder of this section.
An OSPF autonomous system can be configured hierarchically into areas. Each
area runs its own OSPF link-state routing algorithm, with each router in an area
broadcasting its link state to all other routers in that area. Within each area, one or
more area border routers are responsible for routing packets outside the area. Lastly,
exactly one OSPF area in the AS is configured to be the backbone area. The primary
role of the backbone area is to route traffic between the other areas in the AS. The
backbone always contains all area border routers in the AS and may contain nonborder
routers as well. Inter-area routing within the AS requires that the packet be first
routed to an area border router (intra-area routing), then routed through the backbone
to the area border router that is in the destination area, and then routed to the
final destination.
4.6 • ROUTING IN THE INTERNET 389
OSPF is a relatively complex protocol, and our coverage here has been necessarily
brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional details.
4.6.3 Inter-AS Routing: BGP
We just learned how ISPs use RIP and OSPF to determine optimal paths for sourcedestination
pairs that are internal to the same AS. Let’s now examine how paths are
determined for source-destination pairs that span multiple ASs. The Border Gateway
Protocol version 4, specified in RFC 4271 (see also [RFC 4274), is the de facto
standard inter-AS routing protocol in today’s Internet. It is commonly referred to as
BGP4 or simply as BGP. As an inter-AS routing protocol (see Section 4.5.3), BGP
provides each AS a means to
1. Obtain subnet reachability information from neighboring ASs.
2. Propagate the reachability information to all routers internal to the AS.
3. Determine “good” routes to subnets based on the reachability information and
on AS policy.
390 CHAPTER 4 • THE NETWORK LAYER
SETTING OSPF LINK WEIGHTS
Our discussion of link-state routing has implicitly assumed that link weights are set, a routing
algorithm such as OSPF is run, and traffic flows according to the routing tables computed
by the LS algorithm. In terms of cause and effect, the link weights are given (i.e., they
come first) and result (via Dijkstra’s algorithm) in routing paths that minimize overall cost. In
this viewpoint, link weights reflect the cost of using a link (e.g., if link weights are inversely
proportional to capacity, then the use of high-capacity links would have smaller weights
and thus be more attractive from a routing standpoint) and Disjkstra’s algorithm serves to
minimize overall cost.
In practice, the cause and effect relationship between link weights and routing paths
may be reversed, with network operators configuring link weights in order to obtain routing
paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example,
suppose a network operator has an estimate of traffic flow entering the network at each
ingress point and destined for each egress point. The operator may then want to put in
place a specific routing of ingress-to-egress flows that minimizes the maximum utilization
over all of the network’s links. But with a routing algorithm such as OSPF, the operator’s
main “knobs” for tuning the routing of flows through the network are the link weights. Thus,
in order to achieve the goal of minimizing the maximum link utilization, the operator must
find the set of link weights that achieves this goal. This is a reversal of the cause and effect
relationship—the desired routing of flows is known, and the OSPF link weights must be
found such that the OSPF routing algorithm results in this desired routing of flows.
PRINCIPLES IN PRACTICE
Most importantly, BGP allows each subnet to advertise its existence to the rest of
the Internet. A subnet screams “I exist and I am here,” and BGP makes sure that all
the ASs in the Internet know about the subnet and how to get there. If it weren’t for
BGP, each subnet would be isolated—alone and unknown by the rest of the Internet.
BGP Basics
BGP is extremely complex; entire books have been devoted to the subject and many
issues are still not well understood [Yannuzzi 2005]. Furthermore, even after having
read the books and RFCs, you may find it difficult to fully master BGP without having
practiced BGP for many months (if not years) as a designer or administrator of
an upper-tier ISP. Nevertheless, because BGP is an absolutely critical protocol for
the Internet—in essence, it is the protocol that glues the whole thing together—we
need to acquire at least a rudimentary understanding of how it works. We begin by
describing how BGP might work in the context of the simple example network we
studied earlier in Figure 4.32. In this description, we build on our discussion of hierarchical
routing in Section 4.5.3; we encourage you to review that material.
In BGP, pairs of routers exchange routing information over semipermanent
TCP connections using port 179. The semi-permanent TCP connections for the network
in Figure 4.32 are shown in Figure 4.40. There is typically one such BGP TCP
connection for each link that directly connects two routers in two different ASs;
thus, in Figure 4.40, there is a TCP connection between gateway routers 3a and 1c
and another TCP connection between gateway routers 1b and 2a. There are also
semipermanent BGP TCP connections between routers within an AS. In particular,
Figure 4.40 displays a common configuration of one TCP connection for each pair
of routers internal to an AS, creating a mesh of TCP connections within each AS.
For each TCP connection, the two routers at the end of the connection are called
BGP peers, and the TCP connection along with all the BGP messages sent over the
4.6 • ROUTING IN THE INTERNET 391
AS1
AS3
3b
eBGP session
Key:
iBGP session
3c
3a
1a
1c
1b
1d
AS2
2a
2c
2b
Figure 4.40  eBGP and iBGP sessions
VideoNote
Gluing the Internet
together
392 CHAPTER 4 • THE NETWORK LAYER
OBTAINING INTERNET PRESENCE: PUTTING THE PUZZLE TOGETHER
Suppose you have just created a small that has a number of servers, including a public
Web server that describes your company’s products and services, a mail server from which
your employees obtain their email messages, and a DNS server. Naturally, you would like
the entire world to be able to surf your Web site in order to learn about your exciting products
and services. Moreover, you would like your employees to be able to send and
receive email to potential customers throughout the world.
To meet these goals, you first need to obtain Internet connectivity, which is done by
contracting with, and connecting to, a local ISP. Your company will have a gateway
router, which will be connected to a router in your local ISP. This connection might be
a DSL connection through the existing telephone infrastructure, a leased line to the ISP’s
router, or one of the many other access solutions described in Chapter 1. Your local
ISP will also provide you with an IP address range, e.g., a /24 address range consisting
of 256 addresses. Once you have your physical connectivity and your IP address
range, you will assign one of the IP addresses (in your address range) to your Web
server, one to your mail server, one to your DNS server, one to your gateway router,
and other IP addresses to other servers and networking devices in your company’s
network.
In addition to contracting with an ISP, you will also need to contract with an Internet registrar
to obtain a domain name for your company, as described in Chapter 2. For example, if
your company’s name is, say, Xanadu Inc., you will naturally try to obtain the domain name
xanadu.com. Your company must also obtain presence in the DNS system. Specifically,
because outsiders will want to contact your DNS server to obtain the IP addresses of your
servers, you will also need to provide your registrar with the IP address of your DNS server.
Your registrar will then put an entry for your DNS server (domain name and corresponding IP
address) in the .com top-level-domain servers, as described in Chapter 2. After this step is
completed, any user who knows your domain name (e.g., xanadu.com) will be able to
obtain the IP address of your DNS server via the DNS system.
So that people can discover the IP addresses of your Web server, in your DNS server
you will need to include entries that map the host name of your Web server (e.g.,
www.xanadu.com) to its IP address. You will want to have similar entries for other publicly
available servers in your company, including your mail server. In this manner, if Alice
wants to browse your Web server, the DNS system will contact your DNS server, find the
IP address of your Web server, and give it to Alice. Alice can then establish a TCP
connection directly with your Web server.
However, there still remains one other necessary and crucial step to allow outsiders
from around the world access your Web server. Consider what happens when Alice,
who knows the IP address of your Web server, sends an IP datagram (e.g., a TCP SYN
segment) to that IP address. This datagram will be routed through the Internet, visiting a
series of routers in many different ASes, and eventually reach your Web server. When
PRINCIPLES IN PRACTICE
connection is called a BGP session. Furthermore, a BGP session that spans two ASs
is called an external BGP (eBGP) session, and a BGP session between routers in
the same AS is called an internal BGP (iBGP) session. In Figure 4.40, the eBGP
sessions are shown with the long dashes; the iBGP sessions are shown with the short
dashes. Note that BGP session lines in Figure 4.40 do not always correspond to the
physical links in Figure 4.32.
BGP allows each AS to learn which destinations are reachable via its neighboring
ASs. In BGP, destinations are not hosts but instead are CIDRized prefixes, with
each prefix representing a subnet or a collection of subnets. Thus, for example, suppose
there are four subnets attached to AS2: 138.16.64/24, 138.16.65/24,
138.16.66/24, and 138.16.67/24. Then AS2 could aggregate the prefixes for these four
subnets and use BGP to advertise the single prefix to 138.16.64/22 to AS1. As another
example, suppose that only the first three of those four subnets are in AS2 and the
fourth subnet, 138.16.67/24, is in AS3. Then, as described in the Principles and Practice
in Section 4.4.2, because routers use longest-prefix matching for forwarding datagrams,
AS3 could advertise to AS1 the more specific prefix 138.16.67/24 and AS2
could still advertise to AS1 the aggregated prefix 138.16.64/22.
Let’s now examine how BGP would distribute prefix reachability information
over the BGP sessions shown in Figure 4.40. As you might expect, using the eBGP
session between the gateway routers 3a and 1c, AS3 sends AS1 the list of prefixes
that are reachable from AS3; and AS1 sends AS3 the list of prefixes that are reachable
from AS1. Similarly, AS1 and AS2 exchange prefix reachability information
through their gateway routers 1b and 2a. Also as you may expect, when a gateway
router (in any AS) receives eBGP-learned prefixes, the gateway router uses its iBGP
sessions to distribute the prefixes to the other routers in the AS. Thus, all the routers
in AS1 learn about AS3 prefixes, including the gateway router 1b. The gateway
router 1b (in AS1) can therefore re-advertise AS3’s prefixes to AS2. When a router
(gateway or not) learns about a new prefix, it creates an entry for the prefix in its
forwarding table, as described in Section 4.5.3.
4.6 • ROUTING IN THE INTERNET 393
any one of the routers receives the datagram, it is going to look for an entry in its
forwarding table to determine on which outgoing port it should forward the datagram.
Therefore, each of the routers needs to know about the existence of your company’s
/24 prefix (or some aggregate entry). How does a router become aware of your
company’s prefix? As we have just seen, it becomes aware of it from BGP! Specifically,
when your company contracts with a local ISP and gets assigned a prefix (i.e., an
address range), your local ISP will use BGP to advertise this prefix to the ISPs to which
it connects. Those ISPs will then, in turn, use BGP to propagate the advertisement.
Eventually, all Internet routers will know about your prefix (or about some aggregate that
includes your prefix) and thus be able to appropriately forward datagrams destined to
your Web and mail servers.
Path Attributes and BGP Routes
Having now a preliminary understanding of BGP, let’s get a little deeper into it
(while still brushing some of the less important details under the rug!). In BGP, an
autonomous system is identified by its globally unique autonomous system number
(ASN) [RFC 1930]. (Technically, not every AS has an ASN. In particular, a socalled
stub AS that carries only traffic for which it is a source or destination will not
typically have an ASN; we ignore this technicality in our discussion in order to better
see the forest for the trees.) AS numbers, like IP addresses, are assigned by
ICANN regional registries [ICANN 2012].
When a router advertises a prefix across a BGP session, it includes with the prefix
a number of BGP attributes. In BGP jargon, a prefix along with its attributes is
called a route. Thus, BGP peers advertise routes to each other. Two of the more
important attributes are AS-PATH and NEXT-HOP:
• AS-PATH. This attribute contains the ASs through which the advertisement for the
prefix has passed. When a prefix is passed into an AS, the AS adds its ASN to the ASPATH
attribute. For example, consider Figure 4.40 and suppose that prefix
138.16.64/24 is first advertised from AS2 to AS1; if AS1 then advertises the prefix to
AS3, AS-PATH would be AS2 AS1. Routers use the AS-PATH attribute to detect and
prevent looping advertisements; specifically, if a router sees that its AS is contained
in the path list, it will reject the advertisement. As we’ll soon discuss, routers also use
the AS-PATH attribute in choosing among multiple paths to the same prefix.
• Providing the critical link between the inter-AS and intra-AS routing protocols, the
NEXT-HOP attribute has a subtle but important use. The NEXT-HOP is the router
interface that begins the AS-PATH. To gain insight into this attribute, let’s again refer
to Figure 4.40. Consider what happens when the gateway router 3a in AS3 advertises
a route to gateway router 1c in AS1 using eBGP. The route includes the advertised
prefix, which we’ll call x, and an AS-PATH to the prefix. This advertisement also
includes the NEXT-HOP, which is the IP address of the router 3a interface that leads
to 1c. (Recall that a router has multiple IP addresses, one for each of its interfaces.)
Now consider what happens when router 1d learns about this route from iBGP. After
learning about this route to x, router 1d may want to forward packets to x along the
route, that is, router 1d may want to include the entry (x, l) in its forwarding table,
where l is its interface that begins the least-cost path from 1d towards the gateway
router 1c. To determine l, 1d provides the IP address in the NEXT-HOP attribute to
its intra-AS routing module. Note that the intra-AS routing algorithm has determined
the least-cost path to all subnets attached to the routers in AS1, including to the subnet
for the link between 1c and 3a. From this least-cost path from 1d to the 1c-3a subnet,
1d determines its router interface l that begins this path and then adds the entry
(x, l) to its forwarding table. Whew! In summary, the NEXT-HOP attribute is used by
routers to properly configure their forwarding tables.
• Figure 4.41 illustrates another situation where the NEXT-HOP is needed. In this figure,
AS1 and AS2 are connected by two peering links. A router in AS1 could learn
394 CHAPTER 4 • THE NETWORK LAYER
about two different routes to the same prefix x. These two routes could have the same
AS-PATH to x, but could have different NEXT-HOP values corresponding to the different
peering links. Using the NEXT-HOP values and the intra-AS routing algorithm,
the router can determine the cost of the path to each peering link, and then
apply hot-potato routing (see Section 4.5.3) to determine the appropriate interface.
BGP also includes attributes that allow routers to assign preference metrics to
the routes, and an attribute that indicates how the prefix was inserted into BGP at
the origin AS. For a full discussion of route attributes, see [Griffin 2012; Stewart
1999; Halabi 2000; Feamster 2004; RFC 4271].
When a gateway router receives a route advertisement, it uses its import policy
to decide whether to accept or filter the route and whether to set certain attributes
such as the router preference metrics. The import policy may filter a route because
the AS may not want to send traffic over one of the ASs in the route’s AS-PATH.
The gateway router may also filter a route because it already knows of a preferable
route to the same prefix.
BGP Route Selection
As described earlier in this section, BGP uses eBGP and iBGP to distribute routes
to all the routers within ASs. From this distribution, a router may learn about more
than one route to any one prefix, in which case the router must select one of the
4.6 • ROUTING IN THE INTERNET 395
Two peering
links between
AS2 and AS1
Router learns
about a
route to x
Router learns about
another route to x
AS2
AS1
Key:
Route advertisements
message for destination x
Figure 4.41  NEXT-HOP attributes in advertisements are used to determine
which peering link to use
possible routes. The input into this route selection process is the set of all routes that
have been learned and accepted by the router. If there are two or more routes to the
same prefix, then BGP sequentially invokes the following elimination rules until one
route remains:
• Routes are assigned a local preference value as one of their attributes. The local
preference of a route could have been set by the router or could have been
learned by another router in the same AS. This is a policy decision that is left up
to the AS’s network administrator. (We will shortly discuss BGP policy issues in
some detail.) The routes with the highest local preference values are selected.
• From the remaining routes (all with the same local preference value), the route with
the shortest AS-PATH is selected. If this rule were the only rule for route selection,
then BGP would be using a DV algorithm for path determination, where the distance
metric uses the number of AS hops rather than the number of router hops.
• From the remaining routes (all with the same local preference value and the same
AS-PATH length), the route with the closest NEXT-HOP router is selected. Here,
closest means the router for which the cost of the least-cost path, determined by
the intra-AS algorithm, is the smallest. As discussed in Section 4.5.3, this process
is called hot-potato routing.
• If more than one route still remains, the router uses BGP identifiers to select the
route; see [Stewart 1999].
The elimination rules are even more complicated than described above. To avoid
nightmares about BGP, it’s best to learn about BGP selection rules in small doses!
396 CHAPTER 4 • THE NETWORK LAYER
PUTTING IT ALL TOGETHER: HOW DOES AN ENTRY GET INTO A ROUTER’S
FORWARDING TABLE?
Recall that an entry in a router’s forwarding table consists of a prefix (e.g., 138.16.64/22)
and a corresponding router output port (e.g., port 7). When a packet arrives to the router,
the packet’s destination IP address is compared with the prefixes in the forwarding table to
find the one with the longest prefix match. The packet is then forwarded (within the router)
to the router port associated with that prefix. Let’s now summarize how a routing entry
(prefix and associated port) gets entered into a forwarding table. This simple exercise will
tie together a lot of what we just learned about routing and forwarding. To make things
interesting, let’s assume that the prefix is a “foreign prefix,” that is, it does not belong to
the router’s AS but to some other AS.
In order for a prefix to get entered into the router’s forwarding table, the router has to
first become aware of the prefix (corresponding to a subnet or an aggregation of subnets).
As we have just learned, the router becomes aware of the prefix via a BGP route
PRINCIPLES IN PRACTICE
Routing Policy
Let’s illustrate some of the basic concepts of BGP routing policy with a simple example.
Figure 4.42 shows six interconnected autonomous systems: A, B, C, W, X, and Y.
It is important to note that A, B, C, W, X, and Y are ASs, not routers. Let’s assume that
autonomous systems W, X, and Yare stub networks and that A, B, and C are backbone
provider networks. We’ll also assume that A, B, and C, all peer with each other, and
provide full BGP information to their customer networks. All traffic entering a stub
network must be destined for that network, and all traffic leaving a stub network must
have originated in that network. W and Y are clearly stub networks. X is a multihomed
stub network, since it is connected to the rest of the network via two different
providers (a scenario that is becoming increasingly common in practice). However,
like Wand Y, X itself must be the source/destination of all traffic leaving/entering X.
But how will this stub network behavior be implemented and enforced? How will X
be prevented from forwarding traffic between B and C? This can easily be
advertisement. Such an advertisement may be sent to it over an eBGP session (from a
router in another AS) or over an iBGP session (from a router in the same AS).
After the router becomes aware of the prefix, it needs to determine the appropriate output
port to which datagrams destined to that prefix will be forwarded, before it can enter that
prefix in its forwarding table. If the router receives more than one route advertisement for this
prefix, the router uses the BGP route selection process, as described earlier in this subsection,
to find the “best” route for the prefix. Suppose such a best route has been selected. As
described earlier, the selected route includes a NEXT-HOP attribute, which is the IP address of
the first router outside the router’s AS along this best route. As described above, the router
then uses its intra-AS routing protocol (typically OSPF) to determine the shortest path to the
NEXT-HOP router. The router finally determines the port number to associate with the prefix
by identifying the first link along that shortest path. The router can then (finally!) enter the
prefix-port pair into its forwarding table! The forwarding table computed by the routing
processor (see Figure 4.6) is then pushed to the router’s input port line cards.
W A
X
Y
B
Key:
Provider
network
Customer
C network
Figure 4.42  A simple BGP scenario
4.6 • ROUTING IN THE INTERNET 397
398 CHAPTER 4 • THE NETWORK LAYER
WHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS
ROUTING PROTOCOLS?
Having now studied the details of specific inter-AS and intra-AS routing protocols deployed
in today’s Internet, let’s conclude by considering perhaps the most fundamental question we
could ask about these protocols in the first place (hopefully, you have been wondering this
all along, and have not lost the forest for the trees!): Why are different inter-AS and intra-
AS routing protocols used?
The answer to this question gets at the heart of the differences between the goals of
routing within an AS and among ASs:
• Policy. Among ASs, policy issues dominate. It may well be important that traffic originating
in a given AS not be able to pass through another specific AS. Similarly, a
given AS may well want to control what transit traffic it carries between other ASs. We
have seen that BGP carries path attributes and provides for controlled distribution of
routing information so that such policy-based routing decisions can be made. Within an
AS, everything is nominally under the same administrative control, and thus policy
issues play a much less important role in choosing routes within the AS.
• Scale. The ability of a routing algorithm and its data structures to scale to handle routing
to/among large numbers of networks is a critical issue in inter-AS routing. Within
an AS, scalability is less of a concern. For one thing, if a single administrative domain
becomes too large, it is always possible to divide it into two ASs and perform inter-AS
routing between the two new ASs. (Recall that OSPF allows such a hierarchy to be
built by splitting an AS into areas.)
• Performance. Because inter-AS routing is so policy oriented, the quality (for example,
performance) of the routes used is often of secondary concern (that is, a longer or more
costly route that satisfies certain policy criteria may well be taken over a route that is
shorter but does not meet that criteria). Indeed, we saw that among ASs, there is not
even the notion of cost (other than AS hop count) associated with routes. Within a single
AS, however, such policy concerns are of less importance, allowing routing to focus
more on the level of performance realized on a route.
PRINCIPLES IN PRACTICE
accomplished by controlling the manner in which BGP routes are advertised. In particular,
X will function as a stub network if it advertises (to its neighbors B and C) that
it has no paths to any other destinations except itself. That is, even though X may
know of a path, say XCY, that reaches network Y, it will not advertise this path to B.
Since B is unaware that X has a path to Y, B would never forward traffic destined to Y
(or C) via X. This simple example illustrates how a selective route advertisement policy
can be used to implement customer/provider routing relationships.
Let’s next focus on a provider network, say AS B. Suppose that B has learned
(from A) that A has a path AW to W. B can thus install the route BAW into its routing
information base. Clearly, B also wants to advertise the path BAW to its customer,
X, so that X knows that it can route to W via B. But should B advertise the
path BAW to C? If it does so, then C could route traffic to W via CBAW. If A, B,
and C are all backbone providers, than B might rightly feel that it should not have
to shoulder the burden (and cost!) of carrying transit traffic between A and C. B
might rightly feel that it is A’s and C’s job (and cost!) to make sure that C can route
to/from A’s customers via a direct connection between A and C. There are currently
no official standards that govern how backbone ISPs route among themselves. However,
a rule of thumb followed by commercial ISPs is that any traffic flowing across
an ISP’s backbone network must have either a source or a destination (or both) in a
network that is a customer of that ISP; otherwise the traffic would be getting a free
ride on the ISP’s network. Individual peering agreements (that would govern questions
such as those raised above) are typically negotiated between pairs of ISPs and
are often confidential; [Huston 1999a] provides an interesting discussion of peering
agreements. For a detailed description of how routing policy reflects commercial
relationships among ISPs, see [Gao 2001; Dmitiropoulos 2007]. For a discussion of
BGP routing polices from an ISP standpoint, see [Caesar 2005b].
As noted above, BGP is the de facto standard for inter-AS routing for the public
Internet. To see the contents of various BGP routing tables (large!) extracted
from routers in tier-1 ISPs, see http://www.routeviews.org. BGP routing tables
often contain tens of thousands of prefixes and corresponding attributes. Statistics
about the size and characteristics of BGP routing tables are presented in [Potaroo
2012].
This completes our brief introduction to BGP. Understanding BGP is important
because it plays a central role in the Internet. We encourage you to see the references
[Griffin 2012; Stewart 1999; Labovitz 1997; Halabi 2000; Huitema 1998; Gao
2001; Feamster 2004; Caesar 2005b; Li 2007] to learn more about BGP.
4.7 Broadcast and Multicast Routing
Thus far in this chapter, our focus has been on routing protocols that support unicast
(i.e., point-to-point) communication, in which a single source node sends a packet
to a single destination node. In this section, we turn our attention to broadcast and
multicast routing protocols. In broadcast routing, the network layer provides a
service of delivering a packet sent from a source node to all other nodes in the
network; multicast routing enables a single source node to send a copy of a packet
to a subset of the other network nodes. In Section 4.7.1 we’ll consider broadcast
routing algorithms and their embodiment in routing protocols. We’ll examine multicast
routing in Section 4.7.2.
4.7 • BROADCAST AND MULTICAST ROUTING 399
4.7.1 Broadcast Routing Algorithms
Perhaps the most straightforward way to accomplish broadcast communication is
for the sending node to send a separate copy of the packet to each destination, as
shown in Figure 4.43(a). Given N destination nodes, the source node simply makes
N copies of the packet, addresses each copy to a different destination, and then
transmits the N copies to the N destinations using unicast routing. This N-wayunicast
approach to broadcasting is simple—no new network-layer routing protocol,
packet-duplication, or forwarding functionality is needed. There are, however,
several drawbacks to this approach. The first drawback is its inefficiency. If the
source node is connected to the rest of the network via a single link, then N separate
copies of the (same) packet will traverse this single link. It would clearly be more
efficient to send only a single copy of a packet over this first hop and then have the
node at the other end of the first hop make and forward any additional needed
copies. That is, it would be more efficient for the network nodes themselves (rather
than just the source node) to create duplicate copies of a packet. For example, in
Figure 4.43(b), only a single copy of a packet traverses the R1-R2 link. That packet
is then duplicated at R2, with a single copy being sent over links R2-R3 and R2-R4.
The additional drawbacks of N-way-unicast are perhaps more subtle, but no less
important. An implicit assumption of N-way-unicast is that broadcast recipients, and
their addresses, are known to the sender. But how is this information obtained? Most
likely, additional protocol mechanisms (such as a broadcast membership or
destination-registration protocol) would be required. This would add more overhead
and, importantly, additional complexity to a protocol that had initially seemed quite
simple. A final drawback of N-way-unicast relates to the purposes for which broadcast
is to be used. In Section 4.5, we learned that link-state routing protocols use
broadcast to disseminate the link-state information that is used to compute unicast
routes. Clearly, in situations where broadcast is used to create and update unicast
routes, it would be unwise (at best!) to rely on the unicast routing infrastructure to
achieve broadcast.
400 CHAPTER 4 • THE NETWORK LAYER
R2
R3 R4
a. b.
Duplicate
Duplicate creation/transmission
R1
R3 R4
Duplicate
R1
R2
Figure 4.43  Source-duplication versus in-network duplication
Given the several drawbacks of N-way-unicast broadcast, approaches in which
the network nodes themselves play an active role in packet duplication, packet forwarding,
and computation of the broadcast routes are clearly of interest. We’ll
examine several such approaches below and again adopt the graph notation introduced
in Section 4.5. We again model the network as a graph, G = (N,E), where N
is a set of nodes and a collection E of edges, where each edge is a pair of nodes from
N. We’ll be a bit sloppy with our notation and use N to refer to both the set of nodes,
as well as the cardinality (|N|) or size of that set when there is no confusion.
Uncontrolled Flooding
The most obvious technique for achieving broadcast is a flooding approach in
which the source node sends a copy of the packet to all of its neighbors. When a
node receives a broadcast packet, it duplicates the packet and forwards it to all of its
neighbors (except the neighbor from which it received the packet). Clearly, if the
graph is connected, this scheme will eventually deliver a copy of the broadcast
packet to all nodes in the graph. Although this scheme is simple and elegant, it has a
fatal flaw (before you read on, see if you can figure out this fatal flaw): If the graph
has cycles, then one or more copies of each broadcast packet will cycle indefinitely.
For example, in Figure 4.43, R2 will flood to R3, R3 will flood to R4, R4 will flood
to R2, and R2 will flood (again!) to R3, and so on. This simple scenario results in
the endless cycling of two broadcast packets, one clockwise, and one counterclockwise.
But there can be an even more calamitous fatal flaw: When a node is connected
to more than two other nodes, it will create and forward multiple copies of
the broadcast packet, each of which will create multiple copies of itself (at other
nodes with more than two neighbors), and so on. This broadcast storm, resulting
from the endless multiplication of broadcast packets, would eventually result in so
many broadcast packets being created that the network would be rendered useless.
(See the homework questions at the end of the chapter for a problem analyzing the
rate at which such a broadcast storm grows.)
Controlled Flooding
The key to avoiding a broadcast storm is for a node to judiciously choose when
to flood a packet and (e.g., if it has already received and flooded an earlier copy of
a packet) when not to flood a packet. In practice, this can be done in one of several
ways.
In sequence-number-controlled flooding, a source node puts its address (or
other unique identifier) as well as a broadcast sequence number into a broadcast
packet, then sends the packet to all of its neighbors. Each node maintains a list of
the source address and sequence number of each broadcast packet it has already
received, duplicated, and forwarded. When a node receives a broadcast packet, it
first checks whether the packet is in this list. If so, the packet is dropped; if not, the
4.7 • BROADCAST AND MULTICAST ROUTING 401
packet is duplicated and forwarded to all the node’s neighbors (except the node from
which the packet has just been received). The Gnutella protocol, discussed in Chapter
2, uses sequence-number-controlled flooding to broadcast queries in its overlay
network. (In Gnutella, message duplication and forwarding is performed at the
application layer rather than at the network layer.)
A second approach to controlled flooding is known as reverse path forwarding
(RPF) [Dalal 1978], also sometimes referred to as reverse path broadcast (RPB). The
idea behind RPF is simple, yet elegant. When a router receives a broadcast packet
with a given source address, it transmits the packet on all of its outgoing links (except
the one on which it was received) only if the packet arrived on the link that is on its
own shortest unicast path back to the source. Otherwise, the router simply discards
the incoming packet without forwarding it on any of its outgoing links. Such a packet
can be dropped because the router knows it either will receive or has already received
a copy of this packet on the link that is on its own shortest path back to the sender.
(You might want to convince yourself that this will, in fact, happen and that looping
and broadcast storms will not occur.) Note that RPF does not use unicast routing to
actually deliver a packet to a destination, nor does it require that a router know the
complete shortest path from itself to the source. RPF need only know the next neighbor
on its unicast shortest path to the sender; it uses this neighbor’s identity only to
determine whether or not to flood a received broadcast packet.
Figure 4.44 illustrates RPF. Suppose that the links drawn with thick lines represent
the least-cost paths from the receivers to the source (A). Node A initially broadcasts
a source-A packet to nodes C and B. Node B will forward the source-A packet
it has received from A (since A is on its least-cost path to A) to both C and D. B will
ignore (drop, without forwarding) any source-A packets it receives from any other
402 CHAPTER 4 • THE NETWORK LAYER
A
B
D
G
C
F E
Key:
pkt will be forwarded
pkt not forwarded beyond receiving router
Figure 4.44  Reverse path forwarding
4.7 • BROADCAST AND MULTICAST ROUTING 403
a. Broadcast initiated at A b. Broadcast initiated at D
A
B
C
D
G
F E
A
B
C
D
G
F E
Figure 4.45  Broadcast along a spanning tree
nodes (for example, from routers C or D). Let us now consider node C, which will
receive a source-A packet directly from A as well as from B. Since B is not on C’s
own shortest path back to A, C will ignore any source-A packets it receives from B.
On the other hand, when C receives a source-A packet directly from A, it will forward
the packet to nodes B, E, and F.
Spanning-Tree Broadcast
While sequence-number-controlled flooding and RPF avoid broadcast storms, they
do not completely avoid the transmission of redundant broadcast packets. For example,
in Figure 4.44, nodes B, C, D, E, and F receive either one or two redundant
packets. Ideally, every node should receive only one copy of the broadcast packet.
Examining the tree consisting of the nodes connected by thick lines in Figure
4.45(a), you can see that if broadcast packets were forwarded only along links
within this tree, each and every network node would receive exactly one copy of the
broadcast packet—exactly the solution we were looking for! This tree is an example
of a spanning tree—a tree that contains each and every node in a graph. More formally,
a spanning tree of a graph G = (N,E) is a graph G = (N,E) such that E is a
subset of E, G is connected, G contains no cycles, and G contains all the original
nodes in G. If each link has an associated cost and the cost of a tree is the sum of the
link costs, then a spanning tree whose cost is the minimum of all of the graph’s
spanning trees is called (not surprisingly) a minimum spanning tree.
Thus, another approach to providing broadcast is for the network nodes to first
construct a spanning tree. When a source node wants to send a broadcast packet, it
sends the packet out on all of the incident links that belong to the spanning tree. A
node receiving a broadcast packet then forwards the packet to all its neighbors in the
spanning tree (except the neighbor from which it received the packet). Not only
does spanning tree eliminate redundant broadcast packets, but once in place, the
spanning tree can be used by any node to begin a broadcast, as shown in Figures
4.45(a) and 4.45(b). Note that a node need not be aware of the entire tree; it simply
needs to know which of its neighbors in G are spanning-tree neighbors.
The main complexity associated with the spanning-tree approach is the creation
and maintenance of the spanning tree. Numerous distributed spanning-tree algorithms
have been developed [Gallager 1983, Gartner 2003]. We consider only one
simple algorithm here. In the center-based approach to building a spanning tree, a
center node (also known as a rendezvous point or a core) is defined. Nodes then
unicast tree-join messages addressed to the center node. A tree-join message is forwarded
using unicast routing toward the center until it either arrives at a node that
already belongs to the spanning tree or arrives at the center. In either case, the path
that the tree-join message has followed defines the branch of the spanning tree
between the edge node that initiated the tree-join message and the center. One can
think of this new path as being grafted onto the existing spanning tree.
Figure 4.46 illustrates the construction of a center-based spanning tree. Suppose
that node E is selected as the center of the tree. Suppose that node F first joins the tree
and forwards a tree-join message to E. The single link EF becomes the initial spanning
tree. Node B then joins the spanning tree by sending its tree-join message to E.
Suppose that the unicast path route to E from B is via D. In this case, the tree-join
message results in the path BDE being grafted onto the spanning tree. Node A next
joins the spanning group by forwarding its tree-join message towards E. If A’s unicast
path to E is through B, then since B has already joined the spanning tree, the
arrival of A’s tree-join message at B will result in the AB link being immediately
grafted onto the spanning tree. Node C joins the spanning tree next by forwarding
its tree-join message directly to E. Finally, because the unicast routing from G to E
404 CHAPTER 4 • THE NETWORK LAYER
3
2
4
1
5
a. Stepwise construction of spanning tree b. Constructed spanning tree
A
B
C
D
G
F E
A
B
C
D
G
F E
Figure 4.46  Center-based construction of a spanning tree
must be via node D, when G sends its tree-join message to E, the GD link is grafted
onto the spanning tree at node D.
Broadcast Algorithms in Practice
Broadcast protocols are used in practice at both the application and network layers.
Gnutella [Gnutella 2009] uses application-level broadcast in order to broadcast
queries for content among Gnutella peers. Here, a link between two distributed
application-level peer processes in the Gnutella network is actually a TCP connection.
Gnutella uses a form of sequence-number-controlled flooding in which a 16-
bit identifier and a 16-bit payload descriptor (which identifies the Gnutella message
type) are used to detect whether a received broadcast query has been previously
received, duplicated, and forwarded. Gnutella also uses a time-to-live (TTL) field to
limit the number of hops over which a flooded query will be forwarded. When a
Gnutella process receives and duplicates a query, it decrements the TTL field before
forwarding the query. Thus, a flooded Gnutella query will only reach peers that are
within a given number (the initial value of TTL) of application-level hops from the
query initiator. Gnutella’s flooding mechanism is thus sometimes referred to as
limited-scope flooding.
Aform of sequence-number-controlled flooding is also used to broadcast link-state
advertisements (LSAs) in the OSPF [RFC 2328, Perlman 1999] routing algorithm, and in
the Intermediate-System-to-Intermediate-System (IS-IS) routing algorithm [RFC
1142, Perlman 1999]. OSPF uses a 32-bit sequence number, as well as a 16-bit age field
to identify LSAs. Recall that an OSPF node broadcasts LSAs for its attached links periodically,
when a link cost to a neighbor changes, or when a link goes up/down. LSA
sequence numbers are used to detect duplicate LSAs, but also serve a second important
function in OSPF. With flooding, it is possible for an LSA generated by the source at
time t to arrive after a newer LSA that was generated by the same source at time t + d.
The sequence numbers used by the source node allow an older LSA to be distinguished
from a newer LSA. The age field serves a purpose similar to that of a TTL value. The initial
age field value is set to zero and is incremented at each hop as it is flooded, and is also
incremented as it sits in a router’s memory waiting to be flooded. Although we have only
briefly described the LSA flooding algorithm here, we note that designing LSA broadcast
protocols can be very tricky business indeed. [RFC 789; Perlman 1999] describe an incident
in which incorrectly transmitted LSAs by two malfunctioning routers caused an
early version of an LSA flooding algorithm to take down the entire ARPAnet!
4.7.2 Multicast
We’ve seen in the previous section that with broadcast service, packets are delivered
to each and every node in the network. In this section we turn our attention to
multicast service, in which a multicast packet is delivered to only a subset of
network nodes. A number of emerging network applications require the delivery of
packets from one or more senders to a group of receivers. These applications include
4.7 • BROADCAST AND MULTICAST ROUTING 405
bulk data transfer (for example, the transfer of a software upgrade from the software
developer to users needing the upgrade), streaming continuous media (for example,
the transfer of the audio, video, and text of a live lecture to a set of distributed lecture
participants), shared data applications (for example, a whiteboard or teleconferencing
application that is shared among many distributed participants), data feeds
(for example, stock quotes), Web cache updating, and interactive gaming (for example,
distributed interactive virtual environments or multiplayer games).
In multicast communication, we are immediately faced with two problems—
how to identify the receivers of a multicast packet and how to address a packet sent
to these receivers. In the case of unicast communication, the IP address of the
receiver (destination) is carried in each IP unicast datagram and identifies the single
recipient; in the case of broadcast, all nodes need to receive the broadcast packet, so
no destination addresses are needed. But in the case of multicast, we now have multiple
receivers. Does it make sense for each multicast packet to carry the IP
addresses of all of the multiple recipients? While this approach might be workable
with a small number of recipients, it would not scale well to the case of hundreds or
thousands of receivers; the amount of addressing information in the datagram would
swamp the amount of data actually carried in the packet’s payload field. Explicit
identification of the receivers by the sender also requires that the sender know the
identities and addresses of all of the receivers. We will see shortly that there are
cases where this requirement might be undesirable.
For these reasons, in the Internet architecture (and other network architectures
such as ATM [Black 1995]), a multicast packet is addressed using address indirection.
That is, a single identifier is used for the group of receivers, and a copy of the
packet that is addressed to the group using this single identifier is delivered to all of
the multicast receivers associated with that group. In the Internet, the single identifier
that represents a group of receivers is a class D multicast IP address. The group of
receivers associated with a class D address is referred to as a multicast group. The
multicast group abstraction is illustrated in Figure 4.47. Here, four hosts (shown in
shaded color) are associated with the multicast group address of 226.17.30.197 and
will receive all datagrams addressed to that multicast address. The difficulty that we
must still address is the fact that each host has a unique IP unicast address that is completely
independent of the address of the multicast group in which it is participating.
While the multicast group abstraction is simple, it raises a host (pun intended)
of questions. How does a group get started and how does it terminate? How is the
group address chosen? How are new hosts added to the group (either as senders or
receivers)? Can anyone join a group (and send to, or receive from, that group) or is
group membership restricted and, if so, by whom? Do group members know the
identities of the other group members as part of the network-layer protocol? How
do the network nodes interoperate with each other to deliver a multicast datagram to
all group members? For the Internet, the answers to all of these questions involve
the Internet Group Management Protocol [RFC 3376]. So, let us next briefly consider
IGMP and then return to these broader questions.
406 CHAPTER 4 • THE NETWORK LAYER
Internet Group Management Protocol
The IGMP protocol version 3 [RFC 3376] operates between a host and its directly
attached router (informally, we can think of the directly attached router as the firsthop
router that a host would see on a path to any other host outside its own local
network, or the last-hop router on any path to that host), as shown in Figure 4.48.
Figure 4.48 shows three first-hop multicast routers, each connected to its attached
hosts via one outgoing local interface. This local interface is attached to a LAN in
this example, and while each LAN has multiple attached hosts, at most a few of
these hosts will typically belong to a given multicast group at any given time.
IGMP provides the means for a host to inform its attached router that an application
running on the host wants to join a specific multicast group. Given that the scope of
IGMP interaction is limited to a host and its attached router, another protocol is clearly
required to coordinate the multicast routers (including the attached routers) throughout
4.7 • BROADCAST AND MULTICAST ROUTING 407
128.119.40.186
Key:
Router with attached
group member
Router with no attached
group member
128.34.108.63
128.34.108.60
128.59.16.20
mcast group
226.17.30.197
Figure 4.47  The multicast group: A datagram addressed to the group is
delivered to all members of the multicast group
the Internet, so that multicast datagrams are routed to their final destinations. This latter
functionality is accomplished by network-layer multicast routing algorithms, such as
those we will consider shortly. Network-layer multicast in the Internet thus consists of
two complementary components: IGMP and multicast routing protocols.
IGMP has only three message types. Like ICMP, IGMP messages are carried
(encapsulated) within an IP datagram, with an IP protocol number of 2. The membership_
query message is sent by a router to all hosts on an attached interface
(for example, to all hosts on a local area network) to determine the set of all multicast
groups that have been joined by the hosts on that interface. Hosts respond to a membership_
query message with an IGMP membership_report message.
membership_report messages can also be generated by a host when an
application first joins a multicast group without waiting for a membership_query
message from the router. The final type of IGMP message is the leave_group
message. Interestingly, this message is optional. But if it is optional, how does a
router detect when a host leaves the multicast group? The answer to this question is
that the router infers that a host is no longer in the multicast group if it no longer
responds to a membership_query message with the given group address. This is
an example of what is sometimes called soft state in an Internet protocol. In a softstate
protocol, the state (in this case of IGMP, the fact that there are hosts joined to a
given multicast group) is removed via a timeout event (in this case, via a periodic
membership_query message from the router) if it is not explicitly refreshed (in
this case, by a membership_report message from an attached host).
The term soft state was coined by Clark [Clark 1988], who described the notion
of periodic state refresh messages being sent by an end system, and suggested that
408 CHAPTER 4 • THE NETWORK LAYER
Wide-area
multicast
routing
IGMP
IGMP
IGMP
IGMP
Figure 4.48  The two components of network-layer multicast in the
Internet: IGMP and multicast routing protocols
with such refresh messages, state could be lost in a crash and then automatically
restored by subsequent refresh messages—all transparently to the end system and
without invoking any explicit crash-recovery procedures:
“. . . the state information would not be critical in maintaining the desired
type of service associated with the flow. Instead, that type of service would
be enforced by the end points, which would periodically send messages to
ensure that the proper type of service was being associated with the flow. In
this way, the state information associated with the flow could be lost in a
crash without permanent disruption of the service features being used. I call
this concept “soft state,” and it may very well permit us to achieve our primary
goals of survivability and flexibility. . .”
It has been argued that soft-state protocols result in simpler control than hardstate
protocols, which not only require state to be explicitly added and removed, but
also require mechanisms to recover from the situation where the entity responsible
for removing state has terminated prematurely or failed. Interesting discussions of
soft state can be found in [Raman 1999; Ji 2003; Lui 2004].
Multicast Routing Algorithms
The multicast routing problem is illustrated in Figure 4.49. Hosts joined to the multicast
group are shaded in color; their immediately attached router is also shaded in
color. As shown in Figure 4.49, only a subset of routers (those with attached hosts that
are joined to the multicast group) actually needs to receive the multicast traffic. In Figure
4.49, only routers A, B, E, and F need to receive the multicast traffic. Since none
of the hosts attached to router D are joined to the multicast group and since router C
has no attached hosts, neither C nor D needs to receive the multicast group traffic. The
goal of multicast routing, then, is to find a tree of links that connects all of the routers
that have attached hosts belonging to the multicast group. Multicast packets will then
be routed along this tree from the sender to all of the hosts belonging to the multicast
tree. Of course, the tree may contain routers that do not have attached hosts belonging
to the multicast group (for example, in Figure 4.49, it is impossible to connect routers
A, B, E, and F in a tree without involving either router C or D).
In practice, two approaches have been adopted for determining the multicast
routing tree, both of which we have already studied in the context of broadcast
routing, and so we will only mention them in passing here. The two approaches differ
according to whether a single group-shared tree is used to distribute the traffic
for all senders in the group, or whether a source-specific routing tree is constructed
for each individual sender.
• Multicast routing using a group-shared tree. As in the case of spanning-tree broadcast,
multicast routing over a group-shared tree is based on building a tree that
includes all edge routers with attached hosts belonging to the multicast group.
In practice, a center-based approach is used to construct the multicast routing tree,
with edge routers with attached hosts belonging to the multicast group sending
4.7 • BROADCAST AND MULTICAST ROUTING 409
(via unicast) join messages addressed to the center node. As in the broadcast case, a
join message is forwarded using unicast routing toward the center until it either
arrives at a router that already belongs to the multicast tree or arrives at the center.
All routers along the path that the join message follows will then forward received
multicast packets to the edge router that initiated the multicast join. A critical question
for center-based tree multicast routing is the process used to select the center.
Center-selection algorithms are discussed in [Wall 1980; Thaler 1997; Estrin 1997].
• Multicast routing using a source-based tree. While group-shared tree multicast
routing constructs a single, shared routing tree to route packets from all senders,
the second approach constructs a multicast routing tree for each source in the
multicast group. In practice, an RPF algorithm (with source node x) is used to
construct a multicast forwarding tree for multicast datagrams originating at
source x. The RPF broadcast algorithm we studied earlier requires a bit of tweaking
for use in multicast. To see why, consider router D in Figure 4.50. Under
broadcast RPF, it would forward packets to router G, even though router G has
no attached hosts that are joined to the multicast group. While this is not so bad
for this case where D has only a single downstream router, G, imagine what
would happen if there were thousands of routers downstream from D! Each
of these thousands of routers would receive unwanted multicast packets.
410 CHAPTER 4 • THE NETWORK LAYER
A
C
F
B
D
E
Figure 4.49  Multicast hosts, their attached routers, and other routers
(This scenario is not as far-fetched as it might seem. The initial MBone [Casner
1992; Macedonia 1994], the first global multicast network, suffered from precisely
this problem at first.). The solution to the problem of receiving unwanted
multicast packets under RPF is known as pruning. A multicast router that
receives multicast packets and has no attached hosts joined to that group will send
a prune message to its upstream router. If a router receives prune messages from
each of its downstream routers, then it can forward a prune message upstream.
Multicast Routing in the Internet
The first multicast routing protocol used in the Internet was the Distance-Vector Multicast
Routing Protocol (DVMRP) [RFC 1075]. DVMRP implements source-based
trees with reverse path forwarding and pruning. DVMRP uses an RPF algorithm with
pruning, as discussed above. Perhaps the most widely used Internet multicast routing
protocol is the Protocol-Independent Multicast (PIM) routing protocol, which
explicitly recognizes two multicast distribution scenarios. In dense mode [RFC 3973],
multicast group members are densely located; that is, many or most of the routers in
the area need to be involved in routing multicast datagrams. PIM dense mode is a
flood-and-prune reverse path forwarding technique similar in spirit to DVMRP.
4.7 • BROADCAST AND MULTICAST ROUTING 411
A
C
F
Key:
pkt will be forwarded
E G
B
S: source
D
pkt not forwarded beyond receiving router
Figure 4.50  Reverse path forwarding, the multicast case
In sparse mode [RFC 4601], the number of routers with attached group members
is small with respect to the total number of routers; group members are widely
dispersed. PIM sparse mode uses rendezvous points to set up the multicast distribution
tree. In source-specific multicast (SSM) [RFC 3569, RFC 4607], only a
single sender is allowed to send traffic into the multicast tree, considerably simplifying
tree construction and maintenance.
When PIM and DVMP are used within a domain, the network operator can configure
IP multicast routers within the domain, in much the same way that intradomain
unicast routing protocols such as RIP, IS-IS, and OSPF can be configured.
But what happens when multicast routes are needed between different domains? Is
there a multicast equivalent of the inter-domain BGP protocol? The answer is (literally)
yes. [RFC 4271] defines multiprotocol extensions to BGP to allow it to carry
routing information for other protocols, including multicast information. The Multicast
Source Discovery Protocol (MSDP) [RFC 3618, RFC 4611] can be used to connect
together rendezvous points in different PIM sparse mode domains. An excellent
overview of the current state of multicast routing in the Internet is [RFC 5110].
Let us close our discussion of IP multicast by noting that IP multicast has yet to
take off in a big way. For interesting discussions of the Internet multicast service
model and deployment issues, see [Diot 2000, Sharma 2003]. Nonetheless, in spite
of the lack of widespread deployment, network-level multicast is far from “dead.”
Multicast traffic has been carried for many years on Internet 2, and the networks
with which it peers [Internet2 Multicast 2012]. In the United Kingdom, the BBC is
engaged in trials of content distribution via IP multicast [BBC Multicast 2012]. At
the same time, application-level multicast, as we saw with PPLive in Chapter 2 and
in other peer-to-peer systems such as End System Multicast [Chu 2002], provides
multicast distribution of content among peers using application-layer (rather than
network-layer) multicast protocols. Will future multicast services be primarily
implemented in the network layer (in the network core) or in the application layer (at
the network’s edge)? While the current craze for content distribution via peer-to-peer
approaches tips the balance in favor of application-layer multicast at least in the nearterm
future, progress continues to be made in IP multicast, and sometimes the race
ultimately goes to the slow and steady.
4.8 Summary
In this chapter, we began our journey into the network core. We learned that the
network layer involves each and every host and router in the network. Because of
this, network-layer protocols are among the most challenging in the protocol stack.
We learned that a router may need to process millions of flows of packets
between different source-destination pairs at the same time. To permit a router to
process such a large number of flows, network designers have learned over the years
that the router’s tasks should be as simple as possible. Many measures can be taken
412 CHAPTER 4 • THE NETWORK LAYER
to make the router’s job easier, including using a datagram network layer rather than
a virtual-circuit network layer, using a streamlined and fixed-sized header (as in
IPv6), eliminating fragmentation (also done in IPv6), and providing the one and
only best-effort service. Perhaps the most important trick here is not to keep track of
individual flows, but instead base routing decisions solely on hierarchically structured
destination addresses in the datagrams. It is interesting to note that the postal
service has been using this approach for many years.
In this chapter, we also looked at the underlying principles of routing algorithms.
We learned how routing algorithms abstract the computer network to a graph with
nodes and links. With this abstraction, we can exploit the rich theory of shortest-path
routing in graphs, which has been developed over the past 40 years in the operations
research and algorithms communities. We saw that there are two broad approaches: a
centralized (global) approach, in which each node obtains a complete map of the network
and independently applies a shortest-path routing algorithm; and a decentralized
approach, in which individual nodes have only a partial picture of the entire
network, yet the nodes work together to deliver packets along the shortest routes. We
also studied how hierarchy is used to deal with the problem of scale by partitioning
large networks into independent administrative domains called autonomous systems
(ASs). Each AS independently routes its datagrams through the AS, just as each
country independently routes its postal mail through the country. We learned how
centralized, decentralized, and hierarchical approaches are embodied in the principal
routing protocols in the Internet: RIP, OSPF, and BGP. We concluded our study of
routing algorithms by considering broadcast and multicast routing.
Having completed our study of the network layer, our journey now takes us one
step further down the protocol stack, namely, to the link layer. Like the network layer,
the link layer is also part of the network core. But we will see in the next chapter that
the link layer has the much more localized task of moving packets between nodes on
the same link or LAN. Although this task may appear on the surface to be trivial compared
with that of the network layer’s tasks, we will see that the link layer involves a
number of important and fascinating issues that can keep us busy for a long time.
Homework Problems and Questions
Chapter 4 Review Questions
SECTIONS 4.1–4.2
R1. Let’s review some of the terminology used in this textbook. Recall that the
name of a transport-layer packet is segment and that the name of a link-layer
packet is frame. What is the name of a network-layer packet? Recall that both
routers and link-layer switches are called packet switches. What is the
fundamental difference between a router and link-layer switch? Recall that
we use the term routers for both datagram networks and VC networks.
HOMEWORK PROBLEMS AND QUESTIONS 413
R2. What are the two most important network-layer functions in a datagram network?
What are the three most important network-layer functions in a virtualcircuit
network?
R3. What is the difference between routing and forwarding?
R4. Do the routers in both datagram networks and virtual-circuit networks use forwarding
tables? If so, describe the forwarding tables for both classes of networks.
R5. Describe some hypothetical services that the network layer can provide to a
single packet. Do the same for a flow of packets. Are any of your hypothetical
services provided by the Internet’s network layer? Are any provided by
ATM’s CBR service model? Are any provided by ATM’s ABR service
model?
R6. List some applications that would benefit from ATM’s CBR service model.
SECTION 4.3
R7. Discuss why each input port in a high-speed router stores a shadow copy of
the forwarding table.
R8. Three types of switching fabrics are discussed in Section 4.3. List and briefly
describe each type. Which, if any, can send multiple packets across the fabric
in parallel?
R9. Describe how packet loss can occur at input ports. Describe how packet loss
at input ports can be eliminated (without using infinite buffers).
R10. Describe how packet loss can occur at output ports. Can this loss be
prevented by increasing the switch fabric speed?
R11. What is HOL blocking? Does it occur in input ports or output ports?
SECTION 4.4
R12. Do routers have IP addresses? If so, how many?
R13. What is the 32-bit binary equivalent of the IP address 223.1.3.27?
R14. Visit a host that uses DHCP to obtain its IP address, network mask, default
router, and IP address of its local DNS server. List these values.
R15. Suppose there are three routers between a source host and a destination host.
Ignoring fragmentation, an IP datagram sent from the source host to the destination
host will travel over how many interfaces? How many forwarding tables
will be indexed to move the datagram from the source to the destination?
R16. Suppose an application generates chunks of 40 bytes of data every 20 msec,
and each chunk gets encapsulated in a TCP segment and then an IP datagram.
What percentage of each datagram will be overhead, and what percentage
will be application data?
R17. Suppose Host A sends Host B a TCP segment encapsulated in an IP datagram.
When Host B receives the datagram, how does the network layer in Host B
414 CHAPTER 4 • THE NETWORK LAYER
know it should pass the segment (that is, the payload of the datagram) to TCP
rather than to UDP or to something else?
R18. Suppose you purchase a wireless router and connect it to your cable modem.
Also suppose that your ISP dynamically assigns your connected device (that
is, your wireless router) one IP address. Also suppose that you have five PCs
at home that use 802.11 to wirelessly connect to your wireless router. How
are IP addresses assigned to the five PCs? Does the wireless router use NAT?
Why or why not?
R19. Compare and contrast the IPv4 and the IPv6 header fields. Do they have any
fields in common?
R20. It has been said that when IPv6 tunnels through IPv4 routers, IPv6 treats the
IPv4 tunnels as link-layer protocols. Do you agree with this statement? Why
or why not?
SECTION 4.5
R21. Compare and contrast link-state and distance-vector routing algorithms.
R22. Discuss how a hierarchical organization of the Internet has made it possible
to scale to millions of users.
R23. Is it necessary that every autonomous system use the same intra-AS routing
algorithm? Why or why not?
SECTION 4.6
R24. Consider Figure 4.37. Starting with the original table in D, suppose that D
receives from A the following advertisement:
HOMEWORK PROBLEMS AND QUESTIONS 415
Will the table in D change? If so how?
R25. Compare and contrast the advertisements used by RIP and OSPF.
R26. Fill in the blank: RIP advertisements typically announce the number of hops
to various destinations. BGP updates, on the other hand, announce the
__________ to the various destinations.
R27. Why are different inter-AS and intra-AS protocols used in the Internet?
R28. Why are policy considerations as important for intra-AS protocols, such as
OSPF and RIP, as they are for an inter-AS routing protocol like BGP?
Destination Subnet Next Router Number of Hops to Destination
z C 10
w — 1
x — 1
. . . . . . . . . . . .
R29. Define and contrast the following terms: subnet, prefix, and BGP route.
R30. How does BGP use the NEXT-HOP attribute? How does it use the AS-PATH
attribute?
R31. Describe how a network administrator of an upper-tier ISP can implement
policy when configuring BGP.
SECTION 4.7
R32. What is an important difference between implementing the broadcast abstraction
via multiple unicasts, and a single network- (router-) supported broadcast?
R33. For each of the three general approaches we studied for broadcast communication
(uncontrolled flooding, controlled flooding, and spanning-tree broadcast),
are the following statements true or false? You may assume that no
packets are lost due to buffer overflow and all packets are delivered on a link
in the order in which they were sent.
a. A node may receive multiple copies of the same packet.
b. A node may forward multiple copies of a packet over the same
outgoing link.
R34. When a host joins a multicast group, must it change its IP address to that of
the multicast group it is joining?
R35. What are the roles played by the IGMP protocol and a wide-area multicast
routing protocol?
R36. What is the difference between a group-shared tree and a source-based tree in
the context of multicast routing?
Problems
P1. In this question, we consider some of the pros and cons of virtual-circuit and
datagram networks.
a. Suppose that routers were subjected to conditions that might cause them
to fail fairly often. Would this argue in favor of a VC or datagram architecture?
Why?
b. Suppose that a source node and a destination require that a fixed amount
of capacity always be available at all routers on the path between the
source and destination node, for the exclusive use of traffic flowing
between this source and destination node. Would this argue in favor of a
VC or datagram architecture? Why?
c. Suppose that the links and routers in the network never fail and that routing
paths used between all source/destination pairs remains constant. In
this scenario, does a VC or datagram architecture have more control traffic
overhead? Why?
416 CHAPTER 4 • THE NETWORK LAYER
P2. Consider a virtual-circuit network. Suppose the VC number is an 8-bit field.
a. What is the maximum number of virtual circuits that can be carried over a
link?
b. Suppose a central node determines paths and VC numbers at connection
setup. Suppose the same VC number is used on each link along the VC’s
path. Describe how the central node might determine the VC number at connection
setup. Is it possible that there are fewer VCs in progress than the
maximum as determined in part (a) yet there is no common free VC number?
c. Suppose that different VC numbers are permitted in each link along a
VC’s path. During connection setup, after an end-to-end path is determined,
describe how the links can choose their VC numbers and configure their forwarding
tables in a decentralized manner, without reliance on a central node.
P3. A bare-bones forwarding table in a VC network has four columns. What is
the meaning of the values in each of these columns? A bare-bones forwarding
table in a datagram network has two columns. What is the meaning of the
values in each of these columns?
P4. Consider the network below.
a. Suppose that this network is a datagram network. Show the forwarding
table in router A, such that all traffic destined to host H3 is forwarded
through interface 3.
b. Suppose that this network is a datagram network. Can you write down a
forwarding table in router A, such that all traffic from H1 destined to host
H3 is forwarded through interface 3, while all traffic from H2 destined to
host H3 is forwarded through interface 4? (Hint: this is a trick question.)
c. Now suppose that this network is a virtual circuit network and that there is
one ongoing call between H1 and H3, and another ongoing call between
H2 and H3. Write down a forwarding table in router A, such that all traffic
from H1 destined to host H3 is forwarded through interface 3, while all
traffic from H2 destined to host H3 is forwarded through interface 4.
d. Assuming the same scenario as (c), write down the forwarding tables in
nodes B, C, and D.
PROBLEMS 417
B
A
1 3
2 4
2
D
1
2
3
H3
H1
H2
1
1 2
C
P5. Consider a VC network with a 2-bit field for the VC number. Suppose that
the network wants to set up a virtual circuit over four links: link A, link B,
link C, and link D. Suppose that each of these links is currently carrying two
other virtual circuits, and the VC numbers of these other VCs are as follows:
418 CHAPTER 4 • THE NETWORK LAYER
Link A Link B Link C Link D
00 01 10 11
01 10 11 00
In answering the following questions, keep in mind that each of the existing
VCs may only be traversing one of the four links.
a. If each VC is required to use the same VC number on all links along its
path, what VC number could be assigned to the new VC?
b. If each VC is permitted to have different VC numbers in the different links
along its path (so that forwarding tables must perform VC number translation),
how many different combinations of four VC numbers (one for each
of the four links) could be used?
P6. In the text we have used the term connection-oriented service to describe a
transport-layer service and connection service for a network-layer service.
Why the subtle shades in terminology?
P7. Suppose two packets arrive to two different input ports of a router at exactly
the same time. Also suppose there are no other packets anywhere in the
router.
a. Suppose the two packets are to be forwarded to two different output ports.
Is it possible to forward the two packets through the switch fabric at the
same time when the fabric uses a shared bus?
b. Suppose the two packets are to be forwarded to two different output ports.
Is it possible to forward the two packets through the switch fabric at the
same time when the fabric uses a crossbar?
c. Suppose the two packets are to be forwarded to the same output port. Is it
possible to forward the two packets through the switch fabric at the same
time when the fabric uses a crossbar?
P8. In Section 4.3, we noted that the maximum queuing delay is (n–1)D if the
switching fabric is n times faster than the input line rates. Suppose that all
packets are of the same length, n packets arrive at the same time to the n
input ports, and all n packets want to be forwarded to different output ports.
What is the maximum delay for a packet for the (a) memory, (b) bus, and (c)
crossbar switching fabrics?
P9. Consider the switch shown below. Suppose that all datagrams have the same
fixed length, that the switch operates in a slotted, synchronous manner, and
that in one time slot a datagram can be transferred from an input port to an
output port. The switch fabric is a crossbar so that at most one datagram can
PROBLEMS 419
be transferred to a given output port in a time slot, but different output ports
can receive datagrams from different input ports in a single time slot. What is
the minimal number of time slots needed to transfer the packets shown from
input ports to their output ports, assuming any input queue scheduling order
you want (i.e., it need not have HOL blocking)? What is the largest number
of slots needed, assuming the worst-case scheduling order you can devise,
assuming that a non-empty input queue is never idle?
X Y Switch
fabric
Output port X
Output port Y
Output port Z
X
Z Y
P10. Consider a datagram network using 32-bit host addresses. Suppose a router
has four links, numbered 0 through 3, and packets are to be forwarded to the
link interfaces as follows:
Destination Address Range Link Interface
11100000 00000000 00000000 00000000
through 0
11100000 00111111 11111111 11111111
11100000 01000000 00000000 00000000
through 1
11100000 01000000 11111111 11111111
11100000 01000001 00000000 00000000
through 2
11100001 01111111 11111111 11111111
otherwise 3
a. Provide a forwarding table that has five entries, uses longest prefix matching,
and forwards packets to the correct link interfaces.
b. Describe how your forwarding table determines the appropriate link interface
for datagrams with destination addresses:
11001000 10010001 01010001 01010101
11100001 01000000 11000011 00111100
11100001 10000000 00010001 01110111
420 CHAPTER 4 • THE NETWORK LAYER
Prefix Match Interface
1 0
10 1
111 2
otherwise 3
For each of the four interfaces, give the associated range of destination host
addresses and the number of addresses in the range.
P13. Consider a router that interconnects three subnets: Subnet 1, Subnet 2, and
Subnet 3. Suppose all of the interfaces in each of these three subnets are
required to have the prefix 223.1.17/24. Also suppose that Subnet 1 is
required to support at least 60 interfaces, Subnet 2 is to support at least 90
interfaces, and Subnet 3 is to support at least 12 interfaces. Provide three network
addresses (of the form a.b.c.d/x) that satisfy these constraints.
P14. In Section 4.2.2 an example forwarding table (using longest prefix matching)
is given. Rewrite this forwarding table using the a.b.c.d/x notation instead of
the binary string notation.
P15. In Problem P10 you are asked to provide a forwarding table (using longest
prefix matching). Rewrite this forwarding table using the a.b.c.d/x notation
instead of the binary string notation.
P11. Consider a datagram network using 8-bit host addresses. Suppose a router
uses longest prefix matching and has the following forwarding table:
Prefix Match Interface
00 0
010 1
011 2
10 2
11 3
For each of the four interfaces, give the associated range of destination host
addresses and the number of addresses in the range.
P12. Consider a datagram network using 8-bit host addresses. Suppose a
router uses longest prefix matching and has the following forwarding
table:
P16. Consider a subnet with prefix 128.119.40.128/26. Give an example of one
IP address (of form xxx.xxx.xxx.xxx) that can be assigned to this network.
Suppose an ISP owns the block of addresses of the form 128.119.40.64/26.
Suppose it wants to create four subnets from this block, with each block
having the same number of IP addresses. What are the prefixes (of form
a.b.c.d/x) for the four subnets?
P17. Consider the topology shown in Figure 4.17. Denote the three subnets with
hosts (starting clockwise at 12:00) as Networks A, B, and C. Denote the subnets
without hosts as Networks D, E, and F.
a. Assign network addresses to each of these six subnets, with the following
constraints: All addresses must be allocated from 214.97.254/23;
Subnet A should have enough addresses to support 250 interfaces; Subnet
B should have enough addresses to support 120 interfaces; and
Subnet C should have enough addresses to support 120 interfaces. Of
course, subnets D, E and F should each be able to support two interfaces.
For each subnet, the assignment should take the form a.b.c.d/x or
a.b.c.d/x – e.f.g.h/y.
b. Using your answer to part (a), provide the forwarding tables (using longest
prefix matching) for each of the three routers.
P18. Use the whois service at the American Registry for Internet Numbers
(http://www.arin.net/whois) to determine the IP address blocks for three
universities. Can the whois services be used to determine with certainty the
geographical location of a specific IP address? Use www.maxmind.com to
determine the locations of the Web servers at each of these universities.
P19. Consider sending a 2400-byte datagram into a link that has an MTU of
700 bytes. Suppose the original datagram is stamped with the identification
number 422. How many fragments are generated? What are the
values in the various fields in the IP datagram(s) generated related to
fragmentation?
P20. Suppose datagrams are limited to 1,500 bytes (including header) between
source Host A and destination Host B. Assuming a 20-byte IP header, how
many datagrams would be required to send an MP3 consisting of 5 million
bytes? Explain how you computed your answer.
P21. Consider the network setup in Figure 4.22. Suppose that the ISP instead
assigns the router the address 24.34.112.235 and that the network address of
the home network is 192.168.1/24.
a. Assign addresses to all interfaces in the home network.
b. Suppose each host has two ongoing TCP connections, all to port 80 at
host 128.119.40.86. Provide the six corresponding entries in the NAT
translation table.
PROBLEMS 421
P22. Suppose you are interested in detecting the number of hosts behind a NAT.
You observe that the IP layer stamps an identification number sequentially on
each IP packet. The identification number of the first IP packet generated by
a host is a random number, and the identification numbers of the subsequent
IP packets are sequentially assigned. Assume all IP packets generated by
hosts behind the NAT are sent to the outside world.
a. Based on this observation, and assuming you can sniff all packets sent by
the NAT to the outside, can you outline a simple technique that detects the
number of unique hosts behind a NAT? Justify your answer.
b. If the identification numbers are not sequentially assigned but randomly
assigned, would your technique work? Justify your answer.
P23. In this problem we’ll explore the impact of NATs on P2P applications.
Suppose a peer with username Arnold discovers through querying that a peer
with username Bernard has a file it wants to download. Also suppose that
Bernard and Arnold are both behind a NAT. Try to devise a technique that
will allow Arnold to establish a TCP connection with Bernard without
application-specific NAT configuration. If you have difficulty devising such
a technique, discuss why.
P24. Looking at Figure 4.27, enumerate the paths from y to u that do not contain
any loops.
P25. Repeat Problem P24 for paths from x to z, z to u, and z to w.
P26. Consider the following network. With the indicated link costs, use Dijkstra’s
shortest-path algorithm to compute the shortest path from x to all network
nodes. Show how the algorithm works by computing a table similar to
Table 4.3.
x
v
y t
z
u
w
6
12
8
7
8
3
6
4
3
2
4
3
422 CHAPTER 4 • THE NETWORK LAYER
VideoNote
Dijkstra’s algorithm:
discussion and example
P27. Consider the network shown in Problem P26. Using Dijkstra’s algorithm, and
showing your work using a table similar to Table 4.3, do the following:
a. Compute the shortest path from t to all network nodes.
b. Compute the shortest path from u to all network nodes.
c. Compute the shortest path from v to all network nodes.
d. Compute the shortest path from w to all network nodes.
e. Compute the shortest path from y to all network nodes.
f. Compute the shortest path from z to all network nodes.
P28. Consider the network shown below, and assume that each node initially
knows the costs to each of its neighbors. Consider the distance-vector
algorithm and show the distance table entries at node z.
P29. Consider a general topology (that is, not the specific network shown above) and a
synchronous version of the distance-vector algorithm. Suppose that at each iteration,
a node exchanges its distance vectors with its neighbors and receives their
distance vectors. Assuming that the algorithm begins with each node knowing
only the costs to its immediate neighbors, what is the maximum number of iterations
required before the distributed algorithm converges? Justify your answer.
P30. Consider the network fragment shown below. x has only two attached neighbors,
w and y. w has a minimum-cost path to destination u (not shown) of 5,
and y has a minimum-cost path to u of 6. The complete paths from w and y to
u (and between w and y) are not shown. All link costs in the network have
strictly positive integer values.
x y
w
2 2
5
u
z
v
y
2 3
6
2
3
1
x
PROBLEMS 423
a. Give x’s distance vector for destinations w, y, and u.
b. Give a link-cost change for either c(x,w) or c(x,y) such that x will inform
its neighbors of a new minimum-cost path to u as a result of executing the
distance-vector algorithm.
c. Give a link-cost change for either c(x,w) or c(x,y) such that x will not
inform its neighbors of a new minimum-cost path to u as a result of executing
the distance-vector algorithm.
P31. Consider the three-node topology shown in Figure 4.30. Rather than having
the link costs shown in Figure 4.30, the link costs are c(x,y) = 3, c(y,z) = 6,
c(z,x) = 4. Compute the distance tables after the initialization step and after
each iteration of a synchronous version of the distance-vector algorithm (as
we did in our earlier discussion of Figure 4.30).
P32. Consider the count-to-infinity problem in the distance vector routing. Will the
count-to-infinity problem occur if we decrease the cost of a link? Why? How
about if we connect two nodes which do not have a link?
P33. Argue that for the distance-vector algorithm in Figure 4.30, each value in the
distance vector D(x) is non-increasing and will eventually stabilize in a finite
number of steps.
P34. Consider Figure 4.31. Suppose there is another router w, connected to router
y and z. The costs of all links are given as follows: c(x,y) = 4, c(x,z) = 50,
c(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned reverse is used in the
distance-vector routing algorithm.
a. When the distance vector routing is stabilized, router w, y, and z inform their
distances to x to each other. What distance values do they tell each other?
b. Now suppose that the link cost between x and y increases to 60. Will there
be a count-to-infinity problem even if poisoned reverse is used? Why or
why not? If there is a count-to-infinity problem, then how many iterations
are needed for the distance-vector routing to reach a stable state again?
Justify your answer.
c. How do you modify c(y,z) such that there is no count-to-infinity problem
at all if c(y,x) changes from 4 to 60?
P35. Describe how loops in paths can be detected in BGP.
P36. Will a BGP router always choose the loop-free route with the shortest ASpath
length? Justify your answer.
P37. Consider the network shown below. Suppose AS3 and AS2 are running OSPF
for their intra-AS routing protocol. Suppose AS1 and AS4 are running RIP
for their intra-AS routing protocol. Suppose eBGP and iBGP are used for the
inter-AS routing protocol. Initially suppose there is no physical link between
AS2 and AS4.
424 CHAPTER 4 • THE NETWORK LAYER
a. Router 3c learns about prefix x from which routing protocol: OSPF, RIP,
eBGP, or iBGP?
b. Router 3a learns about x from which routing protocol?
c. Router 1c learns about x from which routing protocol?
d. Router 1d learns about x from which routing protocol?
P38. Referring to the previous problem, once router 1d learns about x it will put an
entry (x, I) in its forwarding table.
a. Will I be equal to I1 or I2 for this entry? Explain why in one sentence.
b. Now suppose that there is a physical link between AS2 and AS4, shown by
the dotted line. Suppose router 1d learns that x is accessible via AS2 as
well as via AS3. Will I be set to I1 or I2? Explain why in one sentence.
c. Now suppose there is another AS, called AS5, which lies on the path
between AS2 and AS4 (not shown in diagram). Suppose router 1d learns
that x is accessible via AS2 AS5 AS4 as well as via AS3 AS4. Will I be set
to I1 or I2? Explain why in one sentence.
P39. Consider the following network. ISP B provides national backbone service
to regional ISP A. ISP C provides national backbone service to regional
ISP D. Each ISP consists of one AS. B and C peer with each other in two
places using BGP. Consider traffic going from A to D. B would prefer to
hand that traffic over to C on the West Coast (so that C would have to
absorb the cost of carrying the traffic cross-country), while C would
prefer to get the traffic via its East Coast peering point with B (so that B
would have carried the traffic across the country). What BGP mechanism
might C use, so that B would hand over A-to-D traffic at its East Coast
AS4
AS3
AS1
AS2
x
4b
4c 4a
3c
3b
3a
1c
1b
1d
1a
I1 I2
2c
2a
2b
PROBLEMS 425
peering point? To answer this question, you will need to dig into the BGP
specification.
P40. In Figure 4.42, consider the path information that reaches stub networks W,
X, and Y. Based on the information available at Wand X, what are their
respective views of the network topology? Justify your answer. The topology
view at Y is shown below.
P41. Consider Figure 4.42. B would never forward traffic destined to Y via X
based on BGP routing. But there are some very popular applications for
which data packets go to X first and then flow to Y. Identify one such
application, and describe how data packets follow a path not given by
BGP routing.
P42. In Figure 4.42, suppose that there is another stub network V that is a customer of
ISP A. Suppose that B and C have a peering relationship, and Ais a customer of
both B and C. Suppose that Awould like to have the traffic destined to Wto
come from B only, and the traffic destined to V from either B or C. How should
Aadvertise its routes to B and C? What AS routes does C receive?
P43. Suppose ASs X and Z are not directly connected but instead are connected by
AS Y. Further suppose that X has a peering agreement with Y, and that Y has
W
Y
X
A
C
Stub network
Y’s view of
the topology
ISP B
ISP C
ISP D
ISP A
426 CHAPTER 4 • THE NETWORK LAYER
a peering agreement with Z. Finally, suppose that Z wants to transit all of Y’s
traffic but does not want to transit X’s traffic. Does BGP allow Z to implement
this policy?
P44. Consider the seven-node network (with nodes labeled t to z) in Problem P26.
Show the minimal-cost tree rooted at z that includes (as end hosts) nodes u, v,
w, and y. Informally argue why your tree is a minimal-cost tree.
P45. Consider the two basic approaches identified for achieving broadcast, unicast
emulation and network-layer (i.e., router-assisted) broadcast, and suppose
spanning-tree broadcast is used to achive network-layer broadcast. Consider
a single sender and 32 receivers. Suppose the sender is connected to the
receivers by a binary tree of routers. What is the cost of sending a broadcast
packet, in the cases of unicast emulation and network-layer broadcast, for this
topology? Here, each time a packet (or copy of a packet) is sent over a single
link, it incurs a unit of cost. What topology for interconnecting the sender,
receivers, and routers will bring the cost of unicast emulation and true network-
layer broadcast as far apart as possible? You can choose as many
routers as you’d like.
P46. Consider the operation of the reverse path forwarding (RPF) algorithm in Figure
4.44. Using the same topology, find a set of paths from all nodes to the source
node A (and indicate these paths in a graph using thicker-shaded lines as in Figure
4.44) such that if these paths were the least-cost paths, then node B would
receive a copy of A’s broadcast message from nodes A, C, and D under RPF.
P47. Consider the topology shown in Figure 4.44. Suppose that all links have unit
cost and that node E is the broadcast source. Using arrows like those shown
in Figure 4.44 indicate links over which packets will be forwarded using
RPF, and links over which packets will not be forwarded, given that node E is
the source.
P48. Repeat Problem P47 using the graph from Problem P26. Assume that z is the
broadcast source, and that the link costs are as shown in Problem P26.
P49. Consider the topology shown in Figure 4.46, and suppose that each link has
unit cost. Suppose node C is chosen as the center in a center-based multicast
routing algorithm. Assuming that each attached router uses its least-cost path
to node C to send join messages to C, draw the resulting center-based routing
tree. Is the resulting tree a minimum-cost tree? Justify your answer.
P50. Repeat Problem P49, using the graph from Problem P26. Assume that the
center node is v.
P51. In Section 4.5.1 we studied Dijkstra’s link-state routing algorithm for computing
the unicast paths that are individually the least-cost paths from the
source to all destinations. The union of these paths might be thought of as
forming a least-unicast-cost path tree (or a shortest unicast path tree, if
all link costs are identical). By constructing a counterexample, show that
the least-cost path tree is not always the same as a minimum spanning tree.
PROBLEMS 427
P52. Consider a network in which all nodes are connected to three other nodes. In
a single time step, a node can receive all transmitted broadcast packets from
its neighbors, duplicate the packets, and send them to all of its neighbors
(except to the node that sent a given packet). At the next time step, neighboring
nodes can receive, duplicate, and forward these packets, and so on. Suppose
that uncontrolled flooding is used to provide broadcast in such a
network. At time step t, how many copies of the broadcast packet will be
transmitted, assuming that during time step 1, a single broadcast packet is
transmitted by the source node to its three neighbors.
P53. We saw in Section 4.7 that there is no network-layer protocol that can be used
to identify the hosts participating in a multicast group. Given this, how can
multicast applications learn the identities of the hosts that are participating in
a multicast group?
P54. Design (give a pseudocode description of) an application-level protocol that
maintains the host addresses of all hosts participating in a multicast group.
Specifically identify the network service (unicast or multicast) that is used by
your protocol, and indicate whether your protocol is sending messages inband
or out-of-band (with respect to the application data flow among the
multicast group participants) and why.
P55. What is the size of the multicast address space? Suppose now that two multicast
groups randomly choose a multicast address. What is the probability that
they choose the same address? Suppose now that 1,000 multicast groups are
ongoing at the same time and choose their multicast group addresses at random.
What is the probability that they interfere with each other?
Socket Programming Assignment
At the end of Chapter 2, there are four socket programming assignments. Below,
you will find a fifth assignment which employs ICMP, a protocol discussed in this
chapter.
Assignment 5: ICMP Ping
Ping is a popular networking application used to test from a remote location whether
a particular host is up and reachable. It is also often used to measure latency
between the client host and the target host. It works by sending ICMP “echo
request” packets (i.e., ping packets) to the target host and listening for ICMP “echo
response” replies (i.e., pong packets). Ping measures the RRT, records packet loss,
and calculates a statistical summary of multiple ping-pong exchanges (the minimum,
mean, max, and standard deviation of the round-trip times).
428 CHAPTER 4 • THE NETWORK LAYER
In this lab, you will write your own Ping application in Python. Your application
will use ICMP. But in order to keep your program simple, you will not exactly follow
the official specification in RFC 1739. Note that you will only need to write the client
side of the program, as the functionality needed on the server side is built into almost
all operating systems. You can find full details of this assignment, as well as important
snippets of the Python code, at the Web site http://www.awl.com/kurose-ross.
Programming Assignment
In this programming assignment, you will be writing a “distributed” set of procedures
that implements a distributed asynchronous distance-vector routing for the
network shown below.
You are to write the following routines that will “execute” asynchronously
within the emulated environment provided for this assignment. For node 0, you will
write the routines:
• rtinit0(). This routine will be called once at the beginning of the emulation.
rtinit0() has no arguments. It should initialize your distance table in node 0 to
reflect the direct costs of 1, 3, and 7 to nodes 1, 2, and 3, respectively. In the figure
above, all links are bidirectional and the costs in both directions are identical.
After initializing the distance table and any other data structures needed by
your node 0 routines, it should then send its directly connected neighbors (in
this case, 1, 2, and 3) the cost of its minimum-cost paths to all other network
nodes. This minimum-cost information is sent to neighboring nodes in a routing
update packet by calling the routine tolayer2(), as described in the full assignment.
The format of the routing update packet is also described in the full
assignment.
• rtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node 0
receives a routing packet that was sent to it by one of its directly connected
neighbors. The parameter *rcvdpkt is a pointer to the packet that was received.
rtupdate0() is the “heart” of the distance-vector algorithm. The values it
receives in a routing update packet from some other node i contain i’s current
shortest-path costs to all other network nodes. rtupdate0() uses these received
3 2
0
1
7
3
1
2
1
PROGRAMMING ASSIGNMENT 429
values to update its own distance table (as specified by the distance-vector algorithm).
If its own minimum cost to another node changes as a result of the
update, node 0 informs its directly connected neighbors of this change in minimum
cost by sending them a routing packet. Recall that in the distance-vector
algorithm, only directly connected nodes will exchange routing packets. Thus,
nodes 1 and 2 will communicate with each other, but nodes 1 and 3 will not
communicate with each other.
Similar routines are defined for nodes 1, 2, and 3. Thus, you will write eight procedures
in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtupdate2(),
and rtupdate3(). These routines will together implement a distributed,
asynchronous computation of the distance tables for the topology and costs shown
in the figure on the preceding page.
You can find the full details of the programming assignment, as well as C code
that you will need to create the simulated hardware/software environment, at
http://www.awl.com/kurose-ross. A Java version of the assignment is also available.
Wireshark Labs
In the companion Web site for this textbook, http://www.awl.com/kurose-ross,
you’ll find two Wireshark lab assignments. The first lab examines the operation of
the IP protocol, and the IP datagram format in particular. The second lab explores
the use of the ICMP protocol in the ping and traceroute commands.
430 CHAPTER 4 • THE NETWORK LAYER
What brought you to specialize in networking?
I was working as a programmer at UCLA in the late 1960s. My job was supported by the
US Defense Advanced Research Projects Agency (called ARPA then, called DARPA now). I
was working in the laboratory of Professor Leonard Kleinrock on the Network
Measurement Center of the newly created ARPAnet. The first node of the ARPAnet was
installed at UCLA on September 1, 1969. I was responsible for programming a computer
that was used to capture performance information about the ARPAnet and to report this
information back for comparison with mathematical models and predictions of the performance
of the network.
Several of the other graduate students and I were made responsible for working on
the so-called host-level protocols of the ARPAnet—the procedures and formats that would
allow many different kinds of computers on the network to interact with each other. It was a
fascinating exploration into a new world (for me) of distributed computing and communication.
Did you imagine that IP would become as pervasive as it is today when you first designed
the protocol?
When Bob Kahn and I first worked on this in 1973, I think we were mostly very focused on
the central question: How can we make heterogeneous packet networks interoperate with
one another, assuming we cannot actually change the networks themselves? We hoped that
we could find a way to permit an arbitrary collection of packet-switched networks to be
interconnected in a transparent fashion, so that host computers could communicate end-toend
without having to do any translations in between. I think we knew that we were dealing
with powerful and expandable technology but I doubt we had a clear image of what the
world would be like with hundreds of millions of computers all interlinked on the Internet.
431
Vinton G. Cerf
Vinton G. Cerf is Vice President and Chief Internet Evangelist for
Google. He served for over 16 years at MCI in various positions,
ending up his tenure there as Senior Vice President for Technology
Strategy. He is widely known as the co-designer of the TCP/IP
protocols and the architecture of the Internet. During his time from
1976 to 1982 at the US Department of Defense Advanced
Research Projects Agency (DARPA), he played a key role leading the
development of Internet and Internet-related data packet and security
techniques. He received the US Presidential Medal of Freedom in
2005 and the US National Medal of Technology in 1997. He
holds a BS in Mathematics from Stanford University and an MS and
PhD in computer science from UCLA.
AN INTERVIEW WITH...
What do you now envision for the future of networking and the Internet? What major
challenges/obstacles do you think lie ahead in their development?
I believe the Internet itself and networks in general will continue to proliferate. Already
there is convincing evidence that there will be billions of Internet-enabled devices on the
Internet, including appliances like cell phones, refrigerators, personal digital assistants,
home servers, televisions, as well as the usual array of laptops, servers, and so on. Big challenges
include support for mobility, battery life, capacity of the access links to the network,
and ability to scale the optical core of the network up in an unlimited fashion. Designing an
interplanetary extension of the Internet is a project in which I am deeply engaged at the Jet
Propulsion Laboratory. We will need to cut over from IPv4 [32-bit addresses] to IPv6 [128
bits]. The list is long!
Who has inspired you professionally?
My colleague Bob Kahn; my thesis advisor, Gerald Estrin; my best friend, Steve Crocker
(we met in high school and he introduced me to computers in 1960!); and the thousands of
engineers who continue to evolve the Internet today.
Do you have any advice for students entering the networking/Internet field?
Think outside the limitations of existing systems—imagine what might be possible; but then
do the hard work of figuring out how to get there from the current state of affairs. Dare to
dream: A half dozen colleagues and I at the Jet Propulsion Laboratory have been working
on the design of an interplanetary extension of the terrestrial Internet. It may take decades to
implement this, mission by mission, but to paraphrase: “A man’s reach should exceed his
grasp, or what are the heavens for?”
432
CHAPTER 5
The Link Layer:
Links, Access
Networks, and
LANs
433
In the previous chapter, we learned that the network layer provides a communication
service between any two network hosts. Between the two hosts, datagrams
travel over a series of communication links, some wired and some wireless, starting
at the source host, passing through a series of packet switches (switches and routers)
and ending at the destination host. As we continue down the protocol stack, from the
network layer to the link layer, we naturally wonder how packets are sent across
the individual links that make up the end-to-end communication path. How are the
network-layer datagrams encapsulated in the link-layer frames for transmission over
a single link? Are different link-layer protocols used in the different links along the
communication path? How are transmission conflicts in broadcast links resolved? Is
there addressing at the link layer and, if so, how does the link-layer addressing operate
with the network-layer addressing we learned about in Chapter 4? And what
exactly is the difference between a switch and a router? We’ll answer these and
other important questions in this chapter.
In discussing the link layer, we’ll see that there are two fundamentally different
types of link-layer channels. The first type are broadcast channels, which connect multiple
hosts in wireless LANs, satellite networks, and hybrid fiber-coaxial cable (HFC)
434 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
access networks. Since many hosts are connected to the same broadcast communication
channel, a so-called medium access protocol is needed to coordinate frame
transmission. In some cases, a central controller may be used to coordinate transmissions;
in other cases, the hosts themselves coordinate transmissions. The second type
of link-layer channel is the point-to-point communication link, such as that often
found between two routers connected by a long-distance link, or between a user’s
office computer and the nearby Ethernet switch to which it is connected. Coordinating
access to a point-to-point link is simpler; the reference material on this book’s web site
has a detailed discussion of the Point-to-Point Protocol (PPP), which is used in settings
ranging from dial-up service over a telephone line to high-speed point-to-point
frame transport over fiber-optic links.
We’ll explore several important link-layer concepts and technologies in this chapter.
We’ll dive deeper into error detection and correction, a topic we touched on briefly
in Chapter 3. We’ll consider multiple access networks and switched LANs, including
Ethernet—by far the most prevalent wired LAN technology. We’ll also look at virtual
LANs, and data center networks. Although WiFi, and more generally wireless LANs,
are link-layer topics, we’ll postpone our study of these important topics until Chapter 6.
5.1 Introduction to the Link Layer
Let’s begin with some important terminology. We’ll find it convenient in this chapter to
refer to any device that runs a link-layer (i.e., layer 2) protocol as a node. Nodes include
hosts, routers, switches, and WiFi access points (discussed in Chapter 6). We will
also refer to the communication channels that connect adjacent nodes along the communication
path as links. In order for a datagram to be transferred from source host to
destination host, it must be moved over each of the individual links in the end-to-end
path. As an example, in the company network shown at the bottom of Figure 5.1, consider
sending a datagram from one of the wireless hosts to one of the servers. This datagram
will actually pass through six links: a WiFi link between sending host and WiFi
access point, an Ethernet link between the access point and a link-layer switch; a link
between the link-layer switch and the router, a link between the two routers; an
Ethernet link between the router and a link-layer switch; and finally an Ethernet link
between the switch and the server. Over a given link, a transmitting node encapsulates
the datagram in a link-layer frame and transmits the frame into the link.
In order to gain further insight into the link layer and how it relates to the network
layer, let’s consider a transportation analogy. Consider a travel agent who is planning a
trip for a tourist traveling from Princeton, New Jersey, to Lausanne, Switzerland. The
travel agent decides that it is most convenient for the tourist to take a limousine from
Princeton to JFK airport, then a plane from JFK airport to Geneva’s airport, and finally
a train from Geneva’s airport to Lausanne’s train station. Once the travel agent makes
the three reservations, it is the responsibility of the Princeton limousine company to get
the tourist from Princeton to JFK; it is the responsibility of the airline company to
5.1 • INTRODUCTION TO THE LINK LAYER 435
Figure 5.1  Six link-layer hops between wireless host and server
Mobile Network
National or
Global ISP
Local or
Regional ISP
Enterprise Network
Home Network
get the tourist from JFK to Geneva; and it is the responsibility of the Swiss train service
to get the tourist from Geneva to Lausanne. Each of the three segments of the trip
is “direct” between two “adjacent” locations. Note that the three transportation segments
are managed by different companies and use entirely different transportation
modes (limousine, plane, and train). Although the transportation modes are different,
they each provide the basic service of moving passengers from one location to an
adjacent location. In this transportation analogy, the tourist is a datagram, each transportation
segment is a link, the transportation mode is a link-layer protocol, and the
travel agent is a routing protocol.
5.1.1 The Services Provided by the Link Layer
Although the basic service of any link layer is to move a datagram from one node to
an adjacent node over a single communication link, the details of the provided service
can vary from one link-layer protocol to the next. Possible services that can be
offered by a link-layer protocol include:
• Framing. Almost all link-layer protocols encapsulate each network-layer datagram
within a link-layer frame before transmission over the link. A frame consists
of a data field, in which the network-layer datagram is inserted, and a
number of header fields. The structure of the frame is specified by the link-layer
protocol. We’ll see several different frame formats when we examine specific
link-layer protocols in the second half of this chapter.
• Link access. Amedium access control (MAC) protocol specifies the rules by which
a frame is transmitted onto the link. For point-to-point links that have a single
sender at one end of the link and a single receiver at the other end of the link, the
MAC protocol is simple (or nonexistent)—the sender can send a frame whenever
the link is idle. The more interesting case is when multiple nodes share a single
broadcast link—the so-called multiple access problem. Here, the MAC protocol
serves to coordinate the frame transmissions of the many nodes.
• Reliable delivery. When a link-layer protocol provides reliable delivery service, it
guarantees to move each network-layer datagram across the link without error.
Recall that certain transport-layer protocols (such as TCP) also provide a reliable
delivery service. Similar to a transport-layer reliable delivery service, a link-layer
reliable delivery service can be achieved with acknowledgments and retransmissions
(see Section 3.4). A link-layer reliable delivery service is often used for links
that are prone to high error rates, such as a wireless link, with the goal of correcting
an error locally—on the link where the error occurs—rather than forcing an end-toend
retransmission of the data by a transport- or application-layer protocol. However,
link-layer reliable delivery can be considered an unnecessary overhead for low
bit-error links, including fiber, coax, and many twisted-pair copper links. For this
reason, many wired link-layer protocols do not provide a reliable delivery service.
436 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
• Error detection and correction. The link-layer hardware in a receiving node can
incorrectly decide that a bit in a frame is zero when it was transmitted as a one,
and vice versa. Such bit errors are introduced by signal attenuation and electromagnetic
noise. Because there is no need to forward a datagram that has an error,
many link-layer protocols provide a mechanism to detect such bit errors. This is
done by having the transmitting node include error-detection bits in the frame,
and having the receiving node perform an error check. Recall from Chapters 3
and 4 that the Internet’s transport layer and network layer also provide a limited
form of error detection—the Internet checksum. Error detection in the link layer
is usually more sophisticated and is implemented in hardware. Error correction
is similar to error detection, except that a receiver not only detects when bit
errors have occurred in the frame but also determines exactly where in the frame
the errors have occurred (and then corrects these errors).
5.1.2 Where Is the Link Layer Implemented?
Before diving into our detailed study of the link layer, let’s conclude this introduction
by considering the question of where the link layer is implemented. We’ll focus
here on an end system, since we learned in Chapter 4 that the link layer is implemented
in a router’s line card. Is a host’s link layer implemented in hardware or software?
Is it implemented on a separate card or chip, and how does it interface with
the rest of a host’s hardware and operating system components?
Figure 5.2 shows a typical host architecture. For the most part, the link layer is
implemented in a network adapter, also sometimes known as a network interface
card (NIC). At the heart of the network adapter is the link-layer controller, usually
a single, special-purpose chip that implements many of the link-layer services
(framing, link access, error detection, and so on). Thus, much of a link-layer controller’s
functionality is implemented in hardware. For example, Intel’s 8254x controller
[Intel 2012] implements the Ethernet protocols we’ll study in Section 5.5; the
Atheros AR5006 [Atheros 2012] controller implements the 802.11 WiFi protocols
we’ll study in Chapter 6. Until the late 1990s, most network adapters were physically
separate cards (such as a PCMCIA card or a plug-in card fitting into a PC’s
PCI card slot) but increasingly, network adapters are being integrated onto the host’s
motherboard—a so-called LAN-on-motherboard configuration.
On the sending side, the controller takes a datagram that has been created and
stored in host memory by the higher layers of the protocol stack, encapsulates the
datagram in a link-layer frame (filling in the frame’s various fields), and then
transmits the frame into the communication link, following the link-access protocol.
On the receiving side, a controller receives the entire frame, and extracts the
network-layer datagram. If the link layer performs error detection, then it is
the sending controller that sets the error-detection bits in the frame header and it
is the receiving controller that performs error detection.
5.1 • INTRODUCTION TO THE LINK LAYER 437
Figure 5.2 shows a network adapter attaching to a host’s bus (e.g., a PCI or
PCI-X bus), where it looks much like any other I/O device to the other host components.
Figure 5.2 also shows that while most of the link layer is implemented in
hardware, part of the link layer is implemented in software that runs on the host’s
CPU. The software components of the link layer implement higher-level linklayer
functionality such as assembling link-layer addressing information and activating
the controller hardware. On the receiving side, link-layer software responds
to controller interrupts (e.g., due to the receipt of one or more frames), handling
error conditions and passing a datagram up to the network layer. Thus, the link
layer is a combination of hardware and software—the place in the protocol stack
where software meets hardware. Intel [2012] provides a readable overview (as
well as a detailed description) of the 8254x controller from a software-programming
point of view.
5.2 Error-Detection and -Correction Techniques
In the previous section, we noted that bit-level error detection and correction—
detecting and correcting the corruption of bits in a link-layer frame sent from one
node to another physically connected neighboring node—are two services often
438 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Host
Memory
Host bus
(e.g., PCI)
CPU
Controller
Physical
transmission
Network adapter
Link
Physical
Transport
Network
Link
Application
Figure 5.2  Network adapter: its relationship to other host components
and to protocol stack functionality
provided by the link layer. We saw in Chapter 3 that error-detection and -correction
services are also often offered at the transport layer as well. In this section, we’ll
examine a few of the simplest techniques that can be used to detect and, in some
cases, correct such bit errors. A full treatment of the theory and implementation of
this topic is itself the topic of many textbooks (for example, [Schwartz 1980] or
[Bertsekas 1991]), and our treatment here is necessarily brief. Our goal here is to
develop an intuitive feel for the capabilities that error-detection and -correction
techniques provide, and to see how a few simple techniques work and are used in
practice in the link layer.
Figure 5.3 illustrates the setting for our study. At the sending node, data, D, to
be protected against bit errors is augmented with error-detection and -correction bits
(EDC). Typically, the data to be protected includes not only the datagram passed
down from the network layer for transmission across the link, but also link-level
addressing information, sequence numbers, and other fields in the link frame header.
Both D and EDC are sent to the receiving node in a link-level frame. At the receiving
node, a sequence of bits, D and EDC is received. Note that D and EDC may
differ from the original D and EDC as a result of in-transit bit flips.
The receiver’s challenge is to determine whether or not D is the same as the
original D, given that it has only received D and EDC. The exact wording of the
receiver’s decision in Figure 5.3 (we ask whether an error is detected, not whether
an error has occurred!) is important. Error-detection and -correction techniques
D' EDC'
Detected error
Datagram
D EDC
d data bits
Bit error-prone link
all
bits in D'
OK
?
N
Y
Datagram
HI
Figure 5.3  Error-detection and -correction scenario
5.2 • ERROR-DETECTION AND -CORRECTION TECHNIQUES 439
allow the receiver to sometimes, but not always, detect that bit errors have
occurred. Even with the use of error-detection bits there still may be undetected
bit errors; that is, the receiver may be unaware that the received information contains
bit errors. As a consequence, the receiver might deliver a corrupted datagram
to the network layer, or be unaware that the contents of a field in the frame’s
header has been corrupted. We thus want to choose an error-detection scheme that
keeps the probability of such occurrences small. Generally, more sophisticated
error-detection and-correction techniques (that is, those that have a smaller probability
of allowing undetected bit errors) incur a larger overhead—more computation
is needed to compute and transmit a larger number of error-detection and
-correction bits.
Let’s now examine three techniques for detecting errors in the transmitted data—
parity checks (to illustrate the basic ideas behind error detection and correction),
checksumming methods (which are more typically used in the transport layer), and
cyclic redundancy checks (which are more typically used in the link layer in an
adapter).
5.2.1 Parity Checks
Perhaps the simplest form of error detection is the use of a single parity bit. Suppose
that the information to be sent, D in Figure 5.4, has d bits. In an even parity
scheme, the sender simply includes one additional bit and chooses its value such
that the total number of 1s in the d + 1 bits (the original information plus a parity
bit) is even. For odd parity schemes, the parity bit value is chosen such that there is
an odd number of 1s. Figure 5.4 illustrates an even parity scheme, with the single
parity bit being stored in a separate field.
Receiver operation is also simple with a single parity bit. The receiver need
only count the number of 1s in the received d + 1 bits. If an odd number of 1-
valued bits are found with an even parity scheme, the receiver knows that at least
one bit error has occurred. More precisely, it knows that some odd number of bit
errors have occurred.
But what happens if an even number of bit errors occur? You should convince
yourself that this would result in an undetected error. If the probability of bit
errors is small and errors can be assumed to occur independently from one bit to
the next, the probability of multiple bit errors in a packet would be extremely small.
440 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1
d data bits
Parity
bit
Figure 5.4  One-bit even parity
In this case, a single parity bit might suffice. However, measurements have shown
that, rather than occurring independently, errors are often clustered together in
“bursts.” Under burst error conditions, the probability of undetected errors in a
frame protected by single-bit parity can approach 50 percent [Spragins 1991].
Clearly, a more robust error-detection scheme is needed (and, fortunately, is used
in practice!). But before examining error-detection schemes that are used in practice,
let’s consider a simple generalization of one-bit parity that will provide us
with insight into error-correction techniques.
Figure 5.5 shows a two-dimensional generalization of the single-bit parity
scheme. Here, the d bits in D are divided into i rows and j columns. A parity value is
computed for each row and for each column. The resulting i + j + 1 parity bits comprise
the link-layer frame’s error-detection bits.
Suppose now that a single bit error occurs in the original d bits of information.
With this two-dimensional parity scheme, the parity of both the column
and the row containing the flipped bit will be in error. The receiver can thus not
only detect the fact that a single bit error has occurred, but can use the column
and row indices of the column and row with parity errors to actually identify the
bit that was corrupted and correct that error! Figure 5.5 shows an example in
1 0 1 0 1 1
1 1 1 1 0 0
0 1 1 1 0 1
0 0 1 0 1 0
1 0 1 0 1 1
1 0 1 1 0 0
0 1 1 1 0 1
0 0 1 0 1 0
Row parity
Parity
error
Parity
error
No errors Correctable
single-bit error
d1,1
d2,1
. . .
di,1
di+1,1
. . .
. . .
. . .
. . .
. . .
d1, j
d2, j
. . .
di, j
di+1, j
d1, j+1
d2, j+1
. . .
di, j+1
di+1, j+1
Column parity
Figure 5.5  Two-dimensional even parity
5.2 • ERROR-DETECTION AND -CORRECTION TECHNIQUES 441
which the 1-valued bit in position (2,2) is corrupted and switched to a 0—an
error that is both detectable and correctable at the receiver. Although our discussion
has focused on the original d bits of information, a single error in the parity
bits themselves is also detectable and correctable. Two-dimensional parity can
also detect (but not correct!) any combination of two errors in a packet. Other
properties of the two-dimensional parity scheme are explored in the problems at
the end of the chapter.
The ability of the receiver to both detect and correct errors is known as forward
error correction (FEC). These techniques are commonly used in audio storage and
playback devices such as audio CDs. In a network setting, FEC techniques can be
used by themselves, or in conjunction with link-layer ARQ techniques similar to
those we examined in Chapter 3. FEC techniques are valuable because they can
decrease the number of sender retransmissions required. Perhaps more important,
they allow for immediate correction of errors at the receiver. This avoids having to
wait for the round-trip propagation delay needed for the sender to receive a NAK
packet and for the retransmitted packet to propagate back to the receiver—a potentially
important advantage for real-time network applications [Rubenstein 1998] or
links (such as deep-space links) with long propagation delays. Research examining
the use of FEC in error-control protocols includes [Biersack 1992; Nonnenmacher
1998; Byers 1998; Shacham 1990].
5.2.2 Checksumming Methods
In checksumming techniques, the d bits of data in Figure 5.4 are treated as a
sequence of k-bit integers. One simple checksumming method is to simply sum
these k-bit integers and use the resulting sum as the error-detection bits. The
Internet checksum is based on this approach—bytes of data are treated as 16-bit
integers and summed. The 1s complement of this sum then forms the Internet
checksum that is carried in the segment header. As discussed in Section 3.3, the
receiver checks the checksum by taking the 1s complement of the sum of the
received data (including the checksum) and checking whether the result is all
1 bits. If any of the bits are 0, an error is indicated. RFC 1071 discusses the Internet
checksum algorithm and its implementation in detail. In the TCP and UDP protocols,
the Internet checksum is computed over all fields (header and data fields
included). In IP the checksum is computed over the IP header (since the UDP or
TCP segment has its own checksum). In other protocols, for example, XTP
[Strayer 1992], one checksum is computed over the header and another checksum
is computed over the entire packet.
Checksumming methods require relatively little packet overhead. For example,
the checksums in TCP and UDP use only 16 bits. However, they provide relatively
weak protection against errors as compared with cyclic redundancy check, which is
discussed below and which is often used in the link layer. A natural question at this
point is, Why is checksumming used at the transport layer and cyclic redundancy
442 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
check used at the link layer? Recall that the transport layer is typically implemented
in software in a host as part of the host’s operating system. Because transport-layer
error detection is implemented in software, it is important to have a simple and fast
error-detection scheme such as checksumming. On the other hand, error detection at
the link layer is implemented in dedicated hardware in adapters, which can rapidly
perform the more complex CRC operations. Feldmeier [Feldmeier 1995] presents
fast software implementation techniques for not only weighted checksum codes, but
CRC (see below) and other codes as well.
5.2.3 Cyclic Redundancy Check (CRC)
An error-detection technique used widely in today’s computer networks is based
on cyclic redundancy check (CRC) codes. CRC codes are also known as
polynomial codes, since it is possible to view the bit string to be sent as a polynomial
whose coefficients are the 0 and 1 values in the bit string, with operations on
the bit string interpreted as polynomial arithmetic.
CRC codes operate as follows. Consider the d-bit piece of data, D, that the
sending node wants to send to the receiving node. The sender and receiver must first
agree on an r + 1 bit pattern, known as a generator, which we will denote as G. We
will require that the most significant (leftmost) bit of G be a 1. The key idea behind
CRC codes is shown in Figure 5.6. For a given piece of data, D, the sender will
choose r additional bits, R, and append them to D such that the resulting d + r bit
pattern (interpreted as a binary number) is exactly divisible by G (i.e., has no
remainder) using modulo-2 arithmetic. The process of error checking with CRCs is
thus simple: The receiver divides the d + r received bits by G. If the remainder is
nonzero, the receiver knows that an error has occurred; otherwise the data is accepted
as being correct.
All CRC calculations are done in modulo-2 arithmetic without carries in
addition or borrows in subtraction. This means that addition and subtraction are
identical, and both are equivalent to the bitwise exclusive-or (XOR) of the
operands. Thus, for example,
1011 XOR 0101 = 1110
1001 XOR 1101 = 0100
d bits r bits
D: Data bits to be sent
D • 2r XOR R
R: CRC bits Bit pattern
Mathematical formula
Figure 5.6  CRC
5.2 • ERROR-DETECTION AND -CORRECTION TECHNIQUES 443
Also, we similarly have
1011 – 0101 = 1110
1001 – 1101 = 0100
Multiplication and division are the same as in base-2 arithmetic, except that any
required addition or subtraction is done without carries or borrows. As in regular
binary arithmetic, multiplication by 2k left shifts a bit pattern by k places. Thus,
given D and R, the quantity D  2r XOR R yields the d + r bit pattern shown
in Figure 5.6. We’ll use this algebraic characterization of the d + r bit pattern from
Figure 5.6 in our discussion below.
Let us now turn to the crucial question of how the sender computes R. Recall
that we want to find R such that there is an n such that
D  2r XORR nG
That is, we want to choose R such that G divides into D  2r XOR R without remainder.
If we XOR (that is, add modulo-2, without carry) R to both sides of the above
equation, we get
D  2r nG XOR R
This equation tells us that if we divide D  2r by G, the value of the remainder is precisely
R. In other words, we can calculate R as
Figure 5.7 illustrates this calculation for the case of D = 101110, d = 6, G = 1001,
and r 3. The 9 bits transmitted in this case are 101110 011. You should check these
calculations for yourself and also check that indeed D  2r = 101011  G XOR R.
International standards have been defined for 8-, 12-, 16-, and 32-bit generators,
G. The CRC-32 32-bit standard, which has been adopted in a number of linklevel
IEEE protocols, uses a generator of
GCRC-32 100000100110000010001110110110111
Each of the CRC standards can detect burst errors of fewer than r + 1 bits. (This
means that all consecutive bit errors of r bits or fewer will be detected.) Furthermore,
under appropriate assumptions, a burst of length greater than r + 1 bits is detected with
probability 1 – 0.5r. Also, each of the CRC standards can detect any odd number of bit
errors. See [Williams 1993] for a discussion of implementing CRC checks. The theory
=
=
R = remainder
D  2r
G
=
=
444 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
behind CRC codes and even more powerful codes is beyond the scope of this text. The
text [Schwartz 1980] provides an excellent introduction to this topic.
5.3 Multiple Access Links and Protocols
In the introduction to this chapter, we noted that there are two types of network links:
point-to-point links and broadcast links. A point-to-point link consists of a single
sender at one end of the link and a single receiver at the other end of the link. Many
link-layer protocols have been designed for point-to-point links; the point-to-point protocol
(PPP) and high-level data link control (HDLC) are two such protocols that we’ll
cover later in this chapter. The second type of link, a broadcast link, can have multiple
sending and receiving nodes all connected to the same, single, shared broadcast channel.
The term broadcast is used here because when any one node transmits a frame, the
channel broadcasts the frame and each of the other nodes receives a copy. Ethernet and
wireless LANs are examples of broadcast link-layer technologies. In this section we’ll
take a step back from specific link-layer protocols and first examine a problem of central
importance to the link layer: how to coordinate the access of multiple sending and
receiving nodes to a shared broadcast channel—the multiple access problem. Broadcast
channels are often used in LANs, networks that are geographically concentrated in
a single building (or on a corporate or university campus). Thus, we’ll also look at how
multiple access channels are used in LANs at the end of this section.
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 445
1 0 0 1 1 0 1 1 1 0 0 0 0
1 0 1 0 1 1
1 0 0 1
1 0 1
0 0 0
1 0 1 0
1 0 0 1
1 1 0
0 0 0
1 1 0 0
1 0 0 1
1 0 1 0
1 0 0 1
0 1 1
G
D
R
Figure 5.7  A sample CRC calculation
We are all familiar with the notion of broadcasting—television has been using
it since its invention. But traditional television is a one-way broadcast (that is, one
fixed node transmitting to many receiving nodes), while nodes on a computer network
broadcast channel can both send and receive. Perhaps a more apt human analogy
for a broadcast channel is a cocktail party, where many people gather in a large
room (the air providing the broadcast medium) to talk and listen. A second good
analogy is something many readers will be familiar with—a classroom—where
teacher(s) and student(s) similarly share the same, single, broadcast medium. A central
problem in both scenarios is that of determining who gets to talk (that is, transmit
into the channel), and when. As humans, we’ve evolved an elaborate set of
protocols for sharing the broadcast channel:
“Give everyone a chance to speak.”
“Don’t speak until you are spoken to.”
“Don’t monopolize the conversation.”
“Raise your hand if you have a question.”
“Don’t interrupt when someone is speaking.”
“Don’t fall asleep when someone is talking.”
Computer networks similarly have protocols—so-called multiple access
protocols—by which nodes regulate their transmission into the shared broadcast
channel. As shown in Figure 5.8, multiple access protocols are needed in a wide
variety of network settings, including both wired and wireless access networks, and
satellite networks. Although technically each node accesses the broadcast channel
through its adapter, in this section we will refer to the node as the sending and
receiving device. In practice, hundreds or even thousands of nodes can directly
communicate over a broadcast channel.
Because all nodes are capable of transmitting frames, more than two nodes
can transmit frames at the same time. When this happens, all of the nodes receive
multiple frames at the same time; that is, the transmitted frames collide at all of
the receivers. Typically, when there is a collision, none of the receiving nodes can
make any sense of any of the frames that were transmitted; in a sense, the signals
of the colliding frames become inextricably tangled together. Thus, all the frames
involved in the collision are lost, and the broadcast channel is wasted during the
collision interval. Clearly, if many nodes want to transmit frames frequently,
many transmissions will result in collisions, and much of the bandwidth of the
broadcast channel will be wasted.
In order to ensure that the broadcast channel performs useful work when multiple
nodes are active, it is necessary to somehow coordinate the transmissions of the active
nodes. This coordination job is the responsibility of the multiple access protocol. Over
the past 40 years, thousands of papers and hundreds of PhD dissertations have been
written on multiple access protocols; a comprehensive survey of the first 20 years of
446 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
this body of work is [Rom 1990]. Furthermore, active research in multiple access protocols
continues due to the continued emergence of new types of links, particularly
new wireless links.
Over the years, dozens of multiple access protocols have been implemented
in a variety of link-layer technologies. Nevertheless, we can classify just about
any multiple access protocol as belonging to one of three categories: channel
partitioning protocols, random access protocols, and taking-turns protocols.
We’ll cover these categories of multiple access protocols in the following three
subsections.
Let’s conclude this overview by noting that, ideally, a multiple access protocol
for a broadcast channel of rate R bits per second should have the following desirable
characteristics:
1. When only one node has data to send, that node has a throughput of R bps.
2. When M nodes have data to send, each of these nodes has a throughput of
R/M bps. This need not necessarily imply that each of the M nodes always
Shared wire
(for example, cable access network)
Shared wireless
(for example, WiFi)
Satellite Cocktail party
Head
end
Figure 5.8  Various multiple access channels
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 447
has an instantaneous rate of R/M, but rather that each node should have an
average transmission rate of R/M over some suitably defined interval
of time.
3. The protocol is decentralized; that is, there is no master node that represents a
single point of failure for the network.
4. The protocol is simple, so that it is inexpensive to implement.
5.3.1 Channel Partitioning Protocols
Recall from our early discussion back in Section 1.3 that time-division multiplexing
(TDM) and frequency-division multiplexing (FDM) are two techniques that
can be used to partition a broadcast channel’s bandwidth among all nodes sharing
that channel. As an example, suppose the channel supports N nodes and that the
transmission rate of the channel is R bps. TDM divides time into time frames and
further divides each time frame into N time slots. (The TDM time frame should
not be confused with the link-layer unit of data exchanged between sending and
receiving adapters, which is also called a frame. In order to reduce confusion, in
this subsection we’ll refer to the link-layer unit of data exchanged as a packet.)
Each time slot is then assigned to one of the N nodes. Whenever a node has a
packet to send, it transmits the packet’s bits during its assigned time slot in the
revolving TDM frame. Typically, slot sizes are chosen so that a single packet can
be transmitted during a slot time. Figure 5.9 shows a simple four-node TDM
example. Returning to our cocktail party analogy, a TDM-regulated cocktail party
would allow one partygoer to speak for a fixed period of time, then allow another
partygoer to speak for the same amount of time, and so on. Once everyone had had
a chance to talk, the pattern would repeat.
TDM is appealing because it eliminates collisions and is perfectly fair: Each
node gets a dedicated transmission rate of R/N bps during each frame time. However,
it has two major drawbacks. First, a node is limited to an average rate of
R/N bps even when it is the only node with packets to send. A second drawback
is that a node must always wait for its turn in the transmission sequence—again,
even when it is the only node with a frame to send. Imagine the partygoer who is
the only one with anything to say (and imagine that this is the even rarer circumstance
where everyone wants to hear what that one person has to say). Clearly,
TDM would be a poor choice for a multiple access protocol for this particular
party.
While TDM shares the broadcast channel in time, FDM divides the R bps channel
into different frequencies (each with a bandwidth of R/N) and assigns each frequency
to one of the N nodes. FDM thus creates N smaller channels of R/N bps out
of the single, larger R bps channel. FDM shares both the advantages and drawbacks
of TDM. It avoids collisions and divides the bandwidth fairly among the N nodes.
However, FDM also shares a principal disadvantage with TDM—a node is limited
to a bandwidth of R/N, even when it is the only node with packets to send.
448 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
A third channel partitioning protocol is code division multiple access
(CDMA). While TDM and FDM assign time slots and frequencies, respectively,
to the nodes, CDMA assigns a different code to each node. Each node then uses
its unique code to encode the data bits it sends. If the codes are chosen carefully,
CDMA networks have the wonderful property that different nodes can transmit
simultaneously and yet have their respective receivers correctly receive a sender’s
encoded data bits (assuming the receiver knows the sender’s code) in spite of
interfering transmissions by other nodes. CDMA has been used in military systems
for some time (due to its anti-jamming properties) and now has widespread
civilian use, particularly in cellular telephony. Because CDMA’s use is so tightly
tied to wireless channels, we’ll save our discussion of the technical details of
CDMA until Chapter 6. For now, it will suffice to know that CDMA codes, like
time slots in TDM and frequencies in FDM, can be allocated to the multiple
access channel users.
5.3.2 Random Access Protocols
The second broad class of multiple access protocols are random access protocols.
In a random access protocol, a transmitting node always transmits at the full rate
of the channel, namely, R bps. When there is a collision, each node involved in
the collision repeatedly retransmits its frame (that is, packet) until its frame gets
4KHz
FDM
TDM
Link
4KHz
Slot
All slots labeled “2” are dedicated
to a specific sender-receiver pair.
Frame
1
2
2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
Key:
Figure 5.9  A four-node TDM and FDM example
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 449
through without a collision. But when a node experiences a collision, it doesn’t
necessarily retransmit the frame right away. Instead it waits a random delay
before retransmitting the frame. Each node involved in a collision chooses independent
random delays. Because the random delays are independently chosen, it
is possible that one of the nodes will pick a delay that is sufficiently less than the
delays of the other colliding nodes and will therefore be able to sneak its frame
into the channel without a collision.
There are dozens if not hundreds of random access protocols described in the
literature [Rom 1990; Bertsekas 1991]. In this section we’ll describe a few of the
most commonly used random access protocols—the ALOHA protocols [Abramson
1970; Abramson 1985; Abramson 2009] and the carrier sense multiple access
(CSMA) protocols [Kleinrock 1975b]. Ethernet [Metcalfe 1976] is a popular and
widely deployed CSMA protocol.
Slotted ALOHA
Let’s begin our study of random access protocols with one of the simplest random
access protocols, the slotted ALOHA protocol. In our description of slotted
ALOHA, we assume the following:
• All frames consist of exactly L bits.
• Time is divided into slots of size L/R seconds (that is, a slot equals the time to
transmit one frame).
• Nodes start to transmit frames only at the beginnings of slots.
• The nodes are synchronized so that each node knows when the slots begin.
• If two or more frames collide in a slot, then all the nodes detect the collision
event before the slot ends.
Let p be a probability, that is, a number between 0 and 1. The operation of slotted
ALOHA in each node is simple:
• When the node has a fresh frame to send, it waits until the beginning of the next
slot and transmits the entire frame in the slot.
• If there isn’t a collision, the node has successfully transmitted its frame and thus
need not consider retransmitting the frame. (The node can prepare a new frame
for transmission, if it has one.)
• If there is a collision, the node detects the collision before the end of the slot. The
node retransmits its frame in each subsequent slot with probability p until the
frame is transmitted without a collision.
By retransmitting with probability p, we mean that the node effectively tosses
a biased coin; the event heads corresponds to “retransmit,” which occurs with
450 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
probability p. The event tails corresponds to “skip the slot and toss the coin again
in the next slot”; this occurs with probability (1 – p). All nodes involved in the collision
toss their coins independently.
Slotted ALOHA would appear to have many advantages. Unlike channel partitioning,
slotted ALOHA allows a node to transmit continuously at the full rate, R,
when that node is the only active node. (A node is said to be active if it has frames
to send.) Slotted ALOHA is also highly decentralized, because each node detects
collisions and independently decides when to retransmit. (Slotted ALOHA does,
however, require the slots to be synchronized in the nodes; shortly we’ll discuss an
unslotted version of the ALOHAprotocol, as well as CSMA protocols, none of which
require such synchronization.) Slotted ALOHA is also an extremely simple protocol.
Slotted ALOHA works well when there is only one active node, but how efficient
is it when there are multiple active nodes? There are two possible efficiency
concerns here. First, as shown in Figure 5.10, when there are multiple active
nodes, a certain fraction of the slots will have collisions and will therefore be
“wasted.” The second concern is that another fraction of the slots will be empty
because all active nodes refrain from transmitting as a result of the probabilistic
transmission policy. The only “unwasted” slots will be those in which exactly
one node transmits. A slot in which exactly one node transmits is said to be
a successful slot. The efficiency of a slotted multiple access protocol is defined
to be the long-run fraction of successful slots in the case when there are a large
number of active nodes, each always having a large number of frames to send.
Node 3
Key:
C = Collision slot
E = Empty slot
S = Successful slot
Node 2
Node 1
2 2 2
1 1 1 1
3 3 3
Time
C E C S E C E S S
Figure 5.10  Nodes 1, 2, and 3 collide in the first slot. Node 2 finally
succeeds in the fourth slot, node 1 in the eighth slot, and
node 3 in the ninth slot
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 451
Note that if no form of access control were used, and each node were to immediately
retransmit after each collision, the efficiency would be zero. Slotted ALOHA
clearly increases the efficiency beyond zero, but by how much?
We now proceed to outline the derivation of the maximum efficiency of slotted
ALOHA. To keep this derivation simple, let’s modify the protocol a little and
assume that each node attempts to transmit a frame in each slot with probability p.
(That is, we assume that each node always has a frame to send and that the node
transmits with probability p for a fresh frame as well as for a frame that has
already suffered a collision.) Suppose there are N nodes. Then the probability that
a given slot is a successful slot is the probability that one of the nodes transmits
and that the remaining N – 1 nodes do not transmit. The probability that a given
node transmits is p; the probability that the remaining nodes do not transmit is
(1 – p)N1. Therefore the probability a given node has a success is p(1 – p)N1.
Because there are N nodes, the probability that any one of the N nodes has a success
is Np(1 – p)N1.
Thus, when there are N active nodes, the efficiency of slotted ALOHA is
Np(1 – p)N1. To obtain the maximum efficiency for N active nodes, we have to find
the p* that maximizes this expression. (See the homework problems for a general
outline of this derivation.) And to obtain the maximum efficiency for a large number
of active nodes, we take the limit of Np*(1 – p*)N1 as N approaches infinity.
(Again, see the homework problems.) After performing these calculations, we’ll
find that the maximum efficiency of the protocol is given by 1/e  0.37. That is,
when a large number of nodes have many frames to transmit, then (at best) only
37 percent of the slots do useful work. Thus the effective transmission rate of the
channel is not R bps but only 0.37 R bps! A similar analysis also shows that 37 percent
of the slots go empty and 26 percent of slots have collisions. Imagine the poor
network administrator who has purchased a 100-Mbps slotted ALOHA system,
expecting to be able to use the network to transmit data among a large number of
users at an aggregate rate of, say, 80 Mbps! Although the channel is capable of transmitting
a given frame at the full channel rate of 100 Mbps, in the long run, the
successful throughput of this channel will be less than 37 Mbps.
Aloha
The slotted ALOHA protocol required that all nodes synchronize their transmissions
to start at the beginning of a slot. The first ALOHA protocol [Abramson
1970] was actually an unslotted, fully decentralized protocol. In pure ALOHA,
when a frame first arrives (that is, a network-layer datagram is passed down from
the network layer at the sending node), the node immediately transmits the frame
in its entirety into the broadcast channel. If a transmitted frame experiences a collision
with one or more other transmissions, the node will then immediately (after
completely transmitting its collided frame) retransmit the frame with probability p.
Otherwise, the node waits for a frame transmission time. After this wait, it then
452 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
transmits the frame with probability p, or waits (remaining idle) for another frame
time with probability 1 – p.
To determine the maximum efficiency of pure ALOHA, we focus on an individual
node. We’ll make the same assumptions as in our slotted ALOHA analysis
and take the frame transmission time to be the unit of time. At any given time, the
probability that a node is transmitting a frame is p. Suppose this frame begins transmission
at time t0. As shown in Figure 5.11, in order for this frame to be successfully
transmitted, no other nodes can begin their transmission in the interval of time
[t0 – 1, t0]. Such a transmission would overlap with the beginning of the transmission
of node i’s frame. The probability that all other nodes do not begin a transmission
in this interval is (1 – p)N1. Similarly, no other node can begin a transmission
while node i is transmitting, as such a transmission would overlap with the latter
part of node i’s transmission. The probability that all other nodes do not begin a
transmission in this interval is also (1 – p)N1. Thus, the probability that a given
node has a successful transmission is p(1 – p)2(N1). By taking limits as in the slotted
ALOHA case, we find that the maximum efficiency of the pure ALOHA protocol is
only 1/(2e)—exactly half that of slotted ALOHA. This then is the price to be paid
for a fully decentralized ALOHA protocol.
Carrier Sense Multiple Access (CSMA)
In both slotted and pure ALOHA, a node’s decision to transmit is made independently
of the activity of the other nodes attached to the broadcast channel. In particular,
a node neither pays attention to whether another node happens to be transmitting
when it begins to transmit, nor stops transmitting if another node begins to interfere
with its transmission. In our cocktail party analogy, ALOHA protocols are quite like
Time
Will overlap
with start of
i ’s frame
t0 – 1 t0 t0 + 1
Will overlap
with end of
i ’s frame
Node i frame
Figure 5.11  Interfering transmissions in pure ALOHA
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 453
a boorish partygoer who continues to chatter away regardless of whether other people
are talking. As humans, we have human protocols that allow us not only to
behave with more civility, but also to decrease the amount of time spent “colliding”
with each other in conversation and, consequently, to increase the amount of data
we exchange in our conversations. Specifically, there are two important rules for
polite human conversation:
• Listen before speaking. If someone else is speaking, wait until they are finished.
In the networking world, this is called carrier sensing—a node listens to the
channel before transmitting. If a frame from another node is currently being
transmitted into the channel, a node then waits until it detects no transmissions
for a short amount of time and then begins transmission.
• If someone else begins talking at the same time, stop talking. In the networking
world, this is called collision detection—a transmitting node listens to the channel
while it is transmitting. If it detects that another node is transmitting an interfering
454 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
NORM ABRAMSON AND ALOHANET
Norm Abramson, a PhD engineer, had a passion for surfing and an interest in packet
switching. This combination of interests brought him to the University of Hawaii in
1969. Hawaii consists of many mountainous islands, making it difficult to install and
operate land-based networks. When not surfing, Abramson thought about how to
design a network that does packet switching over radio. The network he designed
had one central host and several secondary nodes scattered over the Hawaiian
Islands. The network had two channels, each using a different frequency band. The
downlink channel broadcasted packets from the central host to the secondary hosts;
and the upstream channel sent packets from the secondary hosts to the central host. In
addition to sending informational packets, the central host also sent on the downstream
channel an acknowledgment for each packet successfully received from the
secondary hosts.
Because the secondary hosts transmitted packets in a decentralized fashion, collisions
on the upstream channel inevitably occurred. This observation led Abramson to
devise the pure ALOHA protocol, as described in this chapter. In 1970, with continued
funding from ARPA, Abramson connected his ALOHAnet to the ARPAnet.
Abramson’s work is important not only because it was the first example of a radio
packet network, but also because it inspired Bob Metcalfe. A few years later,
Metcalfe modified the ALOHA protocol to create the CSMA/CD protocol and the
Ethernet LAN.
CASE HISTORY
frame, it stops transmitting and waits a random amount of time before repeating
the sense-and-transmit-when-idle cycle.
These two rules are embodied in the family of carrier sense multiple access
(CSMA) and CSMA with collision detection (CSMA/CD) protocols [Kleinrock
1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many variations on CSMA and
CSMA/CD have been proposed. Here, we’ll consider a few of the most important,
and fundamental, characteristics of CSMA and CSMA/CD.
The first question that you might ask about CSMA is why, if all nodes perform
carrier sensing, do collisions occur in the first place? After all, a node will
refrain from transmitting whenever it senses that another node is transmitting. The
answer to the question can best be illustrated using space-time diagrams [Molle
1987]. Figure 5.12 shows a space-time diagram of four nodes (A, B, C, D)
attached to a linear broadcast bus. The horizontal axis shows the position of each
node in space; the vertical axis represents time.
A
Time Time
Space
t 0
t 1
B C D
Figure 5.12  Space-time diagram of two CSMA nodes with colliding
transmissions
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 455
At time t0, node B senses the channel is idle, as no other nodes are currently
transmitting. Node B thus begins transmitting, with its bits propagating in both
directions along the broadcast medium. The downward propagation of B’s bits in
Figure 5.12 with increasing time indicates that a nonzero amount of time is needed
for B’s bits actually to propagate (albeit at near the speed of light) along the
broadcast medium. At time t1 (t1 > t0), node D has a frame to send. Although node
B is currently transmitting at time t1, the bits being transmitted by B have yet to
reach D, and thus D senses the channel idle at t1. In accordance with the CSMA
protocol, D thus begins transmitting its frame. A short time later, B’s transmission
begins to interfere with D’s transmission at D. From Figure 5.12, it is evident that
the end-to-end channel propagation delay of a broadcast channel—the time it
takes for a signal to propagate from one of the nodes to another—will play a crucial
role in determining its performance. The longer this propagation delay, the
larger the chance that a carrier-sensing node is not yet able to sense a transmission
that has already begun at another node in the network.
Carrier Sense Multiple Access with Collision Dection (CSMA/CD)
In Figure 5.12, nodes do not perform collision detection; both B and D continue to
transmit their frames in their entirety even though a collision has occurred. When a
node performs collision detection, it ceases transmission as soon as it detects a
collision. Figure 5.13 shows the same scenario as in Figure 5.12, except that the two
nodes each abort their transmission a short time after detecting a collision. Clearly,
adding collision detection to a multiple access protocol will help protocol performance
by not transmitting a useless, damaged (by interference with a frame from
another node) frame in its entirety.
Before analyzing the CSMA/CD protocol, let us now summarize its operation
from the perspective of an adapter (in a node) attached to a broadcast channel:
1. The adapter obtains a datagram from the network layer, prepares a link-layer
frame, and puts the frame adapter buffer.
2. If the adapter senses that the channel is idle (that is, there is no signal energy
entering the adapter from the channel), it starts to transmit the frame. If, on the
other hand, the adapter senses that the channel is busy, it waits until it senses
no signal energy and then starts to transmit the frame.
3. While transmitting, the adapter monitors for the presence of signal energy
coming from other adapters using the broadcast channel.
4. If the adapter transmits the entire frame without detecting signal energy from
other adapters, the adapter is finished with the frame. If, on the other hand, the
adapter detects signal energy from other adapters while transmitting, it aborts
the transmission (that is, it stops transmitting its frame).
5. After aborting, the adapter waits a random amount of time and then returns
to step 2.
456 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
The need to wait a random (rather than fixed) amount of time is hopefully clear—if
two nodes transmitted frames at the same time and then both waited the same fixed
amount of time, they’d continue colliding forever. But what is a good interval
of time from which to choose the random backoff time? If the interval is large and
the number of colliding nodes is small, nodes are likely to wait a large amount
of time (with the channel remaining idle) before repeating the sense-and-transmitwhen-
idle step. On the other hand, if the interval is small and the number of colliding
nodes is large, it’s likely that the chosen random values will be nearly the same,
and transmitting nodes will again collide. What we’d like is an interval that is short
when the number of colliding nodes is small, and long when the number of colliding
nodes is large.
The binary exponential backoff algorithm, used in Ethernet as well as in
DOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves
this problem. Specifically, when transmitting a frame that has already experienced
A
Time Time
Collision
detect/abort
time
Space
t 0
t 1
B C D
Figure 5.13  CSMA with collision detection
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 457
n collisions, a node chooses the value of K at random from {0, 1, 2, . . . . 2n 1}. Thus,
the more collisions experienced by a frame, the larger the interval from which K is
chosen. For Ethernet, the actual amount of time a node waits is K 512 bit times (i.e.,
K times the amount of time needed to send 512 bits into the Ethernet) and the maximum
value that n can take is capped at 10.
Let’s look at an example. Suppose that a node attempts to transmit a frame for the
first time and while transmitting it detects a collision. The node then chooses K 0
with probability 0.5 or chooses K 1 with probability 0.5. If the node chooses K
0, then it immediately begins sensing the channel. If the node chooses K 1, it waits
512 bit times (e.g., 0.01 microseconds for a 100 Mbps Ethernet) before beginning
the sense-and-transmit-when-idle cycle. After a second collision, K is chosen with
equal probability from {0,1,2,3}. After three collisions, K is chosen with equal probability
from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is chosen with equal
probability from {0,1,2, . . . , 1023}. Thus, the size of the sets from which K is chosen
grows exponentially with the number of collisions; for this reason this algorithm
is referred to as binary exponential backoff.
We also note here that each time a node prepares a new frame for transmission,
it runs the CSMA/CD algorithm, not taking into account any collisions that may
have occurred in the recent past. So it is possible that a node with a new frame will
immediately be able to sneak in a successful transmission while several other nodes
are in the exponential backoff state.
CSMA/CD Efficiency
When only one node has a frame to send, the node can transmit at the full channel
rate (e.g., for Ethernet typical rates are 10 Mbps, 100 Mbps, or 1 Gbps). However, if
many nodes have frames to transmit, the effective transmission rate of the channel
can be much less. We define the efficiency of CSMA/CD to be the long-run fraction
of time during which frames are being transmitted on the channel without collisions
when there is a large number of active nodes, with each node having a large number
of frames to send. In order to present a closed-form approximation of the efficiency
of Ethernet, let dprop denote the maximum time it takes signal energy to propagate
between any two adapters. Let dtrans be the time to transmit a maximum-size frame
(approximately 1.2 msecs for a 10 Mbps Ethernet). A derivation of the efficiency of
CSMA/CD is beyond the scope of this book (see [Lam 1980] and [Bertsekas 1991]).
Here we simply state the following approximation:
We see from this formula that as dprop approaches 0, the efficiency approaches 1. This
matches our intuition that if the propagation delay is zero, colliding nodes will abort
Efficiency =
1
1 + 5dprop>dtrans
=
= =
=

-
458 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
immediately without wasting the channel. Also, as dtrans becomes very large, efficiency
approaches 1. This is also intuitive because when a frame grabs the channel,
it will hold on to the channel for a very long time; thus, the channel will be doing
productive work most of the time.
5.3.3 Taking-Turns Protocols
Recall that two desirable properties of a multiple access protocol are (1) when only
one node is active, the active node has a throughput of R bps, and (2) when M nodes
are active, then each active node has a throughput of nearly R/M bps. The ALOHA
and CSMA protocols have this first property but not the second. This has motivated
researchers to create another class of protocols—the taking-turns protocols. As
with random access protocols, there are dozens of taking-turns protocols, and each
one of these protocols has many variations. We’ll discuss two of the more important
protocols here. The first one is the polling protocol. The polling protocol requires
one of the nodes to be designated as a master node. The master node polls each of
the nodes in a round-robin fashion. In particular, the master node first sends a message
to node 1, saying that it (node 1) can transmit up to some maximum number of
frames. After node 1 transmits some frames, the master node tells node 2 it (node 2)
can transmit up to the maximum number of frames. (The master node can determine
when a node has finished sending its frames by observing the lack of a signal on the
channel.) The procedure continues in this manner, with the master node polling each
of the nodes in a cyclic manner.
The polling protocol eliminates the collisions and empty slots that plague
random access protocols. This allows polling to achieve a much higher efficiency.
But it also has a few drawbacks. The first drawback is that the protocol introduces a
polling delay—the amount of time required to notify a node that it can transmit. If,
for example, only one node is active, then the node will transmit at a rate less than
R bps, as the master node must poll each of the inactive nodes in turn each time the
active node has sent its maximum number of frames. The second drawback, which
is potentially more serious, is that if the master node fails, the entire channel
becomes inoperative. The 802.15 protocol and the Bluetooth protocol we will study
in Section 6.3 are examples of polling protocols.
The second taking-turns protocol is the token-passing protocol. In this protocol
there is no master node. Asmall, special-purpose frame known as a token is exchanged
among the nodes in some fixed order. For example, node 1 might always send the token
to node 2, node 2 might always send the token to node 3, and node N might always send
the token to node 1. When a node receives a token, it holds onto the token only if it has
some frames to transmit; otherwise, it immediately forwards the token to the next node.
If a node does have frames to transmit when it receives the token, it sends up to a maximum
number of frames and then forwards the token to the next node. Token passing is
decentralized and highly efficient. But it has its problems as well. For example, the failure
of one node can crash the entire channel. Or if a node accidentally neglects to
5.3 • MULTIPLE ACCESS LINKS AND PROTOCOLS 459
release the token, then some recovery procedure must be invoked to get the token back
in circulation. Over the years many token-passing protocols have been developed,
including the fiber distributed data interface (FDDI) protocol [Jain 1994] and the IEEE
802.5 token ring protocol [IEEE 802.5 2012], and each one had to address these as well
as other sticky issues.
5.3.4 DOCSIS: The Link-Layer Protocol for Cable
Internet Access
In the previous three subsections, we’ve learned about three broad classes of multiple
access protocols: channel partitioning protocols, random access protocols, and
taking turns protocols. A cable access network will make for an excellent case study
here, as we’ll find aspects of each of these three classes of multiple access protocols
with the cable access network!
Recall from Section 1.2.1, that a cable access network typically connects several
thousand residential cable modems to a cable modem termination system
(CMTS) at the cable network headend. The Data-Over-Cable Service Interface
Specifications (DOCSIS) [DOCSIS 2011] specifies the cable data network architecture
and its protocols. DOCSIS uses FDM to divide the downstream (CMTS to
modem) and upstream (modem to CMTS) network segments into multiple frequency
channels. Each downstream channel is 6 MHz wide, with a maximum
throughput of approximately 40 Mbps per channel (although this data rate is seldom
seen at a cable modem in practice); each upstream channel has a maximum channel
width of 6.4 MHz, and a maximum upstream throughput of approximately 30 Mbps.
Each upstream and downstream channel is a broadcast channel. Frames transmitted
on the downstream channel by the CMTS are received by all cable modems receiving
that channel; since there is just a single CMTS transmitting into the downstream
channel, however, there is no multiple access problem. The upstream direction,
however, is more interesting and technically challenging, since multiple cable
modems share the same upstream channel (frequency) to the CMTS, and thus collisions
can potentially occur.
As illustrated in Figure 5.14, each upstream channel is divided into intervals of
time (TDM-like), each containing a sequence of mini-slots during which cable
modems can transmit to the CMTS. The CMTS explicitly grants permission to individual
cable modems to transmit during specific mini-slots. The CMTS accomplishes
this by sending a control message known as a MAP message on a
downstream channel to specify which cable modem (with data to send) can transmit
during which mini-slot for the interval of time specified in the control message.
Since mini-slots are explicitly allocated to cable modems, the CMTS can ensure
there are no colliding transmissions during a mini-slot.
But how does the CMTS know which cable modems have data to send in the
first place? This is accomplished by having cable modems send mini-slot-request
frames to the CMTS during a special set of interval mini-slots that are dedicated
460 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
for this purpose, as shown in Figure 5.14. These mini-slot-request frames are
transmitted in a random access manner and so may collide with each other. A
cable modem can neither sense whether the upstream channel is busy nor detect
collisions. Instead, the cable modem infers that its mini-slot-request frame experienced
a collision if it does not receive a response to the requested allocation in the
next downstream control message. When a collision is inferred, a cable modem
uses binary exponential backoff to defer the retransmission of its mini-slot
-request frame to a future time slot. When there is little traffic on the upstream
channel, a cable modem may actually transmit data frames during slots nominally
assigned for mini-slot-request frames (and thus avoid having to wait for a mini-slot
assignment).
A cable access network thus serves as a terrific example of multiple access protocols
in action—FDM, TDM, random access, and centrally allocated time slots all
within one network!
5.4 Switched Local Area Networks
Having covered broadcast networks and multiple access protocols in the previous
section, let’s turn our attention next to switched local networks. Figure 5.15
shows a switched local network connecting three departments, two servers and a
router with four switches. Because these switches operate at the link layer, they
switch link-layer frames (rather than network-layer datagrams), don’t recognize
5.4 • SWITCHED LOCAL AREA NETWORKS 461
Figure 5.14  Upstream and downstream channels between CMTS and
cable modems
Residences with
cable modems
Minislots
containing
minislot
request frames
Assigned minislots
containing cable
modem upstream
data frames
Cable head end
MAP frame for
interval [t1,t2]
CMTS
Downstream channel i
Upstream channel j
t1 t2
network-layer addresses, and don’t use routing algorithms like RIP or OSPF to
determine paths through the network of layer-2 switches. Instead of using IP
addresses, we will soon see that they use link-layer addresses to forward linklayer
frames through the network of switches. We’ll begin our study of switched
LANs by first covering link-layer addressing (Section 5.4.1). We then examine
the celebrated Ethernet protocol (Section 5.5.2). After examining link-layer
addressing and Ethernet, we’ll look at how link-layer switches operate (Section
5.4.3), and then see (Section 5.4.4) how these switches are often used to build
large-scale LANs.
5.4.1 Link-Layer Addressing and ARP
Hosts and routers have link-layer addresses. Now you might find this surprising,
recalling from Chapter 4 that hosts and routers have network-layer addresses as
well. You might be asking, why in the world do we need to have addresses at both
the network and link layers? In addition to describing the syntax and function of the
link-layer addresses, in this section we hope to shed some light on why the two layers
462 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Mail
server
To external
internet
1 Gbps
1
2 3
4
6 5
1 Gbps
1 Gbps
Electrical Engineering Computer Science
100 Mbps
(fiber)
100 Mbps
(fiber)
100 Mbps
(fiber)
Mixture of 10 Mbps,
100 Mbps, 1 Gbps,
Cat 5 cable
Web
server
Computer Engineering
Figure 5.15  An institutional network connected together by four switches
of addresses are useful and, in fact, indispensable. We’ll also cover the Address Resolution
Protocol (ARP), which provides a mechanism to translate IP addresses to
link-layer addresses.
MAC Addresses
In truth, it is not hosts and routers that have link-layer addresses but rather their
adapters (that is, network interfaces) that have link-layer addresses. A host or
router with multiple network interfaces will thus have multiple link-layer
addresses associated with it, just as it would also have multiple IP addresses associated
with it. It's important to note, however, that link-layer switches do not have
link-layer addresses associated with their interfaces that connect to hosts and
routers. This is because the job of the link-layer switch is to carry datagrams
between hosts and routers; a switch does this job transparently, that is, without the
host or router having to explicitly address the frame to the intervening switch.
This is illustrated in Figure 5.16. A link-layer address is variously called a LAN
address, a physical address, or a MAC address. Because MAC address seems to
be the most popular term, we’ll henceforth refer to link-layer addresses as MAC
addresses. For most LANs (including Ethernet and 802.11 wireless LANs), the
MAC address is 6 bytes long, giving 248 possible MAC addresses. As shown in
Figure 5.16, these 6-byte addresses are typically expressed in hexadecimal notation,
with each byte of the address expressed as a pair of hexadecimal numbers.
Although MAC addresses were designed to be permanent, it is now possible to
5C-66-AB-90-75-B1 88-B2-2F-54-1A-0F
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A
Figure 5.16  Each interface connected to a LAN has a unique MAC
address
5.4 • SWITCHED LOCAL AREA NETWORKS 463
change an adapter’s MAC address via software. For the rest of this section, however,
we’ll assume that an adapter’s MAC address is fixed.
One interesting property of MAC addresses is that no two adapters have the
same address. This might seem surprising given that adapters are manufactured in
many countries by many companies. How does a company manufacturing
adapters in Taiwan make sure that it is using different addresses from a company
manufacturing adapters in Belgium? The answer is that the IEEE manages the
MAC address space. In particular, when a company wants to manufacture
adapters, it purchases a chunk of the address space consisting of 224 addresses for
a nominal fee. IEEE allocates the chunk of 224 addresses by fixing the first 24 bits
of a MAC address and letting the company create unique combinations of the last
24 bits for each adapter.
An adapter’s MAC address has a flat structure (as opposed to a hierarchical
structure) and doesn’t change no matter where the adapter goes. A laptop with an
Ethernet interface always has the same MAC address, no matter where the computer
goes. A smartphone with an 802.11 interface always has the same MAC
address, no matter where the smartphone goes. Recall that, in contrast, IP addresses
have a hierarchical structure (that is, a network part and a host part), and a host’s
IP addresses needs to be changed when the host moves, i.e, changes the network
to which it is attached. An adapter’s MAC address is analogous to a person’s
social security number, which also has a flat addressing structure and which
doesn’t change no matter where the person goes. An IP address is analogous to a
person’s postal address, which is hierarchical and which must be changed whenever
a person moves. Just as a person may find it useful to have both a postal
address and a social security number, it is useful for a host and router interfaces to
have both a network-layer address and a MAC address.
When an adapter wants to send a frame to some destination adapter, the sending
adapter inserts the destination adapter’s MAC address into the frame and then
sends the frame into the LAN. As we will soon see, a switch occassionally broadcasts
an incoming frame onto all of its interfaces. We’ll see in Chapter 6 that
802.11 also broadcasts frames. Thus, an adapter may receive a frame that isn’t
addressed to it. Thus, when an adapter receives a frame, it will check to see
whether the destination MAC address in the frame matches its own MAC address.
If there is a match, the adapter extracts the enclosed datagram and passes the datagram
up the protocol stack. If there isn’t a match, the adapter discards the frame,
without passing the network-layer datagram up. Thus, the destination only will be
interrupted when the frame is received.
However, sometimes a sending adapter does want all the other adapters on the
LAN to receive and process the frame it is about to send. In this case, the sending
adapter inserts a special MAC broadcast address into the destination address field
of the frame. For LANs that use 6-byte addresses (such as Ethernet and 802.11),
the broadcast address is a string of 48 consecutive 1s (that is, FF-FF-FF-FF-FFFF
in hexadecimal notation).
464 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Address Resolution Protocol (ARP)
Because there are both network-layer addresses (for example, Internet IP addresses)
and link-layer addresses (that is, MAC addresses), there is a need to translate
between them. For the Internet, this is the job of the Address Resolution Protocol
(ARP) [RFC 826].
To understand the need for a protocol such as ARP, consider the network shown
in Figure 5.17. In this simple example, each host and router has a single IP address
and single MAC address. As usual, IP addresses are shown in dotted-decimal notation
and MAC addresses are shown in hexadecimal notation. For the purposes of this
discussion, we will assume in this section that the switch broadcasts all frames; that
is, whenever a switch receives a frame on one interface, it forwards the frame on all
of its other interfaces. In the next section, we will provide a more accurate explanation
of how switches operate.
Now suppose that the host with IP address 222.222.222.220 wants to send an IP
datagram to host 222.222.222.222. In this example, both the source and destination
are in the same subnet, in the addressing sense of Section 4.4.2. To send a datagram,
the source must give its adapter not only the IP datagram but also the MAC address
for destination 222.222.222.222. The sending adapter will then construct a linklayer
frame containing the destination’s MAC address and send the frame into
the LAN.
KEEPING THE LAYERS INDEPENDENT
There are several reasons why hosts and router interfaces have MAC addresses in addition
to network-layer addresses. First, LANs are designed for arbitrary network-layer protocols,
not just for IP and the Internet. If adapters were assigned IP addresses rather than “neutral”
MAC addresses, then adapters would not easily be able to support other network-layer
protocols (for example, IPX or DECnet). Second, if adapters were to use network-layer
addresses instead of MAC addresses, the network-layer address would have to be stored
in the adapter RAM and reconfigured every time the adapter was moved (or powered up).
Another option is to not use any addresses in the adapters and have each adapter pass
the data (typically, an IP datagram) of each frame it receives up the protocol stack. The
network layer could then check for a matching network-layer address. One problem with
this option is that the host would be interrupted by every frame sent on the LAN, including
by frames that were destined for other hosts on the same broadcast LAN. In summary, in
order for the layers to be largely independent building blocks in a network architecture,
different layers need to have their own addressing scheme. We have now seen three types
of addresses: host names for the application layer, IP addresses for the network layer, and
MAC addresses for the link layer.
PRINCIPLES IN PRACTICE
5.4 • SWITCHED LOCAL AREA NETWORKS 465
The important question addressed in this section is, How does the sending
host determine the MAC address for the destination host with IP address
222.222.222.222? As you might have guessed, it uses ARP. An ARP module in the
sending host takes any IP address on the same LAN as input, and returns the corresponding
MAC address. In the example at hand, sending host 222.222.222.220
provides its ARP module the IP address 222.222.222.222, and the ARP module
returns the corresponding MAC address 49-BD-D2-C7-56-2A.
So we see that ARP resolves an IP address to a MAC address. In many ways it
is analogous to DNS (studied in Section 2.5), which resolves host names to IP
addresses. However, one important difference between the two resolvers is that
DNS resolves host names for hosts anywhere in the Internet, whereas ARP resolves
IP addresses only for hosts and router interfaces on the same subnet. If a node in
California were to try to use ARP to resolve the IP address for a node in Mississippi,
ARP would return with an error.
Now that we have explained what ARP does, let’s look at how it works. Each
host and router has an ARP table in its memory, which contains mappings of IP
addresses to MAC addresses. Figure 5.18 shows what an ARP table in host
222.222.222.220 might look like. The ARP table also contains a time-to-live (TTL)
value, which indicates when each mapping will be deleted from the table. Note that
a table does not necessarily contain an entry for every host and router on the subnet;
some may have never been entered into the table, and others may have expired.
A typical expiration time for an entry is 20 minutes from when an entry is placed in
an ARP table.
466 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
IP:222.222.222.221
IP:222.222.222.220
IP:222.222.222.223
IP:222.222.222.222
5C-66-AB-90-75-B1
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A
88-B2-2F-54-1A-0F
A
B
C
Figure 5.17  Each interface on a LAN has an IP address and a MAC
address
Now suppose that host 222.222.222.220 wants to send a datagram that is IPaddressed
to another host or router on that subnet. The sending host needs to
obtain the MAC address of the destination given the IP address. This task is easy
if the sender’s ARP table has an entry for the destination node. But what if the
ARP table doesn’t currently have an entry for the destination? In particular, suppose
222.222.222.220 wants to send a datagram to 222.222.222.222. In this case,
the sender uses the ARP protocol to resolve the address. First, the sender constructs
a special packet called an ARP packet. An ARP packet has several fields,
including the sending and receiving IP and MAC addresses. Both ARP query and
response packets have the same format. The purpose of the ARP query packet is
to query all the other hosts and routers on the subnet to determine the MAC
address corresponding to the IP address that is being resolved.
Returning to our example, 222.222.222.220 passes an ARP query packet to
the adapter along with an indication that the adapter should send the packet to the
MAC broadcast address, namely, FF-FF-FF-FF-FF-FF. The adapter encapsulates
the ARP packet in a link-layer frame, uses the broadcast address for the frame’s
destination address, and transmits the frame into the subnet. Recalling our social
security number/postal address analogy, an ARP query is equivalent to a person
shouting out in a crowded room of cubicles in some company (say, AnyCorp):
“What is the social security number of the person whose postal address is Cubicle
13, Room 112, AnyCorp, Palo Alto, California?” The frame containing the ARP
query is received by all the other adapters on the subnet, and (because of the
broadcast address) each adapter passes the ARP packet within the frame up to its
ARP module. Each of these ARP modules checks to see if its IP address matches
the destination IP address in the ARP packet. The one with a match sends back to
the querying host a response ARP packet with the desired mapping. The querying
host 222.222.222.220 can then update its ARP table and send its IP datagram,
encapsulated in a link-layer frame whose destination MAC is that of the host or
router responding to the earlier ARP query.
There are a couple of interesting things to note about the ARP protocol. First,
the query ARP message is sent within a broadcast frame, whereas the response ARP
message is sent within a standard frame. Before reading on you should think about
why this is so. Second, ARP is plug-and-play; that is, an ARP table gets built
automatically—it doesn’t have to be configured by a system administrator. And if
IP Address MAC Address TTL
222.222.222.221 88-B2-2F-54-1A-0F 13:45:00
222.222.222.223 5C-66-AB-90-75-B1 13:52:00
Figure 5.18  A possible ARP table in 222.222.222.220
5.4 • SWITCHED LOCAL AREA NETWORKS 467
a host becomes disconnected from the subnet, its entry is eventually deleted from
the other ARP tables in the subnet.
Students often wonder if ARP is a link-layer protocol or a network-layer protocol.
As we’ve seen, an ARP packet is encapsulated within a link-layer frame
and thus lies architecturally above the link layer. However, an ARP packet has
fields containing link-layer addresses and thus is arguably a link-layer protocol,
but it also contains network-layer addresses and thus is also arguably a networklayer
protocol. In the end, ARP is probably best considered a protocol that straddles
the boundary between the link and network layers—not fitting neatly into
the simple layered protocol stack we studied in Chapter 1. Such are the complexities
of real-world protocols!
Sending a Datagram off the Subnet
It should now be clear how ARP operates when a host wants to send a datagram to
another host on the same subnet. But now let’s look at the more complicated situation
when a host on a subnet wants to send a network-layer datagram to a host off
the subnet (that is, across a router onto another subnet). Let’s discuss this issue in
the context of Figure 5.19, which shows a simple network consisting of two subnets
interconnected by a router.
There are several interesting things to note about Figure 5.19. Each host has
exactly one IP address and one adapter. But, as discussed in Chapter 4, a router has
an IP address for each of its interfaces. For each router interface there is also an ARP
module (in the router) and an adapter. Because the router in Figure 5.19 has two
interfaces, it has two IP addresses, two ARP modules, and two adapters. Of course,
each adapter in the network has its own MAC address.
Also note that Subnet 1 has the network address 111.111.111/24 and that Subnet
2 has the network address 222.222.222/24. Thus all of the interfaces connected
to Subnet 1 have addresses of the form 111.111.111.xxx and all of the interfaces
connected to Subnet 2 have addresses of the form 222.222.222.xxx.
468 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
IP:111.111.111.111 IP:111.111.111.110
IP:111.111.111.112
IP:222.222.222.221
IP:222.222.222.222
74-29-9C-E8-FF-55
CC-49-DE-D0-AB-7D
E6-E9-00-17-BB-4B
1A-23-F9-CD-06-9B
IP:222.222.222.220
88-B2-2F-54-1A-0F
49-BD-D2-C7-56-2A
Figure 5.19  Two subnets interconnected by a router
Now let’s examine how a host on Subnet 1 would send a datagram to a host
on Subnet 2. Specifically, suppose that host 111.111.111.111 wants to send an IP
datagram to a host 222.222.222.222. The sending host passes the datagram to its
adapter, as usual. But the sending host must also indicate to its adapter an appropriate
destination MAC address. What MAC address should the adapter use? One
might be tempted to guess that the appropriate MAC address is that of the adapter
for host 222.222.222.222, namely, 49-BD-D2-C7-56-2A. This guess, however,
would be wrong! If the sending adapter were to use that MAC address, then none
of the adapters on Subnet 1 would bother to pass the IP datagram up to its network
layer, since the frame’s destination address would not match the MAC
address of any adapter on Subnet 1. The datagram would just die and go to datagram
heaven.
If we look carefully at Figure 5.19, we see that in order for a datagram to go
from 111.111.111.111 to a host on Subnet 2, the datagram must first be sent to the
router interface 111.111.111.110, which is the IP address of the first-hop router
on the path to the final destination. Thus, the appropriate MAC address for the
frame is the address of the adapter for router interface 111.111.111.110, namely,
E6-E9-00-17-BB-4B. How does the sending host acquire the MAC address for
111.111.111.110? By using ARP, of course! Once the sending adapter has this
MAC address, it creates a frame (containing the datagram addressed to
222.222.222.222) and sends the frame into Subnet 1. The router adapter on Subnet
1 sees that the link-layer frame is addressed to it, and therefore passes the
frame to the network layer of the router. Hooray—the IP datagram has successfully
been moved from source host to the router! But we are not finished. We still
have to move the datagram from the router to the destination. The router now has
to determine the correct interface on which the datagram is to be forwarded. As
discussed in Chapter 4, this is done by consulting a forwarding table in the router.
The forwarding table tells the router that the datagram is to be forwarded via
router interface 222.222.222.220. This interface then passes the datagram to its
adapter, which encapsulates the datagram in a new frame and sends the frame
into Subnet 2. This time, the destination MAC address of the frame is indeed the
MAC address of the ultimate destination. And how does the router obtain this
destination MAC address? From ARP, of course!
ARP for Ethernet is defined in RFC 826. A nice introduction to ARP is given in
the TCP/IP tutorial, RFC 1180. We’ll explore ARP in more detail in the homework
problems.
5.4.2 Ethernet
Ethernet has pretty much taken over the wired LAN market. In the 1980s and the
early 1990s, Ethernet faced many challenges from other LAN technologies, including
token ring, FDDI, and ATM. Some of these other technologies succeeded in
capturing a part of the LAN market for a few years. But since its invention in the
5.4 • SWITCHED LOCAL AREA NETWORKS 469
VideoNote
Sending a datagram
between subnets:
link-layer and
network-layer
addressing
mid-1970s, Ethernet has continued to evolve and grow and has held on to its
dominant position. Today, Ethernet is by far the most prevalent wired LAN technology,
and it is likely to remain so for the foreseeable future. One might say that
Ethernet has been to local area networking what the Internet has been to global
networking.
There are many reasons for Ethernet’s success. First, Ethernet was the first
widely deployed high-speed LAN. Because it was deployed early, network administrators
became intimately familiar with Ethernet—its wonders and its quirks—
and were reluctant to switch over to other LAN technologies when they came on
the scene. Second, token ring, FDDI, and ATM were more complex and expensive
than Ethernet, which further discouraged network administrators from switching
over. Third, the most compelling reason to switch to another LAN technology
(such as FDDI or ATM) was usually the higher data rate of the new technology;
however, Ethernet always fought back, producing versions that operated at equal
data rates or higher. Switched Ethernet was also introduced in the early 1990s,
which further increased its effective data rates. Finally, because Ethernet has been
so popular, Ethernet hardware (in particular, adapters and switches) has become a
commodity and is remarkably cheap.
The original Ethernet LAN was invented in the mid-1970s by Bob Metcalfe and
David Boggs. The original Ethernet LAN used a coaxial bus to interconnect the
nodes. Bus topologies for Ethernet actually persisted throughout the 1980s and into
the mid-1990s. Ethernet with a bus topology is a broadcast LAN—all transmitted
frames travel to and are processed by all adapters connected to the bus. Recall that
we covered Ethernet's CSMA/CD multiple access protocol with binary exponential
backoff in Section 5.3.2.
By the late 1990s, most companies and universities had replaced their LANs
with Ethernet installations using a hub-based star topology. In such an installation
the hosts (and routers) are directly connected to a hub with twisted-pair copper
wire. A hub is a physical-layer device that acts on individual bits rather than
frames. When a bit, representing a zero or a one, arrives from one interface, the
hub simply re-creates the bit, boosts its energy strength, and transmits the bit onto
all the other interfaces. Thus, Ethernet with a hub-based star topology is also a
broadcast LAN—whenever a hub receives a bit from one of its interfaces, it sends
a copy out on all of its other interfaces. In particular, if a hub receives frames from
two different interfaces at the same time, a collision occurs and the nodes that created
the frames must retransmit.
In the early 2000s Ethernet experienced yet another major evolutionary change.
Ethernet installations continued to use a star topology, but the hub at the center was
replaced with a switch. We’ll be examining switched Ethernet in depth later in this
chapter. For now, we only mention that a switch is not only “collision-less” but
is also a bona-fide store-and-forward packet switch; but unlike routers, which operate
up through layer 3, a switch operates only up through layer 2.
470 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Ethernet Frame Structure
We can learn a lot about Ethernet by examining the Ethernet frame, which is shown in
Figure 5.20. To give this discussion about Ethernet frames a tangible context, let’s consider
sending an IP datagram from one host to another host, with both hosts on the same
Ethernet LAN (for example, the Ethernet LAN in Figure 5.17.) (Although the payload
of our Ethernet frame is an IP datagram, we note that an Ethernet frame can carry
other network-layer packets as well.) Let the sending adapter, adapter A, have the
MAC address AA-AA-AA-AA-AA-AA and the receiving adapter, adapter B, have the
MAC address BB-BB-BB-BB-BB-BB. The sending adapter encapsulates the IP datagram
within an Ethernet frame and passes the frame to the physical layer. The receiving
adapter receives the frame from the physical layer, extracts the IP datagram, and
passes the IP datagram to the network layer. In this context, let’s now examine the six
fields of the Ethernet frame, as shown in Figure 5.20.
• Data field (46 to 1,500 bytes). This field carries the IP datagram. The maximum
transmission unit (MTU) of Ethernet is 1,500 bytes. This means that if the IP
datagram exceeds 1,500 bytes, then the host has to fragment the datagram, as discussed
in Section 4.4.1. The minimum size of the data field is 46 bytes. This
means that if the IP datagram is less than 46 bytes, the data field has to be
“stuffed” to fill it out to 46 bytes. When stuffing is used, the data passed to the
network layer contains the stuffing as well as an IP datagram. The network layer
uses the length field in the IP datagram header to remove the stuffing.
• Destination address (6 bytes). This field contains the MAC address of the destination
adapter, BB-BB-BB-BB-BB-BB. When adapter B receives an Ethernet
frame whose destination address is either BB-BB-BB-BB-BB-BB or the
MAC broadcast address, it passes the contents of the frame’s data field to the
network layer; if it receives a frame with any other MAC address, it discards
the frame.
• Source address (6 bytes). This field contains the MAC address of the adapter that
transmits the frame onto the LAN, in this example, AA-AA-AA-AA-AA-AA.
• Type field (2 bytes). The type field permits Ethernet to multiplex network-layer
protocols. To understand this, we need to keep in mind that hosts can use other
network-layer protocols besides IP. In fact, a given host may support multiple
Preamble CRC
Dest.
address
Source
address
Type
Data
Figure 5.20  Ethernet frame structure
5.4 • SWITCHED LOCAL AREA NETWORKS 471
network-layer protocols using different protocols for different applications. For
this reason, when the Ethernet frame arrives at adapter B, adapter B needs to
know to which network-layer protocol it should pass (that is, demultiplex) the
contents of the data field. IP and other network-layer protocols (for example,
Novell IPX or AppleTalk) each have their own, standardized type number. Furthermore,
the ARP protocol (discussed in the previous section) has its own type
number, and if the arriving frame contains an ARP packet (i.e., has a type field
of 0806 hexadecimal), the ARP packet will be demultiplexed up to the ARP protocol.
Note that the type field is analogous to the protocol field in the networklayer
datagram and the port-number fields in the transport-layer segment; all of
these fields serve to glue a protocol at one layer to a protocol at the layer above.
• Cyclic redundancy check (CRC) (4 bytes). As discussed in Section 5.2.3, the purpose
of the CRC field is to allow the receiving adapter, adapter B, to detect bit
errors in the frame.
• Preamble (8 bytes). The Ethernet frame begins with an 8-byte preamble field.
Each of the first 7 bytes of the preamble has a value of 10101010; the last byte is
10101011. The first 7 bytes of the preamble serve to “wake up” the receiving
adapters and to synchronize their clocks to that of the sender’s clock. Why
should the clocks be out of synchronization? Keep in mind that adapter A aims
to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps, depending on the type
of Ethernet LAN. However, because nothing is absolutely perfect, adapter A will
not transmit the frame at exactly the target rate; there will always be some drift
from the target rate, a drift which is not known a priori by the other adapters on
the LAN. A receiving adapter can lock onto adapter A’s clock simply by locking
onto the bits in the first 7 bytes of the preamble. The last 2 bits of the eighth byte
of the preamble (the first two consecutive 1s) alert adapter B that the “important
stuff” is about to come.
All of the Ethernet technologies provide connectionless service to the network
layer. That is, when adapter Awants to send a datagram to adapter B, adapter Aencapsulates
the datagram in an Ethernet frame and sends the frame into the LAN, without
first handshaking with adapter B. This layer-2 connectionless service is analogous to
IP’s layer-3 datagram service and UDP’s layer-4 connectionless service.
Ethernet technologies provide an unreliable service to the network layer.
Specifically, when adapter B receives a frame from adapter A, it runs the frame
through a CRC check, but neither sends an acknowledgment when a frame passes
the CRC check nor sends a negative acknowledgment when a frame fails the CRC
check. When a frame fails the CRC check, adapter B simply discards the frame.
Thus, adapter A has no idea whether its transmitted frame reached adapter B and
passed the CRC check. This lack of reliable transport (at the link layer) helps to
make Ethernet simple and cheap. But it also means that the stream of datagrams
passed to the network layer can have gaps.
472 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
If there are gaps due to discarded Ethernet frames, does the application at Host
B see gaps as well? As we learned in Chapter 3, this depends on whether the application
is using UDP or TCP. If the application is using UDP, then the application in
Host B will indeed see gaps in the data. On the other hand, if the application is using
TCP, then TCP in Host B will not acknowledge the data contained in discarded
frames, causing TCP in Host A to retransmit. Note that when TCP retransmits data,
the data will eventually return to the Ethernet adapter at which it was discarded.
Thus, in this sense, Ethernet does retransmit data, although Ethernet is unaware of
whether it is transmitting a brand-new datagram with brand-new data, or a datagram
that contains data that has already been transmitted at least once.
Ethernet Technologies
In our discussion above, we’ve referred to Ethernet as if it were a single protocol standard.
But in fact, Ethernet comes in many different flavors, with somewhat bewildering
acronyms such as 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, and
BOB METCALFE AND ETHERNET
As a PhD student at Harvard University in the early 1970s, Bob Metcalfe worked on
the ARPAnet at MIT. During his studies, he also became exposed to Abramson’s work
on ALOHA and random access protocols. After completing his PhD and just before
beginning a job at Xerox Palo Alto Research Center (Xerox PARC), he visited
Abramson and his University of Hawaii colleagues for three months, getting a firsthand
look at ALOHAnet. At Xerox PARC, Metcalfe became exposed to Alto computers,
which in many ways were the forerunners of the personal computers of the
1980s. Metcalfe saw the need to network these computers in an inexpensive manner.
So armed with his knowledge about ARPAnet, ALOHAnet, and random access protocols,
Metcalfe—along with colleague David Boggs—invented Ethernet.
Metcalfe and Boggs’s original Ethernet ran at 2.94 Mbps and linked up to 256
hosts separated by up to one mile. Metcalfe and Boggs succeeded at getting most of
the researchers at Xerox PARC to communicate through their Alto computers.
Metcalfe then forged an alliance between Xerox, Digital, and Intel to establish
Ethernet as a 10 Mbps Ethernet standard, ratified by the IEEE. Xerox did not show
much interest in commercializing Ethernet. In 1979, Metcalfe formed his own company,
3Com, which developed and commercialized networking technology, including
Ethernet technology. In particular, 3Com developed and marketed Ethernet cards in
the early 1980s for the immensely popular IBM PCs. Metcalfe left 3Com in 1990,
when it had 2,000 employees and $400 million in revenue.
CASE HISTORY
5.4 • SWITCHED LOCAL AREA NETWORKS 473
10GBASE-T. These and many other Ethernet technologies have been standardized
over the years by the IEEE 802.3 CSMA/CD (Ethernet) working group [IEEE 802.3
2012]. While these acronyms may appear bewildering, there is actually considerable
order here. The first part of the acronym refers to the speed of the standard: 10, 100,
1000, or 10G, for 10 Megabit (per second), 100 Megabit, Gigabit, and 10 Gigabit
Ethernet, respectively. “BASE” refers to baseband Ethernet, meaning that the physical
media only carries Ethernet traffic; almost all of the 802.3 standards are for baseband
Ethernet. The final part of the acronym refers to the physical media itself; Ethernet is
both a link-layer and a physical-layer specification and is carried over a variety of
physical media including coaxial cable, copper wire, and fiber. Generally, a “T” refers
to twisted-pair copper wires.
Historically, an Ethernet was initially conceived of as a segment of coaxial
cable. The early 10BASE-2 and 10BASE-5 standards specify 10 Mbps Ethernet
over two types of coaxial cable, each limited in length to 500 meters. Longer runs
could be obtained by using a repeater—a physical-layer device that receives a
signal on the input side, and regenerates the signal on the output side. A coaxial
cable, as in Figure 5.20, corresponds nicely to our view of Ethernet as a broadcast
medium—all frames transmitted by one interface are received at other interfaces,
and Ethernet’s CDMA/CD protocol nicely solves the multiple access problem.
Nodes simply attach to the cable, and voila, we have a local area network!
Ethernet has passed through a series of evolutionary steps over the years, and
today’s Ethernet is very different from the original bus-topology designs using coaxial
cable. In most installations today, nodes are connected to a switch via point-topoint
segments made of twisted-pair copper wires or fiber-optic cables, as shown in
Figures 5.15–5.17.
In the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times faster than
10 Mbps Ethernet. The original Ethernet MAC protocol and frame format were preserved,
but higher-speed physical layers were defined for copper wire (100BASE-T)
and fiber (100BASE-FX, 100BASE-SX, 100BASE-BX). Figure 5.21 shows these
different standards and the common Ethernet MAC protocol and frame format.
474 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Physical
Transport
Network
Link
Application
100BASE-TX
100BASE-T4
100BASE-T2
MAC protocol
and frame format
100BASE-SX
100BASE-FX
100BASE-BX
Figure 5.21  100 Mbps Ethernet standards: a common link layer,
different physical layers
100 Mbps Ethernet is limited to a 100 meter distance over twisted pair, and to several
kilometers over fiber, allowing Ethernet switches in different buildings to be
connected.
Gigabit Ethernet is an extension to the highly successful 10 Mbps and 100 Mbps
Ethernet standards. Offering a raw data rate of 1,000 Mbps, Gigabit Ethernet maintains
full compatibility with the huge installed base of Ethernet equipment. The standard
for Gigabit Ethernet, referred to as IEEE 802.3z, does the following:
• Uses the standard Ethernet frame format (Figure 5.20) and is backward compatible
with 10BASE-T and 100BASE-T technologies. This allows for easy
integration of Gigabit Ethernet with the existing installed base of Ethernet
equipment.
• Allows for point-to-point links as well as shared broadcast channels. Pointto-
point links use switches while broadcast channels use hubs, as described
earlier. In Gigabit Ethernet jargon, hubs are called buffered distributors.
• Uses CSMA/CD for shared broadcast channels. In order to have acceptable efficiency,
the maximum distance between nodes must be severely restricted.
• Allows for full-duplex operation at 1,000 Mbps in both directions for point-topoint
channels.
Initially operating over optical fiber, Gigabit Ethernet is now able to run over category
5 UTP cabling. 10 Gbps Ethernet (10GBASE-T) was standardized in 2007,
providing yet higher Ethernet LAN capacities.
Let’s conclude our discussion of Ethernet technology by posing a question that
may have begun troubling you. In the days of bus topologies and hub-based star
topologies, Ethernet was clearly a broadcast link (as defined in Section 5.3) in which
frame collisions occurred when nodes transmitted at the same time. To deal with
these collisions, the Ethernet standard included the CSMA/CD protocol, which is
particularly effective for a wired broadcast LAN spanning a small geographical
region. But if the prevalent use of Ethernet today is a switch-based star topology,
using store-and-forward packet switching, is there really a need anymore for an Ethernet
MAC protocol? As we’ll see shortly, a switch coordinates its transmissions and
never forwards more than one frame onto the same interface at any time. Furthermore,
modern switches are full-duplex, so that a switch and a node can each send
frames to each other at the same time without interference. In other words, in a
switch-based Ethernet LAN there are no collisions and, therefore, there is no need
for a MAC protocol!
As we’ve seen, today’s Ethernets are very different from the original Ethernet
conceived by Metcalfe and Boggs more than 30 years ago—speeds have increased
by three orders of magnitude, Ethernet frames are carried over a variety of media,
switched-Ethernets have become dominant, and now even the MAC protocol is
often unnecessary! Is all of this really still Ethernet? The answer, of course, is “yes,
5.4 • SWITCHED LOCAL AREA NETWORKS 475
by definition.” It is interesting to note, however, that through all of these changes,
there has indeed been one enduring constant that has remained unchanged over
30 years—Ethernet’s frame format. Perhaps this then is the one true and timeless
centerpiece of the Ethernet standard.
5.4.3 Link-Layer Switches
Up until this point, we have been purposefully vague about what a switch actually
does and how it works. The role of the switch is to receive incoming link-layer frames
and forward them onto outgoing links; we’ll study this forwarding function in detail
in this subsection. We’ll see that the switch itself is transparent to the hosts and
routers in the subnet; that is, a host/router addresses a frame to another host/router
(rather than addressing the frame to the switch) and happily sends the frame into the
LAN, unaware that a switch will be receiving the frame and forwarding it. The rate at
which frames arrive to any one of the switch’s output interfaces may temporarily
exceed the link capacity of that interface. To accommodate this problem, switch output
interfaces have buffers, in much the same way that router output interfaces have
buffers for datagrams. Let’s now take a closer look at how switches operate.
Forwarding and Filtering
Filtering is the switch function that determines whether a frame should be forwarded
to some interface or should just be dropped. Forwarding is the switch
function that determines the interfaces to which a frame should be directed, and
then moves the frame to those interfaces. Switch filtering and forwarding are
done with a switch table. The switch table contains entries for some, but not necessarily
all, of the hosts and routers on a LAN. An entry in the switch table contains
(1) a MAC address, (2) the switch interface that leads toward that MAC
address, and (3) the time at which the entry was placed in the table. An example
switch table for the uppermost switch in Figure 5.15 is shown in Figure 5.22.
476 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Figure 5.22  Portion of a switch table for the uppermost switch in
Figure 5.15
Address Interface Time
62-FE-F7-11-89-A3 1 9:32
7C-BA-B2-B4-91-10 3 9:36
.... .... ....
Although this description of frame forwarding may sound similar to our
discussion of datagram forwarding in Chapter 4, we’ll see shortly that there are
important differences. One important difference is that switches forward packets
based on MAC addresses rather than on IP addresses. We will also see that a
switch table is constructed in a very different manner from a router’s forwarding
table.
To understand how switch filtering and forwarding work, suppose a frame with
destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x. The
switch indexes its table with the MAC address DD-DD-DD-DD-DD-DD. There are
three possible cases:
• There is no entry in the table for DD-DD-DD-DD-DD-DD. In this case, the switch
forwards copies of the frame to the output buffers preceding all interfaces except
for interface x. In other words, if there is no entry for the destination address, the
switch broadcasts the frame.
• There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface
x. In this case, the frame is coming from a LAN segment that contains
adapter DD-DD-DD-DD-DD-DD. There being no need to forward the frame to
any of the other interfaces, the switch performs the filtering function by discarding
the frame.
• There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface
yx. In this case, the frame needs to be forwarded to the LAN segment
attached to interface y. The switch performs its forwarding function by putting
the frame in an output buffer that precedes interface y.
Let’s walk through these rules for the uppermost switch in Figure 5.15 and its
switch table in Figure 5.22. Suppose that a frame with destination address 62-FEF7-
11-89-A3 arrives at the switch from interface 1. The switch examines its table
and sees that the destination is on the LAN segment connected to interface 1 (that
is, Electrical Engineering). This means that the frame has already been broadcast on
the LAN segment that contains the destination. The switch therefore filters (that is,
discards) the frame. Now suppose a frame with the same destination address arrives
from interface 2. The switch again examines its table and sees that the destination is
in the direction of interface 1; it therefore forwards the frame to the output buffer
preceding interface 1. It should be clear from this example that as long as the switch
table is complete and accurate, the switch forwards frames towards destinations
without any broadcasting.
In this sense, a switch is “smarter” than a hub. But how does this switch table
get configured in the first place? Are there link-layer equivalents to networklayer
routing protocols? Or must an overworked manager manually configure the
switch table?
5.4 • SWITCHED LOCAL AREA NETWORKS 477
Self-Learning
A switch has the wonderful property (particularly for the already-overworked network
administrator) that its table is built automatically, dynamically, and
autonomously—without any intervention from a network administrator or from a
configuration protocol. In other words, switches are self-learning. This capability is
accomplished as follows:
1. The switch table is initially empty.
2. For each incoming frame received on an interface, the switch stores in its table
(1) the MAC address in the frame’s source address field, (2) the interface from
which the frame arrived, and (3) the current time. In this manner the switch
records in its table the LAN segment on which the sender resides. If every host
in the LAN eventually sends a frame, then every host will eventually get
recorded in the table.
3. The switch deletes an address in the table if no frames are received with that
address as the source address after some period of time (the aging time). In
this manner, if a PC is replaced by another PC (with a different adapter), the
MAC address of the original PC will eventually be purged from the switch
table.
Let’s walk through the self-learning property for the uppermost switch in
Figure 5.15 and its corresponding switch table in Figure 5.22. Suppose at time 9:39
a frame with source address 01-12-23-34-45-56 arrives from interface 2. Suppose
that this address is not in the switch table. Then the switch adds a new entry to the
table, as shown in Figure 5.23.
Continuing with this same example, suppose that the aging time for this switch
is 60 minutes, and no frames with source address 62-FE-F7-11-89-A3 arrive to the
switch between 9:32 and 10:32. Then at time 10:32, the switch removes this address
from its table.
478 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Figure 5.23  Switch learns about the location of an adapter with address
01-12-23-34-45-56
Address Interface Time
01-12-23-34-45-56 2 9:39
62-FE-F7-11-89-A3 1 9:32
7C-BA-B2-B4-91-10 3 9:36
.... .... ....
Switches are plug-and-play devices because they require no intervention
from a network administrator or user. A network administrator wanting to install
a switch need do nothing more than connect the LAN segments to the switch
interfaces. The administrator need not configure the switch tables at the time of
installation or when a host is removed from one of the LAN segments. Switches
are also full-duplex, meaning any switch interface can send and receive at the
same time.
Properties of Link-Layer Switching
Having described the basic operation of a link-layer switch, let’s now consider their
features and properties. We can identify several advantages of using switches, rather
than broadcast links such as buses or hub-based star topologies:
• Elimination of collisions. In a LAN built from switches (and without hubs), there
is no wasted bandwidth due to collisions! The switches buffer frames and never
transmit more than one frame on a segment at any one time. As with a router, the
maximum aggregate throughput of a switch is the sum of all the switch interface
rates. Thus, switches provide a significant performance improvement over LANs
with broadcast links.
• Heterogeneous links. Because a switch isolates one link from another, the different
links in the LAN can operate at different speeds and can run over different
media. For example, the uppermost switch in Figure 5.22 might have three
1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and
one 100BASE-T copper link. Thus, a switch is ideal for mixing legacy equipment
with new equipment.
• Management. In addition to providing enhanced security (see sidebar on Focus
on Security), a switch also eases network management. For example, if an
adapter malfunctions and continually sends Ethernet frames (called a jabbering
adapter), a switch can detect the problem and internally disconnect the malfunctioning
adapter. With this feature, the network administrator need not get
out of bed and drive back to work in order to correct the problem. Similarly, a
cable cut disconnects only that host that was using the cut cable to connect to
the switch. In the days of coaxial cable, many a network manager spent hours
“walking the line” (or more accurately, “crawling the floor”) to find the cable
break that brought down the entire network. As discussed in Chapter 9 (Network
Management), switches also gather statistics on bandwidth usage, collision
rates, and traffic types, and make this information available to the network
manager. This information can be used to debug and correct problems, and to
plan how the LAN should evolve in the future. Researchers are exploring adding
yet more management functionality into Ethernet LANs in prototype deployments
[Casado 2007; Koponen 2011].
5.4 • SWITCHED LOCAL AREA NETWORKS 479
Switches Versus Routers
As we learned in Chapter 4, routers are store-and-forward packet switches that forward
packets using network-layer addresses. Although a switch is also a store-andforward
packet switch, it is fundamentally different from a router in that it forwards
packets using MAC addresses. Whereas a router is a layer-3 packet switch, a switch
is a layer-2 packet switch.
Even though switches and routers are fundamentally different, network administrators
must often choose between them when installing an interconnection device.
For example, for the network in Figure 5.15, the network administrator could just as
easily have used a router instead of a switch to connect the department LANs,
servers, and internet gateway router. Indeed, a router would permit interdepartmental
communication without creating collisions. Given that both switches and routers
are candidates for interconnection devices, what are the pros and cons of the two
approaches?
First consider the pros and cons of switches. As mentioned above, switches are
plug-and-play, a property that is cherished by all the overworked network administrators
of the world. Switches can also have relatively high filtering and forwarding
rates—as shown in Figure 5.24, switches have to process frames only up through
layer 2, whereas routers have to process datagrams up through layer 3. On the other
480 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
SNIFFING A SWITCHED LAN: SWITCH POISONING
When a host is connected to a switch, it typically only receives frames that are being
explicity sent to it. For example, consider a switched LAN in Figure 5.17. When host
A sends a frame to host B, and there is an entry for host B in the switch table, then
the switch will forward the frame only to host B. If host C happens to be running a
sniffer, host C will not be able to sniff this A-to-B frame. Thus, in a switched-LAN environment
(in contrast to a broadcast link environment such as 802.11 LANs or
hub–based Ethernet LANs), it is more difficult for an attacker to sniff frames. However,
because the switch broadcasts frames that have destination addresses that are not in the
switch table, the sniffer at C can still sniff some frames that are not explicitly addressed
to C. Furthermore, a sniffer will be able sniff all Ethernet broadcast frames with broadcast
destination address FF–FF–FF–FF–FF–FF. A well-known attack against a switch,
called switch poisoning, is to send tons of packets to the switch with many different
bogus source MAC addresses, thereby filling the switch table with bogus entries and
leaving no room for the MAC addresses of the legitimate hosts. This causes the switch
to broadcast most frames, which can then be picked up by the sniffer [Skoudis 2006].
As this attack is rather involved even for a sophisticated attacker, switches are significantly
less vulnerable to sniffing than are hubs and wireless LANs.
FOCUS ON SECURITY
hand, to prevent the cycling of broadcast frames, the active topology of a switched
network is restricted to a spanning tree. Also, a large switched network would
require large ARP tables in the hosts and routers and would generate substantial
ARP traffic and processing. Furthermore, switches are susceptible to broadcast
storms—if one host goes haywire and transmits an endless stream of Ethernet
broadcast frames, the switches will forward all of these frames, causing the entire
network to collapse.
Now consider the pros and cons of routers. Because network addressing is
often hierarchical (and not flat, as is MAC addressing), packets do not normally
cycle through routers even when the network has redundant paths. (However,
packets can cycle when router tables are misconfigured; but as we learned in
Chapter 4, IP uses a special datagram header field to limit the cycling.) Thus,
packets are not restricted to a spanning tree and can use the best path between
source and destination. Because routers do not have the spanning tree restriction,
they have allowed the Internet to be built with a rich topology that includes, for
example, multiple active links between Europe and North America. Another feature
of routers is that they provide firewall protection against layer-2 broadcast
storms. Perhaps the most significant drawback of routers, though, is that they are
not plug-and-play—they and the hosts that connect to them need their IP
addresses to be configured. Also, routers often have a larger per-packet processing
time than switches, because they have to process up through the layer-3 fields.
Finally, there are two different ways to pronounce the word router, either as
“rootor” or as “rowter,” and people waste a lot of time arguing over the proper
pronunciation [Perlman 1999].
Given that both switches and routers have their pros and cons (as summarized
in Table 5.1), when should an institutional network (for example, a university campus
network or a corporate campus network) use switches, and when should it use
Host
Application
Host
Transport
Network
Link
Physical
Link
Physical
Network
Switch Router
Link
Physical
Application
Transport
Network
Link
Physical
Figure 5.24  Packet processing in switches, routers, and hosts
5.4 • SWITCHED LOCAL AREA NETWORKS 481
routers? Typically, small networks consisting of a few hundred hosts have a few
LAN segments. Switches suffice for these small networks, as they localize traffic and
increase aggregate throughput without requiring any configuration of IP addresses.
But larger networks consisting of thousands of hosts typically include routers within
the network (in addition to switches). The routers provide a more robust isolation of
traffic, control broadcast storms, and use more “intelligent” routes among the hosts
in the network.
For more discussion of the pros and cons of switched versus routed networks,
as well as a discussion of how switched LAN technology can be extended to accommodate
two orders of magnitude more hosts than today’s Ethernets, see [Meyers
2004; Kim 2008].
5.4.4 Virtual Local Area Networks (VLANs)
In our earlier discussion of Figure 5.15, we noted that modern institutional LANs
are often configured hierarchically, with each workgroup (department) having its
own switched LAN connected to the switched LANs of other groups via a switch
hierarchy. While such a configuration works well in an ideal world, the real
world is often far from ideal. Three drawbacks can be identified in the configuration
in Figure 5.15:
• Lack of traffic isolation. Although the hierarchy localizes group traffic to
within a single switch, broadcast traffic (e.g., frames carrying ARP and DHCP
messages or frames whose destination has not yet been learned by a selflearning
switch) must still traverse the entire institutional network. Limiting
the scope of such broadcast traffic would improve LAN performance. Perhaps
more importantly, it also may be desirable to limit LAN broadcast traffic for
security/privacy reasons. For example, if one group contains the company’s
executive management team and another group contains disgruntled employees
running Wireshark packet sniffers, the network manager may well prefer
that the executives’ traffic never even reaches employee hosts. This type of
isolation could be provided by replacing the center switch in Figure 5.15 with
482 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Hubs Routers Switches
Traffic isolation No Yes Yes
Plug and play Yes No Yes
Optimal routing No Yes No
Table 5.1  Comparison of the typical features of popular interconnection
devices
a router. We’ll see shortly that this isolation also can be achieved via a
switched (layer 2) solution
• Inefficient use of switches. If instead of three groups, the institution had 10
groups, then 10 first-level switches would be required. If each group were
small, say less than 10 people, then a single 96-port switch would likely be
large enough to accommodate everyone, but this single switch would not provide
traffic isolation.
• Managing users. If an employee moves between groups, the physical cabling
must be changed to connect the employee to a different switch in Figure 5.15.
Employees belonging to two groups make the problem even harder.
Fortunately, each of these difficulties can be handled by a switch that supports
virtual local area networks (VLANs). As the name suggests, a switch that supports
VLANs allows multiple virtual local area networks to be defined over a single
physical local area network infrastructure. Hosts within a VLAN communicate
with each other as if they (and no other hosts) were connected to the switch. In a
port-based VLAN, the switch’s ports (interfaces) are divided into groups by the
network manager. Each group constitutes a VLAN, with the ports in each VLAN
forming a broadcast domain (i.e., broadcast traffic from one port can only reach
other ports in the group). Figure 5.25 shows a single switch with 16 ports. Ports
2 to 8 belong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports
1 and 16 are unassigned). This VLAN solves all of the difficulties noted above—
EE and CS VLAN frames are isolated from each other, the two switches in Figure
5.15 have been replaced by a single switch, and if the user at switch port 8
joins the CS Department, the network operator simply reconfigures the VLAN
software so that port 8 is now associated with the CS VLAN. One can easily
1
Electrical Engineering
(VLAN ports 2–8)
Computer Science
(VLAN ports 9–15)
9 15
2 4 8 10 16
Figure 5.25  A single switch with two configured VLANs
5.4 • SWITCHED LOCAL AREA NETWORKS 483
imagine how the VLAN switch is configured and operates—the network manager
declares a port to belong to a given VLAN (with undeclared ports belonging to a
default VLAN) using switch management software, a table of port-to-VLAN
mappings is maintained within the switch; and switch hardware only delivers
frames between ports belonging to the same VLAN.
But by completely isolating the two VLANs, we have introduced a new difficulty!
How can traffic from the EE Department be sent to the CS Department?
One way to handle this would be to connect a VLAN switch port (e.g., port 1 in
Figure 5.25) to an external router and configure that port to belong both the EE
and CS VLANs. In this case, even though the EE and CS departments share the
same physical switch, the logical configuration would look as if the EE and CS
departments had separate switches connected via a router. An IP datagram going
from the EE to the CS department would first cross the EE VLAN to reach the
router and then be forwarded by the router back over the CS VLAN to the CS
host. Fortunately, switch vendors make such configurations easy for the network
manager by building a single device that contains both a VLAN switch and a
router, so a separate external router is not needed. A homework problem at the end
of the chapter explores this scenario in more detail.
Returning again to Figure 5.15, let’s now suppose that rather than having a
separate Computer Engineering department, some EE and CS faculty are housed
in a separate building, where (of course!) they need network access, and (of
course!) they’d like to be part of their department’s VLAN. Figure 5.26 shows a
second 8-port switch, where the switch ports have been defined as belonging to
the EE or the CS VLAN, as needed. But how should these two switches be interconnected?
One easy solution would be to define a port belonging to the CS
VLAN on each switch (similarly for the EE VLAN) and to connect these ports to
each other, as shown in Figure 5.26(a). This solution doesn’t scale, however, since
N VLANS would require N ports on each switch simply to interconnect the two
switches.
A more scalable approach to interconnecting VLAN switches is known as
VLAN trunking. In the VLAN trunking approach shown in Figure 5.26(b), a special
port on each switch (port 16 on the left switch and port 1 on the right switch) is
configured as a trunk port to interconnect the two VLAN switches. The trunk port
belongs to all VLANs, and frames sent to any VLAN are forwarded over the trunk
link to the other switch. But this raises yet another question: How does a switch
know that a frame arriving on a trunk port belongs to a particular VLAN? The IEEE
has defined an extended Ethernet frame format, 802.1Q, for frames crossing a
VLAN trunk. As shown in Figure 5.27, the 802.1Q frame consists of the standard
Ethernet frame with a four-byte VLAN tag added into the header that carries the
identity of the VLAN to which the frame belongs. The VLAN tag is added into a
frame by the switch at the sending side of a VLAN trunk, parsed, and removed by
the switch at the receiving side of the trunk. The VLAN tag itself consists of a 2-byte
484 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
1
16
1
8
1
Electrical Engineering
(VLAN ports 2–8)
b.
a.
Electrical Engineering
(VLAN ports 2, 3, 6)
Trunk
link
Computer Science
(VLAN ports 9–15)
9 15
2 4 8 10 16
1
2
3
4
5
6 8
7
Computer Science
(VLAN ports 4, 5, 7)
Figure 5.26  Connecting two VLAN switches with two VLANs: (a) two
cables (b) trunked
Preamble CRC
Dest.
address
Source
address
Type
Data
Preamble CRC Dest. '
address
Source
address
Type
Tag Control Information
Tag Protocol Identifier
Recomputed
CRT
Data
Figure 5.27  Original Ethernet frame (top), 802.1Q-tagged Ethernet
VLAN frame (below)
5.4 • SWITCHED LOCAL AREA NETWORKS 485
Tag Protocol Identifier (TPID) field (with a fixed hexadecimal value of 81-00), a
2-byte Tag Control Information field that contains a 12-bit VLAN identifier field,
and a 3-bit priority field that is similar in intent to the IP datagram TOS field.
In this discussion, we’ve only briefly touched on VLANs and have focused
on port-based VLANs. We should also mention that VLANs can be defined in
several other ways. In MAC-based VLANs, the network manager specifies the
set of MAC addresses that belong to each VLAN; whenever a device attaches to
a port, the port is connected into the appropriate VLAN based on the MAC
address of the device. VLANs can also be defined based on network-layer protocols
(e.g., IPv4, IPv6, or Appletalk) and other criteria. See the 802.1Q standard
[IEEE 802.1q 2005] for more details.
5.5 Link Virtualization: A Network as a Link
Layer
Because this chapter concerns link-layer protocols, and given that we’re now nearing
the chapter’s end, let’s reflect on how our understanding of the term link has
evolved. We began this chapter by viewing the link as a physical wire connecting
two communicating hosts. In studying multiple access protocols, we saw that multiple
hosts could be connected by a shared wire and that the “wire” connecting the
hosts could be radio spectra or other media. This led us to consider the link a bit
more abstractly as a channel, rather than as a wire. In our study of Ethernet LANs
(Figure 5.15) we saw that the interconnecting media could actually be a rather complex
switched infrastructure. Throughout this evolution, however, the hosts themselves
maintained the view that the interconnecting medium was simply a link-layer
channel connecting two or more hosts. We saw, for example, that an Ethernet host
can be blissfully unaware of whether it is connected to other LAN hosts by a single
short LAN segment (Figure 5.17) or by a geographically dispersed switched LAN
(Figure 5.15) or by a VLAN (Figure 5.26).
In the case of a dialup modem connection between two hosts, the link connecting
the two hosts is actually the telephone network—a logically separate, global
telecommunications network with its own switches, links, and protocol stacks for
data transfer and signaling. From the Internet link-layer point of view, however, the
dial-up connection through the telephone network is viewed as a simple “wire.” In
this sense, the Internet virtualizes the telephone network, viewing the telephone network
as a link-layer technology providing link-layer connectivity between two
Internet hosts. You may recall from our discussion of overlay networks in Chapter 2
that an overlay network similarly views the Internet as a means for providing connectivity
between overlay nodes, seeking to overlay the Internet in the same way
that the Internet overlays the telephone network.
486 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
In this section, we’ll consider Multiprotocol Label Switching (MPLS) networks.
Unlike the circuit-switched telephone network, MPLS is a packet-switched,
virtual-circuit network in its own right. It has its own packet formats and forwarding
behaviors. Thus, from a pedagogical viewpoint, a discussion of MPLS fits well
into a study of either the network layer or the link layer. From an Internet viewpoint,
however, we can consider MPLS, like the telephone network and switched-
Ethernets, as a link-layer technology that serves to interconnect IP devices. Thus,
we’ll consider MPLS in our discussion of the link layer. Frame-relay and ATM
networks can also be used to interconnect IP devices, though they represent a
slightly older (but still deployed) technology and will not be covered here; see the
very readable book [Goralski 1999] for details. Our treatment of MPLS will be necessarily
brief, as entire books could be (and have been) written on these networks.
We recommend [Davie 2000] for details on MPLS. We’ll focus here primarily on
how MPLS servers interconnect to IP devices, although we’ll dive a bit deeper into
the underlying technologies as well.
5.5.1 Multiprotocol Label Switching (MPLS)
Multiprotocol Label Switching (MPLS) evolved from a number of industry efforts in
the mid-to-late 1990s to improve the forwarding speed of IP routers by adopting a
key concept from the world of virtual-circuit networks: a fixed-length label. The goal
was not to abandon the destination-based IP datagram-forwarding infrastructure for
one based on fixed-length labels and virtual circuits, but to augment it by selectively
labeling datagrams and allowing routers to forward datagrams based on fixed-length
labels (rather than destination IP addresses) when possible. Importantly, these techniques
work hand-in-hand with IP, using IP addressing and routing. The IETF unified
these efforts in the MPLS protocol [RFC 3031, RFC 3032], effectively blending
VC techniques into a routed datagram network.
Let’s begin our study of MPLS by considering the format of a link-layer frame
that is handled by an MPLS-capable router. Figure 5.28 shows that a link-layer frame
transmitted between MPLS-capable devices has a small MPLS header added
between the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header. RFC 3032
PPP or Ethernet
header
MPLS header IP header Remainder of link-layer frame
Label Exp S TTL
Figure 5.28  MPLS header: Located between link- and network-layer
headers
5.5 • LINK VIRTUALIZATION: A NETWORK AS A LINK LAYER 487
defines the format of the MPLS header for such links; headers are defined for ATM
and frame-relayed networks as well in other RFCs. Among the fields in the MPLS
header are the label (which serves the role of the virtual-circuit identifier that we
encountered back in Section 4.2.1), 3 bits reserved for experimental use, a single S bit,
which is used to indicate the end of a series of “stacked” MPLS headers (an advanced
topic that we’ll not cover here), and a time-to-live field.
It’s immediately evident from Figure 5.28 that an MPLS-enhanced frame can
only be sent between routers that are both MPLS capable (since a non-MPLScapable
router would be quite confused when it found an MPLS header where it had
expected to find the IP header!). An MPLS-capable router is often referred to as a
label-switched router, since it forwards an MPLS frame by looking up the MPLS
label in its forwarding table and then immediately passing the datagram to the
appropriate output interface. Thus, the MPLS-capable router need not extract the
destination IP address and perform a lookup of the longest prefix match in the forwarding
table. But how does a router know if its neighbor is indeed MPLS capable,
and how does a router know what label to associate with the given IP destination?
To answer these questions, we’ll need to take a look at the interaction among a
group of MPLS-capable routers.
In the example in Figure 5.29, routers R1 through R4 are MPLS capable. R5
and R6 are standard IP routers. R1 has advertised to R2 and R3 that it (R1) can route
to destination A, and that a received frame with MPLS label 6 will be forwarded to
destination A. Router R3 has advertised to router R4 that it can route to destinations
488 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
R4
in
label
out
label
10
12
8
A
D
A
0
0
1
dest
out
interface
R6
R5
R3
R2
D
A
0
0 0
1 1
0
R1
in
label
out
label
6
9
A
D
1
0
10
12
dest
out
interface
in
label
out
label
6 – A 0
dest
out
interface
in
label
out
label
8 6 A 0
dest
out
interface
Figure 5.29  MPLS-enhanced forwarding
A and D, and that incoming frames with MPLS labels 10 and 12, respectively, will
be switched toward those destinations. Router R2 has also advertised to router R4
that it (R2) can reach destination A, and that a received frame with MPLS label 8
will be switched toward A. Note that router R4 is now in the interesting position of
having two MPLS paths to reach A: via interface 0 with outbound MPLS label 10,
and via interface 1 with an MPLS label of 8. The broad picture painted in Figure
5.29 is that IP devices R5, R6, A, and D are connected together via an MPLS infrastructure
(MPLS-capable routers R1, R2, R3, and R4) in much the same way that a
switched LAN or an ATM network can connect together IP devices. And like a
switched LAN or ATM network, the MPLS-capable routers R1 through R4 do so
without ever touching the IP header of a packet.
In our discussion above, we’ve not specified the specific protocol used to distribute
labels among the MPLS-capable routers, as the details of this signaling are
well beyond the scope of this book. We note, however, that the IETF working group
on MPLS has specified in [RFC 3468] that an extension of the RSVP protocol,
known as RSVP-TE [RFC 3209], will be the focus of its efforts for MPLS signaling.
We’ve also not discussed how MPLS actually computes the paths for packets
among MPLS capable routers, nor how it gathers link-state information (e.g.,
amount of link bandwidth unreserved by MPLS) to use in these path computations.
Existing link-state routing algorithms (e.g., OSPF) have been extended to flood this
information to MPLS-capable routers. Interestingly, the actual path computation
algorithms are not standardized, and are currently vendor-specific.
Thus far, the emphasis of our discussion of MPLS has been on the fact that
MPLS performs switching based on labels, without needing to consider the IP
address of a packet. The true advantages of MPLS and the reason for current interest
in MPLS, however, lie not in the potential increases in switching speeds, but
rather in the new traffic management capabilities that MPLS enables. As noted
above, R4 has two MPLS paths to A. If forwarding were performed up at the IP
layer on the basis of IP address, the IP routing protocols we studied in Chapter 4
would specify only a single, least-cost path to A. Thus, MPLS provides the ability
to forward packets along routes that would not be possible using standard IP routing
protocols. This is one simple form of traffic engineering using MPLS [RFC 3346;
RFC 3272; RFC 2702; Xiao 2000], in which a network operator can override normal
IP routing and force some of the traffic headed toward a given destination along
one path, and other traffic destined toward the same destination along another path
(whether for policy, performance, or some other reason).
It is also possible to use MPLS for many other purposes as well. It can be used
to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a
precomputed failover path in response to link failure [Kar 2000; Huang 2002; RFC
3469]. Finally, we note that MPLS can, and has, been used to implement so-called
virtual private networks (VPNs). In implementing a VPN for a customer, an ISP
uses its MPLS-enabled network to connect together the customer’s various networks.
MPLS can be used to isolate both the resources and addressing used by the
5.5 • LINK VIRTUALIZATION: A NETWORK AS A LINK LAYER 489
customer’s VPN from that of other users crossing the ISP’s network; see [DeClercq
2002] for details.
Our discussion of MPLS has been brief, and we encourage you to consult the
references we’ve mentioned. We note that with so many possible uses for MPLS,
it appears that it is rapidly becoming the Swiss Army knife of Internet traffic
engineering!
5.6 Data Center Networking
In recent years, Internet companies such as Google, Microsoft, Facebook, and Amazon
(as well as their counterparts in Asia and Europe) have built massive data centers, each
housing tens to hundreds of thousands of hosts, and concurrently supporting many
distinct cloud applications (e.g., search, email, social networking, and e-commerce).
Each data center has its own data center network that interconnects its hosts with each
other and interconnects the data center with the Internet. In this section, we provide a
brief introduction to data center networking for cloud applications.
The cost of a large data center is huge, exceeding $12 million per month for a
100,000 host data center [Greenberg 2009a]. Of these costs, about 45 percent can be
attributed to the hosts themselves (which need to be replaced every 3–4 years); 25
percent to infrastructure, including transformers, uninterruptable power supplies
(UPS) systems, generators for long-term outages, and cooling systems; 15 percent
for electric utility costs for the power draw; and 15 percent for networking, including
network gear (switches, routers and load balancers), external links, and transit
traffic costs. (In these percentages, costs for equipment are amortized so that a common
cost metric is applied for one-time purchases and ongoing expenses such as
power.) While networking is not the largest cost, networking innovation is the key
to reducing overall cost and maximizing performance [Greenberg 2009a].
The worker bees in a data center are the hosts: They serve content (e.g., Web
pages and videos), store emails and documents, and collectively perform massively
distributed computations (e.g., distributed index computations for search engines).
The hosts in data centers, called blades and resembling pizza boxes, are generally
commodity hosts that include CPU, memory, and disk storage. The hosts are stacked
in racks, with each rack typically having 20 to 40 blades. At the top of each rack
there is a switch, aptly named the Top of Rack (TOR) switch, that interconnects
the hosts in the rack with each other and with other switches in the data center.
Specifically, each host in the rack has a network interface card that connects to its
TOR switch, and each TOR switch has additional ports that can be connected
to other switches. Although today hosts typically have 1 Gbps Ethernet connections
to their TOR switches, 10 Gbps connections may become the norm. Each host is
also assigned its own data-center-internal IP address.
The data center network supports two types of traffic: traffic flowing between
external clients and internal hosts and traffic flowing between internal hosts. To handle
flows between external clients and internal hosts, the data center network includes one
490 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
or more border routers, connecting the data center network to the public Internet. The
data center network therefore interconnects the racks with each other and connects the
racks to the border routers. Figure 5.30 shows an example of a data center network.
Data center network design, the art of designing the interconnection network and protocols
that connect the racks with each other and with the border routers, has become
an important branch of computer networking research in recent years [Al-Fares 2008;
Greenberg 2009a; Greenberg 2009b; Mydotr 2009; Guo 2009; Chen 2010; Abu-Libdeh
2010; Alizadeh 2010; Wang 2010; Farrington 2010; Halperin 2011; Wilson 2011;
Mudigonda 2011; Ballani 2011; Curtis 2011; Raiciu 2011].
Load Balancing
Acloud data center, such as a Google or Microsoft data center, provides many applications
concurrently, such as search, email, and video applications. To support
requests from external clients, each application is associated with a publicly visible
IP address to which clients send their requests and from which they receive
responses. Inside the data center, the external requests are first directed to a load
balancer whose job it is to distribute requests to the hosts, balancing the load across
the hosts as a function of their current load. A large data center will often have several
Internet
A
1 2 3 4 5 6 7 8
C
B
Server racks
TOR switches
Tier-2 switches
Tier-1 switches
Access router
Border router
Load
balancer
Figure 5.30  A data center network with a hierarchical topology
5.6 • DATA CENTER NETWORKING 491
load balancers, each one devoted to a set of specific cloud applications. Such a load
balancer is sometimes referred to as a “layer-4 switch” since it makes decisions
based on the destination port number (layer 4) as well as destination IP address in
the packet. Upon receiving a request for a particular application, the load balancer
forwards it to one of the hosts that handles the application. (A host may then invoke
the services of other hosts to help process the request.) When the host finishes processing
the request, it sends its response back to the load balancer, which in turn
relays the response back to the external client. The load balancer not only balances
the work load across hosts, but also provides a NAT-like function, translating the
public external IP address to the internal IP address of the appropriate host, and then
translating back for packets traveling in the reverse direction back to the clients.
This prevents clients from contacting hosts directly, which has the security benefit
of hiding the internal network structure and preventing clients from directly interacting
with the hosts.
Hierarchical Architecture
For a small data center housing only a few thousand hosts, a simple network consisting
of a border router, a load balancer, and a few tens of racks all interconnected
by a single Ethernet switch could possibly suffice. But to scale to tens to hundreds
of thousands of hosts, a data center often employs a hierarchy of routers and
switches, such as the topology shown in Figure 5.30. At the top of the hierarchy, the
border router connects to access routers (only two are shown in Figure 5.30, but
there can be many more). Below each access router there are three tiers of switches.
Each access router connects to a top-tier switch, and each top-tier switch connects
to multiple second-tier switches and a load balancer. Each second-tier switch in turn
connects to multiple racks via the racks’ TOR switches (third-tier switches). All
links typically use Ethernet for their link-layer and physical-layer protocols, with a
mix of copper and fiber cabling. With such a hierarchical design, it is possible to
scale a data center to hundreds of thousands of hosts.
Because it is critical for a cloud application provider to continually provide
applications with high availability, data centers also include redundant network
equipment and redundant links in their designs (not shown in Figure 5.30).
For example, each TOR switch can connect to two tier-2 switches, and each
access router, tier-1 switch, and tier-2 switch can be duplicated and integrated
into the design [Cisco 2012; Greenberg 2009b]. In the hierarchical design in
Figure 5.30, observe that the hosts below each access router form a single subnet.
In order to localize ARP broadcast traffic, each of these subnets is further
partitioned into smaller VLAN subnets, each comprising a few hundred hosts
[Greenberg 2009a].
Although the conventional hierarchical architecture just described solves the
problem of scale, it suffers from limited host-to-host capacity [Greenberg 2009b]. To
understand this limitation, consider again Figure 5.30, and suppose each host connects
492 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
to its TOR switch with a 1 Gbps link, whereas the links between switches are 10 Gbps
Ethernet links. Two hosts in the same rack can always communicate at a full 1 Gbps,
limited only by the rate of the hosts’ network interface cards. However, if there are
many simultaneous flows in the data center network, the maximum rate between two
hosts in different racks can be much less. To gain insight into this issue, consider a traffic
pattern consisting of 40 simultaneous flows between 40 pairs of hosts in different
racks. Specifically, suppose each of 10 hosts in rack 1 in Figure 5.30 sends a flow to a
corresponding host in rack 5. Similarly, there are ten simultaneous flows between pairs
of hosts in racks 2 and 6, ten simultaneous flows between racks 3 and 7, and ten simultaneous
flows between racks 4 and 8. If each flow evenly shares a link’s capacity with
other flows traversing that link, then the 40 flows crossing the 10 Gbps A-to-B link (as
well as the 10 Gbps B-to-C link) will each only receive 10 Gbps / 40 = 250 Mbps,
which is significantly less than the 1 Gbps network interface card rate. The problem
becomes even more acute for flows between hosts that need to travel higher up the
hierarchy. One possible solution to this limitation is to deploy higher-rate switches and
routers. But this would significantly increase the cost of the data center, because
switches and routers with high port speeds are very expensive.
Supporting high-bandwidth host-to-host communication is important because a
key requirement in data centers is flexibility in placement of computation and services
[Greenberg 2009b; Farrington 2010]. For example, a large-scale Internet search
engine may run on thousands of hosts spread across multiple racks with significant
bandwidth requirements between all pairs of hosts. Similarly, a cloud computing service
such as EC2 may wish to place the multiple virtual machines comprising a customer’s
service on the physical hosts with the most capacity irrespective of their
location in the data center. If these physical hosts are spread across multiple racks, network
bottlenecks as described above may result in poor performance.
Trends in Data Center Networking
In order to reduce the cost of data centers, and at the same time improve their delay and
throughput performance, Internet cloud giants such as Google, Facebook, Amazon, and
Microsoft are continually deploying new data center network designs. Although these
designs are proprietary, many important trends can nevertheless be identified.
One such trend is to deploy new interconnection architectures and network protocols
that overcome the drawbacks of the traditional hierarchical designs. One such
approach is to replace the hierarchy of switches and routers with a fully connected
topology [Al-Fares 2008; Greenberg 2009b; Guo 2009], such as the topology shown in
Figure 5.31. In this design, each tier-1 switch connects to all of the tier-2 switches so
that (1) host-to-host traffic never has to rise above the switch tiers, and (2) with n tier-1
switches, between any two tier-2 switches there are n disjoint paths. Such a design can
significantly improve the host-to-host capacity. To see this, consider again our example
of 40 flows. The topology in Figure 5.31 can handle such a flow pattern since there are
four distinct paths between the first tier-2 switch and the second tier-2 switch, together
5.6 • DATA CENTER NETWORKING 493
providing an aggregate capacity of 40 Gbps between the first two tier-2 switches. Such
a design not only alleviates the host-to-host capacity limitation, but also creates a more
flexible computation and service environment in which communication between any
two racks not connected to the same switch is logically equivalent, irrespective of their
locations in the data center.
Another major trend is to employ shipping container–based modular data centers
(MDCs) [YouTube 2009; Waldrop 2007]. In an MDC, a factory builds, within a standard
12-meter shipping container, a “mini data center” and ships the container to the
data center location. Each container has up to a few thousand hosts, stacked in tens of
racks, which are packed closely together. At the data center location, multiple containers
are interconnected with each other and also with the Internet. Once a prefabricated
container is deployed at a data center, it is often difficult to service. Thus, each container
is designed for graceful performance degradation: as components (servers and
switches) fail over time, the container continues to operate but with degraded performance.
When many components have failed and performance has dropped below a
threshold, the entire container is removed and replaced with a fresh one.
Building a data center out of containers creates new networking challenges.
With an MDC, there are two types of networks: the container-internal networks
within each of the containers and the core network connecting each container [Guo
2009; Farrington 2010]. Within each container, at the scale of up to a few thousand
hosts, it is possible to build a fully connected network (as described above) using
inexpensive commodity Gigabit Ethernet switches. However, the design of the core
network, interconnecting hundreds to thousands of containers while providing high
host-to-host bandwidth across containers for typical workloads, remains a challenging
problem. A hybrid electrical/optical switch architecture for interconnecting the
containers is proposed in [Farrington 2010].
When using highly interconnected topologies, one of the major issues is designing
routing algorithms among the switches. One possibility [Greenberg 2009b] is to
494 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
1 2 3 4 5 6 7 8
Server racks
TOR switches
Tier-2 switches
Tier-1 switches
Figure 5.31  Highly-interconnected data network topology
use a form of random routing. Another possibility [Guo 2009] is to deploy multiple
network interface cards in each host, connect each host to multiple low-cost commodity
switches, and allow the hosts themselves to intelligently route traffic among the
switches. Variations and extensions of these approaches are currently being deployed
in contemporary data centers. Many more innovations in data center design are likely
to come; interested readers are encouraged to read the many recent papers on data center
network design.
5.7 Retrospective: A Day in the Life of a Web Page
Request
Now that we’ve covered the link layer in this chapter, and the network, transport and
application layers in earlier chapters, our journey down the protocol stack is complete!
In the very beginning of this book (Section 1.1), we wrote “much of this book
is concerned with computer network protocols,” and in the first five chapters, we’ve
certainly seen that this is indeed the case! Before heading into the topical chapters in
second part of this book, we’d like to wrap up our journey down the protocol stack
by taking an integrated, holistic view of the protocols we’ve learned about so far.
One way then to take this “big picture” view is to identify the many (many!) protocols
that are involved in satisfying even the simplest request: downloading a web page.
Figure 5.32 illustrates our setting: a student, Bob, connects a laptop to his school’s
Ethernet switch and downloads a web page (say the home page of www.google.com).
As we now know, there’s a lot going on “under the hood” to satisfy this seemingly
simple request. AWireshark lab at the end of this chapter examines trace files containing
a number of the packets involved in similar scenarios in more detail.
5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet
Let’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable
connected to the school’s Ethernet switch, which in turn is connected to the school’s
router, as shown in Figure 5.32. The school’s router is connected to an ISP, in this
example, comcast.net. In this example, comcast.net is providing the DNS service
for the school; thus, the DNS server resides in the Comcast network rather than the
school network. We’ll assume that the DHCP server is running within the router, as
is often the case.
When Bob first connects his laptop to the network, he can’t do anything (e.g.,
download a Web page) without an IP address. Thus, the first network-related action
taken by Bob’s laptop is to run the DHCP protocol to obtain an IP address, as well
as other information, from the local DHCP server:
1. The operating system on Bob’s laptop creates a DHCP request message (Section
4.4.2) and puts this message within a UDP segment (Section 3.3) with
destination port 67 (DHCP server) and source port 68 (DHCP client). The UDP
segment is then placed within an IP datagram (Section 4.4.1) with a broadcast
5.7 • RETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST 495
VideoNote
A day in the life of a
Web page request
IP destination address (255.255.255.255) and a source IP address of 0.0.0.0,
since Bob’s laptop doesn’t yet have an IP address.
2. The IP datagram containing the DHCP request message is then placed within
an Ethernet frame (Section 5.4.2). The Ethernet frame has a destination MAC
addresses of FF:FF:FF:FF:FF:FF so that the frame will be broadcast to all
devices connected to the switch (hopefully including a DHCP server); the
frame’s source MAC address is that of Bob’s laptop, 00:16:D3:23:68:8A.
3. The broadcast Ethernet frame containing the DHCP request is the first frame
sent by Bob’s laptop to the Ethernet switch. The switch broadcasts the incoming
frame on all outgoing ports, including the port connected to the router.
4. The router receives the broadcast Ethernet frame containing the DHCP request
on its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram is
extracted from the Ethernet frame. The datagram’s broadcast IP destination
address indicates that this IP datagram should be processed by upper layer protocols
at this node, so the datagram’s payload (a UDP segment) is thus demultiplexed
(Section 3.2) up to UDP, and the DHCP request message is extracted
from the UDP segment. The DHCP server now has the DHCP request message.
5. Let’s suppose that the DHCP server running within the router can allocate IP
addresses in the CIDR (Section 4.4.2) block 68.85.2.0/24. In this example, all
IP addresses used within the school are thus within Comcast’s address block.
496 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
00:22:6B:45:1F:1B
68.85.2.1
00:16:D3:23:68:8A
68.85.2.101
comcast.net
DNS server
68.87.71.226
www.google.com
Web server
64.233.169.105
School network
68.80.2.0/24
Comcast’s network
68.80.0.0/13
Google’s network
64.233.160.0/19
1–7
8–13
18–24
14–17
Figure 5.32  A day in the life of a Web page request: network setting
and actions
Let’s suppose the DHCP server allocates address 68.85.2.101 to Bob’s laptop.
The DHCP server creates a DHCPACK message (Section 4.4.2) containing
this IP address, as well as the IP address of the DNS server (68.87.71.226), the
IP address for the default gateway router (68.85.2.1), and the subnet block
(68.85.2.0/24) (equivalently, the “network mask”). The DHCP message is put
inside a UDP segment, which is put inside an IP datagram, which is put inside
an Ethernet frame. The Ethernet frame has a source MAC address of the
router’s interface to the home network (00:22:6B:45:1F:1B) and a destination
MAC address of Bob’s laptop (00:16:D3:23:68:8A).
6. The Ethernet frame containing the DHCP ACK is sent (unicast) by the router
to the switch. Because the switch is self-learning (Section 5.4.3) and previously
received an Ethernet frame (containing the DHCP request) from Bob’s
laptop, the switch knows to forward a frame addressed to 00:16:D3:23:68:8A
only to the output port leading to Bob’s laptop.
7. Bob’s laptop receives the Ethernet frame containing the DHCP ACK, extracts
the IP datagram from the Ethernet frame, extracts the UDP segment from the
IP datagram, and extracts the DHCP ACK message from the UDP segment.
Bob’s DHCP client then records its IP address and the IP address of its DNS
server. It also installs the address of the default gateway into its IP forwarding
table (Section 4.1). Bob’s laptop will send all datagrams with destination
address outside of its subnet 68.85.2.0/24 to the default gateway. At this point,
Bob’s laptop has initialized its networking components and is ready to begin
processing the Web page fetch. (Note that only the last two DHCP steps of the
four presented in Chapter 4 are actually necessary.)
5.7.2 Still Getting Started: DNS and ARP
When Bob types the URL for www.google.com into his Web browser, he begins the
long chain of events that will eventually result in Google’s home page being displayed
by his Web browser. Bob’s Web browser begins the process by creating a
TCP socket (Section 2.7) that will be used to send the HTTP request (Section 2.2)
to www.google.com. In order to create the socket, Bob’s laptop will need to know
the IP address of www.google.com. We learned in Section 2.5, that the DNS protocol
is used to provide this name-to-IP-address translation service.
8. The operating system on Bob’s laptop thus creates a DNS query message (Section
2.5.3), putting the string “www.google.com” in the question section of the DNS
message. This DNS message is then placed within a UDP segment with a destination
port of 53 (DNS server). The UDP segment is then placed within an IP datagram
with an IP destination address of 68.87.71.226 (the address of the DNS server
returned in the DHCP ACK in step 5) and a source IP address of 68.85.2.101.
9. Bob’s laptop then places the datagram containing the DNS query message in
an Ethernet frame. This frame will be sent (addressed, at the link layer) to the
gateway router in Bob’s school’s network. However, even though Bob’s laptop
5.7 • RETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST 497
knows the IP address of the school’s gateway router (68.85.2.1) via the DHCP
ACK message in step 5 above, it doesn’t know the gateway router’s MAC
address. In order to obtain the MAC address of the gateway router, Bob’s laptop
will need to use the ARP protocol (Section 5.4.1).
10. Bob’s laptop creates an ARP query message with a target IP address of
68.85.2.1 (the default gateway), places the ARP message within an Ethernet
frame with a broadcast destination address (FF:FF:FF:FF:FF:FF) and sends the
Ethernet frame to the switch, which delivers the frame to all connected
devices, including the gateway router.
11. The gateway router receives the frame containing the ARP query message on the
interface to the school network, and finds that the target IP address of 68.85.2.1 in
the ARP message matches the IP address of its interface. The gateway router thus
prepares an ARPreply, indicating that its MAC address of 00:22:6B:45:1F:1B
corresponds to IP address 68.85.2.1. It places the ARP reply message in an
Ethernet frame, with a destination address of 00:16:D3:23:68:8A (Bob’s laptop)
and sends the frame to the switch, which delivers the frame to Bob’s laptop.
12. Bob’s laptop receives the frame containing the ARP reply message and extracts
the MAC address of the gateway router (00:22:6B:45:1F:1B) from the ARP
reply message.
13. Bob’s laptop can now ( finally!) address the Ethernet frame containing the DNS
query to the gateway router’s MAC address. Note that the IP datagram in this frame
has an IP destination address of 68.87.71.226 (the DNS server), while the frame has
a destination address of 00:22:6B:45:1F:1B (the gateway router). Bob’s laptop
sends this frame to the switch, which delivers the frame to the gateway router.
5.7.3 Still Getting Started: Intra-Domain Routing to the
DNS Server
14. The gateway router receives the frame and extracts the IP datagram containing
the DNS query. The router looks up the destination address of this datagram
(68.87.71.226) and determines from its forwarding table that the datagram should
be sent to the leftmost router in the Comcast network in Figure 5.32. The IP datagram
is placed inside a link-layer frame appropriate for the link connecting the
school’s router to the leftmost Comcast router and the frame is sent over this link.
15. The leftmost router in the Comcast network receives the frame, extracts the IP
datagram, examines the datagram’s destination address (68.87.71.226) and
determines the outgoing interface on which to forward the datagram towards
the DNS server from its forwarding table, which has been filled in by Comcast’s
intra-domain protocol (such as RIP, OSPF or IS-IS, Section 4.6) as well
as the Internet’s inter-domain protocol, BGP.
16. Eventually the IP datagram containing the DNS query arrives at the DNS server.
The DNS server extracts the DNS query message, looks up the name
www.google.com in its DNS database (Section 2.5), and finds the DNS resource
498 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
record that contains the IP address (64.233.169.105) for www.google.com.
(assuming that it is currently cached in the DNS server). Recall that this cached
data originated in the authoritative DNS server (Section 2.5.2) for googlecom.
The DNS server forms a DNS reply message containing this hostname-to-IPaddress
mapping, and places the DNS reply message in a UDP segment, and the
segment within an IP datagram addressed to Bob’s laptop (68.85.2.101). This
datagram will be forwarded back through the Comcast network to the school’s
router and from there, via the Ethernet switch to Bob’s laptop.
17. Bob’s laptop extracts the IP address of the server www.google.com from the
DNS message. Finally, after a lot of work, Bob’s laptop is now ready to contact
the www.google.com server!
5.7.4 Web Client-Server Interaction: TCP and HTTP
18. Now that Bob’s laptop has the IP address of www.google.com, it can create the
TCP socket (Section 2.7) that will be used to send the HTTPGET message
(Section 2.2.3) to www.google.com. When Bob creates the TCP socket, the TCP
in Bob’s laptop must first perform a three-way handshake (Section 3.5.6) with
the TCP in www.google.com. Bob’s laptop thus first creates a TCP SYN segment
with destination port 80 (for HTTP), places the TCP segment inside an IP datagram
with a destination IP address of 64.233.169.105 (www.google.com), places
the datagram inside a frame with a destination MAC address of
00:22:6B:45:1F:1B (the gateway router) and sends the frame to the switch.
19. The routers in the school network, Comcast’s network, and Google’s network
forward the datagram containing the TCP SYN towards www.google.com,
using the forwarding table in each router, as in steps 14–16 above. Recall that
the router forwarding table entries governing forwarding of packets over the
inter-domain link between the Comcast and Google networks are determined
by the BGP protocol (Section 4.6.3).
20. Eventually, the datagram containing the TCP SYN arrives at www.google.com.
The TCP SYN message is extracted from the datagram and demultiplexed to the
welcome socket associated with port 80. A connection socket (Section 2.7) is
created for the TCP connection between the Google HTTP server and Bob’s
laptop. ATCP SYNACK (Section 3.5.6) segment is generated, placed inside a
datagram addressed to Bob’s laptop, and finally placed inside a link-layer frame
appropriate for the link connecting www.google.com to its first-hop router.
21. The datagram containing the TCP SYNACK segment is forwarded through the
Google, Comcast, and school networks, eventually arriving at the Ethernet card
in Bob’s laptop. The datagram is demultiplexed within the operating system to
the TCP socket created in step 18, which enters the connected state.
22. With the socket on Bob’s laptop now (finally!) ready to send bytes to www.google
.com, Bob’s browser creates the HTTP GET message (Section 2.2.3) containing the
URLto be fetched. The HTTPGET message is then written into the socket, with the
5.7 • RETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST 499
GET message becoming the payload of a TCP segment. The TCP segment is placed
in a datagram and sent and delivered to www.google.com as in steps 18–20 above.
23. The HTTP server at www.google.com reads the HTTP GET message from the
TCP socket, creates an HTTP response message (Section 2.2), places the
requested Web page content in the body of the HTTP response message, and
sends the message into the TCP socket.
24. The datagram containing the HTTP reply message is forwarded through the Google,
Comcast, and school networks, and arrives at Bob’s laptop. Bob’s Web browser program
reads the HTTP response from the socket, extracts the html for the Web page
from the body of the HTTP response, and finally ( finally!) displays the Web page!
Our scenario above has covered a lot of networking ground! If you’ve understood
most or all of the above example, then you’ve also covered a lot of ground since you
first read Section 1.1, where we wrote “much of this book is concerned with computer
network protocols” and you may have wondered what a protocol actually was! As
detailed as the above example might seem, we’ve omitted a number of possible additional
protocols (e.g., NAT running in the school’s gateway router, wireless access to
the school’s network, security protocols for accessing the school network or encrypting
segments or datagrams, network management protocols), and considerations (Web
caching, the DNS hierarchy) that one would encounter in the public Internet. We’ll
cover a number of these topics and more in the second part of this book.
Lastly, we note that our example above was an integrated and holistic, but also
very “nuts and bolts,” view of many of the protocols that we’ve studied in the first
part of this book. The example focused more on the “how” than the “why.” For a
broader, more reflective view on the design of network protocols in general, see
[Clark 1988, RFC 5218].
5.8 Summary
In this chapter, we’ve examined the link layer—its services, the principles underlying
its operation, and a number of important specific protocols that use these principles
in implementing link-layer services.
We saw that the basic service of the link layer is to move a network-layer datagram
from one node (host, switch, router, WiFi access point) to an adjacent node.
We saw that all link-layer protocols operate by encapsulating a network-layer datagram
within a link-layer frame before transmitting the frame over the link to the
adjacent node. Beyond this common framing function, however, we learned that different
link-layer protocols provide very different link access, delivery, and transmission
services. These differences are due in part to the wide variety of link types over
which link-layer protocols must operate. A simple point-to-point link has a single
sender and receiver communicating over a single “wire.” A multiple access link is
shared among many senders and receivers; consequently, the link-layer protocol for
a multiple access channel has a protocol (its multiple access protocol) for coordinating
link access. In the case of MPLS, the “link” connecting two adjacent nodes (for
500 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
example, two IP routers that are adjacent in an IP sense—that they are next-hop IP
routers toward some destination) may actually be a network in and of itself. In one
sense, the idea of a network being considered as a link should not seem odd. A telephone
link connecting a home modem/computer to a remote modem/router, for
example, is actually a path through a sophisticated and complex telephone network.
Among the principles underlying link-layer communication, we examined errordetection
and -correction techniques, multiple access protocols, link-layer addressing,
virtualization (VLANs), and the construction of extended switched LANs and
data center networks. Much of the focus today at the link layer is on these switched
networks. In the case of error detection/correction, we examined how it is possible to
add additional bits to a frame’s header in order to detect, and in some cases correct,
bit-flip errors that might occur when the frame is transmitted over the link. We covered
simple parity and checksumming schemes, as well as the more robust cyclic
redundancy check. We then moved on to the topic of multiple access protocols. We
identified and studied three broad approaches for coordinating access to a broadcast
channel: channel partitioning approaches (TDM, FDM), random access approaches
(the ALOHA protocols and CSMA protocols), and taking-turns approaches (polling
and token passing). We studied the cable access network and found that it uses many
of these multiple access methods. We saw that a consequence of having multiple
nodes share a single broadcast channel was the need to provide node addresses at the
link layer. We learned that link-layer addresses were quite different from networklayer
addresses and that, in the case of the Internet, a special protocol (ARP—the
Address Resolution Protocol) is used to translate between these two forms of
addressing and studied the hugely successful Ethernet protocol in detail. We then
examined how nodes sharing a broadcast channel form a LAN and how multiple
LANs can be connected together to form larger LANs—all without the intervention
of network-layer routing to interconnect these local nodes.We also learned how multiple
virtual LANs can be created on a single physical LAN infrastructure.
We ended our study of the link layer by focusing on how MPLS networks provide
link-layer services when they interconnect IP routers and an overview of the network
designs for today’s massive data centers.We wrapped up this chapter (and
indeed the first five chapters) by identifying the many protocols that are needed to
fetch a simple Web page. Having covered the link layer, our journey down the protocol
stack is now over! Certainly, the physical layer lies below the link layer, but the
details of the physical layer are probably best left for another course (for example, in
communication theory, rather than computer networking). We have, however,
touched upon several aspects of the physical layer in this chapter and in Chapter 1
(our discussion of physical media in Section 1.2). We’ll consider the physical layer
again when we study wireless link characteristics in the next chapter.
Although our journey down the protocol stack is over, our study of computer
networking is not yet at an end. In the following four chapters we cover wireless
networking, multimedia networking, network security, and network management.
These four topics do not fit conveniently into any one layer; indeed, each topic
crosscuts many layers. Understanding these topics (billed as advanced topics in
5.8 • SUMMARY 501
some networking texts) thus requires a firm foundation in all layers of the protocol
stack—a foundation that our study of the link layer has now completed!
Homework Problems and Questions
Chapter 5 Review Questions
SECTIONS 5.1–5.2
R1. Consider the transportation analogy in Section 5.1.1. If the passenger is
analagous to a datagram, what is analogous to the link layer frame?
R2. If all the links in the Internet were to provide reliable delivery service, would
the TCP reliable delivery service be redundant? Why or why not?
R3. What are some of the possible services that a link-layer protocol can offer to
the network layer? Which of these link-layer services have corresponding
services in IP? In TCP?
SECTION 5.3
R4. Suppose two nodes start to transmit at the same time a packet of length L
over a broadcast channel of rate R. Denote the propagation delay between the
two nodes as dprop. Will there be a collision if dprop < L/R? Why or why not?
R5. In Section 5.3, we listed four desirable characteristics of a broadcast channel.
Which of these characteristics does slotted ALOHA have? Which of these
characteristics does token passing have?
R6. In CSMA/CD, after the fifth collision, what is the probability that a node
chooses K = 4? The result K = 4 corresponds to a delay of how many seconds
on a 10 Mbps Ethernet?
R7. Describe polling and token-passing protocols using the analogy of cocktail
party interactions.
R8. Why would the token-ring protocol be inefficient if a LAN had a very large
perimeter?
SECTION 5.4
R9. How big is the MAC address space? The IPv4 address space? The IPv6
address space?
R10. Suppose nodes A, B, and C each attach to the same broadcast LAN (through
their adapters). If A sends thousands of IP datagrams to B with each encapsulating
frame addressed to the MAC address of B, will C’s adapter process
these frames? If so, will C’s adapter pass the IP datagrams in these frames to
the network layer C? How would your answers change if A sends frames with
the MAC broadcast address?
502 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
R11. Why is an ARP query sent within a broadcast frame? Why is an ARP
response sent within a frame with a specific destination MAC address?
R12. For the network in Figure 5.19, the router has two ARP modules, each with
its own ARP table. Is it possible that the same MAC address appears in both
tables?
R13. Compare the frame structures for 10BASE-T, 100BASE-T, and Gigabit Ethernet.
How do they differ?
R14. Consider Figure 5.15. How many subnetworks are there, in the addressing
sense of Section 4.4?
R15. What is the maximum number of VLANs that can be configured on a switch
supporting the 802.1Q protocol? Why?
R16. Suppose that N switches supporting K VLAN groups are to be connected via
a trunking protocol. How many ports are needed to connect the switches?
Justify your answer.
Problems
P1. Suppose the information content of a packet is the bit pattern 1110 0110 1001
1101 and an even parity scheme is being used. What would the value of the
field containing the parity bits be for the case of a two-dimensional parity
scheme? Your answer should be such that a minimum-length checksum field
is used.
P2. Show (give an example other than the one in Figure 5.5) that two-dimensional
parity checks can correct and detect a single bit error. Show (give an example
of) a double-bit error that can be detected but not corrected.
P3. Suppose the information portion of a packet (D in Figure 5.3) contains 10
bytes consisting of the 8-bit unsigned binary ASCII representation of string
“Networking.” Compute the Internet checksum for this data.
P4. Consider the previous problem, but instead suppose these 10 bytes contain
a. the binary representation of the numbers 1 through 10.
b. the ASCII representation of the letters B through K (uppercase).
c. the ASCII representation of the letters b through k (lowercase).
Compute the Internet checksum for this data.
P5. Consider the 7-bit generator, G=10011, and suppose that D has the value
1010101010. What is the value of R?
P6. Consider the previous problem, but suppose that D has the value
a. 1001010101.
b. 0101101010.
c. 1010100000.
PROBLEMS 503
P7. In this problem, we explore some of the properties of the CRC. For the generator
G (=1001) given in Section 5.2.3, answer the following questions.
a. Why can it detect any single bit error in data D?
b. Can the above G detect any odd number of bit errors? Why?
P8. In Section 5.3, we provided an outline of the derivation of the efficiency of
slotted ALOHA. In this problem we’ll complete the derivation.
a. Recall that when there are N active nodes, the efficiency of slotted ALOHA
is Np(1 – p)N–1. Find the value of p that maximizes this expression.
b. Using the value of p found in (a), find the efficiency of slotted ALOHA by
letting N approach infinity. Hint: (1 – 1/N)N approaches 1/e as N
approaches infinity.
P9. Show that the maximum efficiency of pure ALOHA is 1/(2e). Note: This
problem is easy if you have completed the problem above!
P10. Consider two nodes, A and B, that use the slotted ALOHA protocol to contend
for a channel. Suppose node A has more data to transmit than node B,
and node A’s retransmission probability pA is greater than node B’s retransmission
probability, pB.
a. Provide a formula for node A’s average throughput. What is the total efficiency
of the protocol with these two nodes?
b. If pA= 2pB, is node A’s average throughput twice as large as that of node B?
Why or why not? If not, how can you choose pA and pB to make that happen?
c. In general, suppose there are N nodes, among which node A has retransmission
probability 2p and all other nodes have retransmission probability
p. Provide expressions to compute the average throughputs of node A and
of any other node.
P11. Suppose four active nodes—nodes A, B, C and D—are competing for access to
a channel using slotted ALOHA. Assume each node has an infinite number
of packets to send. Each node attempts to transmit in each slot with probability
p. The first slot is numbered slot 1, the second slot is numbered slot 2, and so on.
a. What is the probability that node A succeeds for the first time in slot 5?
b. What is the probability that some node (either A, B, C or D) succeeds in slot 4?
c. What is the probability that the first success occurs in slot 3?
d. What is the efficiency of this four-node system?
P12. Graph the efficiency of slotted ALOHA and pure ALOHA as a function of p
for the following values of N:
a. N15.
b. N25.
c. N35.
504 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
Subnet 3
E
F
C
Subnet 2
D
A
B
Subnet 1
Figure 5.33  Three subnets, interconnected by routers
P13. Consider a broadcast channel with N nodes and a transmission rate of R bps.
Suppose the broadcast channel uses polling (with an additional polling node)
for multiple access. Suppose the amount of time from when a node completes
transmission until the subsequent node is permitted to transmit (that is, the
polling delay) is dpoll. Suppose that within a polling round, a given node is
allowed to transmit at most Q bits. What is the maximum throughput of the
broadcast channel?
P14. Consider three LANs interconnected by two routers, as shown in Figure 5.33.
a. Assign IP addresses to all of the interfaces. For Subnet 1 use addresses of
the form 192.168.1.xxx; for Subnet 2 uses addresses of the form
192.168.2.xxx; and for Subnet 3 use addresses of the form 192.168.3.xxx.
b. Assign MAC addresses to all of the adapters.
c. Consider sending an IP datagram from Host E to Host B. Suppose all of
the ARP tables are up to date. Enumerate all the steps, as done for the
single-router example in Section 5.4.1.
d. Repeat (c), now assuming that the ARP table in the sending host is empty
(and the other tables are up to date).
P15. Consider Figure 5.33. Now we replace the router between subnets 1 and 2
with a switch S1, and label the router between subnets 2 and 3 as R1.
PROBLEMS 505
a. Consider sending an IP datagram from Host E to Host F. Will Host E ask
router R1 to help forward the datagram? Why? In the Ethernet frame containing
the IP datagram, what are the source and destination IP and MAC
addresses?
b. Suppose E would like to send an IP datagram to B, and assume that E’s
ARP cache does not contain B’s MAC address. Will E perform an ARP
query to find B’s MAC address? Why? In the Ethernet frame (containing
the IP datagram destined to B) that is delivered to router R1, what are the
source and destination IP and MAC addresses?
c. Suppose Host Awould like to send an IP datagram to Host B, and neither A’s
ARP cache contains B’s MAC address nor does B’s ARP cache contain A’s
MAC address. Further suppose that the switch S1’s forwarding table contains
entries for Host B and router R1 only. Thus, Awill broadcast an ARP request
message. What actions will switch S1 perform once it receives the ARP
request message? Will router R1 also receive this ARP request message? If so,
will R1 forward the message to Subnet 3? Once Host B receives this ARP
request message, it will send back to Host Aan ARP response message. But
will it send an ARP query message to ask for A’s MAC address? Why? What
will switch S1 do once it receives an ARP response message from Host B?
P16. Consider the previous problem, but suppose now that the router between subnets
2 and 3 is replaced by a switch. Answer questions (a)–(c) in the previous
problem in this new context.
P17. Recall that with the CSMA/CD protocol, the adapter waits K  512 bit times after
a collision, where K is drawn randomly. For K = 100, how long does the adapter
wait until returning to Step 2 for a 10 Mbps broadcast channel? For a 100 Mbps
broadcast channel?
P18. Suppose nodes Aand B are on the same 10 Mbps broadcast channel, and the
propagation delay between the two nodes is 325 bit times. Suppose CSMA/CD
and Ethernet packets are used for this broadcast channel. Suppose node Abegins
transmitting a frame and, before it finishes, node B begins transmitting a frame.
Can Afinish transmitting before it detects that B has transmitted? Why or why
not? If the answer is yes, then Aincorrectly believes that its frame was successfully
transmitted without a collision. Hint: Suppose at time t = 0 bits, Abegins
transmitting a frame. In the worst case, Atransmits a minimum-sized frame of
512 + 64 bit times. So Awould finish transmitting the frame at t = 512 + 64 bit
times. Thus, the answer is no, if B’s signal reaches Abefore bit time t = 512 + 64
bits. In the worst case, when does B’s signal reach A?
P19. Suppose nodes A and B are on the same 10 Mbps broadcast channel, and
the propagation delay between the two nodes is 245 bit times. Suppose A
and B send Ethernet frames at the same time, the frames collide, and then
A and B choose different values of K in the CSMA/CD algorithm. Assuming
506 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
VideoNote
Sending a datagram
between subnets: linklayer
and network-layer
addressing
no other nodes are active, can the retransmissions from A and B collide? For
our purposes, it suffices to work out the following example. Suppose A and B
begin transmission at t = 0 bit times. They both detect collisions at t = 245 bit
times. Suppose KA = 0 and KB = 1. At what time does B schedule its retransmission?
At what time does A begin transmission? (Note: The nodes must wait
for an idle channel after returning to Step 2—see protocol.) At what time does
A’s signal reach B? Does B refrain from transmitting at its scheduled time?
P20. In this problem, you will derive the efficiency of a CSMA/CD-like multiple access
protocol. In this protocol, time is slotted and all adapters are synchronized to the
slots. Unlike slotted ALOHA, however, the length of a slot (in seconds) is much
less than a frame time (the time to transmit a frame). Let S be the length of a slot.
Suppose all frames are of constant length L = kRS, where R is the transmission rate
of the channel and k is a large integer. Suppose there are N nodes, each with an
infinite number of frames to send. We also assume that dprop< S, so that all nodes
can detect a collision before the end of a slot time. The protocol is as follows:
• If, for a given slot, no node has possession of the channel, all nodes
contend for the channel; in particular, each node transmits in the slot with
probability p. If exactly one node transmits in the slot, that node takes
possession of the channel for the subsequent k – 1 slots and transmits its
entire frame.
• If some node has possession of the channel, all other nodes refrain from
transmitting until the node that possesses the channel has finished transmitting
its frame. Once this node has transmitted its frame, all nodes
contend for the channel.
Note that the channel alternates between two states: the productive state,
which lasts exactly k slots, and the nonproductive state, which lasts for a
random number of slots. Clearly, the channel efficiency is the ratio of k/(k + x),
where x is the expected number of consecutive unproductive slots.
a. For fixed N and p, determine the efficiency of this protocol.
b. For fixed N, determine the p that maximizes the efficiency.
c. Using the p (which is a function of N) found in (b), determine the efficiency
as N approaches infinity.
d. Show that this efficiency approaches 1 as the frame length becomes
large.
P21. Consider Figure 5.33 in problem P14. Provide MAC addresses and IP
addresses for the interfaces at Host A, both routers, and Host F. Suppose Host
A sends a datagram to Host F. Give the source and destination MAC addresses
in the frame encapsulating this IP datagram as the frame is transmitted (i) from
A to the left router, (ii) from the left router to the right router, (iii) from the
right router to F. Also give the source and destination IP addresses in the IP
datagram encapsulated within the frame at each of these points in time.
PROBLEMS 507
P22. Suppose now that the leftmost router in Figure 5.33 is replaced by a switch.
Hosts A, B, C, and D and the right router are all star-connected into this
switch. Give the source and destination MAC addresses in the frame encapsulating
this IP datagram as the frame is transmitted (i) from A to the switch,
(ii) from the switch to the right router, (iii) from the right router to F. Also
give the source and destination IP addresses in the IP datagram encapsulated
within the frame at each of these points in time.
P23. Consider Figure 5.15. Suppose that all links are 100 Mbps. What is the maximum
total aggregate throughput that can be achieved among the 9 hosts and
2 servers in this network? You can assume that any host or serrver can send
to any other host or server. Why?
P24. Suppose the three departmental switches in Figure 5.15 are replaced by
hubs. All links are 100 Mbps. Now answer the questions posed in problem
P23.
P25. Suppose that all the switches in Figure 5.15 are replaced by hubs. All links
are 100 Mbps. Now answer the questions posed in problem P23.
P26. Let’s consider the operation of a learning switch in the context of a network
in which 6 nodes labeled A through F are star connected into an Ethernet
switch. Suppose that (i) B sends a frame to E, (ii) E replies with a
frame to B, (iii) A sends a frame to B, (iv) B replies with a frame to A. The
switch table is initially empty. Show the state of the switch table before
and after each of these events. For each of these events, identify the link(s)
on which the transmitted frame will be forwarded, and briefly justify your
answers.
P27. In this problem, we explore the use of small packets for Voice-over-IP applications.
One of the drawbacks of a small packet size is that a large fraction of
link bandwidth is consumed by overhead bytes. To this end, suppose that the
packet consists of P bytes and 5 bytes of header.
a. Consider sending a digitally encoded voice source directly. Suppose the
source is encoded at a constant rate of 128 kbps. Assume each packet is
entirely filled before the source sends the packet into the network. The
time required to fill a packet is the packetization delay. In terms of L,
determine the packetization delay in milliseconds.
b. Packetization delays greater than 20 msec can cause a noticeable and
unpleasant echo. Determine the packetization delay for L = 1,500 bytes
(roughly corresponding to a maximum-sized Ethernet packet) and for
L = 50 (corresponding to an ATM packet).
c. Calculate the store-and-forward delay at a single switch for a link rate of
R = 622 Mbps for L = 1,500 bytes, and for L = 50 bytes.
d. Comment on the advantages of using a small packet size.
508 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
P28. Consider the single switch VLAN in Figure 5.25, and assume an external
router is connected to switch port 1. Assign IP addresses to the EE and CS
hosts and router interface. Trace the steps taken at both the network layer and
the link layer to transfer an IP datagram from an EE host to a CS host (Hint:
reread the discussion of Figure 5.19 in the text).
P29. Consider the MPLS network shown in Figure 5.29, and suppose that routers
R5 and R6 are now MPLS enabled. Suppose that we want to perform traffic
engineering so that packets from R6 destined for A are switched to A via
R6-R4-R3-R1, and packets from R5 destined for A are switched via
R5-R4-R2-R1. Show the MPLS tables in R5 and R6, as well as the modified
table in R4, that would make this possible.
P30. Consider again the same scenario as in the previous problem, but suppose
that packets from R6 destined for D are switched via R6-R4-R3, while packets
from R5 destined to D are switched via R4-R2-R1-R3. Show the MPLS
tables in all routers that would make this possible.
P31. In this problem, you will put together much of what you have learned about
Internet protocols. Suppose you walk into a room, connect to Ethernet, and
want to download a Web page. What are all the protocol steps that take place,
starting from powering on your PC to getting the Web page? Assume there is
nothing in our DNS or browser caches when you power on your PC. (Hint:
the steps include the use of Ethernet, DHCP, ARP, DNS, TCP, and HTTP protocols.)
Explicitly indicate in your steps how you obtain the IP and MAC
addresses of a gateway router.
P32. Consider the data center network with hierarchical topology in Figure 5.30.
Suppose now there are 80 pairs of flows, with ten flows between the first and
ninth rack, ten flows between the second and tenth rack, and so on. Further
suppose that all links in the network are 10 Gbps, except for the links
between hosts and TOR switches, which are 1 Gbps.
a. Each flow has the same data rate; determine the maximum rate of a flow.
b. For the same traffic pattern, determine the maximum rate of a flow for the
highly interconnected topology in Figure 5.31.
c. Now suppose there is a similar traffic pattern, but involving 20 hosts on
each hosts and 160 pairs of flows. Determine the maximum flow rates for
the two topologies.
P33. Consider the hierarchical network in Figure 5.30 and suppose that the data
center needs to support email and video distribution among other applications.
Suppose four racks of servers are reserved for email and four racks are
reserved for video. For each of the applications, all four racks must lie below
a single tier-2 switch since the tier-2 to tier-1 links do not have sufficient
bandwidth to support the intra-application traffic. For the email application,
PROBLEMS 509
510 CHAPTER 5 • THE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS
suppose that for 99.9 percent of the time only three racks are used, and that
the video application has identical usage patterns.
a. For what fraction of time does the email application need to use a fourth
rack? How about for the video application?
b. Assuming email usage and video usage are independent, for what fraction
of time do (equivalently, what is the probability that) both applications
need their fourth rack?
c. Suppose that it is acceptable for an application to have a shortage of
servers for 0.001 percent of time or less (causing rare periods of performance
degradation for users).
Discuss how the topology in Figure 5.31 can be used so that only seven
racks are collectively assigned to the two applications (assuming that the
topology can support all the traffic).
Wireshark Labs
At the companion Web site for this textbook, http://www.awl.com/kurose-ross,
you’ll find a Wireshark lab that examines the operation of the IEEE 802.3 protocol
and the Wireshark frame format. A second Wireshark lab examines packet traces
taken in a home network scenario.
Simon S. Lam
AN INTERVIEW WITH...
Simon S. Lam is Professor and Regents Chair in Computer Sciences
at the University of Texas at Austin. From 1971 to 1974, he was
with the ARPA Network Measurement Center at UCLA, where he
worked on satellite and radio packet switching. He led a research
group that invented secure sockets and prototyped, in 1993, the first
secure sockets layer named Secure Network Programming, which
won the 2004 ACM Software System Award. His research interests
are in design and analysis of network protocols and security services.
He received his BSEE from Washington State University and his
MS and PhD from UCLA. He was elected to the National Academy
of Engineering in 2007.
511
Why did you decide to specialize in networking?
When I arrived at UCLA as a new graduate student in Fall 1969, my intention was to study
control theory. Then I took the queuing theory classes of Leonard Kleinrock and was very
impressed by him. For a while, I was working on adaptive control of queuing systems as a possible
thesis topic. In early 1972, Larry Roberts initiated the ARPAnet Satellite System project
(later called Packet Satellite). Professor Kleinrock asked me to join the project. The first thing
we did was to introduce a simple, yet realistic, backoff algorithm to the slotted ALOHA protocol.
Shortly thereafter, I found many interesting research problems, such as ALOHA’s instability
problem and need for adaptive backoff, which would form the core of my thesis.
You were active in the early days of the Internet in the 1970s, beginning with your
student days at UCLA. What was it like then? Did people have any inkling of what
the Internet would become?
The atmosphere was really no different from other system-building projects I have seen in
industry and academia. The initially stated goal of the ARPAnet was fairly modest, that is,
to provide access to expensive computers from remote locations so that many more scientists
could use them. However, with the startup of the Packet Satellite project in 1972 and
the Packet Radio project in 1973, ARPA’s goal had expanded substantially. By 1973, ARPA
was building three different packet networks at the same time, and it became necessary for
Vint Cerf and Bob Kahn to develop an interconnection strategy.
Back then, all of these progressive developments in networking were viewed (I believe)
as logical rather than magical. No one could have envisioned the scale of the Internet and
power of personal computers today. It was a decade before appearance of the first PCs. To put
things in perspective, most students submitted their computer programs as decks of punched
cards for batch processing. Only some students had direct access to computers, which were
typically housed in a restricted area. Modems were slow and still a rarity. As a graduate student,
I had only a phone on my desk, and I used pencil and paper to do most of my work.
512
Where do you see the field of networking and the Internet heading in the future?
In the past, the simplicity of the Internet’s IP protocol was its greatest strength in vanquishing
competition and becoming the de facto standard for internetworking. Unlike competitors,
such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer
networking technology, because it offers only a best-effort datagram service. Thus, any
packet network can connect to the Internet.
Today, IP’s greatest strength is actually a shortcoming. IP is like a straitjacket that confines
the Internet’s development to specific directions. In recent years, many researchers
have redirected their efforts to the application layer only. There is also a great deal of
research on wireless ad hoc networks, sensor networks, and satellite networks. These networks
can be viewed either as stand-alone systems or link-layer systems, which can flourish
because they are outside of the IP straitjacket.
Many people are excited about the possibility of P2P systems as a platform for novel
Internet applications. However, P2P systems are highly inefficient in their use of Internet
resources. A concern of mine is whether the transmission and switching capacity of the
Internet core will continue to increase faster than the traffic demand on the Internet as it
grows to interconnect all kinds of devices and support future P2P-enabled applications.
Without substantial overprovisioning of capacity, ensuring network stability in the presence
of malicious attacks and congestion will continue to be a significant challenge.
The Internet’s phenomenal growth also requires the allocation of new IP addresses at a
rapid rate to network operators and enterprises worldwide. At the current rate, the pool of unallocated
IPv4 addresses would be depleted in a few years. When that happens, large contiguous
blocks of address space can only be allocated from the IPv6 address space. Since adoption of
IPv6 is off to a slow start, due to lack of incentives for early adopters, IPv4 and IPv6 will most
likely co-exist on the Internet for many years to come. Successful migration from an IPv4-dominant
Internet to an IPv6-dominant Internet will require a substantial global effort.
What is the most challenging part of your job?
The most challenging part of my job as a professor is teaching and motivating every student
in my class, and every doctoral student under my supervision, rather than just the high
achievers. The very bright and motivated may require a little guidance but not much else. I
often learn more from these students than they learn from me. Educating and motivating the
underachievers present a major challenge.
What impacts do you foresee technology having on learning in the future?
Eventually, almost all human knowledge will be accessible through the Internet, which will
be the most powerful tool for learning. This vast knowledge base will have the potential of
leveling the playing field for students all over the world. For example, motivated students in
any country will be able to access the best-class Web sites, multimedia lectures, and teaching
materials. Already, it was said that the IEEE and ACM digital libraries have accelerated
the development of computer science researchers in China. In time, the Internet will transcend
all geographic barriers to learning.
CHAPTER 6
Wireless and
Mobile
Networks
513
In the telephony world, the past 15 years have arguably been the golden years of cellular
telephony. The number of worldwide mobile cellular subscribers increased
from 34 million in 1993 to nearly 5.5 billion subscribers by 2011, with the number
of cellular subscribers now surpassing the number of wired telephone lines. The
many advantages of cell phones are evident to all—anywhere, anytime, untethered
access to the global telephone network via a highly portable lightweight device.
With the advent of laptops, palmtops, smartphones, and their promise of anywhere,
anytime, untethered access to the global Internet, is a similar explosion in the use of
wireless Internet devices just around the corner?
Regardless of the future growth of wireless Internet devices, it’s already clear
that wireless networks and the mobility-related services they enable are here to stay.
From a networking standpoint, the challenges posed by these networks, particularly
at the link layer and the network layer, are so different from traditional wired computer
networks that an individual chapter devoted to the study of wireless and
mobile networks (i.e., this chapter) is appropriate.
We’ll begin this chapter with a discussion of mobile users, wireless links, and
networks, and their relationship to the larger (typically wired) networks to which
they connect. We’ll draw a distinction between the challenges posed by the wireless
nature of the communication links in such networks, and by the mobility that these
wireless links enable. Making this important distinction—between wireless and
514 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
mobility—will allow us to better isolate, identify, and master the key concepts in
each area. Note that there are indeed many networked environments in which the
network nodes are wireless but not mobile (e.g., wireless home or office networks
with stationary workstations and large displays), and that there are limited forms of
mobility that do not require wireless links (e.g., a worker who uses a wired laptop at
home, shuts down the laptop, drives to work, and attaches the laptop to the company’s
wired network). Of course, many of the most exciting networked environments
are those in which users are both wireless and mobile—for example, a
scenario in which a mobile user (say in the back seat of car) maintains a Voice-over-IP
call and multiple ongoing TCP connections while racing down the autobahn at
160 kilometers per hour. It is here, at the intersection of wireless and mobility, that
we’ll find the most interesting technical challenges!
We’ll begin by illustrating the setting in which we’ll consider wireless communication
and mobility—a network in which wireless (and possibly mobile) users are
connected into the larger network infrastructure by a wireless link at the network’s
edge. We’ll then consider the characteristics of this wireless link in Section 6.2. We
include a brief introduction to code division multiple access (CDMA), a sharedmedium
access protocol that is often used in wireless networks, in Section 6.2. In
Section 6.3, we’ll examine the link-level aspects of the IEEE 802.11 (WiFi) wireless
LAN standard in some depth; we’ll also say a few words about Bluetooth and
other wireless personal area networks. In Section 6.4, we’ll provide an overview of
cellular Internet access, including 3G and emerging 4G cellular technologies that
provide both voice and high-speed Internet access. In Section 6.5, we’ll turn our
attention to mobility, focusing on the problems of locating a mobile user, routing to
the mobile user, and “handing off” the mobile user who dynamically moves from
one point of attachment to the network to another. We’ll examine how these mobility
services are implemented in the mobile IP standard and in GSM, in Sections 6.6
and 6.7, respectively. Finally, we’ll consider the impact of wireless links and mobility
on transport-layer protocols and networked applications in Section 6.8.
6.1 Introduction
Figure 6.1 shows the setting in which we’ll consider the topics of wireless data communication
and mobility. We’ll begin by keeping our discussion general enough to
cover a wide range of networks, including both wireless LANs such as IEEE 802.11
and cellular networks such as a 3G network; we’ll drill down into a more detailed
discussion of specific wireless architectures in later sections. We can identify the
following elements in a wireless network:
• Wireless hosts. As in the case of wired networks, hosts are the end-system devices
that run applications. A wireless host might be a laptop, palmtop, smartphone, or
desktop computer. The hosts themselves may or may not be mobile.
6.1 • INTRODUCTION 515
• Wireless links. A host connects to a base station (defined below) or to another
wireless host through a wireless communication link. Different wireless link
technologies have different transmission rates and can transmit over different
distances. Figure 6.2 shows two key characteristics (coverage area and link rate)
of the more popular wireless network standards. (The figure is only meant to provide
a rough idea of these characteristics. For example, some of these types of
networks are only now being deployed, and some link rates can increase or
decrease beyond the values shown depending on distance, channel conditions,
and the number of users in the wireless network.) We’ll cover these standards
PUBLIC WIFI ACCESS: COMING SOON TO A LAMP POST NEAR YOU?
WiFi hotspots—public locations where users can find 802.11 wireless access—are
becoming increasingly common in hotels, airports, and cafés around the world. Most
college campuses offer ubiquitous wireless access, and it’s hard to find a hotel that
doesn’t offer wireless Internet access.
Over the past decade a number of cities have designed, deployed, and operated
municipal WiFi networks. The vision of providing ubiquitous WiFi access to the community
as a public service (much like streetlights)—helping to bridge the digital divide by
providing Internet access to all citizens and to promote economic development—is
compelling. Many cities around the world, including Philadelphia, Toronto, Hong
Kong, Minneapolis, London, and Auckland, have plans to provide ubiquitous wireless
within the city, or have already done so to varying degrees. The goal in Philadelphia
was to “turn Philadelphia into the nation’s largest WiFi hotspot and help to improve
education, bridge the digital divide, enhance neighborhood development, and reduce
the costs of government.” The ambitious program—an agreement between the city,
Wireless Philadelphia (a nonprofit entity), and the Internet Service Provider Earthlink—
built an operational network of 802.11b hotspots on streetlamp pole arms and traffic
control devices that covered 80 percent of the city. But financial and operational concerns
caused the network to be sold to a group of private investors in 2008, who later
sold the network back to the city in 2010. Other cities, such as Minneapolis, Toronto,
Hong Kong, and Auckland, have had success with smaller-scale efforts.
The fact that 802.11 networks operate in the unlicensed spectrum (and hence can
be deployed without purchasing expensive spectrum use rights) would seem to make
them financially attractive. However, 802.11 access points (see Section 6.3) have
much shorter ranges than 3G cellular base stations (see Section 6.4), requiring a larger
number of deployed endpoints to cover the same geographic region. Cellular data
networks providing Internet access, on the other hand, operate in the licensed spectrum.
Cellular providers pay billions of dollars for spectrum access rights for their networks,
making cellular data networks a business rather than municipal undertaking.
CASE HISTORY
later in the first half of this chapter; we’ll also consider other wireless link characteristics
(such as their bit error rates and the causes of bit errors) in Section 6.2.
In Figure 6.1, wireless links connect wireless hosts located at the edge of the network
into the larger network infrastructure. We hasten to add that wireless links
are also sometimes used within a network to connect routers, switches, and other
network equipment. However, our focus in this chapter will be on the use of
wireless communication at the network edge, as it is here that many of the most
exciting technical challenges, and most of the growth, are occurring.
• Base station. The base station is a key part of the wireless network infrastructure.
Unlike the wireless host and wireless link, a base station has no obvious counterpart
in a wired network. Abase station is responsible for sending and receiving data (e.g.,
packets) to and from a wireless host that is associated with that base station. A base
station will often be responsible for coordinating the transmission of multiple wireless
hosts with which it is associated. When we say a wireless host is “associated”
with a base station, we mean that (1) the host is within the wireless communication
516 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Network
infrastructure
Key:
Wireless access point
Coverage area
Wireless host
Wireless host in motion
Figure 6.1  Elements of a wireless network
distance of the base station, and (2) the host uses that base station to relay data
between it (the host) and the larger network. Cell towers in cellular networks and
access points in 802.11 wireless LANs are examples of base stations.
In Figure 6.1, the base station is connected to the larger network (e.g., the Internet,
corporate or home network, or telephone network), thus functioning as a
link-layer relay between the wireless host and the rest of the world with which
the host communicates.
Hosts associated with a base station are often referred to as operating in
infrastructure mode, since all traditional network services (e.g., address assignment
and routing) are provided by the network to which a host is connected via
the base station. In ad hoc networks, wireless hosts have no such infrastructure
with which to connect. In the absence of such infrastructure, the hosts themselves
must provide for services such as routing, address assignment, DNS-like name
translation, and more.
When a mobile host moves beyond the range of one base station and into the
range of another, it will change its point of attachment into the larger network
(i.e., change the base station with which it is associated)—a process referred to as
handoff. Such mobility raises many challenging questions. If a host can move,
how does one find the mobile host’s current location in the network so that data
can be forwarded to that mobile host? How is addressing performed, given that a
host can be in one of many possible locations? If the host moves during a TCP
6.1 • INTRODUCTION 517
802.11a,g
802.11n
802.11b
802.15.1
3G: UMTS/WCDMA, CDMA2000
2G: IS-95, CDMA, GSM
Indoor Outdoor Mid range
outdoor
Long range
outdoor
10–30m 50–200m 200m–4Km 5Km–20Km
54 Mbps
4 Mbps
5–11 Mbps
200 Mbps
1 Mbps
384 Kbps
Enhanced 3G: HSPA
4G: LTE
802.11a,g point-to-point
Figure 6.2  Link characteristics of selected wireless network standards
connection or phone call, how is data routed so that the connection continues
uninterrupted? These and many (many!) other questions make wireless and
mobile networking an area of exciting networking research.
• Network infrastructure. This is the larger network with which a wireless host
may wish to communicate.
Having discussed the “pieces” of a wireless network, we note that these
pieces can be combined in many different ways to form different types of wireless
networks. You may find a taxonomy of these types of wireless networks useful as
you read on in this chapter, or read/learn more about wireless networks beyond
this book. At the highest level we can classify wireless networks according to two
criteria: (i) whether a packet in the wireless network crosses exactly one wireless
hop or multiple wireless hops, and (ii) whether there is infrastructure such as a base
station in the network:
• Single-hop, infrastructure-based. These networks have a base station that is
connected to a larger wired network (e.g., the Internet). Furthermore, all communication
is between this base station and a wireless host over a single wireless
hop. The 802.11 networks you use in the classroom, café, or library; and
the 3G cellular data networks that we will learn about shortly all fall in this
category.
• Single-hop, infrastructure-less. In these networks, there is no base station that is
connected to a wireless network. However, as we will see, one of the nodes in
this single-hop network may coordinate the transmissions of the other nodes.
Bluetooth networks (which we will study in Section 6.3.6) and 802.11 networks
in ad hoc mode are single-hop, infrastructure-less networks.
• Multi-hop, infrastructure-based. In these networks, a base station is present that
is wired to the larger network. However, some wireless nodes may have to relay
their communication through other wireless nodes in order to communicate via
the base station. Some wireless sensor networks and so-called wireless mesh
networks fall in this category.
• Multi-hop, infrastructure-less. There is no base station in these networks, and
nodes may have to relay messages among several other nodes in order to reach a
destination. Nodes may also be mobile, with connectivity changing among
nodes—a class of networks known as mobile ad hoc networks (MANETs). If
the mobile nodes are vehicles, the network is a vehicular ad hoc network
(VANET). As you might imagine, the development of protocols for such networks
is challenging and is the subject of much ongoing research.
In this chapter, we’ll mostly confine ourselves to single-hop networks, and then
mostly to infrastructure-based networks.
518 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Let’s now dig deeper into the technical challenges that arise in wireless and
mobile networks. We’ll begin by first considering the individual wireless link, deferring
our discussion of mobility until later in this chapter.
6.2 Wireless Links and Network Characteristics
Let’s begin by considering a simple wired network, say a home network, with a
wired Ethernet switch (see Section 5.4) interconnecting the hosts. If we replace
the wired Ethernet with a wireless 802.11 network, a wireless network interface
would replace the host’s wired Ethernet interface, and an access point would
replace the Ethernet switch, but virtually no changes would be needed at the network
layer or above. This suggests that we focus our attention on the link layer
when looking for important differences between wired and wireless networks.
Indeed, we can find a number of important differences between a wired link and a
wireless link:
• Decreasing signal strength. Electromagnetic radiation attenuates as it passes
through matter (e.g., a radio signal passing through a wall). Even in free space,
the signal will disperse, resulting in decreased signal strength (sometimes
referred to as path loss) as the distance between sender and receiver increases.
• Interference from other sources. Radio sources transmitting in the same frequency
band will interfere with each other. For example, 2.4 GHz wireless
phones and 802.11b wireless LANs transmit in the same frequency band. Thus,
the 802.11b wireless LAN user talking on a 2.4 GHz wireless phone can expect
that neither the network nor the phone will perform particularly well. In addition
to interference from transmitting sources, electromagnetic noise within the environment
(e.g., a nearby motor, a microwave) can result in interference.
• Multipath propagation. Multipath propagation occurs when portions of the
electromagnetic wave reflect off objects and the ground, taking paths of different
lengths between a sender and receiver. This results in the blurring of the received
signal at the receiver. Moving objects between the sender and receiver can cause
multipath propagation to change over time.
For a detailed discussion of wireless channel characteristics, models, and measurements,
see [Anderson 1995].
The discussion above suggests that bit errors will be more common in wireless
links than in wired links. For this reason, it is perhaps not surprising that wireless
link protocols (such as the 802.11 protocol we’ll examine in the following section)
employ not only powerful CRC error detection codes, but also link-level reliabledata-
transfer protocols that retransmit corrupted frames.
6.2 • WIRELESS LINKS AND NETWORK CHARACTERISTICS 519
520 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Figure 6.3  Bit error rate, transmission rate, and SNR
10–7
10–6
10–5
10–4
10–3
10–2
10–1
0 10 20 30 40
SNR (dB)
BER
QAM16
(4 Mbps)
QAM256
(8 Mbps)
BPSK
(1Mpbs)
Having considered the impairments that can occur on a wireless channel, let’s
next turn our attention to the host receiving the wireless signal. This host receives an
electromagnetic signal that is a combination of a degraded form of the original signal
transmitted by the sender (degraded due to the attenuation and multipath propagation
effects that we discussed above, among others) and background noise in the environment.
The signal-to-noise ratio (SNR) is a relative measure of the strength of the
received signal (i.e., the information being transmitted) and this noise. The SNR is
typically measured in units of decibels (dB), a unit of measure that some think is used
by electrical engineers primarily to confuse computer scientists. The SNR, measured
in dB, is twenty times the ratio of the base-10 logarithm of the amplitude of the
received signal to the amplitude of the noise. For our purposes here, we need only
know that a larger SNR makes it easier for the receiver to extract the transmitted signal
from the background noise.
Figure 6.3 (adapted from [Holland 2001]) shows the bit error rate (BER)—
roughly speaking, the probability that a transmitted bit is received in error at the
receiver—versus the SNR for three different modulation techniques for encoding
information for transmission on an idealized wireless channel. The theory of modulation
and coding, as well as signal extraction and BER, is well beyond the scope of
this text (see [Schwartz 1980] for a discussion of these topics). Nonetheless, Figure
6.3 illustrates several physical-layer characteristics that are important in understanding
higher-layer wireless communication protocols:
• For a given modulation scheme, the higher the SNR, the lower the BER. Since
a sender can increase the SNR by increasing its transmission power, a sender
can decrease the probability that a frame is received in error by increasing its
transmission power. Note, however, that there is arguably little practical gain
in increasing the power beyond a certain threshold, say to decrease the BER
from 10-12 to 10-13. There are also disadvantages associated with increasing the
transmission power: More energy must be expended by the sender (an important
concern for battery-powered mobile users), and the sender’s transmissions
are more likely to interfere with the transmissions of another sender (see
Figure 6.4(b)).
• For a given SNR, a modulation technique with a higher bit transmission rate
(whether in error or not) will have a higher BER. For example, in Figure 6.3,
with an SNR of 10 dB, BPSK modulation with a transmission rate of 1 Mbps has
a BER of less than 10-7, while with QAM16 modulation with a transmission rate
of 4 Mbps, the BER is 10-1, far too high to be practically useful. However, with
an SNR of 20 dB, QAM16 modulation has a transmission rate of 4 Mbps and a
BER of 10-7, while BPSK modulation has a transmission rate of only 1 Mbps and
a BER that is so low as to be (literally) “off the charts.” If one can tolerate a BER
of 10-7, the higher transmission rate offered by QAM16 would make it the preferred
modulation technique in this situation. These considerations give rise to
the final characteristic, described next.
• Dynamic selection of the physical-layer modulation technique can be used to
adapt the modulation technique to channel conditions. The SNR (and hence the
BER) may change as a result of mobility or due to changes in the environment.
Adaptive modulation and coding are used in cellular data systems and in the
802.11 WiFi and 3G cellular data networks that we’ll study in Sections 6.3 and
6.4. This allows, for example, the selection of a modulation technique that
provides the highest transmission rate possible subject to a constraint on the
BER, for given channel characteristics.
A higher and time-varying bit error rate is not the only difference between a
wired and wireless link. Recall that in the case of wired broadcast links, all nodes
receive the transmissions from all other nodes. In the case of wireless links, the situation
is not as simple, as shown in Figure 6.4. Suppose that Station A is transmitting
to Station B. Suppose also that Station C is transmitting to Station B. With the socalled
hidden terminal problem, physical obstructions in the environment (for
example, a mountain or a building) may prevent A and C from hearing each other’s
transmissions, even though A’s and C’s transmissions are indeed interfering at the
destination, B. This is shown in Figure 6.4(a). A second scenario that results in
undetectable collisions at the receiver results from the fading of a signal’s strength
as it propagates through the wireless medium. Figure 6.4(b) illustrates the case
where Aand C are placed such that their signals are not strong enough to detect each
other’s transmissions, yet their signals are strong enough to interfere with each other
at station B. As we’ll see in Section 6.3, the hidden terminal problem and fading
6.2 • WIRELESS LINKS AND NETWORK CHARACTERISTICS 521
make multiple access in a wireless network considerably more complex than in a
wired network.
6.2.1 CDMA
Recall from Chapter 5 that when hosts communicate over a shared medium, a protocol
is needed so that the signals sent by multiple senders do not interfere at the
receivers. In Chapter 5 we described three classes of medium access protocols:
channel partitioning, random access, and taking turns. Code division multiple
access (CDMA) belongs to the family of channel partitioning protocols. It is prevalent
in wireless LAN and cellular technologies. Because CDMA is so important in
the wireless world, we’ll take a quick look at CDMA now, before getting into specific
wireless access technologies in the subsequent sections.
In a CDMA protocol, each bit being sent is encoded by multiplying the bit by
a signal (the code) that changes at a much faster rate (known as the chipping rate)
than the original sequence of data bits. Figure 6.5 shows a simple, idealized
CDMA encoding/decoding scenario. Suppose that the rate at which original data
bits reach the CDMA encoder defines the unit of time; that is, each original data
bit to be transmitted requires a one-bit slot time. Let di be the value of the data bit
for the ith bit slot. For mathematical convenience, we represent a data bit with a 0
value as –1. Each bit slot is further subdivided into M mini-slots; in Figure 6.5, M
= 8, although in practice M is much larger. The CDMA code used by the sender
consists of a sequence of M values, cm, m = 1, . . . , M, each taking a +1 or –1
522 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
A
A
C
B C
Location
a. b.
0
Signal strength
B
Figure 6.4  Hidden terminal problem caused by obstacle (a) and
fading (b)
value. In the example in Figure 6.5, the M-bit CDMA code being used by the
sender is (1, 1, 1, –1, 1, –1, –1, –1).
To illustrate how CDMA works, let us focus on the ith data bit, di. For the mth
mini-slot of the bit-transmission time of di, the output of the CDMA encoder, Zi,m, is
the value of di multiplied by the mth bit in the assigned CDMA code, cm:
Zi, m = di  cm (6.1)
6.2 • WIRELESS LINKS AND NETWORK CHARACTERISTICS 523
Figure 6.5  A simple CDMA example: sender encoding, receiver
decoding
1 1 1 1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
1 1 1
Time slot 1
received input
Time slot 0
received input
Code
1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
Data bits 1 1 1
Code 1 1 1 1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
d1 = -1
d0 = 1
Time slot 1
Sender Channel output Zi,m
Receiver
Zi,m = di • cm
Zi,m • cm
d
M i
m=1
M
=
S
Time slot 1
channel output
Time slot 0
channel output
Time slot 0
d1 = -1
d0 = 1
In a simple world, with no interfering senders, the receiver would receive the
encoded bits, Zi,m, and recover the original data bit, di, by computing:
(6.2)
The reader might want to work through the details of the example in Figure 6.5 to
see that the original data bits are indeed correctly recovered at the receiver using
Equation 6.2.
The world is far from ideal, however, and as noted above, CDMA must work in
the presence of interfering senders that are encoding and transmitting their data using
a different assigned code. But how can a CDMA receiver recover a sender’s original
data bits when those data bits are being tangled with bits being transmitted by other
senders? CDMA works under the assumption that the interfering transmitted bit
signals are additive. This means, for example, that if three senders send a 1 value, and
a fourth sender sends a –1 value during the same mini-slot, then the received
signal at all receivers during that mini-slot is a 2 (since 1  1  1  1 = 2). In the
presence of multiple senders, sender s computes its encoded transmissions, Zs
i,m, in
exactly the same manner as in Equation 6.1. The value received at a receiver during
the mth mini-slot of the ith bit slot, however, is now the sum of the transmitted bits
from all N senders during that mini-slot:
Amazingly, if the senders’ codes are chosen carefully, each receiver can recover the
data sent by a given sender out of the aggregate signal simply by using the sender’s
code in exactly the same manner as in Equation 6.2:
(6.3)
as shown in Figure 6.6, for a two-sender CDMA example. The M-bit CDMA code being
used by the upper sender is (1, 1, 1, –1, 1, –1, –1, –1), while the CDMA code being used
by the lower sender is (1, –1, 1, 1, 1, –1, 1, 1). Figure 6.6 illustrates a receiver recovering
the original data bits from the upper sender. Note that the receiver is able to extract
the data from sender 1 in spite of the interfering transmission from sender 2.
Recall our cocktail analogy from Chapter 5. A CDMA protocol is similar to having
partygoers speaking in multiple languages; in such circumstances humans are
actually quite good at locking into the conversation in the language they understand,
while filtering out the remaining conversations. We see here that CDMA is a partitioning
protocol in that it partitions the codespace (as opposed to time or frequency) and
assigns each node a dedicated piece of the codespace.
di =
1
M
a
M
m=1
Zi, m
*  cm
Z* i, m = a
N
s=1
Zs
i, m
di =
1
M
a
M
m=1
Zi, m  cm
524 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.2 • WIRELESS LINKS AND NETWORK CHARACTERISTICS 525
Receiver 1
1 1 1 1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
Time slot 1
received input
Time slot 0
received input
Data bits
Data bits
1 1 1 1
-1 -1 -1 -1
1 1 1 1
-1 -1 -1 -1
Code
Senders
1 1 1
-1
1 1 1
-1
1
-1 -1
1 1 1 1 1
Code
Code
+
-2
2 2 2 2 2
-2
2
-2
2 2 2 2 2
-2
2
Channel, Zi,m *
Zi,m = di • cm
Zi,m • cm
d
M i
m=1
M
=
S
d1 = -1
d0 = 1
d1 = 1 2
1
* 1
2 2 2
Zi,m = di • cm 1 1 1
d0 = 1 2
1
1
d1 = -1
d0 = 1
1
1
Figure 6.6  A two-sender CDMA example
Our discussion here of CDMA is necessarily brief; in practice a number of
difficult issues must be addressed. First, in order for the CDMA receivers to be
able to extract a particular sender’s signal, the CDMA codes must be carefully
chosen. Second, our discussion has assumed that the received signal strengths
from various senders are the same; in reality this can be difficult to achieve. There
is a considerable body of literature addressing these and other issues related to
CDMA; see [Pickholtz 1982; Viterbi 1995] for details.
6.3 WiFi: 802.11 Wireless LANs
Pervasive in the workplace, the home, educational institutions, cafés, airports, and
street corners, wireless LANs are now one of the most important access network
technologies in the Internet today. Although many technologies and standards for
wireless LANs were developed in the 1990s, one particular class of standards has
clearly emerged as the winner: the IEEE 802.11 wireless LAN, also known as
WiFi. In this section, we’ll take a close look at 802.11 wireless LANs, examining
its frame structure, its medium access protocol, and its internetworking of 802.11
LANs with wired Ethernet LANs.
There are several 802.11 standards for wireless LAN technology, including
802.11b, 802.11a, and 802.11g. Table 6.1 summarizes the main characteristics of
these standards. 802.11g is by far the most popular technology. A number of dualmode
(802.11a/g) and tri-mode (802.11a/b/g) devices are also available.
The three 802.11 standards share many characteristics. They all use the same
medium access protocol, CSMA/CA, which we’ll discuss shortly. All three use the
same frame structure for their link-layer frames as well. All three standards have the
ability to reduce their transmission rate in order to reach out over greater distances.
And all three standards allow for both “infrastructure mode” and “ad hoc mode,” as
we’ll also shortly discuss. However, as shown in Table 6.1, the three standards have
some major differences at the physical layer.
The 802.11b wireless LAN has a data rate of 11 Mbps and operates in the
unlicensed frequency band of 2.4–2.485 GHz, competing for frequency spectrum
with 2.4 GHz phones and microwave ovens. 802.1la wireless LANs can run at
526 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Table 6.1  Summary of IEEE 802.11 standards
Standard Frequency Range (United States) Data Rate
802.11b 2.4–2.485 GHz up to 11 Mbps
802.11a 5.1–5.8 GHz up to 54 Mbps
802.11g 2.4–2.485 GHz up to 54 Mbps
significantly higher bit rates, but do so at higher frequencies. By operating at a
higher frequency, 802.11a LANs have a shorter transmission distance for a given
power level and suffer more from multipath propagation. 802.11g LANs, operating
in the same lower-frequency band as 802.11b and being backwards compatible
with 802.11b (so one can upgrade 802.11b clients incrementally) yet with the
higher-speed transmission rates of 802.11a, allows users to have their cake and
eat it too.
A relatively new WiFi standard, 802.11n [IEEE 802.11n 2012], uses multipleinput
multiple-output (MIMO) antennas; i.e., two or more antennas on the
sending side and two or more antennas on the receiving side that are transmitting/
receiving different signals [Diggavi 2004]. Depending on the modulation
scheme used, transmission rates of several hundred megabits per second are possible
with 802.11n.
6.3.1 The 802.11 Architecture
Figure 6.7 illustrates the principal components of the 802.11 wireless LAN architecture.
The fundamental building block of the 802.11 architecture is the basic
service set (BSS). A BSS contains one or more wireless stations and a central
6.3 • WIFI: 802.11 WIRELESS LANS 527
Internet
Switch or router
AP
BSS 1
BSS 2
AP
Figure 6.7  IEEE 802.11 LAN architecture
base station, known as an access point (AP) in 802.11 parlance. Figure 6.7
shows the AP in each of two BSSs connecting to an interconnection device (such
as a switch or router), which in turn leads to the Internet. In a typical home network,
there is one AP and one router (typically integrated together as one unit)
that connects the BSS to the Internet.
As with Ethernet devices, each 802.11 wireless station has a 6-byte MAC
address that is stored in the firmware of the station’s adapter (that is, 802.11 network
interface card). Each AP also has a MAC address for its wireless interface. As with
Ethernet, these MAC addresses are administered by IEEE and are (in theory) globally
unique.
As noted in Section 6.1, wireless LANs that deploy APs are often referred to
as infrastructure wireless LANs, with the “infrastructure” being the APs along
with the wired Ethernet infrastructure that interconnects the APs and a router.
Figure 6.8 shows that IEEE 802.11 stations can also group themselves together
to form an ad hoc network—a network with no central control and with no connections
to the “outside world.” Here, the network is formed “on the fly,” by
mobile devices that have found themselves in proximity to each other, that have
a need to communicate, and that find no preexisting network infrastructure in
their location. An ad hoc network might be formed when people with laptops get
together (for example, in a conference room, a train, or a car) and want to
exchange data in the absence of a centralized AP. There has been tremendous
interest in ad hoc networking, as communicating portable devices continue to
proliferate. In this section, though, we’ll focus our attention on infrastructure
wireless LANs.
528 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
BSS
Figure 6.8  An IEEE 802.11 ad hoc network
Channels and Association
In 802.11, each wireless station needs to associate with an AP before it can send or
receive network-layer data. Although all of the 802.11 standards use association,
we’ll discuss this topic specifically in the context of IEEE 802.11b/g.
When a network administrator installs an AP, the administrator assigns a oneor
two-word Service Set Identifier (SSID) to the access point. (When you “view
available networks” in Microsoft Windows XP, for example, a list is displayed
showing the SSID of each AP in range.) The administrator must also assign a channel
number to the AP. To understand channel numbers, recall that 802.11 operates in
the frequency range of 2.4 GHz to 2.485 GHz. Within this 85 MHz band, 802.11
defines 11 partially overlapping channels. Any two channels are non-overlapping if
and only if they are separated by four or more channels. In particular, the set of
channels 1, 6, and 11 is the only set of three non-overlapping channels. This means
that an administrator could create a wireless LAN with an aggregate maximum
transmission rate of 33 Mbps by installing three 802.11b APs at the same physical
location, assigning channels 1, 6, and 11 to the APs, and interconnecting each of the
APs with a switch.
Now that we have a basic understanding of 802.11 channels, let’s describe an
interesting (and not completely uncommon) situation—that of a WiFi jungle. A
WiFi jungle is any physical location where a wireless station receives a sufficiently
strong signal from two or more APs. For example, in many cafés in New York City,
a wireless station can pick up a signal from numerous nearby APs. One of the APs
might be managed by the café, while the other APs might be in residential apartments
near the café. Each of these APs would likely be located in a different IP subnet
and would have been independently assigned a channel.
Now suppose you enter such a WiFi jungle with your portable computer,
seeking wireless Internet access and a blueberry muffin. Suppose there are five
APs in the WiFi jungle. To gain Internet access, your wireless station needs to join
exactly one of the subnets and hence needs to associate with exactly one of the
APs. Associating means the wireless station creates a virtual wire between itself
and the AP. Specifically, only the associated AP will send data frames (that is,
frames containing data, such as a datagram) to your wireless station, and your
wireless station will send data frames into the Internet only through the associated
AP. But how does your wireless station associate with a particular AP? And more
fundamentally, how does your wireless station know which APs, if any, are out
there in the jungle?
The 802.11 standard requires that an AP periodically send beacon frames, each
of which includes the AP’s SSID and MAC address. Your wireless station, knowing
that APs are sending out beacon frames, scans the 11 channels, seeking beacon
frames from any APs that may be out there (some of which may be transmitting on
the same channel—it’s a jungle out there!). Having learned about available APs
6.3 • WIFI: 802.11 WIRELESS LANS 529
from the beacon frames, you (or your wireless host) select one of the APs for
association.
The 802.11 standard does not specify an algorithm for selecting which of the
available APs to associate with; that algorithm is left up to the designers of the 802.11
firmware and software in your wireless host. Typically, the host chooses the AP
whose beacon frame is received with the highest signal strength. While a high signal
strength is good (see, e.g., Figure 6.3), signal strength is not the only AP characteristic
that will determine the performance a host receives. In particular, it’s possible that
the selected AP may have a strong signal, but may be overloaded with other affiliated
hosts (that will need to share the wireless bandwidth at that AP), while an unloaded
AP is not selected due to a slightly weaker signal. A number of alternative ways of
choosing APs have thus recently been proposed [Vasudevan 2005; Nicholson 2006;
Sundaresan 2006]. For an interesting and down-to-earth discussion of how signal
strength is measured, see [Bardwell 2004].
The process of scanning channels and listening for beacon frames is known as
passive scanning (see Figure 6.9a). A wireless host can also perform active scanning,
by broadcasting a probe frame that will be received by all APs within the wireless
host’s range, as shown in Figure 6.9b. APs respond to the probe request frame
with a probe response frame. The wireless host can then choose the AP with which
to associate from among the responding APs.
After selecting the AP with which to associate, the wireless host sends an association
request frame to the AP, and the AP responds with an association response
frame. Note that this second request/response handshake is needed with active scanning,
since an AP responding to the initial probe request frame doesn’t know which
of the (possibly many) responding APs the host will choose to associate with, in
much the same way that a DHCP client can choose from among multiple DHCP
servers (see Figure 4.21). Once associated with an AP, the host will want to join the
subnet (in the IP addressing sense of Section 4.4.2) to which the AP belongs. Thus,
the host will typically send a DHCP discovery message (see Figure 4.21) into the
subnet via the AP in order to obtain an IP address on the subnet. Once the address is
obtained, the rest of the world then views that host simply as another host with an IP
address in that subnet.
In order to create an association with a particular AP, the wireless station may
be required to authenticate itself to the AP. 802.11 wireless LANs provide a number
of alternatives for authentication and access. One approach, used by many companies,
is to permit access to a wireless network based on a station’s MAC address. A
second approach, used by many Internet cafés, employs usernames and passwords.
In both cases, the AP typically communicates with an authentication server, relaying
information between the wireless end-point station and the authentication server
using a protocol such as RADIUS [RFC 2865] or DIAMETER [RFC 3588]. Separating
the authentication server from the AP allows one authentication server to
serve many APs, centralizing the (often sensitive) decisions of authentication and
access within the single server, and keeping AP costs and complexity low. We’ll see
530 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
in Section 8.8 that the new IEEE 802.11i protocol defining security aspects of the
802.11 protocol family takes precisely this approach.
6.3.2 The 802.11 MAC Protocol
Once a wireless station is associated with an AP, it can start sending and receiving
data frames to and from the access point. But because multiple stations may want to
transmit data frames at the same time over the same channel, a multiple access protocol
is needed to coordinate the transmissions. Here, a station is either a wireless station
or an AP. As discussed in Chapter 5 and Section 6.2.1, broadly speaking there
are three classes of multiple access protocols: channel partitioning (including
CDMA), random access, and taking turns. Inspired by the huge success of Ethernet
and its random access protocol, the designers of 802.11 chose a random access protocol
for 802.11 wireless LANs. This random access protocol is referred to as CSMA
with collision avoidance, or more succinctly as CSMA/CA. As with Ethernet’s
CSMA/CD, the “CSMA” in CSMA/CA stands for “carrier sense multiple access,”
meaning that each station senses the channel before transmitting, and refrains from
transmitting when the channel is sensed busy. Although both Ethernet and 802.11 use
carrier-sensing random access, the two MAC protocols have important differences.
6.3 • WIFI: 802.11 WIRELESS LANS 531
1
1
2 3
H1
AP 1 AP 2
BBS 1
a. Passive scanning
1. Beacon frames sent from APs
2. Association Request frame sent:
H1 to selected AP
3. Association Response frame sent:
Selected AP to H1
a. Active scanning
1. Probe Request frame broadcast from H1
2. Probes Response frame sent from APs
3. Association Request frame sent:
H1 to selected AP
4. Association Response frame sent:
Selected AP to H1
BBS 2
2
2
3 4
H1
AP 1 AP 2
BBS 1 BBS 2
1
Figure 6.9  Active and passive scanning for access points
First, instead of using collision detection, 802.11 uses collision-avoidance techniques.
Second, because of the relatively high bit error rates of wireless channels,
802.11 (unlike Ethernet) uses a link-layer acknowledgment/retransmission (ARQ)
scheme. We’ll describe 802.11’s collision-avoidance and link-layer acknowledgment
schemes below.
Recall from Sections 5.3.2 and 5.4.2 that with Ethernet’s collision-detection
algorithm, an Ethernet station listens to the channel as it transmits. If, while transmitting,
it detects that another station is also transmitting, it aborts its transmission
and tries to transmit again after waiting a small, random amount of time. Unlike the
802.3 Ethernet protocol, the 802.11 MAC protocol does not implement collision
detection. There are two important reasons for this:
• The ability to detect collisions requires the ability to send (the station’s own signal)
and receive (to determine whether another station is also transmitting) at the
same time. Because the strength of the received signal is typically very small
compared to the strength of the transmitted signal at the 802.11 adapter, it is
costly to build hardware that can detect a collision.
• More importantly, even if the adapter could transmit and listen at the same time
(and presumably abort transmission when it senses a busy channel), the adapter
would still not be able to detect all collisions, due to the hidden terminal problem
and fading, as discussed in Section 6.2.
Because 802.11wireless LANs do not use collision detection, once a station
begins to transmit a frame, it transmits the frame in its entirety; that is, once a station
gets started, there is no turning back. As one might expect, transmitting entire
frames (particularly long frames) when collisions are prevalent can significantly
degrade a multiple access protocol’s performance. In order to reduce the likelihood
of collisions, 802.11 employs several collision-avoidance techniques, which we’ll
shortly discuss.
Before considering collision avoidance, however, we’ll first need to examine
802.11’s link-layer acknowledgment scheme. Recall from Section 6.2 that when a
station in a wireless LAN sends a frame, the frame may not reach the destination
station intact for a variety of reasons. To deal with this non-negligible chance of failure,
the 802.11 MAC protocol uses link-layer acknowledgments. As shown in
Figure 6.10, when the destination station receives a frame that passes the CRC, it
waits a short period of time known as the Short Inter-frame Spacing (SIFS) and
then sends back an acknowledgment frame. If the transmitting station does not
receive an acknowledgment within a given amount of time, it assumes that an error
has occurred and retransmits the frame, using the CSMA/CA protocol to access the
channel. If an acknowledgment is not received after some fixed number of retransmissions,
the transmitting station gives up and discards the frame.
532 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Having discussed how 802.11 uses link-layer acknowledgments, we’re now in
a position to describe the 802.11 CSMA/CA protocol. Suppose that a station (wireless
station or an AP) has a frame to transmit.
1. If initially the station senses the channel idle, it transmits its frame after a short
period of time known as the Distributed Inter-frame Space (DIFS); see
Figure 6.10.
2. Otherwise, the station chooses a random backoff value using binary exponential
backoff (as we encountered in Section 5.3.2) and counts down this value
when the channel is sensed idle. While the channel is sensed busy, the counter
value remains frozen.
3. When the counter reaches zero (note that this can only occur while the channel
is sensed idle), the station transmits the entire frame and then waits for an
acknowledgment.
6.3 • WIFI: 802.11 WIRELESS LANS 533
Destination
DIFS
SIFS
data
ack
Source
Figure 6.10  802.11 uses link-layer acknowledgments
4. If an acknowledgment is received, the transmitting station knows that its frame
has been correctly received at the destination station. If the station has another
frame to send, it begins the CSMA/CA protocol at step 2. If the acknowledgment
isn’t received, the transmitting station reenters the backoff phase in step
2, with the random value chosen from a larger interval.
Recall that under Ethernet’s CSMA/CD, multiple access protocol (Section
5.3.2), a station begins transmitting as soon as the channel is sensed idle. With
CSMA/CA, however, the station refrains from transmitting while counting down,
even when it senses the channel to be idle.Why do CSMA/CD and CDMA/CA take
such different approaches here?
To answer this question, let’s consider a scenario in which two stations each
have a data frame to transmit, but neither station transmits immediately because
each senses that a third station is already transmitting. With Ethernet’s CSMA/CD,
the two stations would each transmit as soon as they detect that the third station has
finished transmitting. This would cause a collision, which isn’t a serious issue in
CSMA/CD, since both stations would abort their transmissions and thus avoid the
useless transmissions of the remainders of their frames. In 802.11, however, the situation
is quite different. Because 802.11 does not detect a collision and abort transmission,
a frame suffering a collision will be transmitted in its entirety. The goal in
802.11 is thus to avoid collisions whenever possible. In 802.11, if the two stations
sense the channel busy, they both immediately enter random backoff, hopefully
choosing different backoff values. If these values are indeed different, once the
channel becomes idle, one of the two stations will begin transmitting before the
other, and (if the two stations are not hidden from each other) the “losing station”
will hear the “winning station’s” signal, freeze its counter, and refrain from transmitting
until the winning station has completed its transmission. In this manner, a
costly collision is avoided. Of course, collisions can still occur with 802.11 in this
scenario: The two stations could be hidden from each other, or the two stations
could choose random backoff values that are close enough that the transmission
from the station starting first have yet to reach the second station. Recall that we
encountered this problem earlier in our discussion of random access algorithms in
the context of Figure 5.12.
Dealing with Hidden Terminals: RTS and CTS
The 802.11 MAC protocol also includes a nifty (but optional) reservation scheme
that helps avoid collisions even in the presence of hidden terminals. Let’s investigate
this scheme in the context of Figure 6.11, which shows two wireless stations
and one access point. Both of the wireless stations are within range of the AP
(whose coverage is shown as a shaded circle) and both have associated with the AP.
However, due to fading, the signal ranges of wireless stations are limited to the
534 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
interiors of the shaded circles shown in Figure 6.11. Thus, each of the wireless stations
is hidden from the other, although neither is hidden from the AP.
Let’s now consider why hidden terminals can be problematic. Suppose Station
H1 is transmitting a frame and halfway through H1’s transmission, Station H2 wants
to send a frame to the AP. H2, not hearing the transmission from H1, will first wait a
DIFS interval and then transmit the frame, resulting in a collision. The channel will
therefore be wasted during the entire period of H1’s transmission as well as during
H2’s transmission.
In order to avoid this problem, the IEEE 802.11 protocol allows a station to use
a short Request to Send (RTS) control frame and a short Clear to Send (CTS) control
frame to reserve access to the channel. When a sender wants to send a DATA
frame, it can first send an RTS frame to the AP, indicating the total time required to
transmit the DATA frame and the acknowledgment (ACK) frame. When the AP
receives the RTS frame, it responds by broadcasting a CTS frame. This CTS frame
serves two purposes: It gives the sender explicit permission to send and also
instructs the other stations not to send for the reserved duration.
Thus, in Figure 6.12, before transmitting a DATA frame, H1 first broadcasts an
RTS frame, which is heard by all stations in its circle, including the AP. The AP then
responds with a CTS frame, which is heard by all stations within its range, including
H1 and H2. Station H2, having heard the CTS, refrains from transmitting for the
time specified in the CTS frame. The RTS, CTS, DATA, and ACK frames are shown
in Figure 6.12.
6.3 • WIFI: 802.11 WIRELESS LANS 535
H1 AP H2
Figure 6.11  Hidden terminal example: H1 is hidden from H2,
and vice versa
The use of the RTS and CTS frames can improve performance in two important
ways:
• The hidden station problem is mitigated, since a long DATA frame is transmitted
only after the channel has been reserved.
536 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Destination All other nodes
Defer access
Source
DIFS
ACK
SIFS
SIFS
SIFS
DATA
CTS
CTS
ACK
RTS
Figure 6.12  Collision avoidance using the RTS and CTS frames
• Because the RTS and CTS frames are short, a collision involving an RTS or CTS
frame will last only for the duration of the short RTS or CTS frame. Once the
RTS and CTS frames are correctly transmitted, the following DATA and ACK
frames should be transmitted without collisions.
You are encouraged to check out the 802.11 applet in the textbook’s companion Web
site. This interactive applet illustrates the CSMA/CA protocol, including the
RTS/CTS exchange sequence.
Although the RTS/CTS exchange can help reduce collisions, it also introduces
delay and consumes channel resources. For this reason, the RTS/CTS exchange is
only used (if at all) to reserve the channel for the transmission of a long DATA
frame. In practice, each wireless station can set an RTS threshold such that the
RTS/CTS sequence is used only when the frame is longer than the threshold. For
many wireless stations, the default RTS threshold value is larger than the maximum
frame length, so the RTS/CTS sequence is skipped for all DATA frames sent.
Using 802.11 as a Point-to-Point Link
Our discussion so far has focused on the use of 802.11 in a multiple access setting.
We should mention that if two nodes each have a directional antenna, they can point
their directional antennas at each other and run the 802.11 protocol over what is
essentially a point-to-point link. Given the low cost of commodity 802.11 hardware,
the use of directional antennas and an increased transmission power allow 802.11 to
be used as an inexpensive means of providing wireless point-to-point connections
over tens of kilometers distance. [Raman 2007] describes such a multi-hop wireless
network operating in the rural Ganges plains in India that contains point-to-point
802.11 links.
6.3.3 The IEEE 802.11 Frame
Although the 802.11 frame shares many similarities with an Ethernet frame, it also
contains a number of fields that are specific to its use for wireless links. The 802.11
frame is shown in Figure 6.13. The numbers above each of the fields in the frame
represent the lengths of the fields in bytes; the numbers above each of the subfields
in the frame control field represent the lengths of the subfields in bits. Let’s now
examine the fields in the frame as well as some of the more important subfields in
the frame’s control field.
Payload and CRC Fields
At the heart of the frame is the payload, which typically consists of an IP datagram
or an ARP packet. Although the field is permitted to be as long as 2,312 bytes, it is
6.3 • WIFI: 802.11 WIRELESS LANS 537
typically fewer than 1,500 bytes, holding an IP datagram or an ARP packet. As with
an Ethernet frame, an 802.11 frame includes a 32-bit cyclic redundancy check
(CRC) so that the receiver can detect bit errors in the received frame. As we’ve seen,
bit errors are much more common in wireless LANs than in wired LANs, so the
CRC is even more useful here.
Address Fields
Perhaps the most striking difference in the 802.11 frame is that it has four address
fields, each of which can hold a 6-byte MAC address. But why four address fields?
Doesn’t a source MAC field and destination MAC field suffice, as they do for
Ethernet? It turns out that three address fields are needed for internetworking purposes—
specifically, for moving the network-layer datagram from a wireless station
through an AP to a router interface. The fourth address field is used when APs forward
frames to each other in ad hoc mode. Since we are only considering infrastructure
networks here, let’s focus our attention on the first three address fields. The
802.11 standard defines these fields as follows:
• Address 2 is the MAC address of the station that transmits the frame. Thus, if a
wireless station transmits the frame, that station’s MAC address is inserted in the
address 2 field. Similarly, if an AP transmits the frame, the AP’s MAC address is
inserted in the address 2 field.
• Address 1 is the MAC address of the wireless station that is to receive the
frame. Thus if a mobile wireless station transmits the frame, address 1 contains
the MAC address of the destination AP. Similarly, if an AP transmits the frame,
address 1 contains the MAC address of the destination wireless station.
538 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Frame
control
2
2 2 4 1 1 1 1 1 1 1 1
2 6 6 6 2 6 0-2312 4
Frame (numbers indicate field length in bytes):
Address
Duration 1 Payload CRC
Protocol
version
To
AP
From
AP
More
frag
Power
mgt
More
data
Address
2
Address
3
Address
4
Seq
control
Type Subtype Retry WEP Rsvd
Frame control field expanded (numbers indicate field length in bits):
Figure 6.13  The 802.11 frame
• To understand address 3, recall that the BSS (consisting of the AP and wireless stations)
is part of a subnet, and that this subnet connects to other subnets via some
router interface. Address 3 contains the MAC address of this router interface.
To gain further insight into the purpose of address 3, let’s walk through an internetworking
example in the context of Figure 6.14. In this figure, there are two APs,
each of which is responsible for a number of wireless stations. Each of the APs has
a direct connection to a router, which in turn connects to the global Internet. We
should keep in mind that an AP is a link-layer device, and thus neither “speaks” IP
nor understands IP addresses. Consider now moving a datagram from the router
interface R1 to the wireless Station H1. The router is not aware that there is an AP
between it and H1; from the router’s perspective, H1 is just a host in one of the subnets
to which it (the router) is connected.
• The router, which knows the IP address of H1 (from the destination address of
the datagram), uses ARP to determine the MAC address of H1, just as in an
ordinary Ethernet LAN. After obtaining H1’s MAC address, router interface R1
encapsulates the datagram within an Ethernet frame. The source address field of
this frame contains R1’s MAC address, and the destination address field contains
H1’s MAC address.
6.3 • WIFI: 802.11 WIRELESS LANS 539
Internet
Router
AP
H1
R1
BSS 1
BSS 2
AP
Figure 6.14  The use of address fields in 802.11 frames: Sending
frames between H1 and R1
• When the Ethernet frame arrives at the AP, the AP converts the 802.3 Ethernet
frame to an 802.11 frame before transmitting the frame into the wireless channel.
The AP fills in address 1 and address 2 with H1’s MAC address and its
own MAC address, respectively, as described above. For address 3, the AP
inserts the MAC address of R1. In this manner, H1 can determine (from
address 3) the MAC address of the router interface that sent the datagram into
the subnet.
Now consider what happens when the wireless station H1 responds by moving a
datagram from H1 to R1.
• H1 creates an 802.11 frame, filling the fields for address 1 and address 2 with the
AP’s MAC address and H1’s MAC address, respectively, as described above. For
address 3, H1 inserts R1’s MAC address.
• When the AP receives the 802.11 frame, it converts the frame to an Ethernet
frame. The source address field for this frame is H1’s MAC address, and the destination
address field is R1’s MAC address. Thus, address 3 allows the AP to
determine the appropriate destination MAC address when constructing the Ethernet
frame.
In summary, address 3 plays a crucial role for internetworking the BSS with a wired
LAN.
Sequence Number, Duration, and Frame Control Fields
Recall that in 802.11, whenever a station correctly receives a frame from another
station, it sends back an acknowledgment. Because acknowledgments can get lost,
the sending station may send multiple copies of a given frame. As we saw in our discussion
of the rdt2.1 protocol (Section 3.4.1), the use of sequence numbers allows
the receiver to distinguish between a newly transmitted frame and the retransmission
of a previous frame. The sequence number field in the 802.11 frame thus serves
exactly the same purpose here at the link layer as it did in the transport layer in
Chapter 3.
Recall that the 802.11 protocol allows a transmitting station to reserve the channel
for a period of time that includes the time to transmit its data frame and the time
to transmit an acknowledgment. This duration value is included in the frame’s duration
field (both for data frames and for the RTS and CTS frames).
As shown in Figure 6.13, the frame control field includes many subfields. We’ll
say just a few words about some of the more important subfields; for a more complete
discussion, you are encouraged to consult the 802.11 specification [Held 2001;
Crow 1997; IEEE 802.11 1999]. The type and subtype fields are used to distinguish
the association, RTS, CTS, ACK, and data frames. The to and from fields are used
to define the meanings of the different address fields. (These meanings change
540 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
depending on whether ad hoc or infrastructure modes are used and, in the case of
infrastructure mode, whether a wireless station or an AP is sending the frame.)
Finally the WEP field indicates whether encryption is being used or not. (WEP is
discussed in Chapter 8.)
6.3.4 Mobility in the Same IP Subnet
In order to increase the physical range of a wireless LAN, companies and universities
will often deploy multiple BSSs within the same IP subnet. This naturally raises the
issue of mobility among the BSSs—how do wireless stations seamlessly move from
one BSS to another while maintaining ongoing TCP sessions? As we’ll see in this subsection,
mobility can be handled in a relatively straightforward manner when the BSSs
are part of the subnet. When stations move between subnets, more sophisticated mobility
management protocols will be needed, such as those we’ll study in Sections 6.5
and 6.6.
Let’s now look at a specific example of mobility between BSSs in the same
subnet. Figure 6.15 shows two interconnected BSSs with a host, H1, moving
from BSS1 to BSS2. Because in this example the interconnection device that
connects the two BSSs is not a router, all of the stations in the two BSSs, including
the APs, belong to the same IP subnet. Thus, when H1 moves from BSS1 to
BSS2, it may keep its IP address and all of its ongoing TCP connections. If the
interconnection device were a router, then H1 would have to obtain a new IP
address in the subnet in which it was moving. This address change would disrupt
(and eventually terminate) any on-going TCP connections at H1. In Section 6.6,
we’ll see how a network-layer mobility protocol, such as mobile IP, can be used
to avoid this problem.
6.3 • WIFI: 802.11 WIRELESS LANS 541
BSS 1 BSS 2
H1
Switch
AP 1 AP 2
Figure 6.15  Mobility in the same subnet
But what specifically happens when H1 moves from BSS1 to BSS2? As H1
wanders away from AP1, H1 detects a weakening signal from AP1 and starts to scan
for a stronger signal. H1 receives beacon frames from AP2 (which in many corporate
and university settings will have the same SSID as AP1). H1 then disassociates
with AP1 and associates with AP2, while keeping its IP address and maintaining its
ongoing TCP sessions.
This addresses the handoff problem from the host and AP viewpoint. But what
about the switch in Figure 6.15? How does it know that the host has moved from
one AP to another? As you may recall from Chapter 5, switches are “self-learning”
and automatically build their forwarding tables. This self-learning feature nicely
handles occasional moves (for example, when an employee gets transferred from
one department to another); however, switches were not designed to support highly
mobile users who want to maintain TCP connections while moving between BSSs.
To appreciate the problem here, recall that before the move, the switch has an entry
in its forwarding table that pairs H1’s MAC address with the outgoing switch interface
through which H1 can be reached. If H1 is initially in BSS1, then a datagram
destined to H1 will be directed to H1 via AP1. Once H1 associates with BSS2, however,
its frames should be directed to AP2. One solution (a bit of a hack, really) is
for AP2 to send a broadcast Ethernet frame with H1’s source address to the switch
just after the new association. When the switch receives the frame, it updates its forwarding
table, allowing H1 to be reached via AP2. The 802.11f standards group is
developing an inter-AP protocol to handle these and related issues.
6.3.5 Advanced Features in 802.11
We’ll wrap up our coverage of 802.11 with a short discussion of two advanced capabilities
found in 802.11 networks. As we’ll see, these capabilities are not completely
specified in the 802.11 standard, but rather are made possible by mechanisms specified
in the standard. This allows different vendors to implement these capabilities
using their own (proprietary) approaches, presumably giving them an edge over the
competition.
802.11 Rate Adaptation
We saw earlier in Figure 6.3 that different modulation techniques (with the different
transmission rates that they provide) are appropriate for different SNR scenarios.
Consider for example a mobile 802.11 user who is initially 20 meters away
from the base station, with a high signal-to-noise ratio. Given the high SNR, the
user can communicate with the base station using a physical-layer modulation technique
that provides high transmission rates while maintaining a low BER. This is
one happy user! Suppose now that the user becomes mobile, walking away from
542 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
the base station, with the SNR falling as the distance from the base station
increases. In this case, if the modulation technique used in the 802.11 protocol
operating between the base station and the user does not change, the BER will
become unacceptably high as the SNR decreases, and eventually no transmitted
frames will be received correctly.
For this reason, some 802.11 implementations have a rate adaptation capability
that adaptively selects the underlying physical-layer modulation technique to use
based on current or recent channel characteristics. If a node sends two frames in a
row without receiving an acknowledgment (an implicit indication of bit errors on
the channel), the transmission rate falls back to the next lower rate. If 10 frames in a
row are acknowledged, or if a timer that tracks the time since the last fallback
expires, the transmission rate increases to the next higher rate. This rate adaptation
mechanism shares the same “probing” philosophy as TCP’s congestion-control
mechanism—when conditions are good (reflected by ACK receipts), the transmission
rate is increased until something “bad” happens (the lack of ACK receipts);
when something “bad” happens, the transmission rate is reduced. 802.11 rate adaptation
and TCP congestion control are thus similar to the young child who is constantly
pushing his/her parents for more and more (say candy for a young child, later
curfew hours for the teenager) until the parents finally say “Enough!” and the child
backs off (only to try again later after conditions have hopefully improved!). Anumber
of other schemes have also been proposed to improve on this basic automatic
rate-adjustment scheme [Kamerman 1997; Holland 2001; Lacage 2004].
Power Management
Power is a precious resource in mobile devices, and thus the 802.11 standard provides
power-management capabilities that allow 802.11 nodes to minimize the
amount of time that their sense, transmit, and receive functions and other circuitry
need to be “on.” 802.11 power management operates as follows. A node is able to
explicitly alternate between sleep and wake states (not unlike a sleepy student in a
classroom!). A node indicates to the access point that it will be going to sleep by setting
the power-management bit in the header of an 802.11 frame to 1. A timer in the
node is then set to wake up the node just before the AP is scheduled to send its beacon
frame (recall that an AP typically sends a beacon frame every 100 msec). Since
the AP knows from the set power-transmission bit that the node is going to sleep, it
(the AP) knows that it should not send any frames to that node, and will buffer any
frames destined for the sleeping host for later transmission.
A node will wake up just before the AP sends a beacon frame, and quickly enter
the fully active state (unlike the sleepy student, this wakeup requires only 250
microseconds [Kamerman 1997]!). The beacon frames sent out by the AP contain a
list of nodes whose frames have been buffered at the AP. If there are no buffered
frames for the node, it can go back to sleep. Otherwise, the node can explicitly
6.3 • WIFI: 802.11 WIRELESS LANS 543
request that the buffered frames be sent by sending a polling message to the AP.
With an inter-beacon time of 100 msec, a wakeup time of 250 microseconds, and a
similarly small time to receive a beacon frame and check to ensure that there are no
buffered frames, a node that has no frames to send or receive can be asleep 99% of
the time, resulting in a significant energy savings.
6.3.6 Personal Area Networks: Bluetooth and Zigbee
As illustrated in Figure 6.2, the IEEE 802.11 WiFi standard is aimed at communication
among devices separated by up to 100 meters (except when 802.11 is used in
a point-to-point configuration with a directional antenna). Two other IEEE 802
protocols—Bluetooth and Zigbee (defined in the IEEE 802.15.1 and IEEE 802.15.4
standards [IEEE 802.15 2012]) and WiMAX (defined in the IEEE 802.16 standard
[IEEE 802.16d 2004; IEEE 802.16e 2005])—are standards for communicating
over shorter and longer distances, respectively. We will touch on WiMAX briefly
when we discuss cellular data networks in Section 6.4, and so here, we will focus
on networks for shorter distances.
Bluetooth
An IEEE 802.15.1 network operates over a short range, at low power, and at low
cost. It is essentially a low-power, short-range, low-rate “cable replacement” technology
for interconnecting notebooks, peripheral devices, cellular phones, and smartphones,
whereas 802.11 is a higher-power, medium-range, higher-rate “access”
technology. For this reason, 802.15.1 networks are sometimes referred to as wireless
personal area networks (WPANs). The link and physical layers of 802.15.1 are based
on the earlier Bluetooth specification for personal area networks [Held 2001, Bisdikian
2001]. 802.15.1 networks operate in the 2.4 GHz unlicensed radio band in a
TDM manner, with time slots of 625 microseconds. During each time slot, a sender
transmits on one of 79 channels, with the channel changing in a known but pseudorandom
manner from slot to slot. This form of channel hopping, known as
frequency-hopping spread spectrum (FHSS), spreads transmissions in time over
the frequency spectrum. 802.15.1 can provide data rates up to 4 Mbps.
802.15.1 networks are ad hoc networks: No network infrastructure (e.g., an
access point) is needed to interconnect 802.15.1 devices. Thus, 802.15.1 devices
must organize themselves. 802.15.1 devices are first organized into a piconet of up
to eight active devices, as shown in Figure 6.16. One of these devices is designated
as the master, with the remaining devices acting as slaves. The master node truly
rules the piconet—its clock determines time in the piconet, it can transmit in each
odd-numbered slot, and a slave can transmit only after the master has communicated
with it in the previous slot and even then the slave can only transmit to the master.
In addition to the slave devices, there can also be up to 255 parked devices in the
544 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
network. These devices cannot communicate until their status has been changed
from parked to active by the master node.
For more information about 802.15.1 WPANs, the interested reader should consult
the Bluetooth references [Held 2001, Bisdikian 2001] or the official IEEE
802.15 Web site [IEEE 802.15 2012].
Zigbee
A second personal area network standardized by the IEEE is the 802.14.5 standard
[IEEE 802.15 2012] known as Zigbee. While Bluetooth networks provide a “cable
replacement” data rate of over a Megabit per second, Zigbee is targeted at lowerpowered,
lower-data-rate, lower-duty-cycle applications than Bluetooth. While we
may tend to think that “bigger and faster is better,” not all network applications need
high bandwidth and the consequent higher costs (both economic and power costs).
For example, home temperature and light sensors, security devices, and wallmounted
switches are all very simple, low-power, low-duty-cycle, low-cost devices.
Zigbee is thus well-suited for these devices. Zigbee defines channel rates of 20, 40,
100, and 250 Kbps, depending on the channel frequency.
Nodes in a Zigbee network come in two flavors. So-called “reduced-function
devices” operate as slave devices under the control of a single “full-function
device,” much as Bluetooth slave devices. A full-function device can operate as a
master device as in Bluetooth by controlling multiple slave devices, and multiple
full-function devices can additionally be configured into a mesh network in
which full-function devices route frames amongst themselves. Zigbee shares
6.3 • WIFI: 802.11 WIRELESS LANS 545
Radius of
coverage
Master device
Slave device
Parked device
Key:
M
M
S
S
S S
P
P
P
P
P
Figure 6.16  A Bluetooth piconet
many protocol mechanisms that we’ve already encountered in other link-layer
protocols: beacon frames and link-layer acknowledgments (similar to 802.11),
carrier-sense random access protocols with binary exponential backoff (similar
to 802.11 and Ethernet), and fixed, guaranteed allocation of time slots (similar to
DOCSIS).
Zigbee networks can be configured in many different ways. Let’s consider the
simple case of a single full-function device controlling multiple reduced-function
devices in a time-slotted manner using beacon frames. Figure 6.17 shows the case
where the Zigbee network divides time into recurring super frames, each of which
begins with a beacon frame. Each beacon frame divides the super frame into an
active period (during which devices may transmit) and an inactive period (during
which all devices, including the controller, can sleep and thus conserve power). The
active period consists of 16 time slots, some of which are used by devices in a
CSMA/CA random access manner, and some of which are allocated by the controller
to specific devices, thus providing guaranteed channel access for those
devices. More details about Zigbee networks can be found at [Baronti 2007, IEEE
802.15.4 2012].
6.4 Cellular Internet Access
In the previous section we examined how an Internet host can access the Internet
when inside a WiFi hotspot—that is, when it is within the vicinity of an 802.11
access point. But most WiFi hotspots have a small coverage area of between 10 and
100 meters in diameter. What do we do then when we have a desperate need for
wireless Internet access and we cannot access a WiFi hotspot?
Given that cellular telephony is now ubiquitous in many areas throughout the
world, a natural strategy is to extend cellular networks so that they support not
only voice telephony but wireless Internet access as well. Ideally, this Internet
access would be at a reasonably high speed and would provide for seamless
546 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Figure 6.17  Zigbee 802.14.4 super-frame structure
Beacon
Contention slots Guaranteed slots Inactive period
Super frame
mobility, allowing users to maintain their TCP sessions while traveling, for
example, on a bus or a train. With sufficiently high upstream and downstream bit
rates, the user could even maintain video-conferencing sessions while roaming
about. This scenario is not that far-fetched. As of 2012, many cellular telephony
providers in the U.S. offer their subscribers a cellular Internet access service for
under $50 per month with typical downstream and upstream bit rates in the hundreds
of kilobits per second. Data rates of several megabits per second are
becoming available as broadband data services such as those we will cover here
become more widely deployed.
In this section, we provide a brief overview of current and emerging cellular
Internet access technologies. Our focus here will be on both the wireless first hop as
well as the network that connects the wireless first hop into the larger telephone network
and/or the Internet; in Section 6.7 we’ll consider how calls are routed to a user
moving between base stations. Our brief discussion will necessarily provide only a
simplified and high-level description of cellular technologies. Modern cellular communications,
of course, has great breadth and depth, with many universities offering
several courses on the topic. Readers seeking a deeper understanding are encouraged
to see [Goodman 1997; Kaaranen 2001; Lin 2001; Korhonen 2003; Schiller 2003;
Scourias 2012; Turner 2012; Akyildiz 2010], as well as the particularly excellent and
exhaustive reference [Mouly 1992].
6.4.1 An Overview of Cellular Network Architecture
In our description of cellular network architecture in this section, we’ll adopt the terminology
of the Global System for Mobile Communications (GSM) standards. (For
history buffs, the GSM acronym was originally derived from Groupe Spécial Mobile,
until the more anglicized name was adopted, preserving the original acronym letters.)
In the 1980s, Europeans recognized the need for a pan-European digital cellular
telephony system that would replace the numerous incompatible analog cellular
telephony systems, leading to the GSM standard [Mouly 1992]. Europeans deployed
GSM technology with great success in the early 1990s, and since then GSM has
grown to be the 800-pound gorilla of the cellular telephone world, with more than
80% of all cellular subscribers worldwide using GSM.
When people talk about cellular technology, they often classify the technology
as belonging to one of several “generations.” The earliest generations were designed
primarily for voice traffic. First generation (1G) systems were analog FDMA systems
designed exclusively for voice-only communication. These 1G systems are
almost extinct now, having been replaced by digital 2G systems. The original 2G
systems were also designed for voice, but later extended (2.5G) to support data (i.e.,
Internet) as well as voice service. The 3G systems that currently are being deployed
also support voice and data, but with an ever increasing emphasis on data capabilities
and higher-speed radio access links.
6.4 • CELLULAR INTERNET ACCESS 547
Cellular Network Architecture, 2G: Voice Connections to the
Telephone Network
The term cellular refers to the fact that the region covered by a cellular network
is partitioned into a number of geographic coverage areas, known as cells, shown
as hexagons on the left side of Figure 6.18. As with the 802.11WiFi standard we
studied in Section 6.3.1, GSM has its own particular nomenclature. Each cell
contains a base transceiver station (BTS) that transmits signals to and receives
signals from the mobile stations in its cell. The coverage area of a cell depends
548 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
3G CELLULAR MOBILE VERSUS WIRELESS LANS
Many cellular mobile phone operators are deploying 3G cellular mobile systems with
indoor data rates of 2 Mbps and outdoor data rates of 384 kbps and higher. These 3G
systems are being deployed in licensed radio-frequency bands, with some operators
paying considerable sums to governments for spectrum-use licenses. 3G systems allow
users to access the Internet from remote outdoor locations while on the move, in a
manner similar to today’s cellular phone access. For example, 3G technology permits a
user to access road map information while driving a car, or movie theater information
while sunbathing on a beach. Nevertheless, one may question the extent to which 3G
systems will be used, given their cost and the fact that users may often have simultaneous
access to both wireless LANs and 3G:
• The emerging wireless LAN infrastructure may become nearly ubiquitous. IEEE 802.11
wireless LANs, operating at 54 Mbps, are enjoying widespread deployment. Almost
all portable computers and smartphones are factory-equipped with 802.11 LAN
capabilities. Furthermore, emerging Internet appliances—such as wireless cameras
and picture frames—will also have small and low-powered wireless LAN capabilities.
• Wireless LAN base stations can also handle mobile phone appliances. Many
phones are already capable of connecting to the cellular phone network or to an
IP network either natively or using a Skype-like Voice-over-IP service, thus bypassing
the operator’s cellular voice and 3G data services.
Of course, many other experts believe that 3G not only will be a major success,
but will also dramatically revolutionize the way we work and live. Most likely,
both WiFi and 3G will both become prevalent wireless technologies, with roaming
wireless devices automatically selecting the access technology that provides the
best service at their current physical location.
CASE HISTORY
6.4 • CELLULAR INTERNET ACCESS 549
on many factors, including the transmitting power of the BTS, the transmitting
power of the user devices, obstructing buildings in the cell, and the height of base
station antennas. Although Figure 6.18 shows each cell containing one base
transceiver station residing in the middle of the cell, many systems today place
the BTS at corners where three cells intersect, so that a single BTS with directional
antennas can service three cells.
The GSM standard for 2G cellular systems uses combined FDM/TDM (radio)
for the air interface. Recall from Chapter 1 that, with pure FDM, the channel is partitioned
into a number of frequency bands with each band devoted to a call. Also
recall from Chapter 1 that, with pure TDM, time is partitioned into frames with each
frame further partitioned into slots and each call being assigned the use of a particular
slot in the revolving frame. In combined FDM/TDM systems, the channel is partitioned
into a number of frequency sub-bands; within each sub-band, time is
partitioned into frames and slots. Thus, for a combined FDM/TDM system, if
the channel is partitioned into F sub-bands and time is partitioned into T slots, then
BSC
BSC
MSC
Key: Base transceiver station
(BTS)
Base station controller
(BSC)
Mobile switching center
(MSC)
Mobile subscribers
Gateway
MSC
Base Station System
(BSS)
Base Station System (BSS)
Public telephone
network
G
Figure 6.18  Components of the GSM 2G cellular network architecture
the channel will be able to support F.T simultaneous calls. Recall that we saw in
Section 5.3.4 that cable access networks also use a combined FDM/TDM approach.
GSM systems consist of 200-kHz frequency bands with each band supporting eight
TDM calls. GSM encodes speech at 13 kbps and 12.2 kbps.
A GSM network’s base station controller (BSC) will typically service several
tens of base transceiver stations. The role of the BSC is to allocate BTS radio channels
to mobile subscribers, perform paging (finding the cell in which a mobile user
is resident), and perform handoff of mobile users—a topic we’ll cover shortly in
Section 6.7.2. The base station controller and its controlled base transceiver stations
collectively constitute a GSM base station system (BSS).
As we’ll see in Section 6.7, the mobile switching center (MSC) plays the central
role in user authorization and accounting (e.g., determining whether a mobile
device is allowed to connect to the cellular network), call establishment and teardown,
and handoff. A single MSC will typically contain up to five BSCs, resulting
in approximately 200K subscribers per MSC. A cellular provider’s network will
have a number of MSCs, with special MSCs known as gateway MSCs connecting
the provider’s cellular network to the larger public telephone network.
6.4.2 3G Cellular Data Networks: Extending the Internet to
Cellular Subscribers
Our discussion in Section 6.4.1 focused on connecting cellular voice users to the
public telephone network. But, of course, when we’re on the go, we’d also like to
read email, access the Web, get location-dependent services (e.g., maps and restaurant
recommendations) and perhaps even watch streaming video. To do this, our
smartphone will need to run a full TCP/IP protocol stack (including the physical
link, network, transport, and application layers) and connect into the Internet via
the cellular data network. The topic of cellular data networks is a rather bewildering
collection of competing and ever-evolving standards as one generation (and
half-generation) succeeds the former and introduces new technologies and services
with new acronyms. To make matters worse, there’s no single official body that
sets requirements for 2.5G, 3G, 3.5G, or 4G technologies, making it hard to sort
out the differences among competing standards. In our discussion below, we’ll
focus on the UMTS (Universal Mobile Telecommunications Service) 3G standards
developed by the 3rd Generation Partnership project (3GPP) [3GPP 2012], a
widely deployed 3G technology.
Let’s take a top-down look at 3G cellular data network architecture shown in
Figure 6.19.
3G Core Network
The 3G core cellular data network connects radio access networks to the public Internet.
The core network interoperates with components of the existing cellular voice
550 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.4 • CELLULAR INTERNET ACCESS 551
Gateway
MSC
G
Key:
Serving GPRS
Support Node
(SGSN)
Gateway GPRS
Support Node
(GGSN)
Radio Network
Controller (RNC) SGSN GGSN
G
G
MSC
Public telephone
network
Radio Interface
(WCDMA, HSPA)
Radio Access Network
Universal Terrestrial Radio
Access Network (UTRAN)
Core Network
General Packet Radio Service
(GPRS) Core Network
Public
Internet
Public
Internet
Figure 6.19  3G system architecture
network (in particular, the MSC) that we previously encountered in Figure 6.18.
Given the considerable amount of existing infrastructure (and profitable services!) in
the existing cellular voice network, the approach taken by the designers of 3G data
services is clear: leave the existing core GSM cellular voice network untouched,
adding additional cellular data functionality in parallel to the existing cellular voice
network. The alternative—integrating new data services directly into the core of the
existing cellular voice network—would have raised the same challenges encountered
in Section 4.4.4, where we discussed integrating new (IPv6) and legacy (IPv4) technologies
in the Internet.
There are two types of nodes in the 3G core network: Serving GPRS Support
Nodes (SGSNs) and Gateway GPRS Support Nodes (GGSNs). (GPRS
stands for Generalized Packet Radio Service, an early cellular data service in 2G
networks; here we discuss the evolved version of GPRS in 3G networks). An
SGSN is responsible for delivering datagrams to/from the mobile nodes in the
radio access network to which the SGSN is attached. The SGSN interacts with
the cellular voice network’s MSC for that area, providing user authorization and
handoff, maintaining location (cell) information about active mobile nodes, and
performing datagram forwarding between mobile nodes in the radio access network
and a GGSN. The GGSN acts as a gateway, connecting multiple SGSNs
into the larger Internet. A GGSN is thus the last piece of 3G infrastructure that a
datagram originating at a mobile node encounters before entering the larger Internet.
To the outside world, the GGSN looks like any other gateway router; the
mobility of the 3G nodes within the GGSN’s network is hidden from the outside
world behind the GGSN.
3G Radio Access Network: The Wireless Edge
The 3G radio access network is the wireless first-hop network that we see as a
3G user. The Radio Network Controller (RNC) typically controls several cell
base transceiver stations similar to the base stations that we encountered in 2G
systems (but officially known in 3G UMTS parlance as a “Node Bs”—a rather
non-descriptive name!). Each cell’s wireless link operates between the mobile
nodes and a base transceiver station, just as in 2G networks. The RNC connects
to both the circuit-switched cellular voice network via an MSC, and to the
packet-switched Internet via an SGSN. Thus, while 3G cellular voice and cellular
data services use different core networks, they share a common first/last-hop
radio access network.
A significant change in 3G UMTS over 2G networks is that rather than using
GSM’s FDMA/TDMA scheme, UMTS uses a CDMA technique known as Direct
Sequence Wideband CDMA (DS-WCDMA) [Dahlman 1998] within TDMA
slots; TDMA slots, in turn, are available on multiple frequencies—an interesting
use of all three dedicated channel-sharing approaches that we earlier identified in
Chapter 5 and similar to the approach taken in wired cable access networks (see
Section 5.3.4). This change requires a new 3G cellular wireless-access network
operating in parallel with the 2G BSS radio network shown in Figure 6.19. The
data service associated with the WCDMA specification is known as HSP (High
Speed Packet Access) and promises downlink data rates of up to 14 Mbps.
Details regarding 3G networks can be found at the 3rd Generation Partnership
Project (3GPP) Web site [3GPP 2012].
552 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.4.3 On to 4G: LTE
With 3G systems now being deployed worldwide, can 4G systems be far behind?
Certainly not! Indeed, the design, early testing, and initial deployment of 4G systems
are already underway. The 4G Long-Term Evolution (LTE) standard put forward
by the 3GPP has two important innovations over 3G systems:
• Evolved Packet Core (EPC) [3GPP Network Architecture 2012]. The EPC is
a simplified all-IP core network that unifies the separate circuit-switched cellular
voice network and the packet-switched cellular data network shown in
Figure 6.19. It is an “all-IP” network in that both voice and data will be carried
in IP datagrams. As we’ve seen in Chapter 4 and will study in more detail
in Chapter 7, IP’s “best effort” service model is not inherently well-suited to
the stringent performance requirements of Voice-over-IP (VoIP) traffic unless
network resources are carefully managed to avoid (rather than react to) congestion.
Thus, a key task of the EPC is to manage network resources to provide
this high quality of service. The EPC also makes a clear separation
between the network control and user data planes, with many of the mobility
support features that we will study in Section 6.7 being implemented in the
control plane. The EPC allows multiple types of radio access networks,
including legacy 2G and 3G radio access networks, to attach to the core
network. Two very readable introductions to the EPC are [Motorola 2007;
Alcatel-Lucent 2009].
• LTE Radio Access Network. LTE uses a combination of frequency division
multiplexing and time division multiplexing on the downstream channel,
known as orthogonal frequency division multiplexing (OFDM) [Rohde 2008;
Ericsson 2011]. (The term “orthogonal” comes from the fact the signals being
sent on different frequency channels are created so that they interfere very little
with each other, even when channel frequencies are tightly spaced). In LTE,
each active mobile node is allocated one or more 0.5 ms time slots in one or
more of the channel frequencies. Figure 6.20 shows an allocation of eight time
slots over four frequencies. By being allocated increasingly more time slots
(whether on the same frequency or on different frequencies), a mobile node is
able to achieve increasingly higher transmission rates. Slot (re)allocation
among mobile nodes can be performed as often as once every millisecond. Different
modulation schemes can also be used to change the transmission rate;
see our earlier discussion of Figure 6.3 and dynamic selection of modulation
schemes in WiFi networks. Another innovation in the LTE radio network is the
use of sophisticated multiple-input, multiple output (MIMO) antennas. The
maximum data rate for an LTE user is 100 Mbps in the downstream direction
and 50 Mbps in the upstream direction, when using 20 MHz worth of wireless
spectrum.
6.4 • CELLULAR INTERNET ACCESS 553
The particular allocation of time slots to mobile nodes is not mandated by the
LTE standard. Instead, the decision of which mobile nodes will be allowed to
transmit in a given time slot on a given frequency is determined by the scheduling
algorithms provided by the LTE equipment vendor and/or the network operator.
With opportunistic scheduling [Bender 2000; Kolding 2003; Kulkarni 2005],
matching the physical-layer protocol to the channel conditions between the
sender and receiver and choosing the receivers to which packets will be sent
based on channel conditions allow the radio network controller to make best use
of the wireless medium. In addition, user priorities and contracted levels of service
(e.g., silver, gold, or platinum) can be used in scheduling downstream packet
transmissions. In addition to the LTE capabilities described above, LTE-Advanced
allows for downstream bandwidths of hundreds of Mbps by allocating aggregated
channels to a mobile node [Akyildiz 2010].
An additional 4G wireless technology—WiMAX (World Interoperability for
Microwave Access)—is a family of IEEE 802.16 standards that differ significantly
from LTE. Whether LTE or WiMAX becomes the 4G technology of choice is still to
be seen, but at the time of this writing (spring 2012), LTE appears to have significantly
more momentum. A detailed discussion of WiMAX can be found on this
book’s Web site.
554 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
f1
f2
f3
f4
f5
f6
0 0.5 1.0 1.5 2.0 2.5 9.0 9.5 10.0
Figure 6.20  Twenty 0.5 ms slots organized into 10 ms frames at each
frequency. An eight-slot allocation is shown shaded.
6.5 Mobility Management: Principles
Having covered the wireless nature of the communication links in a wireless network,
it’s now time to turn our attention to the mobility that these wireless links
enable. In the broadest sense, a mobile node is one that changes its point of attachment
into the network over time. Because the term mobility has taken on many
meanings in both the computer and telephony worlds, it will serve us well first to
consider several dimensions of mobility in some detail.
• From the network layer’s standpoint, how mobile is a user? A physically mobile
user will present a very different set of challenges to the network layer, depending
on how he or she moves between points of attachment to the network. At one
end of the spectrum in Figure 6.21, a user may carry a laptop with a wireless network
interface card around in a building. As we saw in Section 6.3.4, this user is
not mobile from a network-layer perspective. Moreover, if the user associates
with the same access point regardless of location, the user is not even mobile
from the perspective of the link layer.
At the other end of the spectrum, consider the user zooming along the autobahn
in a BMW at 150 kilometers per hour, passing through multiple wireless access
networks and wanting to maintain an uninterrupted TCP connection to a remote
application throughout the trip. This user is definitely mobile! In between these
extremes is a user who takes a laptop from one location (e.g., office or dormitory)
into another (e.g., coffeeshop, classroom) and wants to connect into the
network in the new location. This user is also mobile (although less so than the
BMW driver!) but does not need to maintain an ongoing connection while moving
between points of attachment to the network. Figure 6.21 illustrates this spectrum
of user mobility from the network layer’s perspective.
6.5 • MOBILITY MANAGEMENT: PRINCIPLES 555
User moves only
within same wireless
access network
No mobility High mobility
User moves between
access networks,
shutting down while
moving between
networks
User moves between
access networks,
while maintaining
ongoing connections
Figure 6.21  Various degrees of mobility, from the network layer’s point
of view
• How important is it for the mobile node’s address to always remain the same?
With mobile telephony, your phone number—essentially the network-layer
address of your phone—remains the same as you travel from one provider’s
mobile phone network to another. Must a laptop similarly maintain the same IP
address while moving between IP networks?
The answer to this question will depend strongly on the applications being run.
For the BMW driver who wants to maintain an uninterrupted TCP connection to
a remote application while zipping along the autobahn, it would be convenient to
maintain the same IP address. Recall from Chapter 3 that an Internet application
needs to know the IP address and port number of the remote entity with which it
is communicating. If a mobile entity is able to maintain its IP address as it
moves, mobility becomes invisible from the application standpoint. There is
great value to this transparency—an application need not be concerned with a
potentially changing IP address, and the same application code serves mobile
and nonmobile connections alike. We’ll see in the following section that mobile
IP provides this transparency, allowing a mobile node to maintain its permanent
IP address while moving among networks.
On the other hand, a less glamorous mobile user might simply want to turn off an
office laptop, bring that laptop home, power up, and work from home. If the laptop
functions primarily as a client in client-server applications (e.g., send/read e-mail,
browse the Web, Telnet to a remote host) from home, the particular IP address used
by the laptop is not that important. In particular, one could get by fine with an
address that is temporarily allocated to the laptop by the ISP serving the home. We
saw in Section 4.4 that DHCP already provides this functionality.
• What supporting wired infrastructure is available? In all of our scenarios above,
we’ve implicitly assumed that there is a fixed infrastructure to which the mobile
user can connect—for example, the home’s ISP network, the wireless access network
in the office, or the wireless access networks lining the autobahn. What if
no such infrastructure exists? If two users are within communication proximity
of each other, can they establish a network connection in the absence of any other
network-layer infrastructure? Ad hoc networking provides precisely these capabilities.
This rapidly developing area is at the cutting edge of mobile networking
research and is beyond the scope of this book. [Perkins 2000] and the IETF
Mobile Ad Hoc Network (manet) working group Web pages [manet 2012] provide
thorough treatments of the subject.
In order to illustrate the issues involved in allowing a mobile user to maintain
ongoing connections while moving between networks, let’s consider a human
analogy. A twenty-something adult moving out of the family home becomes
mobile, living in a series of dormitories and/or apartments, and often changing
addresses. If an old friend wants to get in touch, how can that friend find the
address of her mobile friend? One common way is to contact the family, since a
556 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.5 • MOBILITY MANAGEMENT: PRINCIPLES 557
mobile adult will often register his or her current address with the family (if for no
other reason than so that the parents can send money to help pay the rent!). The
family home, with its permanent address, becomes that one place that others can
go as a first step in communicating with the mobile adult. Later communication
from the friend may be either indirect (for example, with mail being sent first to
the parents’ home and then forwarded to the mobile adult) or direct (for example,
with the friend using the address obtained from the parents to send mail directly to
her mobile friend).
In a network setting, the permanent home of a mobile node (such as a laptop
or smartphone) is known as the home network, and the entity within the home
network that performs the mobility management functions discussed below on
behalf of the mobile node is known as the home agent. The network in which
the mobile node is currently residing is known as the foreign (or visited) network,
and the entity within the foreign network that helps the mobile node with
the mobility management functions discussed below is known as a foreign agent.
For mobile professionals, their home network might likely be their company network,
while the visited network might be the network of a colleague they are visiting.
A correspondent is the entity wishing to communicate with the mobile node.
Figure 6.22 illustrates these concepts, as well as addressing concepts considered
below. In Figure 6.22, note that agents are shown as being collocated with routers
(e.g., as processes running on routers), but alternatively they could be executing on
other hosts or servers in the network.
6.5.1 Addressing
We noted above that in order for user mobility to be transparent to network applications,
it is desirable for a mobile node to keep its address as it moves from one network
to another. When a mobile node is resident in a foreign network, all traffic
addressed to the node’s permanent address now needs to be routed to the foreign
network. How can this be done? One option is for the foreign network to advertise
to all other networks that the mobile node is resident in its network. This could be
via the usual exchange of intradomain and interdomain routing information and
would require few changes to the existing routing infrastructure. The foreign network
could simply advertise to its neighbors that it has a highly specific route to the
mobile node’s permanent address (that is, essentially inform other networks that it
has the correct path for routing datagrams to the mobile node’s permanent address;
see Section 4.4). These neighbors would then propagate this routing information
throughout the network as part of the normal procedure of updating routing information
and forwarding tables. When the mobile node leaves one foreign network
and joins another, the new foreign network would advertise a new, highly specific
route to the mobile node, and the old foreign network would withdraw its routing
information regarding the mobile node.
558 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
This solves two problems at once, and it does so without making significant
changes to the network-layer infrastructure. Other networks know the location of
the mobile node, and it is easy to route datagrams to the mobile node, since the forwarding
tables will direct datagrams to the foreign network. A significant drawback,
however, is that of scalability. If mobility management were to be the responsibility
of network routers, the routers would have to maintain forwarding table entries for
potentially millions of mobile nodes, and update these entries as nodes move. Some
additional drawbacks are explored in the problems at the end of this chapter.
An alternative approach (and one that has been adopted in practice) is to push
mobility functionality from the network core to the network edge—a recurring
theme in our study of Internet architecture. A natural way to do this is via the mobile
node’s home network. In much the same way that parents of the mobile twentysomething
track their child’s location, the home agent in the mobile node’s home
network can track the foreign network in which the mobile node resides. A protocol
Home agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186 Permanent address:
128.119.40.186
Foreign agent
Care-of address:
79.129.13.2
Correspondent
Wide area
network
Figure 6.22  Initial elements of a mobile network architecture
6.5 • MOBILITY MANAGEMENT: PRINCIPLES 559
between the mobile node (or a foreign agent representing the mobile node) and the
home agent will certainly be needed to update the mobile node’s location.
Let’s now consider the foreign agent in more detail. The conceptually simplest
approach, shown in Figure 6.22, is to locate foreign agents at the edge routers in the
foreign network. One role of the foreign agent is to create a so-called care-of address
(COA) for the mobile node, with the network portion of the COA matching that of
the foreign network. There are thus two addresses associated with a mobile node, its
permanent address (analogous to our mobile youth’s family’s home address) and its
COA, sometimes known as a foreign address (analogous to the address of the house
in which our mobile youth is currently residing). In the example in Figure 6.22, the
permanent address of the mobile node is 128.119.40.186. When visiting network
79.129.13/24, the mobile node has a COA of 79.129.13.2. A second role of the foreign
agent is to inform the home agent that the mobile node is resident in its (the foreign
agent’s) network and has the given COA. We’ll see shortly that the COA will be
used to “reroute” datagrams to the mobile node via its foreign agent.
Although we have separated the functionality of the mobile node and the foreign
agent, it is worth noting that the mobile node can also assume the responsibilities
of the foreign agent. For example, the mobile node could obtain a COA in the
foreign network (for example, using a protocol such as DHCP) and itself inform the
home agent of its COA.
6.5.2 Routing to a Mobile Node
We have now seen how a mobile node obtains a COA and how the home agent can
be informed of that address. But having the home agent know the COA solves only
part of the problem. How should datagrams be addressed and forwarded to the
mobile node? Since only the home agent (and not network-wide routers) knows the
location of the mobile node, it will no longer suffice to simply address a datagram to
the mobile node’s permanent address and send it into the network-layer infrastructure.
Something more must be done. Two approaches can be identified, which we
will refer to as indirect and direct routing.
Indirect Routing to a Mobile Node
Let’s first consider a correspondent that wants to send a datagram to a mobile node.
In the indirect routing approach, the correspondent simply addresses the datagram
to the mobile node’s permanent address and sends the datagram into the network,
blissfully unaware of whether the mobile node is resident in its home network or is
visiting a foreign network; mobility is thus completely transparent to the correspondent.
Such datagrams are first routed, as usual, to the mobile node’s home network.
This is illustrated in step 1 in Figure 6.23.
Let’s now turn our attention to the home agent. In addition to being responsible
for interacting with a foreign agent to track the mobile node’s COA, the home agent
560 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
has another very important function. Its second job is to be on the lookout for arriving
datagrams addressed to nodes whose home network is that of the home agent but
that are currently resident in a foreign network. The home agent intercepts these
datagrams and then forwards them to a mobile node in a two-step process. The datagram
is first forwarded to the foreign agent, using the mobile node’s COA (step 2 in
Figure 6.23), and then forwarded from the foreign agent to the mobile node (step 3
in Figure 6.23).
It is instructive to consider this rerouting in more detail. The home agent will
need to address the datagram using the mobile node’s COA, so that the network
layer will route the datagram to the foreign network. On the other hand, it is
desirable to leave the correspondent’s datagram intact, since the application receiving
the datagram should be unaware that the datagram was forwarded via the home
agent. Both goals can be satisfied by having the home agent encapsulate the correspondent’s
original complete datagram within a new (larger) datagram. This larger
Home
agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186 Permanent address:
128.119.40.186
Foreign
agent
Care-of
address:
79.129.13.2
Wide area
network
Correspondent
1
2
4
3
Figure 6.23  Indirect routing to a mobile node
6.5 • MOBILITY MANAGEMENT: PRINCIPLES 561
datagram is addressed and delivered to the mobile node’s COA. The foreign agent,
who “owns” the COA, will receive and decapsulate the datagram—that is, remove
the correspondent’s original datagram from within the larger encapsulating datagram
and forward (step 3 in Figure 6.23) the original datagram to the mobile node.
Figure 6.24 shows a correspondent’s original datagram being sent to the home network,
an encapsulated datagram being sent to the foreign agent, and the original
datagram being delivered to the mobile node. The sharp reader will note that the
encapsulation/decapsulation described here is identical to the notion of tunneling,
discussed in Chapter 4 in the context of IP multicast and IPv6.
Let’s next consider how a mobile node sends datagrams to a correspondent.
This is quite simple, as the mobile node can address its datagram directly to the correspondent
(using its own permanent address as the source address, and the correspondent’s
address as the destination address). Since the mobile node knows the
correspondent’s address, there is no need to route the datagram back through the
home agent. This is shown as step 4 in Figure 6.23.
Let’s summarize our discussion of indirect routing by listing the new networklayer
functionality required to support mobility.
Home
agent
Permanent address:
128.119.40.186 Permanent address:
128.119.40.186
Foreign
agent
Correspondent
dest: 128.119.40.186
dest: 79.129.13.2 dest: 128.119.40.186
dest: 128.119.40.186
Care-of address:
79.129.13.2
Figure 6.24  Encapsulation and decapsulation
• A mobile-node–to–foreign-agent protocol. The mobile node will register with
the foreign agent when attaching to the foreign network. Similarly, a mobile
node will deregister with the foreign agent when it leaves the foreign network.
• A foreign-agent–to–home-agent registration protocol. The foreign agent will
register the mobile node’s COA with the home agent. A foreign agent need not
explicitly deregister a COA when a mobile node leaves its network, because the
subsequent registration of a new COA, when the mobile node moves to a new
network, will take care of this.
• A home-agent datagram encapsulation protocol. Encapsulation and forwarding
of the correspondent’s original datagram within a datagram addressed to
the COA.
• A foreign-agent decapsulation protocol. Extraction of the correspondent’s original
datagram from the encapsulating datagram, and the forwarding of the original
datagram to the mobile node.
The previous discussion provides all the pieces—foreign agents, the home
agent, and indirect forwarding—needed for a mobile node to maintain an ongoing
connection while moving among networks. As an example of how these
pieces fit together, assume the mobile node is attached to foreign network A, has
registered a COA in network A with its home agent, and is receiving datagrams
that are being indirectly routed through its home agent. The mobile node now
moves to foreign network B and registers with the foreign agent in network B,
which informs the home agent of the mobile node’s new COA. From this point
on, the home agent will reroute datagrams to foreign network B. As far as a correspondent
is concerned, mobility is transparent—datagrams are routed via the
same home agent both before and after the move. As far as the home agent is concerned,
there is no disruption in the flow of datagrams—arriving datagrams are
first forwarded to foreign network A; after the change in COA, datagrams are forwarded
to foreign network B. But will the mobile node see an interrupted flow of
datagrams as it moves between networks? As long as the time between the
mobile node’s disconnection from network A (at which point it can no longer
receive datagrams via A) and its attachment to network B (at which point it will
register a new COA with its home agent) is small, few datagrams will be lost.
Recall from Chapter 3 that end-to-end connections can suffer datagram loss due
to network congestion. Hence occasional datagram loss within a connection
when a node moves between networks is by no means a catastrophic problem. If
loss-free communication is required, upper-layer mechanisms will recover from
datagram loss, whether such loss results from network congestion or from user
mobility.
An indirect routing approach is used in the mobile IP standard [RFC 5944], as
discussed in Section 6.6.
562 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.5 • MOBILITY MANAGEMENT: PRINCIPLES 563
Direct Routing to a Mobile Node
The indirect routing approach illustrated in Figure 6.23 suffers from an inefficiency
known as the triangle routing problem—datagrams addressed to the mobile node
must be routed first to the home agent and then to the foreign network, even when a
much more efficient route exists between the correspondent and the mobile node. In
the worst case, imagine a mobile user who is visiting the foreign network of a colleague.
The two are sitting side by side and exchanging data over the network. Datagrams
from the correspondent (in this case the colleague of the visitor) are routed to
the mobile user’s home agent and then back again to the foreign network!
Direct routing overcomes the inefficiency of triangle routing, but does so at the
cost of additional complexity. In the direct routing approach, a correspondent agent in
the correspondent’s network first learns the COA of the mobile node. This can be done
by having the correspondent agent query the home agent, assuming that (as in the case
of indirect routing) the mobile node has an up-to-date value for its COA registered with
its home agent. It is also possible for the correspondent itself to perform the function of
the correspondent agent, just as a mobile node could perform the function of the foreign
agent. This is shown as steps 1 and 2 in Figure 6.25. The correspondent agent then
tunnels datagrams directly to the mobile node’s COA, in a manner analogous to the tunneling
performed by the home agent, steps 3 and 4 in Figure 6.25.
While direct routing overcomes the triangle routing problem, it introduces two
important additional challenges:
• A mobile-user location protocol is needed for the correspondent agent to query
the home agent to obtain the mobile node’s COA (steps 1 and 2 in Figure 6.25).
• When the mobile node moves from one foreign network to another, how will data
now be forwarded to the new foreign network? In the case of indirect routing, this
problem was easily solved by updating the COA maintained by the home
agent. However, with direct routing, the home agent is queried for the COA by
the correspondent agent only once, at the beginning of the session. Thus, updating
the COA at the home agent, while necessary, will not be enough to solve the
problem of routing data to the mobile node’s new foreign network.
One solution would be to create a new protocol to notify the correspondent of
the changing COA. An alternate solution, and one that we’ll see adopted in practice in
GSM networks, works as follows. Suppose data is currently being forwarded to the
mobile node in the foreign network where the mobile node was located when the session
first started (step 1 in Figure 6.26). We’ll identify the foreign agent in that foreign
network where the mobile node was first found as the anchor foreign agent.
When the mobile node moves to a new foreign network (step 2 in Figure 6.26), the
mobile node registers with the new foreign agent (step 3), and the new foreign agent
provides the anchor foreign agent with the mobile node’s new COA (step 4). When
564 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Home
agent
Home network:
128.119.40/24
Visited network:
79.129.13/24
Mobile node
Permanent address:
128.119.40.186
Key:
Permanent address:
128.119.40.186
Foreign
agent
Care-of address:
79.129.13.2
Wide area
network
Correspondent
Control messages
Correspondent
agent
1
2 3
Data flow
4
Figure 6.25  Direct routing to a mobile user
the anchor foreign agent receives an encapsulated datagram for a departed mobile
node, it can then re-encapsulate the datagram and forward it to the mobile node
(step 5) using the new COA. If the mobile node later moves yet again to a new foreign
network, the foreign agent in that new visited network would then contact the
anchor foreign agent in order to set up forwarding to this new foreign network.
6.6 Mobile IP
The Internet architecture and protocols for supporting mobility, collectively known
as mobile IP, are defined primarily in RFC 5944 for IPv4. Mobile IP is a flexible
standard, supporting many different modes of operation (for example, operation
6.6 • MOBILE IP 565
Home
agent
Home network: Foreign network
being visited at
session start:
New foreign
network:
Anchor
foreign
agent
New foreign agent
Wide area
network
Correspondent
Correspondent
agent
1
4
2
3
5
Figure 6.26  Mobile transfer between networks with direct routing
with or without a foreign agent), multiple ways for agents and mobile nodes to discover
each other, use of single or multiple COAs, and multiple forms of encapsulation.
As such, mobile IP is a complex standard, and would require an entire book to
describe in detail; indeed one such book is [Perkins 1998b]. Our modest goal here is
to provide an overview of the most important aspects of mobile IP and to illustrate
its use in a few common-case scenarios.
The mobile IP architecture contains many of the elements we have considered
above, including the concepts of home agents, foreign agents, care-of addresses, and
encapsulation/decapsulation. The current standard [RFC 5944] specifies the use of
indirect routing to the mobile node.
The mobile IP standard consists of three main pieces:
• Agent discovery. Mobile IP defines the protocols used by a home or foreign agent
to advertise its services to mobile nodes, and protocols for mobile nodes to solicit
the services of a foreign or home agent.
566 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
• Registration with the home agent. Mobile IP defines the protocols used by the
mobile node and/or foreign agent to register and deregister COAs with a mobile
node’s home agent.
• Indirect routing of datagrams. The standard also defines the manner in which
datagrams are forwarded to mobile nodes by a home agent, including rules for
forwarding datagrams, rules for handling error conditions, and several forms of
encapsulation [RFC 2003, RFC 2004].
Security considerations are prominent throughout the mobile IP standard. For
example, authentication of a mobile node is clearly needed to ensure that a malicious
user does not register a bogus care-of address with a home agent, which
could cause all datagrams addressed to an IP address to be redirected to the malicious
user. Mobile IP achieves security using many of the mechanisms that we will
examine in Chapter 8, so we will not address security considerations in our discussion
below.
Agent Discovery
A mobile IP node arriving to a new network, whether attaching to a foreign network
or returning to its home network, must learn the identity of the corresponding foreign
or home agent. Indeed it is the discovery of a new foreign agent, with a new
network address, that allows the network layer in a mobile node to learn that it has
moved into a new foreign network. This process is known as agent discovery.
Agent discovery can be accomplished in one of two ways: via agent advertisement
or via agent solicitation.
With agent advertisement, a foreign or home agent advertises its services
using an extension to the existing router discovery protocol [RFC 1256]. The
agent periodically broadcasts an ICMP message with a type field of 9 (router discovery)
on all links to which it is connected. The router discovery message contains
the IP address of the router (that is, the agent), thus allowing a mobile node
to learn the agent’s IP address. The router discovery message also contains a
mobility agent advertisement extension that contains additional information
needed by the mobile node. Among the more important fields in the extension are
the following:
• Home agent bit (H). Indicates that the agent is a home agent for the network in
which it resides.
• Foreign agent bit (F). Indicates that the agent is a foreign agent for the network
in which it resides.
• Registration required bit (R). Indicates that a mobile user in this network must
register with a foreign agent. In particular, a mobile user cannot obtain a careof
address in the foreign network (for example, using DHCP) and assume the
6.6 • MOBILE IP 567
functionality of the foreign agent for itself, without registering with the foreign
agent.
• M, G encapsulation bits. Indicate whether a form of encapsulation other than IPin-
IP encapsulation will be used.
• Care-of address (COA) fields. A list of one or more care-of addresses provided
by the foreign agent. In our example below, the COA will be associated with the
foreign agent, who will receive datagrams sent to the COA and then forward
them to the appropriate mobile node. The mobile user will select one of these
addresses as its COA when registering with its home agent.
Figure 6.27 illustrates some of the key fields in the agent advertisement message.
With agent solicitation, a mobile node wanting to learn about agents without
waiting to receive an agent advertisement can broadcast an agent solicitation message,
which is simply an ICMP message with type value 10. An agent receiving the
solicitation will unicast an agent advertisement directly to the mobile node, which
can then proceed as if it had received an unsolicited advertisement.
Type = 9 Code = 0
Type = 16 Length Sequence number
Registration lifetime RBHFMGrT Reserved
bits
Checksum
Standard
ICMP fields
0 8 16 24
Router address
0 or more care-of addresses Mobility agent
advertisement
extension
Figure 6.27  ICMP router discovery message with mobility agent
advertisement extension
568 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
Registration with the Home Agent
Once a mobile IP node has received a COA, that address must be registered with the
home agent. This can be done either via the foreign agent (who then registers
the COA with the home agent) or directly by the mobile IP node itself. We consider
the former case below. Four steps are involved.
1. Following the receipt of a foreign agent advertisement, a mobile node sends a
mobile IP registration message to the foreign agent. The registration message is
carried within a UDP datagram and sent to port 434. The registration message
carries a COA advertised by the foreign agent, the address of the home agent
(HA), the permanent address of the mobile node (MA), the requested lifetime
of the registration, and a 64-bit registration identification. The requested registration
lifetime is the number of seconds that the registration is to be valid. If
the registration is not renewed at the home agent within the specified lifetime,
the registration will become invalid. The registration identifier acts like a
sequence number and serves to match a received registration reply with a registration
request, as discussed below.
2. The foreign agent receives the registration message and records the mobile node’s
permanent IP address. The foreign agent now knows that it should be looking for
datagrams containing an encapsulated datagram whose destination address
matches the permanent address of the mobile node. The foreign agent then sends a
mobile IP registration message (again, within a UDP datagram) to port 434 of the
home agent. The message contains the COA, HA, MA, encapsulation format
requested, requested registration lifetime, and registration identification.
3. The home agent receives the registration request and checks for authenticity
and correctness. The home agent binds the mobile node’s permanent IP address
with the COA; in the future, datagrams arriving at the home agent and
addressed to the mobile node will now be encapsulated and tunneled to the
COA. The home agent sends a mobile IP registration reply containing the HA,
MA, actual registration lifetime, and the registration identification of the
request that is being satisfied with this reply.
4. The foreign agent receives the registration reply and then forwards it to the
mobile node.
At this point, registration is complete, and the mobile node can receive datagrams
sent to its permanent address. Figure 6.28 illustrates these steps. Note that the
home agent specifies a lifetime that is smaller than the lifetime requested by the
mobile node.
A foreign agent need not explicitly deregister a COA when a mobile node
leaves its network. This will occur automatically, when the mobile node moves to a
new network (whether another foreign network or its home network) and registers a
new COA.
6.6 • MOBILE IP 569
The mobile IP standard allows many additional scenarios and capabilities in
addition to those described previously. The interested reader should consult [Perkins
1998b; RFC 5944].
Home agent
HA: 128.119.40.7
Mobile agent
MA: 128.119.40.186
Visited network:
79.129.13/24
ICMP agent adv.
COA: 79.129.13.2
. . .
COA: 79.129.13.2
HA:128.119.40.7
MA: 128.119.40.186
Lifetime: 9999
identification: 714
. . .
Registration req.
COA: 79.129.13.2
HA:128.119.40.7
MA: 128.119.40.186
Lifetime: 9999
identification: 714
encapsulation format
. . .
Registration req.
Time Time Time
HA: 128.119.40.7
MA: 128.119.40.186
Lifetime: 4999
identification: 714
encapsulation format
. . .
Registration reply
HA: 128.119.40.7
MA: 128.119.40.186
Lifetime: 4999
identification: 714
. . .
Registration reply
Foreign agent
COA: 79.129.13.2
Figure 6.28  Agent advertisement and mobile IP registration
570 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
6.7 Managing Mobility in Cellular Networks
Having examined how mobility is managed in IP networks, let’s now turn our attention
to networks with an even longer history of supporting mobility—cellular
telephony networks. Whereas we focused on the first-hop wireless link in cellular
networks in Section 6.4, we’ll focus here on mobility, using the GSM cellular network
architecture [Goodman 1997; Mouly 1992; Scourias 2012; Kaaranen 2001;
Korhonen 2003; Turner 2012] as our case study, since it is a mature and widely
deployed technology. As in the case of mobile IP, we’ll see that a number of the fundamental
principles we identified in Section 6.5 are embodied in GSM’s network
architecture.
Like mobile IP, GSM adopts an indirect routing approach (see Section 6.5.2),
first routing the correspondent’s call to the mobile user’s home network and from
there to the visited network. In GSM terminology, the mobile users’s home network
is referred to as the mobile user’s home public land mobile network (home
PLMN). Since the PLMN acronym is a bit of a mouthful, and mindful of our quest
to avoid an alphabet soup of acronyms, we’ll refer to the GSM home PLMN simply
as the home network. The home network is the cellular provider with which the
mobile user has a subscription (i.e., the provider that bills the user for monthly cellular
service). The visited PLMN, which we’ll refer to simply as the visited network,
is the network in which the mobile user is currently residing.
As in the case of mobile IP, the responsibilities of the home and visited networks
are quite different.
• The home network maintains a database known as the home location register
(HLR), which contains the permanent cell phone number and subscriber profile
information for each of its subscribers. Importantly, the HLR also contains
information about the current locations of these subscribers. That is, if a
mobile user is currently roaming in another provider’s cellular network, the
HLR contains enough information to obtain (via a process we’ll describe
shortly) an address in the visited network to which a call to the mobile user
should be routed. As we’ll see, a special switch in the home network, known
as the Gateway Mobile services Switching Center (GMSC) is contacted by
a correspondent when a call is placed to a mobile user. Again, in our quest to
avoid an alphabet soup of acronyms, we’ll refer to the GMSC here by a more
descriptive term, home MSC.
• The visited network maintains a database known as the visitor location register
(VLR). The VLR contains an entry for each mobile user that is currently in the
portion of the network served by the VLR. VLR entries thus come and go as
mobile users enter and leave the network. A VLR is usually co-located with the
mobile switching center (MSC) that coordinates the setup of a call to and from
the visited network.
6.7 • MANAGING MOBILITY IN CELLULAR NETWORKS 571
In practice, a provider’s cellular network will serve as a home network for its subscribers
and as a visited network for mobile users whose subscription is with a different
cellular provider.
6.7.1 Routing Calls to a Mobile User
We’re now in a position to describe how a call is placed to a mobile GSM user in a
visited network. We’ll consider a simple example below; more complex scenarios are
described in [Mouly 1992]. The steps, as illustrated in Figure 6.29, are as follows:
1. The correspondent dials the mobile user’s phone number. This number itself
does not refer to a particular telephone line or location (after all, the phone
number is fixed and the user is mobile!). The leading digits in the number are
sufficient to globally identify the mobile’s home network. The call is routed
from the correspondent through the PSTN to the home MSC in the mobile’s
home network. This is the first leg of the call.
2. The home MSC receives the call and interrogates the HLR to determine the
location of the mobile user. In the simplest case, the HLR returns the mobile
Mobile
user
Visited
network
Home
network
Public switched
telephone
network
1
3
Correspondent
VLR
HLR
2
Figure 6.29  Placing a call to a mobile user: indirect routing
572 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
station roaming number (MSRN), which we will refer to as the roaming
number. Note that this number is different from the mobile’s permanent phone
number, which is associated with the mobile’s home network. The roaming
number is ephemeral: It is temporarily assigned to a mobile when it enters a visited
network. The roaming number serves a role similar to that of the care-of
address in mobile IP and, like the COA, is invisible to the correspondent and the
mobile. If HLR does not have the roaming number, it returns the address of the
VLR in the visited network. In this case (not shown in Figure 6.29), the home
MSC will need to query the VLR to obtain the roaming number of the mobile
node. But how does the HLR get the roaming number or the VLR address in the
first place? What happens to these values when the mobile user moves to
another visited network? We’ll consider these important questions shortly.
3. Given the roaming number, the home MSC sets up the second leg of the call
through the network to the MSC in the visited network. The call is completed,
being routed from the correspondent to the home MSC, and from there to the
visited MSC, and from there to the base station serving the mobile user.
An unresolved question in step 2 is how the HLR obtains information about
the location of the mobile user. When a mobile telephone is switched on or enters a
part of a visited network that is covered by a new VLR, the mobile must register
with the visited network. This is done through the exchange of signaling messages
between the mobile and the VLR. The visited VLR, in turn, sends a location update
request message to the mobile’s HLR. This message informs the HLR of either the
roaming number at which the mobile can be contacted, or the address of the VLR
(which can then later be queried to obtain the mobile number). As part of this
exchange, the VLR also obtains subscriber information from the HLR about the
mobile and determines what services (if any) should be accorded the mobile user
by the visited network.
6.7.2 Handoffs in GSM
A handoff occurs when a mobile station changes its association from one base station
to another during a call. As shown in Figure 6.30, a mobile’s call is initially
(before handoff) routed to the mobile through one base station (which we’ll refer to
as the old base station), and after handoff is routed to the mobile through another
base station (which we’ll refer to as the new base station). Note that a handoff
between base stations results not only in the mobile transmitting/receiving to/from a
new base station, but also in the rerouting of the ongoing call from a switching point
within the network to the new base station. Let’s initially assume that the old and
new base stations share the same MSC, and that the rerouting occurs at this MSC.
There may be several reasons for handoff to occur, including (1) the signal
between the current base station and the mobile may have deteriorated to such an
extent that the call is in danger of being dropped, and (2) a cell may have become
6.7 • MANAGING MOBILITY IN CELLULAR NETWORKS 573
overloaded, handling a large number of calls. This congestion may be alleviated by
handing off mobiles to less congested nearby cells.
While it is associated with a base station, a mobile periodically measures the
strength of a beacon signal from its current base station as well as beacon signals from
nearby base stations that it can “hear.” These measurements are reported once or twice
a second to the mobile’s current base station. Handoff in GSM is initiated by the old
base station based on these measurements, the current loads of mobiles in nearby cells,
and other factors [Mouly 1992]. The GSM standard does not specify the specific algorithm
to be used by a base station to determine whether or not to perform handoff.
Figure 6.31 illustrates the steps involved when a base station does decide to
hand off a mobile user:
1. The old base station (BS) informs the visited MSC that a handoff is to be performed
and the BS (or possible set of BSs) to which the mobile is to be handed off.
2. The visited MSC initiates path setup to the new BS, allocating the resources
needed to carry the rerouted call, and signaling the new BS that a handoff is
about to occur.
3. The new BS allocates and activates a radio channel for use by the mobile.
4. The new BS signals back to the visited MSC and the old BS that the visited-
MSC-to-new-BS path has been established and that the mobile should be
informed of the impending handoff. The new BS provides all of the information
that the mobile will need to associate with the new BS.
5. The mobile is informed that it should perform a handoff. Note that up until this
point, the mobile has been blissfully unaware that the network has been laying
the groundwork (e.g., allocating a channel in the new BS and allocating a path
from the visited MSC to the new BS) for a handoff.
6. The mobile and the new BS exchange one or more messages to fully activate
the new channel in the new BS.
Old BS New BS
Old
routing
New
routing
VLR
Figure 6.30  Handoff scenario between base stations with a common
MSC
574 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
7. The mobile sends a handoff complete message to the new BS, which is forwarded
up to the visited MSC. The visited MSC then reroutes the ongoing call
to the mobile via the new BS.
8. The resources allocated along the path to the old BS are then released.
Let’s conclude our discussion of handoff by considering what happens when the
mobile moves to a BS that is associated with a different MSC than the old BS, and what
happens when this inter-MSC handoff occurs more than once. As shown in Figure 6.32,
GSM defines the notion of an anchor MSC. The anchor MSC is the MSC visited by
the mobile when a call first begins; the anchor MSC thus remains unchanged during
the call. Throughout the call’s duration and regardless of the number of inter-MSC
transfers performed by the mobile, the call is routed from the home MSC to the anchor
MSC, and then from the anchor MSC to the visited MSC where the mobile is currently
located. When a mobile moves from the coverage area of one MSC to another, the
ongoing call is rerouted from the anchor MSC to the new visited MSC containing the
new base station. Thus, at all times there are at most three MSCs (the home MSC, the
anchor MSC, and the visited MSC) between the correspondent and the mobile. Figure
6.32 illustrates the routing of a call among the MSCs visited by a mobile user.
Rather than maintaining a single MSC hop from the anchor MSC to the current
MSC, an alternative approach would have been to simply chain the MSCs visited by
the mobile, having an old MSC forward the ongoing call to the new MSC each time
the mobile moves to a new MSC. Such MSC chaining can in fact occur in IS-41 cellular
networks, with an optional path minimization step to remove MSCs between the
anchor MSC and the current visited MSC [Lin 2001].
Let’s wrap up our discussion of GSM mobility management with a comparison
of mobility management in GSM and Mobile IP. The comparison in Table 6.2 indicates
that although IP and cellular networks are fundamentally different in many
ways, they share a surprising number of common functional elements and overall
approaches in handling mobility.
Old
BS
New
BS
1
5
8 7
2
3
6
4
VLR
Figure 6.31  Steps in accomplishing a handoff between base stations
with a common MSC
6.8 • WIRELESS AND MOBILITY: IMPACT ON HIGHER-LAYER PROTOCOLS 575
6.8 Wireless and Mobility: Impact on Higher-
Layer Protocols
In this chapter, we’ve seen that wireless networks differ significantly from their wired
counterparts at both the link layer (as a result of wireless channel characteristics such as
fading, multipath, and hidden terminals) and at the network layer (as a result of mobile
users who change their points of attachment to the network). But are there important differences
at the transport and application layers? It’s tempting to think that these differences
will be minor, since the network layer provides the same best-effort delivery
service model to upper layers in both wired and wireless networks. Similarly, if protocols
such as TCP or UDP are used to provide transport-layer services to applications in
both wired and wireless networks, then the application layer should remain unchanged as
well. In one sense our intuition is right—TCP and UDP can (and do) operate in networks
with wireless links. On the other hand, transport protocols in general, and TCP in particular,
can sometimes have very different performance in wired and wireless networks,
and it is here, in terms of performance, that differences are manifested. Let’s see why.
Recall that TCP retransmits a segment that is either lost or corrupted on the path
between sender and receiver. In the case of mobile users, loss can result from either
Home network Correspondent
a. Before handoff
Anchor
MSC
PSTN
b. After handoff
Correspondent
Anchor
MSC
PSTN
Home network
Figure 6.32  Rerouting via the anchor MSC
576 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
network congestion (router buffer overflow) or from handoff (e.g., from delays in
rerouting segments to a mobile’s new point of attachment to the network). In all
cases, TCP’s receiver-to-sender ACK indicates only that a segment was not received
intact; the sender is unaware of whether the segment was lost due to congestion,
during handoff, or due to detected bit errors. In all cases, the sender’s response is
the same—to retransmit the segment. TCP’s congestion-control response is also the
same in all cases—TCP decreases its congestion window, as discussed in Section
3.7. By unconditionally decreasing its congestion window, TCP implicitly assumes
that segment loss results from congestion rather than corruption or handoff. We saw
in Section 6.2 that bit errors are much more common in wireless networks than in
wired networks. When such bit errors occur or when handoff loss occurs, there’s
really no reason for the TCP sender to decrease its congestion window (and thus
decrease its sending rate). Indeed, it may well be the case that router buffers are
empty and packets are flowing along the end-to-end path unimpeded by congestion.
Researchers realized in the early to mid 1990s that given high bit error rates on
wireless links and the possibility of handoff loss, TCP’s congestion-control response
could be problematic in a wireless setting. Three broad classes of approaches are
possible for dealing with this problem:
• Local recovery. Local recovery protocols recover from bit errors when and where
(e.g., at the wireless link) they occur, e.g., the 802.11 ARQ protocol we studied
GSM element Comment on GSM element Mobile IP element
Home system Network to which the mobile user’s permanent phone Home network
number belongs.
Gateway mobile switching center or Home MSC: point of contact to obtain routable address of Home agent
simply home MSC, Home mobile user. HLR: database in home system containing permanent
location register (HLR) phone number, profile information, current location of mobile user,
subscription information.
Visited system Network other than home system where mobile user is currently residing. Visited network.
Visited mobile services switching center, Visited MSC: responsible for setting up calls to/from mobile nodes Foreign agent
Visitor location register (VLR) in cells associated with MSC. VLR: temporary database entry in
visited system, containing subscription information for each
visiting mobile user.
Mobile station roaming number Routable address for telephone call segment between home MSC Care-of address
(MSRN) or simply roaming number and visited MSC, visible to neither the mobile nor the correspondent.
Table 6.2  Commonalities between mobile IP and GSM mobility
6.8 • WIRELESS AND MOBILITY: IMPACT ON HIGHER-LAYER PROTOCOLS 577
in Section 6.3, or more sophisticated approaches that use both ARQ and FEC
[Ayanoglu 1995].
• TCP sender awareness of wireless links. In the local recovery approaches, the
TCP sender is blissfully unaware that its segments are traversing a wireless link.
An alternative approach is for the TCP sender and receiver to be aware of the
existence of a wireless link, to distinguish between congestive losses occurring in
the wired network and corruption/loss occurring at the wireless link, and to
invoke congestion control only in response to congestive wired-network losses.
[Balakrishnan 1997] investigates various types of TCP, assuming that end systems
can make this distinction. [Liu 2003] investigates techniques for distinguishing
between losses on the wired and wireless segments of an end-to-end path.
• Split-connection approaches. In a split-connection approach [Bakre 1995], the
end-to-end connection between the mobile user and the other end point is broken
into two transport-layer connections: one from the mobile host to the wireless
access point, and one from the wireless access point to the other communication
end point (which we’ll assume here is a wired host). The end-to-end connection
is thus formed by the concatenation of a wireless part and a wired part. The transport
layer over the wireless segment can be a standard TCP connection [Bakre
1995], or a specially tailored error recovery protocol on top of UDP. [Yavatkar
1994] investigates the use of a transport-layer selective repeat protocol over the
wireless connection. Measurements reported in [Wei 2006] indicate that split
TCP connections are widely used in cellular data networks, and that significant
improvements can indeed be made through the use of split TCP connections.
Our treatment of TCP over wireless links has been necessarily brief here. Indepth
surveys of TCP challenges and solutions in wireless networks can be found in
[Hanabali 2005; Leung 2006]. We encourage you to consult the references for
details of this ongoing area of research.
Having considered transport-layer protocols, let us next consider the effect of
wireless and mobility on application-layer protocols. Here, an important consideration
is that wireless links often have relatively low bandwidths, as we saw in Figure
6.2. As a result, applications that operate over wireless links, particularly over cellular
wireless links, must treat bandwidth as a scarce commodity. For example, a Web
server serving content to a Web browser executing on a 3G phone will likely not be
able to provide the same image-rich content that it gives to a browser operating over
a wired connection. Although wireless links do provide challenges at the application
layer, the mobility they enable also makes possible a rich set of location-aware and
context-aware applications [Chen 2000; Baldauf 2007]. More generally, wireless
and mobile networks will play a key role in realizing the ubiquitous computing environments
of the future [Weiser 1991]. It’s fair to say that we’ve only seen the tip of
the iceberg when it comes to the impact of wireless and mobile networks on networked
applications and their protocols!
6.9 Summary
Wireless and mobile networks have revolutionized telephony and are having an
increasingly profound impact in the world of computer networks as well. With their
anytime, anywhere, untethered access into the global network infrastructure, they
are not only making network access more ubiquitous, they are also enabling an
exciting new set of location-dependent services. Given the growing importance of
wireless and mobile networks, this chapter has focused on the principles, common
link technologies, and network architectures for supporting wireless and mobile
communication.
We began this chapter with an introduction to wireless and mobile networks,
drawing an important distinction between the challenges posed by the wireless
nature of the communication links in such networks, and by the mobility that these
wireless links enable. This allowed us to better isolate, identify, and master the key
concepts in each area. We focused first on wireless communication, considering the
characteristics of a wireless link in Section 6.2. In Sections 6.3 and 6.4, we examined
the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard, two
IEEE 802.15 personal area networks (Bluetooth and Zigbee), and 3G and 4G cellular
Internet access. We then turned our attention to the issue of mobility. In Section
6.5, we identified several forms of mobility, with points along this spectrum posing
different challenges and admitting different solutions. We considered the problems
of locating and routing to a mobile user, as well as approaches for handing off the
mobile user who dynamically moves from one point of attachment to the network to
another. We examined how these issues were addressed in the mobile IP standard
and in GSM, in Sections 6.6 and 6.7, respectively. Finally, we considered the impact
of wireless links and mobility on transport-layer protocols and networked applications
in Section 6.8.
Although we have devoted an entire chapter to the study of wireless and mobile
networks, an entire book (or more) would be required to fully explore this exciting
and rapidly expanding field. We encourage you to delve more deeply into this field
by consulting the many references provided in this chapter.
Homework Problems and Questions
Chapter 6 Review Questions
SECTION 6.1
R1. What does it mean for a wireless network to be operating in “infrastructure
mode?” If the network is not in infrastructure mode, what mode of operation
is it in, and what is the difference between that mode of operation and infrastructure
mode?
578 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
R2. What are the four types of wireless networks identified in our taxonomy in
Section 6.1? Which of these types of wireless networks have you used?
SECTION 6.2
R3. What are the differences between the following types of wireless channel
impairments: path loss, multipath propagation, interference from other
sources?
R4. As a mobile node gets farther and farther away from a base station, what are
two actions that a base station could take to ensure that the loss probability of
a transmitted frame does not increase?
SECTIONS 6.3 AND 6.4
R5. Describe the role of the beacon frames in 802.11.
R6. True or false: Before an 802.11 station transmits a data frame, it must first
send an RTS frame and receive a corresponding CTS frame.
R7. Why are acknowledgments used in 802.11 but not in wired Ethernet?
R8. True or false: Ethernet and 802.11 use the same frame structure.
R9. Describe how the RTS threshold works.
R10. Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard
DATA and ACK frames. Would there be any advantage to using the CTS and
RTS frames? Why or why not?
R11. Section 6.3.4 discusses 802.11 mobility, in which a wireless station moves
from one BSS to another within the same subnet. When the APs are interconnected
with a switch, an AP may need to send a frame with a spoofed MAC
address to get the switch to forward the frame properly. Why?
R12. What are the differences between a master device in a Bluetooth network and
a base station in an 802.11 network?
R13. What is meant by a super frame in the 802.15.4 Zigbee standard?
R14. What is the role of the “core network” in the 3G cellular data architecture?
R15. What is the role of the RNC in the 3G cellular data network architecture?
What role does the RNC play in the cellular voice network?
SECTIONS 6.5 AND 6.6
R16. If a node has a wireless connection to the Internet, does that node have to be
mobile? Explain. Suppose that a user with a laptop walks around her house
with her laptop, and always accesses the Internet through the same access
point. Is this user mobile from a network standpoint? Explain.
R17. What is the difference between a permanent address and a care-of address?
Who assigns a care-of address?
HOMEWORK PROBLEMS AND QUESTIONS 579
R18. Consider a TCP connection going over Mobile IP. True or false: The TCP
connection phase between the correspondent and the mobile host goes
through the mobile’s home network, but the data transfer phase is directly
between the correspondent and the mobile host, bypassing the home network.
SECTION 6.7
R19. What are the purposes of the HLR and VLR in GSM networks? What elements
of mobile IP are similar to the HLR and VLR?
R20. What is the role of the anchor MSC in GSM networks?
SECTION 6.8
R21. What are three approaches that can be taken to avoid having a single wireless
link degrade the performance of an end-to-end transport-layer TCP connection?
Problems
P1. Consider the single-sender CDMA example in Figure 6.5. What would be the
sender’s output (for the 2 data bits shown) if the sender’s CDMA code were
(1, –1, 1, –1, 1, –1, 1, –1)?
P2. Consider sender 2 in Figure 6.6. What is the sender’s output to the channel
(before it is added to the signal from sender 1), Z2
i,m?
P3. Suppose that the receiver in Figure 6.6 wanted to receive the data being sent
by sender 2. Show (by calculation) that the receiver is indeed able to recover
sender 2’s data from the aggregate channel signal by using sender 2’s code.
P4. For the two-sender, two-receiver example, give an example of two CDMA
codes containing 1 and 1 values that do not allow the two receivers to
extract the original transmitted bits from the two CDMA senders.
P5. Suppose there are two ISPs providing WiFi access in a particular café, with
each ISP operating its own AP and having its own IP address block.
a. Further suppose that by accident, each ISP has configured its AP to operate
over channel 11. Will the 802.11 protocol completely break down in
this situation? Discuss what happens when two stations, each associated
with a different ISP, attempt to transmit at the same time.
b. Now suppose that one AP operates over channel 1 and the other over
channel 11. How do your answers change?
P6. In step 4 of the CSMA/CA protocol, a station that successfully transmits a
frame begins the CSMA/CA protocol for a second frame at step 2, rather than
at step 1. What rationale might the designers of CSMA/CA have had in mind
by having such a station not transmit the second frame immediately (if the
channel is sensed idle)?
580 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
P7. Suppose an 802.11b station is configured to always reserve the channel with the
RTS/CTS sequence. Suppose this station suddenly wants to transmit 1,000 bytes
of data, and all other stations are idle at this time. As a function of SIFS and
DIFS, and ignoring propagation delay and assuming no bit errors, calculate the
time required to transmit the frame and receive the acknowledgment.
P8. Consider the scenario shown in Figure 6.33, in which there are four wireless
nodes, A, B, C, and D. The radio coverage of the four nodes is shown via the
shaded ovals; all nodes share the same frequency. When A transmits, it can
only be heard/received by B; when B transmits, both A and C can
hear/receive from B; when C transmits, both B and D can hear/receive from
C; when D transmits, only C can hear/receive from D.
Suppose now that each node has an infinite supply of messages that it wants
to send to each of the other nodes. If a message’s destination is not an immediate
neighbor, then the message must be relayed. For example, if A wants to
send to D, a message from A must first be sent to B, which then sends the
message to C, which then sends the message to D. Time is slotted, with a
message transmission time taking exactly one time slot, e.g., as in slotted
Aloha. During a slot, a node can do one of the following: (i) send a message;
(ii) receive a message (if exactly one message is being sent to it), (iii) remain
silent. As always, if a node hears two or more simultaneous transmissions, a
collision occurs and none of the transmitted messages are received successfully.
You can assume here that there are no bit-level errors, and thus if
exactly one message is sent, it will be received correctly by those within the
transmission radius of the sender.
a. Suppose now that an omniscient controller (i.e., a controller that knows
the state of every node in the network) can command each node to do
whatever it (the omniscient controller) wishes, i.e., to send a message, to
receive a message, or to remain silent. Given this omniscient controller,
what is the maximum rate at which a data message can be transferred
from C to A, given that there are no other messages between any other
source/destination pairs?
PROBLEMS 581
A B C D
Figure 6.33  Scenario for problem P8
b. Suppose now that A sends messages to B, and D sends messages to C.
What is the combined maximum rate at which data messages can flow
from A to B and from D to C?
c. Suppose now that A sends messages to B, and C sends messages to D.
What is the combined maximum rate at which data messages can flow
from A to B and from C to D?
d. Suppose now that the wireless links are replaced by wired links. Repeat
questions (a) through (c) again in this wired scenario.
e. Now suppose we are again in the wireless scenario, and that for every data
message sent from source to destination, the destination will send an ACK
message back to the source (e.g., as in TCP). Also suppose that each ACK
message takes up one slot. Repeat questions (a) – (c) above for this scenario.
P9. Describe the format of the 802.15.1 Bluetooth frame. You will have to do
some reading outside of the text to find this information. Is there anything in
the frame format that inherently limits the number of active nodes in an
802.15.1 network to eight active nodes? Explain.
P10. Consider the following idealized LTE scenario. The downstream channel
(see Figure 6.20) is slotted in time, across F frequencies. There are four
nodes, A, B, C, and D, reachable from the base station at rates of 10 Mbps,
5 Mbps, 2.5 Mbps, and 1 Mbps, respectively, on the downstream channel.
These rates assume that the base station utilizes all time slots available on
all F frequencies to send to just one station. The base station has an infinite
amount of data to send to each of the nodes, and can send to any one of
these four nodes using any of the F frequencies during any time slot in the
downstream sub-frame.
a. What is the maximum rate at which the base station can send to the nodes,
assuming it can send to any node it chooses during each time slot? Is your
solution fair? Explain and define what you mean by “fair.”
b. If there is a fairness requirement that each node must receive an equal
amount of data during each one second interval, what is the average
transmission rate by the base station (to all nodes) during the downstream
sub-frame? Explain how you arrived at your answer.
c. Suppose that the fairness criterion is that any node can receive at most
twice as much data as any other node during the sub-frame. What is the
average transmission rate by the base station (to all nodes) during the subframe?
Explain how you arrived at your answer.
P11. In Section 6.5, one proposed solution that allowed mobile users to maintain
their IP addresses as they moved among foreign networks was to have a foreign
network advertise a highly specific route to the mobile user and use the
existing routing infrastructure to propagate this information throughout the
582 CHAPTER 6 • WIRELESS AND MOBILE NETWORKS
network. We identified scalability as one concern. Suppose that when a
mobile user moves from one network to another, the new foreign network
advertises a specific route to the mobile user, and the old foreign network
withdraws its route. Consider how routing information propagates in a
distance-vector algorithm (particularly for the case of interdomain routing
among networks that span the globe).
a. Will other routers be able to route datagrams immediately to the new foreign
network as soon as the foreign network begins advertising its route?
b. Is it possible for different routers to believe that different foreign networks
contain the mobile user?
c. Discuss the timescale over which other routers in the network will eventually
learn the path to the mobile users.
P12. Suppose the correspondent in Figure 6.22 were mobile. Sketch the additional
network-layer infrastructure that would be needed to route the datagram from
the original mobile user to the (now mobile) correspondent. Show the structure
of the datagram(s) between the original mobile user and the (now
mobile) correspondent, as in Figure 6.23.
P13. In mobile IP, what effect will mobility have on end-to-end delays of datagrams
between the source and destination?
P14. Consider the chaining example discussed at the end of Section 6.7.2. Suppose
a mobile user visits foreign networks A, B, and C, and that a correspondent
begins a connection to the mobile user when it is resident in foreign network A.
List the sequence of messages between foreign agents, and between foreign
agents and the home agent as the mobile user moves from network A to network
B to network C. Next, suppose chaining is not performed, and the correspondent
(as well as the home agent) must be explicitly notified of the
changes in the mobile user’s care-of address. List the sequence of messages
that would need to be exchanged in this second scenario.
P15. Consider two mobile nodes in a foreign network having a foreign agent. Is it
possible for the two mobile nodes to use the same care-of address in mobile
IP? Explain your answer.
P16. In our discussion of how the VLR updated the HLR with information about
the mobile’s current location, what are the advantages and disadvantages of
providing the MSRN as opposed to the address of the VLR to the HLR?
Wireshark Lab
At the companion Web site for this textbook, http://www.awl.com/kurose-ross,
you’ll find a Wireshark lab for this chapter that captures and studies the 802.11
frames exchanged between a wireless laptop and an access point.
WIRESHARK LAB 583
Please describe a few of the most exciting projects you have worked on during your
career. What were the biggest challenges?
In the mid-90s at USC and ISI, I had the great fortune to work with the likes of Steve
Deering, Mark Handley, and Van Jacobson on the design of multicast routing protocols (in
particular, PIM). I tried to carry many of the architectural design lessons from multicast into
the design of ecological monitoring arrays, where for the first time I really began to take
applications and multidisciplinary research seriously. That interest in jointly innovating in
the social and technological space is what interests me so much about my latest area of
research, mobile health. The challenges in these projects were as diverse as the problem
domains, but what they all had in common was the need to keep our eyes open to whether
we had the problem definition right as we iterated between design and deployment, prototype
and pilot. None of them were problems that could be solved analytically, with simulation
or even in constructed laboratory experiments. They all challenged our ability to retain
584
Deborah Estrin
Deborah Estrin is Professor of Computer Science at UCLA, the
Jon Postel Chair in Computer Networks, Director of the Center for
Embedded Networked Sensing (CENS), and co-founder of the nonprofit
openmhealth.org. She received her Ph.D. (1985) in Computer
Science from M.I.T., and her B.S. (1980) from UC Berkeley. Estrin’s
early research focused on the design of network protocols, including
multicast and inter-domain routing. In 2002 Estrin founded the
NSF-funded Science and Technology Center, CENS (http://cens
.ucla.edu), to develop and explore environmental monitoring technologies and applications.
Currently Estrin and collaborators are developing participatory sensing systems, leveraging
the programmability, proximity, and pervasiveness of mobile phones; the primary deployment
contexts are mobile health (http://openmhealth.org), community data gathering, and STEM
education (http://mobilizingcs.org). Professor Estrin is an elected member of the American
Academy of Arts and Sciences (2007) and the National Academy of Engineering (2009).
She is a fellow of the IEEE, ACM, and AAAS. She was selected as the first ACM-W Athena
Lecturer (2006), awarded the Anita Borg Institute’s Women of Vision Award for Innovation
(2007), inducted into the WITI hall of fame (2008) and awarded Doctor Honoris Causa from
EPFL (2008) and Uppsala University (2011).
AN INTERVIEW WITH...
clean architectures in the presence of messy problems and contexts, and they all called for
extensive collaboration.
What changes and innovations do you see happening in wireless networks and
mobility in the future?
I have never put much faith into predicting the future, but I would say we might see the end
of feature phones (i.e., those that are not programmable and are used only for voice and text
messaging) as smart phones become more and more powerful and the primary point of
Internet access for many. I also think that we will see the continued proliferation of embedded
SIMs by which all sorts of devices have the ability to communicate via the cellular network
at low data rates.
Where do you see the future of networking and the Internet?
The efforts in named data and software-defined networking will emerge to create a more
manageable, evolvable, and richer infrastructure and more generally represent moving the
role of architecture higher up in the stack. In the beginnings of the Internet, architecture was
layer 4 and below, with applications being more siloed/monolithic, sitting on top. Now data
and analytics dominate transport.
What people inspired you professionally?
There are three people who come to mind. First, Dave Clark, the secret sauce and unsung
hero of the Internet community. I was lucky to be around in the early days to see him act as
the “organizing principle” of the IAB and Internet governance; the priest of rough consensus
and running code. Second, Scott Shenker, for his intellectual brilliance, integrity, and
persistence. I strive for, but rarely attain, his clarity in defining problems and solutions. He
is always the first person I email for advice on matters large and small. Third, my sister
Judy Estrin, who had the creativity and courage to spend her career bringing ideas and concepts
to market. Without the Judys of the world the Internet technologies would never have
transformed our lives.
What are your recommendations for students who want careers in computer science
and networking?
First, build a strong foundation in your academic work, balanced with any and every realworld
work experience you can get. As you look for a working environment, seek opportunities
in problem areas you really care about and with smart teams that you can learn from.
585
This page intentionally left blank
CHAPTER 7
Multimedia
Networking
587
People in all corners of the world are currently using the Internet to watch movies
and television shows on demand. Internet movie and television distribution companies
such as Netflix and Hulu in North America and Youku and Kankan in China
have practically become household names. But people are not only watching
Internet videos, they are using sites like YouTube to upload and distribute their own
user-generated content, becoming Internet video producers as well as consumers.
Moreover, network applications such as Skype, Google Talk, and QQ (enormously
popular in China) allow people to not only make “telephone calls” over the Internet,
but to also enhance those calls with video and multi-person conferencing. In
fact, we can safely predict that by the end of the current decade almost all video distribution
and voice conversations will take place end-to-end over the Internet, often
to wireless devices connected to the Internet via 4G and WiFi access networks.
We begin this chapter with a taxonomy of multimedia applications in Section 7.1.
We’ll see that a multimedia application can be classified as either streaming stored
audio/video, conversational voice/video-over-IP, or streaming live audio/video.
We’ll see that each of these classes of applications has its own unique service
requirements that differ significantly from those of traditional elastic applications
such as e-mail, Web browsing, and remote login. In Section 7.2, we’ll examine
video streaming in some detail. We’ll explore many of the underlying principles
behind video streaming, including client buffering, prefetching, and adapting video
quality to available bandwidth. We will also investigate Content Distribution Networks
(CDNs), which are used extensively today by the leading video streaming
systems. We then examine the YouTube, Netflix, and Kankan systems as case
studies for streaming video. In Section 7.3, we investigate conversational voice and
video, which, unlike elastic applications, are highly sensitive to end-to-end delay
but can tolerate occasional loss of data. Here we’ll examine how techniques such as
adaptive playout, forward error correction, and error concealment can mitigate
against network-induced packet loss and delay. We’ll also examine Skype as a case
study. In Section 7.4, we’ll study RTP and SIP, two popular protocols for real-time
conversational voice and video applications. In Section 7.5, we’ll investigate mechanisms
within the network that can be used to distinguish one class of traffic (e.g.,
delay-sensitive applications such as conversational voice) from another (e.g., elastic
applications such as browsing Web pages), and provide differentiated service among
multiple classes of traffic.
7.1 Multimedia Networking Applications
We define a multimedia network application as any network application that
employs audio or video. In this section, we provide a taxonomy of multimedia applications.
We’ll see that each class of applications in the taxonomy has its own unique
set of service requirements and design issues. But before diving into an in-depth discussion
of Internet multimedia applications, it is useful to consider the intrinsic
characteristics of the audio and video media themselves.
7.1.1 Properties of Video
Perhaps the most salient characteristic of video is its high bit rate. Video distributed
over the Internet typically ranges from 100 kbps for low-quality video conferencing
to over 3 Mbps for streaming high-definition movies. To get a sense of how
video bandwidth demands compare with those of other Internet applications, let’s
briefly consider three different users, each using a different Internet application. Our
first user, Frank, is going quickly through photos posted on his friends’ Facebook
pages. Let’s assume that Frank is looking at a new photo every 10 seconds, and that
photos are on average 200 Kbytes in size. (As usual, throughout this discussion we
make the simplifying assumption that 1 Kbyte = 8,000 bits.) Our second user,
Martha, is streaming music from the Internet (“the cloud”) to her smartphone. Let’s
assume Martha is listening to many MP3 songs, one after the other, each encoded at
a rate of 128 kbps. Our third user, Victor, is watching a video that has been encoded
at 2 Mbps. Finally, let’s suppose that the session length for all three users is 4,000
seconds (approximately 67 minutes). Table 7.1 compares the bit rates and the total
bytes transferred for these three users. We see that video streaming consumes by far
588 CHAPTER 7 • MULTIMEDIA NETWORKING
the most bandwidth, having a bit rate of more than ten times greater than that of the
Facebook and music-streaming applications. Therefore, when designing networked
video applications, the first thing we must keep in mind is the high bit-rate requirements
of video. Given the popularity of video and its high bit rate, it is perhaps not
surprising that Cisco predicts [Cisco 2011] that streaming and stored video will be
approximately 90 percent of global consumer Internet traffic by 2015.
Another important characteristic of video is that it can be compressed, thereby
trading off video quality with bit rate. A video is a sequence of images, typically
being displayed at a constant rate, for example, at 24 or 30 images per second. An
uncompressed, digitally encoded image consists of an array of pixels, with each
pixel encoded into a number of bits to represent luminance and color. There are two
types of redundancy in video, both of which can be exploited by video compression.
Spatial redundancy is the redundancy within a given image. Intuitively, an
image that consists of mostly white space has a high degree of redundancy and can
be efficiently compressed without significantly sacrificing image quality. Temporal
redundancy reflects repetition from image to subsequent image. If, for example, an
image and the subsequent image are exactly the same, there is no reason to reencode
the subsequent image; it is instead more efficient simply to indicate during
encoding that the subsequent image is exactly the same. Today’s off-the-shelf compression
algorithms can compress a video to essentially any bit rate desired. Of
course, the higher the bit rate, the better the image quality and the better the overall
user viewing experience.
We can also use compression to create multiple versions of the same video,
each at a different quality level. For example, we can use compression to create, say,
three versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users
can then decide which version they want to watch as a function of their current
available bandwidth. Users with high-speed Internet connections might choose the
3 Mbps version; users watching the video over 3G with a smartphone might choose
the 300 kbps version. Similarly, the video in a video conference application can be
compressed “on-the-fly” to provide the best video quality given the available endto-
end bandwidth between conversing users.
7.1 • MULTIMEDIA NETWORKING APPLICATIONS 589
Bit rate Bytes transferred in 67 min
Facebook Frank 160 kbps 80 Mbytes
Martha Music 128 kbps 64 Mbytes
Victor Video 2 Mbps 1 Gbyte
Table 7.1  Comparison of bit-rate requirements of three Internet applications
7.1.2 Properties of Audio
Digital audio (including digitized speech and music) has significantly lower bandwidth
requirements than video. Digital audio, however, has its own unique properties
that must be considered when designing multimedia network applications. To
understand these properties, let’s first consider how analog audio (which humans
and musical instruments generate) is converted to a digital signal:
• The analog audio signal is sampled at some fixed rate, for example, at 8,000
samples per second. The value of each sample is an arbitrary real number.
• Each of the samples is then rounded to one of a finite number of values. This
operation is referred to as quantization. The number of such finite values—
called quantization values—is typically a power of two, for example, 256 quantization
values.
• Each of the quantization values is represented by a fixed number of bits. For
example, if there are 256 quantization values, then each value—and hence each
audio sample—is represented by one byte. The bit representations of all the samples
are then concatenated together to form the digital representation of the signal.
As an example, if an analog audio signal is sampled at 8,000 samples per
second and each sample is quantized and represented by 8 bits, then the resulting
digital signal will have a rate of 64,000 bits per second. For playback through
audio speakers, the digital signal can then be converted back—that is, decoded—
to an analog signal. However, the decoded analog signal is only an approximation
of the original signal, and the sound quality may be noticeably degraded (for
example, high-frequency sounds may be missing in the decoded signal). By
increasing the sampling rate and the number of quantization values, the decoded
signal can better approximate the original analog signal. Thus (as with video),
there is a trade-off between the quality of the decoded signal and the bit-rate and
storage requirements of the digital signal.
The basic encoding technique that we just described is called pulse code modulation
(PCM). Speech encoding often uses PCM, with a sampling rate of 8,000 samples per
second and 8 bits per sample, resulting in a rate of 64 kbps. The audio compact disk
(CD) also uses PCM, with a sampling rate of 44,100 samples per second with 16 bits
per sample; this gives a rate of 705.6 kbps for mono and 1.411 Mbps for stereo.
PCM-encoded speech and music, however, are rarely used in the Internet.
Instead, as with video, compression techniques are used to reduce the bit rates of the
stream. Human speech can be compressed to less than 10 kbps and still be intelligible.
A popular compression technique for near CD-quality stereo music is MPEG 1
layer 3, more commonly known as MP3. MP3 encoders can compress to many different
rates; 128 kbps is the most common encoding rate and produces very little
sound degradation. A related standard is Advanced Audio Coding (AAC), which
has been popularized by Apple. As with video, multiple versions of a prerecorded
audio stream can be created, each at a different bit rate.
590 CHAPTER 7 • MULTIMEDIA NETWORKING
Although audio bit rates are generally much less than those of video, users are
generally much more sensitive to audio glitches than video glitches. Consider, for
example, a video conference taking place over the Internet. If, from time to time, the
video signal is lost for a few seconds, the video conference can likely proceed without
too much user frustration. If, however, the audio signal is frequently lost, the
users may have to terminate the session.
7.1.3 Types of Multimedia Network Applications
The Internet supports a large variety of useful and entertaining multimedia applications.
In this subsection, we classify multimedia applications into three broad categories:
(i) streaming stored audio/video, (ii) conversational voice/video-over-IP,
and (iii) streaming live audio/video. As we will soon see, each of these application
categories has its own set of service requirements and design issues.
Streaming Stored Audio and Video
To keep the discussion concrete, we focus here on streaming stored video, which
typically combines video and audio components. Streaming stored audio (such as
streaming music) is very similar to streaming stored video, although the bit rates are
typically much lower.
In this class of applications, the underlying medium is prerecorded video, such
as a movie, a television show, a prerecorded sporting event, or a prerecorded usergenerated
video (such as those commonly seen on YouTube). These prerecorded
videos are placed on servers, and users send requests to the servers to view the videos
on demand. Many Internet companies today provide streaming video, including
YouTube (Google), Netflix, and Hulu. By some estimates, streaming stored video
makes up over 50 percent of the downstream traffic in the Internet access networks
today [Cisco 2011]. Streaming stored video has three key distinguishing features.
• Streaming. In a streaming stored video application, the client typically begins
video playout within a few seconds after it begins receiving the video from the
server. This means that the client will be playing out from one location in
the video while at the same time receiving later parts of the video from the
server. This technique, known as streaming, avoids having to download
the entire video file (and incurring a potentially long delay) before playout begins.
• Interactivity. Because the media is prerecorded, the user may pause, reposition
forward, reposition backward, fast-forward, and so on through the video content.
The time from when the user makes such a request until the action manifests itself
at the client should be less than a few seconds for acceptable responsiveness.
• Continuous playout. Once playout of the video begins, it should proceed
according to the original timing of the recording. Therefore, data must be
received from the server in time for its playout at the client; otherwise, users
7.1 • MULTIMEDIA NETWORKING APPLICATIONS 591
experience video frame freezing (when the client waits for the delayed frames)
or frame skipping (when the client skips over delayed frames).
By far, the most important performance measure for streaming video is average
throughput. In order to provide continuous playout, the network must provide an
average throughput to the streaming application that is at least as large the bit rate of
the video itself. As we will see in Section 7.2, by using buffering and prefetching, it
is possible to provide continuous playout even when the throughput fluctuates, as
long as the average throughput (averaged over 5–10 seconds) remains above the
video rate [Wang 2008].
For many streaming video applications, prerecorded video is stored on, and
streamed from, a CDN rather than from a single data center. There are also many
P2P video streaming applications for which the video is stored on users’ hosts
(peers), with different chunks of video arriving from different peers that may spread
around the globe. Given the prominence of Internet video streaming, we will
explore video streaming in some depth in Section 7.2, paying particular attention to
client buffering, prefetching, adapting quality to bandwidth availability, and CDN
distribution.
Conversational Voice- and Video-over-IP
Real-time conversational voice over the Internet is often referred to as Internet
telephony, since, from the user’s perspective, it is similar to the traditional circuitswitched
telephone service. It is also commonly called Voice-over-IP (VoIP). Conversational
video is similar, except that it includes the video of the participants as
well as their voices. Most of today’s voice and video conversational systems allow
users to create conferences with three or more participants. Conversational voice
and video are widely used in the Internet today, with the Internet companies Skype,
QQ, and Google Talk boasting hundreds of millions of daily users.
In our discussion of application service requirements in Chapter 2 (Figure 2.4),
we identified a number of axes along which application requirements can be classified.
Two of these axes—timing considerations and tolerance of data loss—are particularly
important for conversational voice and video applications. Timing
considerations are important because audio and video conversational applications
are highly delay-sensitive. For a conversation with two or more interacting speakers,
the delay from when a user speaks or moves until the action is manifested at the
other end should be less than a few hundred milliseconds. For voice, delays smaller
than 150 milliseconds are not perceived by a human listener, delays between 150
and 400 milliseconds can be acceptable, and delays exceeding 400 milliseconds can
result in frustrating, if not completely unintelligible, voice conversations.
On the other hand, conversational multimedia applications are loss-tolerant—
occasional loss only causes occasional glitches in audio/video playback, and these
losses can often be partially or fully concealed. These delay-sensitive but loss-tolerant
592 CHAPTER 7 • MULTIMEDIA NETWORKING
characteristics are clearly different from those of elastic data applications such as Web
browsing, e-mail, social networks, and remote login. For elastic applications, long
delays are annoying but not particularly harmful; the completeness and integrity of the
transferred data, however, are of paramount importance. We will explore conversational
voice and video in more depth in Section 7.3, paying particular attention to how
adaptive playout, forward error correction, and error concealment can mitigate against
network-induced packet loss and delay.
Streaming Live Audio and Video
This third class of applications is similar to traditional broadcast radio and television,
except that transmission takes place over the Internet. These applications allow
a user to receive a live radio or television transmission—such as a live sporting
event or an ongoing news event—transmitted from any corner of the world. Today,
thousands of radio and television stations around the world are broadcasting content
over the Internet.
Live, broadcast-like applications often have many users who receive the same
audio/video program at the same time. Although the distribution of live audio/video
to many receivers can be efficiently accomplished using the IP multicasting techniques
described in Section 4.7, multicast distribution is more often accomplished
today via application-layer multicast (using P2P networks or CDNs) or through multiple
separate unicast streams. As with streaming stored multimedia, the network
must provide each live multimedia flow with an average throughput that is larger
than the video consumption rate. Because the event is live, delay can also be an issue,
although the timing constraints are much less stringent than those for conversational
voice. Delays of up to ten seconds or so from when the user chooses to view a live
transmission to when playout begins can be tolerated. We will not cover streaming
live media in this book because many of the techniques used for streaming live
media—initial buffering delay, adaptive bandwidth use, and CDN distribution—are
similar to those for streaming stored media.
7.2 Streaming Stored Video
For streaming video applications, prerecorded videos are placed on servers, and
users send requests to these servers to view the videos on demand. The user may
watch the video from beginning to end without interruption, may stop watching the
video well before it ends, or interact with the video by pausing or repositioning to a
future or past scene. Streaming video systems can be classified into three categories:
UDP streaming, HTTP streaming, and adaptive HTTP streaming. Although all
three types of systems are used in practice, the majority of today’s systems employ
HTTP streaming and adaptive HTTP streaming.
7.2 • STREAMING STORED VIDEO 593
A common characteristic of all three forms of video streaming is the extensive
use of client-side application buffering to mitigate the effects of varying end-to-end
delays and varying amounts of available bandwidth between server and client. For
streaming video (both stored and live), users generally can tolerate a small severalsecond
initial delay between when the client requests a video and when video playout
begins at the client. Consequently, when the video starts to arrive at the client,
the client need not immediately begin playout, but can instead build up a reserve of
video in an application buffer. Once the client has built up a reserve of several seconds
of buffered-but-not-yet-played video, the client can then begin video playout.
There are two important advantages provided by such client buffering. First, clientside
buffering can absorb variations in server-to-client delay. If a particular piece of
video data is delayed, as long as it arrives before the reserve of received-but-notyet-
played video is exhausted, this long delay will not be noticed. Second, if the
server-to-client bandwidth briefly drops below the video consumption rate, a user
can continue to enjoy continuous playback, again as long as the client application
buffer does not become completely drained.
Figure 7.1 illustrates client-side buffering. In this simple example, suppose that
video is encoded at a fixed bit rate, and thus each video block contains video frames
that are to be played out over the same fixed amount of time, . The server transmits
the first video block at , the second block at , the third block at
, and so on. Once the client begins playout, each block should be played out
time units after the previous block in order to reproduce the timing of the original
recorded video. Because of the variable end-to-end network delays, different video
blocks experience different delays. The first video block arrives at the client at t1
and the second block arrives at . The network delay for the ith block is the horizontal
distance between the time the block was transmitted by the server and the
t2

t0 + 2
t0 t0 + 

594 CHAPTER 7 • MULTIMEDIA NETWORKING
Variable
network
delay
Client
playout
delay
Constant bit
rate video
transmission
by server
1 2 3 4 5 6 7 8 9
10
11
12
Constant bit
rate video
playout
by client
Time
Video block number
t0 t0+2? t1 t2 t3
t0+? t1+? t3+?
Video
reception
at client
Figure 7.1  Client playout delay in video streaming
time it is received at the client; note that the network delay varies from one video
block to another. In this example, if the client were to begin playout as soon as the
first block arrived at , then the second block would not have arrived in time to be
played out at out at . In this case, video playout would either have to stall
(waiting for block 1 to arrive) or block 1 could be skipped—both resulting in undesirable
playout impairments. Instead, if the client were to delay the start of playout
until , when blocks 1 through 6 have all arrived, periodic playout can proceed with
all blocks having been received before their playout time.
7.2.1 UDP Streaming
We only briefly discuss UDP streaming here, referring the reader to more in-depth discussions
of the protocols behind these systems where appropriate. With UDP streaming,
the server transmits video at a rate that matches the client’s video consumption rate
by clocking out the video chunks over UDP at a steady rate. For example, if the video
consumption rate is 2 Mbps and each UDP packet carries 8,000 bits of video, then the
server would transmit one UDP packet into its socket every (8000 bits)/(2 Mbps) =
4 msec. As we learned in Chapter 3, because UDP does not employ a congestion-control
mechanism, the server can push packets into the network at the consumption rate of the
video without the rate-control restrictions of TCP. UDP streaming typically uses a small
client-side buffer, big enough to hold less than a second of video.
Before passing the video chunks to UDP, the server will encapsulate the video
chunks within transport packets specially designed for transporting audio and video,
using the Real-Time Transport Protocol (RTP) [RFC 3550] or a similar (possibly
proprietary) scheme. We delay our coverage of RTP until Section 7.3, where we discuss
RTP in the context of conversational voice and video systems.
Another distinguishing property of UDP streaming is that in addition to the serverto-
client video stream, the client and server also maintain, in parallel, a separate control
connection over which the client sends commands regarding session state changes
(such as pause, resume, reposition, and so on). This control connection is in many ways
analogous to the FTP control connection we studied in Chapter 2. The Real-Time
Streaming Protocol (RTSP) [RFC 2326], explained in some detail in the companion
Web site for this textbook, is a popular open protocol for such a control connection.
Although UDP streaming has been employed in many open-source systems and
proprietary products, it suffers from three significant drawbacks. First, due to the
unpredictable and varying amount of available bandwidth between server and client,
constant-rate UDP streaming can fail to provide continuous playout. For example,
consider the scenario where the video consumption rate is 1 Mbps and the serverto-
client available bandwidth is usually more than 1 Mbps, but every few minutes
the available bandwidth drops below 1 Mbps for several seconds. In such a scenario,
a UDP streaming system that transmits video at a constant rate of 1 Mbps over
RTP/UDP would likely provide a poor user experience, with freezing or skipped
frames soon after the available bandwidth falls below 1 Mbps. The second drawback
t3
t1 + 
t1
7.2 • STREAMING STORED VIDEO 595
of UDP streaming is that it requires a media control server, such as an RTSP server,
to process client-to-server interactivity requests and to track client state (e.g., the
client’s playout point in the video, whether the video is being paused or played, and
so on) for each ongoing client session. This increases the overall cost and complexity
of deploying a large-scale video-on-demand system. The third drawback is that
many firewalls are configured to block UDP traffic, preventing the users behind
these firewalls from receiving UDP video.
7.2.2 HTTP Streaming
In HTTP streaming, the video is simply stored in an HTTP server as an ordinary file
with a specific URL. When a user wants to see the video, the client establishes a
TCP connection with the server and issues an HTTP GET request for that URL. The
server then sends the video file, within an HTTP response message, as quickly as
possible, that is, as quickly as TCP congestion control and flow control will allow.
On the client side, the bytes are collected in a client application buffer. Once the
number of bytes in this buffer exceeds a predetermined threshold, the client application
begins playback—specifically, it periodically grabs video frames from
the client application buffer, decompresses the frames, and displays them on the
user’s screen.
We learned in Chapter 3 that when transferring a file over TCP, the server-toclient
transmission rate can vary significantly due to TCP’s congestion control mechanism.
In particular, it is not uncommon for the transmission rate to vary in a
“saw-tooth” manner (for example, Figure 3.53) associated with TCP congestion control.
Furthermore, packets can also be significantly delayed due to TCP’s retransmission
mechanism. Because of these characteristics of TCP, the conventional wisdom in
the 1990s was that video streaming would never work well over TCP. Over time, however,
designers of streaming video systems learned that TCP’s congestion control and
reliable-data transfer mechanisms do not necessarily preclude continuous playout
when client buffering and prefetching (discussed in the next section) are used.
The use of HTTP over TCP also allows the video to traverse firewalls and NATs
more easily (which are often configured to block most UDP traffic but to allow most
HTTP traffic). Streaming over HTTP also obviates the need for a media control
server, such as an RTSP server, reducing the cost of a large-scale deployment over
the Internet. Due to all of these advantages, most video streaming applications
today—including YouTube and Netflix—use HTTP streaming (over TCP) as its
underlying streaming protocol.
Prefetching Video
We just learned, client-side buffering can be used to mitigate the effects of varying
end-to-end delays and varying available bandwidth. In our earlier example in
Figure 7.1, the server transmits video at the rate at which the video is to be played
596 CHAPTER 7 • MULTIMEDIA NETWORKING
7.2 • STREAMING STORED VIDEO 597
out. However, for streaming stored video, the client can attempt to download the
video at a rate higher than the consumption rate, thereby prefetching video
frames that are to be consumed in the future. This prefetched video is naturally
stored in the client application buffer. Such prefetching occurs naturally with TCP
streaming, since TCP’s congestion avoidance mechanism will attempt to use all of
the available bandwidth between server and client.
To gain some insight into prefetching, let’s take a look at a simple example.
Suppose the video consumption rate is 1 Mbps but the network is capable of delivering
the video from server to client at a constant rate of 1.5 Mbps. Then the client
will not only be able to play out the video with a very small playout delay, but will
also be able to increase the amount of buffered video data by 500 Kbits every
second. In this manner, if in the future the client receives data at a rate of less than 1
Mbps for a brief period of time, the client will be able to continue to provide continuous
playback due to the reserve in its buffer. [Wang 2008] shows that when the
average TCP throughput is roughly twice the media bit rate, streaming over TCP
results in minimal starvation and low buffering delays.
Client Application Buffer and TCP Buffers
Figure 7.2 illustrates the interaction between client and server for HTTP streaming.
At the server side, the portion of the video file in white has already been sent into
the server’s socket, while the darkened portion is what remains to be sent. After
“passing through the socket door,” the bytes are placed in the TCP send buffer
before being transmitted into the Internet, as described in Chapter 3. In Figure 7.2,
Video file
Web server
Client
TCP send
buffer
TCP receive
buffer
TCP application
buffer
Frames read
out periodically
from buffer,
decompressed,
and displayed
on screen
Figure 7.2  Streaming stored video over HTTP/TCP
because the TCP send buffer is shown to be full, the server is momentarily prevented
from sending more bytes from the video file into the socket. On the client side, the
client application (media player) reads bytes from the TCP receive buffer (through
its client socket) and places the bytes into the client application buffer. At the same
time, the client application periodically grabs video frames from the client application
buffer, decompresses the frames, and displays them on the user’s screen. Note that
if the client application buffer is larger than the video file, then the whole process of
moving bytes from the server’s storage to the client’s application buffer is equivalent
to an ordinary file download over HTTP—the client simply pulls the video off
the server as fast as TCP will allow!
Consider now what happens when the user pauses the video during the
streaming process. During the pause period, bits are not removed from the client
application buffer, even though bits continue to enter the buffer from the server. If
the client application buffer is finite, it may eventually become full, which will
cause “back pressure” all the way back to the server. Specifically, once the client
application buffer becomes full, bytes can no longer be removed from the
client TCP receive buffer, so it too becomes full. Once the client receive TCP buffer
becomes full, bytes can no longer be removed from the client TCP send buffer, so
it also becomes full. Once the TCP send buffer becomes full, the server cannot send
any more bytes into the socket. Thus, if the user pauses the video, the server may
be forced to stop transmitting, in which case the server will be blocked until the
user resumes the video.
In fact, even during regular playback (that is, without pausing), if the client
application buffer becomes full, back pressure will cause the TCP buffers to
become full, which will force the server to reduce its rate. To determine the
resulting rate, note that when the client application removes f bits, it creates room
for f bits in the client application buffer, which in turn allows the server to send f
additional bits. Thus, the server send rate can be no higher than the video consumption
rate at the client. Therefore, a full client application buffer indirectly
imposes a limit on the rate that video can be sent from server to client when
streaming over HTTP.
Analysis of Video Streaming
Some simple modeling will provide more insight into initial playout delay and
freezing due to application buffer depletion. As shown in Figure 7.3, let B denote
the size (in bits) of the client’s application buffer, and let Q denote the number of
bits that must be buffered before the client application begins playout. (Of course,
Q < B.) Let r denote the video consumption rate—the rate at which the client draws
bits out of the client application buffer during playback. So, for example, if the
video’s frame rate is 30 frames/sec, and each (compressed) frame is 100,000 bits,
then r = 3 Mbps. To see the forest through the trees, we’ll ignore TCP’s send and
receive buffers.
598 CHAPTER 7 • MULTIMEDIA NETWORKING
Let’s assume that the server sends bits at a constant rate x whenever the client
buffer is not full. (This is a gross simplification, since TCP’s send rate varies due to
congestion control; we’ll examine more realistic time-dependent rates x(t) in the
problems at the end of this chapter.) Suppose at time t = 0, the application buffer is
empty and video begins arriving to the client application buffer. We now ask at what
time does playout begin? And while we are at it, at what time does the
client application buffer become full?
First, let’s determine , the time when Q bits have entered the application
buffer and playout begins. Recall that bits arrive to the client application buffer at
rate x and no bits are removed from this buffer before playout begins. Thus, the
amount of time required to build up Q bits (the initial buffering delay) is Q/x.
Now let’s determine , the point in time when the client application buffer
becomes full. We first observe that if x < r (that is, if the server send rate is less than
the video consumption rate), then the client buffer will never become full! Indeed,
starting at time , the buffer will be depleted at rate r and will only be filled at rate
x < r. Eventually the client buffer will empty out entirely, at which time the video
will freeze on the screen while the client buffer waits another seconds to build up
Q bits of video. Thus, when the available rate in the network is less than the video
rate, playout will alternate between periods of continuous playout and periods of
freezing. In a homework problem, you will be asked to determine the length of each
continuous playout and freezing period as a function of Q, r, and x. Now let’s determine
for when x > r. In this case, starting at time , the buffer increases from Q to
B at rate x rsince bits are being depleted at rate r but are arriving at rate x, as
shown in Figure 7.3. Given these hints, you will be asked in a homework problem
to determine , the time the client buffer becomes full. Note that when the available
rate in the network is more than the video rate, after the initial buffering delay, the
user will enjoy continuous playout until the video ends.
tf
-
tf tp
tp
tp
tf
tp =
tp
t = tp t = tf
7.2 • STREAMING STORED VIDEO 599
Fill rate = x Depletion rate = r
Video
server
Internet
Q
B
Client application buffer
Figure 7.3  Analysis of client-side buffering for video streaming
Early Termination and Repositioning the Video
HTTP streaming systems often make use of the HTTP byte-range header in the
HTTP GET request message, which specifies the specific range of bytes the client
currently wants to retrieve from the desired video. This is particularly useful when
the user wants to reposition (that is, jump) to a future point in time in the video.
When the user repositions to a new position, the client sends a new HTTP request,
indicating with the byte-range header from which byte in the file should the server
send data. When the server receives the new HTTP request, it can forget about any
earlier request and instead send bytes beginning with the byte indicated in the byterange
request.
While we are on the subject of repositioning, we briefly mention that when a user
repositions to a future point in the video or terminates the video early, some
prefetched-but-not-yet-viewed data transmitted by the server will go unwatched—a
waste of network bandwidth and server resources. For example, suppose that the client
buffer is full with B bits at some time t0 into the video, and at this time the user repositions
to some instant t > t0 + B/r into the video, and then watches the video to completion
from that point on. In this case, all B bits in the buffer will be unwatched and the
bandwidth and server resources that were used to transmit those B bits have been completely
wasted. There is significant wasted bandwidth in the Internet due to early termination,
which can be quite costly, particularly for wireless links [Ihm 2011]. For this
reason, many streaming systems use only a moderate-size client application buffer, or
will limit the amount of prefetched video using the byte-range header in HTTP
requests [Rao 2011].
Repositioning and early termination are analogous to cooking a large meal, eating
only a portion of it, and throwing the rest away, thereby wasting food. So the
next time your parents criticize you for wasting food by not eating all your dinner,
you can quickly retort by saying they are wasting bandwidth and server resources
when they reposition while watching movies over the Internet! But, of course, two
wrongs do not make a right—both food and bandwidth are not to be wasted!
7.2.3 Adaptive Streaming and DASH
Although HTTP streaming, as described in the previous subsection, has been extensively
deployed in practice (for example, by YouTube since its inception), it has a
major shortcoming: All clients receive the same encoding of the video, despite the
large variations in the amount of bandwidth available to a client, both across different
clients and also over time for the same client. This has led to the development of
a new type of HTTP-based streaming, often referred to as Dynamic Adaptive
Streaming over HTTP (DASH). In DASH, the video is encoded into several different
versions, with each version having a different bit rate and, correspondingly, a
different quality level. The client dynamically requests chunks of video segments of
a few seconds in length from the different versions. When the amount of available
600 CHAPTER 7 • MULTIMEDIA NETWORKING
bandwidth is high, the client naturally selects chunks from a high-rate version; and
when the available bandwidth is low, it naturally selects from a low-rate version.
The client selects different chunks one at a time with HTTP GET request messages
[Akhshabi 2011].
On one hand, DASH allows clients with different Internet access rates to stream
in video at different encoding rates. Clients with low-speed 3G connections can
receive a low bit-rate (and low-quality) version, and clients with fiber connections
can receive a high-quality version. On the other hand, DASH allows a client to adapt
to the available bandwidth if the end-to-end bandwidth changes during the session.
This feature is particularly important for mobile users, who typically see their bandwidth
availability fluctuate as they move with respect to the base stations. Comcast,
for example, has deployed an adaptive streaming system in which each video source
file is encoded into 8 to 10 different MPEG-4 formats, allowing the highest quality
video format to be streamed to the client, with adaptation being performed in
response to changing network and device conditions.
With DASH, each video version is stored in the HTTP server, each with a different
URL. The HTTP server also has a manifest file, which provides a URL for each
version along with its bit rate. The client first requests the manifest file and learns
about the various versions. The client then selects one chunk at a time by specifying a
URL and a byte range in an HTTP GET request message for each chunk. While downloading
chunks, the client also measures the received bandwidth and runs a rate determination
algorithm to select the chunk to request next. Naturally, if the client has a lot
of video buffered and if the measured receive bandwidth is high, it will choose a
chunk from a high-rate version. And naturally if the client has little video buffered and
the measured received bandwidth is low, it will choose a chunk from a low-rate version.
DASH therefore allows the client to freely switch among different quality levels.
Since a sudden drop in bit rate by changing versions may result in noticeable visual
quality degradation, the bit-rate reduction may be achieved using multiple intermediate
versions to smoothly transition to a rate where the client’s consumption rate drops
below its available receive bandwidth. When the network conditions improve, the
client can then later choose chunks from higher bit-rate versions.
By dynamically monitoring the available bandwidth and client buffer level, and
adjusting the transmission rate with version switching, DASH can often achieve
continuous playout at the best possible quality level without frame freezing or skipping.
Furthermore, since the client (rather than the server) maintains the intelligence
to determine which chunk to send next, the scheme also improves server-side scalability.
Another benefit of this approach is that the client can use the HTTP byte-range
request to precisely control the amount of prefetched video that it buffers locally.
We conclude our brief discussion of DASH by mentioning that for many implementations,
the server not only stores many versions of the video but also separately
stores many versions of the audio. Each audio version has its own quality level and bit
rate and has its own URL. In these implementations, the client dynamically selects
both video and audio chunks, and locally synchronizes audio and video playout.
7.2 • STREAMING STORED VIDEO 601
7.2.4 Content Distribution Networks
Today, many Internet video companies are distributing on-demand multi-Mbps
streams to millions of users on a daily basis. YouTube, for example, with a library
of hundreds of millions of videos, distributes hundreds of millions of video streams
to users around the world every day [Ding 2011]. Streaming all this traffic to locations
all over the world while providing continuous playout and high interactivity is
clearly a challenging task.
For an Internet video company, perhaps the most straightforward approach to
providing streaming video service is to build a single massive data center, store all
of its videos in the data center, and stream the videos directly from the data center to
clients worldwide. But there are three major problems with this approach. First, if
the client is far from the data center, server-to-client packets will cross many communication
links and likely pass through many ISPs, with some of the ISPs possibly
located on different continents. If one of these links provides a throughput that is
less than the video consumption rate, the end-to-end throughput will also be below
the consumption rate, resulting in annoying freezing delays for the user. (Recall
from Chapter 1 that the end-to-end throughput of a stream is governed by the
throughput in the bottleneck link.) The likelihood of this happening increases as the
number of links in the end-to-end path increases. A second drawback is that a popular
video will likely be sent many times over the same communication links. Not
only does this waste network bandwidth, but the Internet video company itself will
be paying its provider ISP (connected to the data center) for sending the same bytes
into the Internet over and over again. A third problem with this solution is that a single
data center represents a single point of failure—if the data center or its links to
the Internet goes down, it would not be able to distribute any video streams.
In order to meet the challenge of distributing massive amounts of video data to
users distributed around the world, almost all major video-streaming companies
make use of Content Distribution Networks (CDNs). A CDN manages servers in
multiple geographically distributed locations, stores copies of the videos (and other
types of Web content, including documents, images, and audio) in its servers, and
attempts to direct each user request to a CDN location that will provide the best user
experience. The CDN may be a private CDN, that is, owned by the content provider
itself; for example, Google’s CDN distributes YouTube videos and other types of
content. The CDN may alternatively be a third-party CDN that distributes content
on behalf of multiple content providers; Akamai’s CDN, for example, is a thirdparty
CDN that distributes Netflix and Hulu content, among others. A very readable
overview of modern CDNs is [Leighton 2009].
CDNs typically adopt one of two different server placement philosophies
[Huang 2008]:
• Enter Deep. One philosophy, pioneered by Akamai, is to enter deep into the
access networks of Internet Service Providers, by deploying server clusters in
access ISPs all over the world. (Access networks are described in Section 1.3.)
602 CHAPTER 7 • MULTIMEDIA NETWORKING
Akamai takes this approach with clusters in approximately 1,700 locations. The
goal is to get close to end users, thereby improving user-perceived delay and
throughput by decreasing the number of links and routers between the end user and
the CDN cluster from which it receives content. Because of this highly distributed
design, the task of maintaining and managing the clusters becomes challenging.
GOOGLE’S NETWORK INFRASTRUCTURE
To support its vast array of cloud services—including search, gmail, calendar,
YouTube video, maps, documents, and social networks—Google has deployed an
extensive private network and CDN infrastructure. Google’s CDN infrastructure has
three tiers of server clusters:
• Eight “mega data centers,” with six located in the United States and two located
in Europe [Google Locations 2012], with each data center having on the
order of 100,000 servers. These mega data centers are responsible for serving
dynamic (and often personalized) content, including search results and gmail
messages.
• About 30 “bring-home” clusters (see discussion in 7.2.4), with each cluster consisting
on the order of 100–500 servers [Adhikari 2011a]. The cluster locations
are distributed around the world, with each location typically near multiple
tier-1 ISP PoPs. These clusters are responsible for serving static content,
including YouTube videos [Adhikari 2011a].
• Many hundreds of “enter-deep” clusters (see discussion in 7.2.4), with each
cluster located within an access ISP. Here a cluster typically consists of tens of
servers within a single rack. These enter-deep servers perform TCP splitting (see
Section 3.7) and serve static content [Chen 2011], including the static portions
of Web pages that embody search results.
All of these data centers and cluster locations are networked together with Google’s
own private network, as part of one enormous AS (AS 15169). When a user makes a
search query, often the query is first sent over the local ISP to a nearby enter-deep
cache, from where the static content is retrieved; while providing the static content to
the client, the nearby cache also forwards the query over Google’s private network to
one of the mega data centers, from where the personalized search results are retrieved.
For a YouTube video, the video itself may come from one of the bring-home caches,
whereas portions of the Web page surrounding the video may come from the nearby
enter-deep cache, and the advertisements surrounding the video come from the data
centers. In summary, except for the local ISPs, the Google cloud services are largely
provided by a network infrastructure that is independent of the public Internet.
CASE STUDY
7.2 • STREAMING STORED VIDEO 603
• Bring Home. A second design philosophy, taken by Limelight and many other
CDN companies, is to bring the ISPs home by building large clusters at a smaller
number (for example, tens) of key locations and connecting these clusters using
a private high-speed network. Instead of getting inside the access ISPs, these
CDNs typically place each cluster at a location that is simultaneously near the
PoPs (see Section 1.3) of many tier-1 ISPs, for example, within a few miles of
both AT&T and Verizon PoPs in a major city. Compared with the enter-deep
design philosophy, the bring-home design typically results in lower maintenance
and management overhead, possibly at the expense of higher delay and lower
throughput to end users.
Once its clusters are in place, the CDN replicates content across its clusters. The
CDN may not want to place a copy of every video in each cluster, since some videos
are rarely viewed or are only popular in some countries. In fact, many CDNs do not
push videos to their clusters but instead use a simple pull strategy: If a client
requests a video from a cluster that is not storing the video, then the cluster retrieves
the video (from a central repository or from another cluster) and stores a copy
locally while streaming the video to the client at the same time. Similar to Internet
caches (see Chapter 2), when a cluster’s storage becomes full, it removes videos that
are not frequently requested.
CDN Operation
Having identified the two major approaches toward deploying a CDN, let’s now
dive down into the nuts and bolts of how a CDN operates. When a browser in
a user’s host is instructed to retrieve a specific video (identified by a URL), the
CDN must intercept the request so that it can (1) determine a suitable CDN
server cluster for that client at that time, and (2) redirect the client’s request to
a server in that cluster. We’ll shortly discuss how a CDN can determine a suitable
cluster. But first let’s examine the mechanics behind intercepting and redirecting
a request.
Most CDNs take advantage of DNS to intercept and redirect requests; an interesting
discussion of such a use of the DNS is [Vixie 2009]. Let’s consider a simple
example to illustrate how DNS is typically involved. Suppose a content provider,
NetCinema, employs the third-party CDN company, KingCDN, to distribute its
videos to its customers. On the NetCinema Web pages, each of its videos is assigned
a URL that includes the string “video” and a unique identifier for the video itself; for
example, Transformers 7 might be assigned http://video.netcinema.com/6Y7B23V.
Six steps then occur, as shown in Figure 7.4:
1. The user visits the Web page at NetCinema.
2. When the user clicks on the link http://video.netcinema.com/6Y7B23V, the
user’s host sends a DNS query for video.netcinema.com.
604 CHAPTER 7 • MULTIMEDIA NETWORKING
Figure 7.4  DNS redirects a user’s request to a CDN server
3. The user’s Local DNS Server (LDNS) relays the DNS query to an authoritative
DNS server for NetCinema, which observes the string “video” in the
hostname video.netcinema.com. To “hand over” the DNS query to KingCDN,
instead of returning an IP address, the NetCinema authoritative DNS server
returns to the LDNS a hostname in the KingCDN’s domain, for example,
a1105.kingcdn.com.
4. From this point on, the DNS query enters into KingCDN’s private DNS
infrastructure. The user’s LDNS then sends a second query, now for
a1105.kingcdn.com, and KingCDN’s DNS system eventually returns the
IP addresses of a KingCDN content server to the LDNS. It is thus here,
within the KingCDN’s DNS system, that the CDN server from which the
client will receive its content is specified.
5. The LDNS forwards the IP address of the content-serving CDN node to the
user’s host.
6. Once the client receives the IP address for a KingCDN content server, it
establishes a direct TCP connection with the server at that IP address and
issues an HTTP GET request for the video. If DASH is used, the server will
first send to the client a manifest file with a list of URLs, one for each
version of the video, and the client will dynamically select chunks from the
different versions.
Local
DNS server
NetCinema authoritative
DNS server
www.NetCinema.com
KingCDN authoritative
server
KingCDN content
distribution server
2
5
6
3
1
4
7.2 • STREAMING STORED VIDEO 605
Cluster Selection Strategies
At the core of any CDN deployment is a cluster selection strategy, that is, a mechanism
for dynamically directing clients to a server cluster or a data center within the
CDN. As we just saw, the CDN learns the IP address of the client’s LDNS server via
the client’s DNS lookup. After learning this IP address, the CDN needs to select an
appropriate cluster based on this IP address. CDNs generally employ proprietary
cluster selection strategies. We now briefly survey a number of natural approaches,
each of which has its own advantages and disadvantages.
One simple strategy is to assign the client to the cluster that is geographically
closest. Using commercial geo-location databases (such as Quova [Quova 2012]
and Max-Mind [MaxMind 2012]), each LDNS IP address is mapped to a geographic
location. When a DNS request is received from a particular LDNS, the CDN
chooses the geographically closest cluster, that is, the cluster that is the fewest kilometers
from the LDNS “as the bird flies.” Such a solution can work reasonably well
for a large fraction of the clients [Agarwal 2009]. However, for some clients, the
solution may perform poorly, since the geographically closest cluster may not be the
closest cluster along the network path. Furthermore, a problem inherent with all
DNS-based approaches is that some end-users are configured to use remotely
located LDNSs [Shaikh 2001; Mao 2002], in which case the LDNS location may be
far from the client’s location. Moreover, this simple strategy ignores the variation in
delay and available bandwidth over time of Internet paths, always assigning the
same cluster to a particular client.
In order to determine the best cluster for a client based on the current traffic
conditions, CDNs can instead perform periodic real-time measurements of delay
and loss performance between their clusters and clients. For instance, a CDN can
have each of its clusters periodically send probes (for example, ping messages or
DNS queries) to all of the LDNSs around the world. One drawback of this approach
is that many LDNSs are configured to not respond to such probes.
An alternative to sending extraneous traffic for measuring path properties is to
use the characteristics of recent and ongoing traffic between the clients and CDN
servers. For instance, the delay between a client and a cluster can be estimated by
examining the gap between server-to-client SYNACK and client-to-server ACK
during the TCP three-way handshake. Such solutions, however, require redirecting
clients to (possibly) suboptimal clusters from time to time in order to measure the
properties of paths to these clusters. Although only a small number of requests need
to serve as probes, the selected clients can suffer significant performance degradation
when receiving content (video or otherwise) [Andrews 2002; Krishnan 2009].
Another alternative for cluster-to-client path probing is to use DNS query traffic to
measure the delay between clients and clusters. Specifically, during the DNS phase
(within Step 4 in Figure 7.4), the client’s LDNS can be occasionally directed to different
DNS authoritative servers installed at the various cluster locations, yielding
DNS traffic that can then be measured between the LDNS and these cluster locations.
606 CHAPTER 7 • MULTIMEDIA NETWORKING
In this scheme, the DNS servers continue to return the optimal cluster for the client,
so that delivery of videos and other Web objects does not suffer [Huang 2010].
A very different approach to matching clients with CDN servers is to use IP
anycast [RFC 1546]. The idea behind IP anycast is to have the routers in the Internet
route the client’s packets to the “closest” cluster, as determined by BGP. Specifically,
as shown in Figure 7.5, during the IP-anycast configuration stage, the CDN
company assigns the same IP address to each of its clusters, and uses standard BGP
to advertise this IP address from each of the different cluster locations. When a BGP
router receives multiple route advertisements for this same IP address, it treats these
advertisements as providing different paths to the same physical location (when, in
fact, the advertisements are for different paths to different physical locations).
Following standard operating procedures, the BGP router will then pick the “best”
(for example, closest, as determined by AS-hop counts) route to the IP address
according to its local route selection mechanism. For example, if one BGP route
AS1
AS3
3b
3c
3a
1a
1c
1b
1d
AS2
AS4
2a
2c
4a 4c
4b
Advertise
212.21.21.21
CDN Server B
CDN Server A
Advertise
212.21.21.21
Receive BGP
advertisements for
212.21.21.21 from
AS1 and from AS4.
Forward towards
Server B since it is
closer.
2b
Figure 7.5  Using IP anycast to route clients to closest CDN cluster
7.2 • STREAMING STORED VIDEO 607
(corresponding to one location) is only one AS hop away from the router, and all
other BGP routes (corresponding to other locations) are two or more AS hops away,
then the BGP router would typically choose to route packets to the location that
needs to traverse only one AS (see Section 4.6). After this initial configuration phase,
the CDN can do its main job of distributing content. When any client wants to see
any video, the CDN’s DNS returns the anycast address, no matter where the client is
located. When the client sends a packet to that IP address, the packet is routed to the
“closest” cluster as determined by the preconfigured forwarding tables, which were
configured with BGP as just described. This approach has the advantage of finding
the cluster that is closest to the client rather than the cluster that is closest to the
client’s LDNS. However, the IP anycast strategy again does not take into account the
dynamic nature of the Internet over short time scales [Ballani 2006].
Besides network-related considerations such as delay, loss, and bandwidth performance,
there are many additional important factors that go into designing a cluster
selection strategy. Load on the clusters is one such factor—clients should not be
directed to overloaded clusters. ISP delivery cost is another factor—the clusters may
be chosen so that specific ISPs are used to carry CDN-to-client traffic, taking into
account the different cost structures in the contractual relationships between ISPs
and cluster operators.
7.2.5 Case Studies: Netflix, YouTube, and Kankan
We conclude our discussion of streaming stored video by taking a look at three
highly successful large-scale deployments: Netflix, YouTube, and Kankan. We’ll see
that all these systems take very different approaches, yet employ many of the underlying
principles discussed in this section.
Netflix
Generating almost 30 percent of the downstream U.S. Internet traffic in 2011, Netflix
has become the leading service provider for online movies and TV shows in the United
States [Sandvine 2011]. In order to rapidly deploy its large-scale service, Netflix has
made extensive use of third-party cloud services and CDNs. Indeed, Netflix is an interesting
example of a company deploying a large-scale online service by renting servers,
bandwidth, storage, and database services from third parties while using hardly any
infrastructure of its own. The following discussion is adapted from a very readable
measurement study of the Netflix architecture [Adhikari 2012]. As we’ll see, Netflix
employs many of the techniques covered earlier in this section, including video distribution
using a CDN (actually multiple CDNs) and adaptive streaming over HTTP.
Figure 7.6 shows the basic architecture of the Netflix video-streaming platform.
It has four major components: the registration and payment servers, the Amazon
cloud, multiple CDN providers, and clients. In its own hardware infrastructure, Netflix
maintains registration and payment servers, which handle registration of new
608 CHAPTER 7 • MULTIMEDIA NETWORKING
Figure 7.6  Netflix video streaming platform
accounts and capture credit-card payment information. Except for these basic functions,
Netflix runs its online service by employing machines (or virtual machines) in
the Amazon cloud. Some of the functions taking place in the Amazon cloud include:
• Content ingestion. Before Netflix can distribute a movie to its customers, it
must first ingest and process the movie. Netflix receives studio master versions
of movies and uploads them to hosts in the Amazon cloud.
• Content processing. The machines in the Amazon cloud create many different
formats for each movie, suitable for a diverse array of client video players running
on desktop computers, smartphones, and game consoles connected to televisions.
A different version is created for each of these formats and at multiple
bit rates, allowing for adaptive streaming over HTTP using DASH.
• Uploading versions to the CDNs. Once all of the versions of a movie have
been created, the hosts in the Amazon cloud upload the versions to the CDNs.
To deliver the movies to its customers on demand, Netflix makes extensive use of
CDN technology. In fact, as of this writing in 2012, Netflix employs not one but three
third-party CDN companies at the same time—Akamai, Limelight, and Level-3.
Having described the components of the Netflix architecture, let’s take a closer
look at the interaction between the client and the various servers that are involved in
Amazon Cloud
CDN server
CDN server
Upload
versions
to CDNs
Netflix
registration and
payment servers
CDN server
Client
Manifest
Registration file
and payment
Video
chunks
(DASH)
7.2 • STREAMING STORED VIDEO 609
movie delivery. The Web pages for browsing the Netflix video library are served
from servers in the Amazon cloud. When the user selects a movie to “Play Now,”
the user’s client obtains a manifest file, also from servers in the Amazon cloud. The
manifest file includes a variety of information, including a ranked list of CDNs and
the URLs for the different versions of the movie, which are used for DASH playback.
The ranking of the CDNs is determined by Netflix, and may change from one
streaming session to the next. Typically the client will select the CDN that is ranked
highest in the manifest file. After the client selects a CDN, the CDN leverages DNS
to redirect the client to a specific CDN server, as described in Section 7.2.4. The
client and that CDN server then interact using DASH. Specifically, as described in
Section 7.2.3, the client uses the byte-range header in HTTP GET request messages,
to request chunks from the different versions of the movie. Netflix uses chunks that
are approximately four-seconds long [Adhikari 2012]. While the chunks are being
downloaded, the client measures the received throughput and runs a rate-determination
algorithm to determine the quality of the next chunk to request.
Netflix embodies many of the key principles discussed earlier in this section,
including adaptive streaming and CDN distribution. Netflix also nicely illustrates
how a major Internet service, generating almost 30 percent of Internet traffic, can
run almost entirely on a third-party cloud and third-party CDN infrastructures, using
very little infrastructure of its own!
YouTube
With approximately half a billion videos in its library and half a billion video views
per day [Ding 2011], YouTube is indisputably the world’s largest video-sharing site.
YouTube began its service in April 2005 and was acquired by Google in November
2006. Although the Google/YouTube design and protocols are proprietary, through
several independent measurement efforts we can gain a basic understanding about
how YouTube operates [Zink 2009; Torres 2011; Adhikari 2011a].
As with Netflix, YouTube makes extensive use of CDN technology to distribute
its videos [Torres 2011]. Unlike Netflix, however, Google does not
employ third-party CDNs but instead uses its own private CDN to distribute
YouTube videos. Google has installed server clusters in many hundreds of different
locations. From a subset of about 50 of these locations, Google distributes
YouTube videos [Adhikari 2011a]. Google uses DNS to redirect a customer
request to a specific cluster, as described in Section 7.2.4. Most of the time,
Google’s cluster selection strategy directs the client to the cluster for which the
RTT between client and cluster is the lowest; however, in order to balance the
load across clusters, sometimes the client is directed (via DNS) to a more distant
cluster [Torres 2011]. Furthermore, if a cluster does not have the requested video,
instead of fetching it from somewhere else and relaying it to the client, the cluster
may return an HTTP redirect message, thereby redirecting the client to
another cluster [Torres 2011].
610 CHAPTER 7 • MULTIMEDIA NETWORKING
YouTube employs HTTP streaming, as discussed in Section 7.2.2. YouTube
often makes a small number of different versions available for a video, each with a
different bit rate and corresponding quality level. As of 2011, YouTube does not
employ adaptive streaming (such as DASH), but instead requires the user to manually
select a version. In order to save bandwidth and server resources that would be
wasted by repositioning or early termination, YouTube uses the HTTP byte range
request to limit the flow of transmitted data after a target amount of video is
prefetched.
A few million videos are uploaded to YouTube every day. Not only are
YouTube videos streamed from server to client over HTTP, but YouTube uploaders
also upload their videos from client to server over HTTP. YouTube processes each
video it receives, converting it to a YouTube video format and creating multiple versions
at different bit rates. This processing takes place entirely within Google data
centers. Thus, in stark contrast to Netflix, which runs its service almost entirely on
third-party infrastructures, Google runs the entire YouTube service within its own
vast infrastructure of data centers, private CDN, and private global network
interconnecting its data centers and CDN clusters. (See the case study on Google’s
network infrastructure in Section 7.2.4.)
Kankan
We just saw that for both the Netflix and YouTube services, servers operated by
CDNs (either third-party or private CDNs) stream videos to clients. Netflix and
YouTube not only have to pay for the server hardware (either directly through ownership
or indirectly through rent), but also for the bandwidth the servers use to distribute
the videos. Given the scale of these services and the amount of bandwidth
they are consuming, such a “client-server” deployment is extremely costly.
We conclude this section by describing an entirely different approach for providing
video on demand over the Internet at a large scale—one that allows the service
provider to significantly reduce its infrastructure and bandwidth costs. As you might
suspect, this approach uses P2P delivery instead of client-server (via CDNs) delivery.
P2P video delivery is used with great success by several companies in China, including
Kankan (owned and operated by Xunlei), PPTV (formerly PPLive), and PPs (formerly
PPstream). Kankan, currently the leading P2P-based video-on-demand provider
in China, has over 20 million unique users viewing its videos every month.
At a high level, P2P video streaming is very similar to BitTorrent file downloading
(discussed in Chapter 2). When a peer wants to see a video, it contacts a
tracker (which may be centralized or peer-based using a DHT) to discover other
peers in the system that have a copy of that video. This peer then requests chunks
of the video file in parallel from these other peers that have the file. Different from
downloading with BitTorrent, however, requests are preferentially made for
chunks that are to be played back in the near future in order to ensure continuous
playback.
7.2 • STREAMING STORED VIDEO 611
The Kankan design employs a tracker and its own DHT for tracking content.
Swarm sizes for the most popular content involve tens of thousands of peers, typically
larger than the largest swarms in BitTorrent [Dhungel 2012]. The Kankan protocols—
for communication between peer and tracker, between peer and DHT, and
among peers—are all proprietary. Interestingly, for distributing video chunks among
peers, Kankan uses UDP whenever possible, leading to massive amounts of UDP
traffic within China’s Internet [Zhang M 2010].
7.3 Voice-over-IP
Real-time conversational voice over the Internet is often referred to as Internet
telephony, since, from the user’s perspective, it is similar to the traditional
circuit-switched telephone service. It is also commonly called Voice-over-IP
(VoIP). In this section we describe the principles and protocols underlying VoIP.
Conversational video is similar in many respects to VoIP, except that it includes
the video of the participants as well as their voices. To keep the discussion focused
and concrete, we focus here only on voice in this section rather than combined
voice and video.
7.3.1 Limitations of the Best-Effort IP Service
The Internet’s network-layer protocol, IP, provides best-effort service. That is to say
the service makes its best effort to move each datagram from source to destination
as quickly as possible but makes no promises whatsoever about getting the packet
to the destination within some delay bound or about a limit on the percentage of
packets lost. The lack of such guarantees poses significant challenges to the design
of real-time conversational applications, which are acutely sensitive to packet delay,
jitter, and loss.
In this section, we’ll cover several ways in which the performance of
VoIP over a best-effort network can be enhanced. Our focus will be on application-
layer techniques, that is, approaches that do not require any changes in the
network core or even in the transport layer at the end hosts. To keep the discussion
concrete, we’ll discuss the limitations of best-effort IP service in the context
of a specific VoIP example. The sender generates bytes at a rate of 8,000 bytes
per second; every 20 msecs the sender gathers these bytes into a chunk. A chunk
and a special header (discussed below) are encapsulated in a UDP segment, via a
call to the socket interface. Thus, the number of bytes in a chunk is (20 msecs)·
(8,000 bytes/sec) = 160 bytes, and a UDP segment is sent every 20 msecs.
If each packet makes it to the receiver with a constant end-to-end delay, then
packets arrive at the receiver periodically every 20 msecs. In these ideal conditions,
612 CHAPTER 7 • MULTIMEDIA NETWORKING
the receiver can simply play back each chunk as soon as it arrives. But unfortunately,
some packets can be lost and most packets will not have the same end-to-end
delay, even in a lightly congested Internet. For this reason, the receiver must take
more care in determining (1) when to play back a chunk, and (2) what to do with a
missing chunk.
Packet Loss
Consider one of the UDP segments generated by our VoIP application. The UDP
segment is encapsulated in an IP datagram. As the datagram wanders through the
network, it passes through router buffers (that is, queues) while waiting for transmission
on outbound links. It is possible that one or more of the buffers in the path
from sender to receiver is full, in which case the arriving IP datagram may be discarded,
never to arrive at the receiving application.
Loss could be eliminated by sending the packets over TCP (which provides
for reliable data transfer) rather than over UDP. However, retransmission mechanisms
are often considered unacceptable for conversational real-time audio applications
such as VoIP, because they increase end-to-end delay [Bolot 1996].
Furthermore, due to TCP congestion control, packet loss may result in a reduction
of the TCP sender’s transmission rate to a rate that is lower than the
receiver’s drain rate, possibly leading to buffer starvation. This can have a severe
impact on voice intelligibility at the receiver. For these reasons, most existing
VoIP applications run over UDP by default. [Baset 2006] reports that UDP is
used by Skype unless a user is behind a NAT or firewall that blocks UDP
segments (in which case TCP is used).
But losing packets is not necessarily as disastrous as one might think.
Indeed, packet loss rates between 1 and 20 percent can be tolerated, depending
on how voice is encoded and transmitted, and on how the loss is concealed at the
receiver. For example, forward error correction (FEC) can help conceal packet
loss. We’ll see below that with FEC, redundant information is transmitted along
with the original information so that some of the lost original data can be recovered
from the redundant information. Nevertheless, if one or more of the links
between sender and receiver is severely congested, and packet loss exceeds 10 to
20 percent (for example, on a wireless link), then there is really nothing that can
be done to achieve acceptable audio quality. Clearly, best-effort service has its
limitations.
End-to-End Delay
End-to-end delay is the accumulation of transmission, processing, and queuing
delays in routers; propagation delays in links; and end-system processing delays.
For real-time conversational applications, such as VoIP, end-to-end delays smaller
than 150 msecs are not perceived by a human listener; delays between 150 and 400
7.3 • VOICE-OVER-IP 613
msecs can be acceptable but are not ideal; and delays exceeding 400 msecs can seriously
hinder the interactivity in voice conversations. The receiving side of a VoIP
application will typically disregard any packets that are delayed more than a certain
threshold, for example, more than 400 msecs. Thus, packets that are delayed by
more than the threshold are effectively lost.
Packet Jitter
A crucial component of end-to-end delay is the varying queuing delays that a packet
experiences in the network’s routers. Because of these varying delays, the time from
when a packet is generated at the source until it is received at the receiver can fluctuate
from packet to packet, as shown in Figure 7.1. This phenomenon is called jitter.
As an example, consider two consecutive packets in our VoIP application. The sender
sends the second packet 20 msecs after sending the first packet. But at the receiver,
the spacing between these packets can become greater than 20 msecs. To see this,
suppose the first packet arrives at a nearly empty queue at a router, but just before the
second packet arrives at the queue a large number of packets from other sources
arrive at the same queue. Because the first packet experiences a small queuing delay
and the second packet suffers a large queuing delay at this router, the first and second
packets become spaced by more than 20 msecs. The spacing between consecutive
packets can also become less than 20 msecs. To see this, again consider two consecutive
packets. Suppose the first packet joins the end of a queue with a large number of
packets, and the second packet arrives at the queue before this first packet is transmitted
and before any packets from other sources arrive at the queue. In this case, our
two packets find themselves one right after the other in the queue. If the time it takes
to transmit a packet on the router’s outbound link is less than 20 msecs, then the spacing
between first and second packets becomes less than 20 msecs.
The situation is analogous to driving cars on roads. Suppose you and your
friend are each driving in your own cars from San Diego to Phoenix. Suppose
you and your friend have similar driving styles, and that you both drive at
100 km/hour, traffic permitting. If your friend starts out one hour before you,
depending on intervening traffic, you may arrive at Phoenix more or less than one
hour after your friend.
If the receiver ignores the presence of jitter and plays out chunks as soon as
they arrive, then the resulting audio quality can easily become unintelligible at the
receiver. Fortunately, jitter can often be removed by using sequence numbers,
timestamps, and a playout delay, as discussed below.
7.3.2 Removing Jitter at the Receiver for Audio
For our VoIP application, where packets are being generated periodically, the
receiver should attempt to provide periodic playout of voice chunks in the presence
614 CHAPTER 7 • MULTIMEDIA NETWORKING
of random network jitter. This is typically done by combining the following two
mechanisms:
• Prepending each chunk with a timestamp. The sender stamps each chunk with
the time at which the chunk was generated.
• Delaying playout of chunks at the receiver. As we saw in our earlier discussion
of Figure 7.1, the playout delay of the received audio chunks must be long
enough so that most of the packets are received before their scheduled playout
times. This playout delay can either be fixed throughout the duration of the audio
session or vary adaptively during the audio session lifetime.
We now discuss how these three mechanisms, when combined, can alleviate or
even eliminate the effects of jitter. We examine two playback strategies: fixed playout
delay and adaptive playout delay.
Fixed Playout Delay
With the fixed-delay strategy, the receiver attempts to play out each chunk exactly q
msecs after the chunk is generated. So if a chunk is timestamped at the sender at
time t, the receiver plays out the chunk at time t + q, assuming the chunk has arrived
by that time. Packets that arrive after their scheduled playout times are discarded
and considered lost.
What is a good choice for q? VoIP can support delays up to about 400 msecs,
although a more satisfying conversational experience is achieved with smaller values
of q. On the other hand, if q is made much smaller than 400 msecs, then many packets
may miss their scheduled playback times due to the network-induced packet jitter.
Roughly speaking, if large variations in end-to-end delay are typical, it is preferable to
use a large q; on the other hand, if delay is small and variations in delay are also small,
it is preferable to use a small q, perhaps less than 150 msecs.
The trade-off between the playback delay and packet loss is illustrated in
Figure 7.7. The figure shows the times at which packets are generated and played
out for a single talk spurt. Two distinct initial playout delays are considered. As
shown by the leftmost staircase, the sender generates packets at regular intervals—
say, every 20 msecs. The first packet in this talk spurt is received at time r.
As shown in the figure, the arrivals of subsequent packets are not evenly spaced
due to the network jitter.
For the first playout schedule, the fixed initial playout delay is set to p – r. With
this schedule, the fourth packet does not arrive by its scheduled playout time, and
the receiver considers it lost. For the second playout schedule, the fixed initial playout
delay is set to p – r. For this schedule, all packets arrive before their scheduled
playout times, and there is therefore no loss.
7.3 • VOICE-OVER-IP 615
616 CHAPTER 7 • MULTIMEDIA NETWORKING
Packets
generated
Time
Packets
r p p'
Playout
schedule
p–r
Playout
schedule
p'–r
Packets
received
Missed
playout
Figure 7.7  Packet loss for different fixed playout delays
Adaptive Playout Delay
The previous example demonstrates an important delay-loss trade-off that arises
when designing a playout strategy with fixed playout delays. By making the initial
playout delay large, most packets will make their deadlines and there will therefore
be negligible loss; however, for conversational services such as VoIP, long delays
can become bothersome if not intolerable. Ideally, we would like the playout delay
to be minimized subject to the constraint that the loss be below a few percent.
The natural way to deal with this trade-off is to estimate the network delay and
the variance of the network delay, and to adjust the playout delay accordingly at the
beginning of each talk spurt. This adaptive adjustment of playout delays at the
beginning of the talk spurts will cause the sender’s silent periods to be compressed
and elongated; however, compression and elongation of silence by a small amount
is not noticeable in speech.
Following [Ramjee 1994], we now describe a generic algorithm that the
receiver can use to adaptively adjust its playout delays. To this end, let
ti = the timestamp of the ith packet = the time the packet was generated by the
sender
ri = the time packet i is received by receiver
pi = the time packet i is played at receiver
The end-to-end network delay of the ith packet is ri – ti. Due to network jitter,
this delay will vary from packet to packet. Let di denote an estimate of the average
network delay upon reception of the ith packet. This estimate is constructed from
the timestamps as follows:
di = (1 – u) di–1 + u (ri – ti)
where u is a fixed constant (for example, u = 0.01). Thus di is a smoothed average
of the observed network delays r1 – t1, . . . , ri – ti. The estimate places more weight
on the recently observed network delays than on the observed network delays of the
distant past. This form of estimate should not be completely unfamiliar; a similar
idea is used to estimate round-trip times in TCP, as discussed in Chapter 3. Let vi
denote an estimate of the average deviation of the delay from the estimated average
delay. This estimate is also constructed from the timestamps:
vi = (1 – u) vi–1 + u | ri – ti – di |
The estimates di and vi are calculated for every packet received, although they are
used only to determine the playout point for the first packet in any talk spurt.
Once having calculated these estimates, the receiver employs the following
algorithm for the playout of packets. If packet i is the first packet of a talk spurt, its
playout time, pi, is computed as:
pi = ti + di + Kvi
where K is a positive constant (for example, K = 4). The purpose of the Kvi term is to
set the playout time far enough into the future so that only a small fraction of the arriving
packets in the talk spurt will be lost due to late arrivals. The playout point for any
subsequent packet in a talk spurt is computed as an offset from the point in time when
the first packet in the talk spurt was played out. In particular, let
qi = pi – ti
be the length of time from when the first packet in the talk spurt is generated until it
is played out. If packet j also belongs to this talk spurt, it is played out at time
pj = tj + qi
The algorithm just described makes perfect sense assuming that the receiver can
tell whether a packet is the first packet in the talk spurt. This can be done by examining
the signal energy in each received packet.
7.3.3 Recovering from Packet Loss
We have discussed in some detail how a VoIP application can deal with packet jitter.
We now briefly describe several schemes that attempt to preserve acceptable audio
7.3 • VOICE-OVER-IP 617
quality in the presence of packet loss. Such schemes are called loss recovery schemes.
Here we define packet loss in a broad sense: Apacket is lost either if it never arrives at
the receiver or if it arrives after its scheduled playout time. Our VoIP example will
again serve as a context for describing loss recovery schemes.
As mentioned at the beginning of this section, retransmitting lost packets may not
be feasible in a real-time conversational application such as VoIP. Indeed, retransmitting
a packet that has missed its playout deadline serves absolutely no purpose. And
retransmitting a packet that overflowed a router queue cannot normally be accomplished
quickly enough. Because of these considerations, VoIP applications often use
some type of loss anticipation scheme. Two types of loss anticipation schemes are
forward error correction (FEC) and interleaving.
Forward Error Correction (FEC)
The basic idea of FEC is to add redundant information to the original packet stream.
For the cost of marginally increasing the transmission rate, the redundant information
can be used to reconstruct approximations or exact versions of some of the lost packets.
Following [Bolot 1996] and [Perkins 1998], we now outline two simple FEC mechanisms.
The first mechanism sends a redundant encoded chunk after every n chunks. The
redundant chunk is obtained by exclusive OR-ing the n original chunks [Shacham
1990]. In this manner if any one packet of the group of n + 1 packets is lost, the receiver
can fully reconstruct the lost packet. But if two or more packets in a group are lost, the
receiver cannot reconstruct the lost packets. By keeping n + 1, the group size, small, a
large fraction of the lost packets can be recovered when loss is not excessive. However,
the smaller the group size, the greater the relative increase of the transmission rate. In
particular, the transmission rate increases by a factor of 1/n, so that, if n = 3, then the
transmission rate increases by 33 percent. Furthermore, this simple scheme increases
the playout delay, as the receiver must wait to receive the entire group of packets before
it can begin playout. For more practical details about how FEC works for multimedia
transport see [RFC 5109].
The second FEC mechanism is to send a lower-resolution audio stream as the
redundant information. For example, the sender might create a nominal audio
stream and a corresponding low-resolution, low-bit rate audio stream. (The nominal
stream could be a PCM encoding at 64 kbps, and the lower-quality stream could be
a GSM encoding at 13 kbps.) The low-bit rate stream is referred to as the redundant
stream. As shown in Figure 7.8, the sender constructs the nth packet by taking the
nth chunk from the nominal stream and appending to it the (n – 1)st chunk from the
redundant stream. In this manner, whenever there is nonconsecutive packet loss, the
receiver can conceal the loss by playing out the low-bit rate encoded chunk that
arrives with the subsequent packet. Of course, low-bit rate chunks give lower quality
than the nominal chunks. However, a stream of mostly high-quality chunks,
occasional low-quality chunks, and no missing chunks gives good overall audio
quality. Note that in this scheme, the receiver only has to receive two packets before
playback, so that the increased playout delay is small. Furthermore, if the low-bit
618 CHAPTER 7 • MULTIMEDIA NETWORKING
rate encoding is much less than the nominal encoding, then the marginal increase in
the transmission rate will be small.
In order to cope with consecutive loss, we can use a simple variation. Instead of
appending just the (n – 1)st low-bit rate chunk to the nth nominal chunk, the sender
can append the (n – 1)st and (n – 2)nd low-bit rate chunk, or append the (n – 1)st
and (n – 3)rd low-bit rate chunk, and so on. By appending more low-bit rate chunks
to each nominal chunk, the audio quality at the receiver becomes acceptable for a
wider variety of harsh best-effort environments. On the other hand, the additional
chunks increase the transmission bandwidth and the playout delay.
Interleaving
As an alternative to redundant transmission, a VoIP application can send interleaved
audio. As shown in Figure 7.9, the sender resequences units of audio data before transmission,
so that originally adjacent units are separated by a certain distance in the transmitted
stream. Interleaving can mitigate the effect of packet losses. If, for example,
units are 5 msecs in length and chunks are 20 msecs (that is, four units per chunk), then
the first chunk could contain units 1, 5, 9, and 13; the second chunk could contain units
2, 6, 10, and 14; and so on. Figure 7.9 shows that the loss of a single packet from an
interleaved stream results in multiple small gaps in the reconstructed stream, as
opposed to the single large gap that would occur in a noninterleaved stream.
Interleaving can significantly improve the perceived quality of an audio stream
[Perkins 1998]. It also has low overhead. The obvious disadvantage of interleaving is
that it increases latency. This limits its use for conversational applications such as VoIP,
although it can perform well for streaming stored audio. A major advantage of interleaving
is that it does not increase the bandwidth requirements of a stream.
1
1 1
1
1
2
2
2 2
3
3
loss
3 4
1 2 3 4
3 4
4
Redundancy
Received
stream
Original
stream
Reconstructed
stream
Figure 7.8  Piggybacking lower-quality redundant information
7.3 • VOICE-OVER-IP 619
Error Concealment
Error concealment schemes attempt to produce a replacement for a lost packet that
is similar to the original. As discussed in [Perkins 1998], this is possible since audio
signals, and in particular speech, exhibit large amounts of short-term self-similarity.
As such, these techniques work for relatively small loss rates (less than 15 percent),
and for small packets (4–40 msecs). When the loss length approaches the length of a
phoneme (5–100 msecs) these techniques break down, since whole phonemes may
be missed by the listener.
Perhaps the simplest form of receiver-based recovery is packet repetition.
Packet repetition replaces lost packets with copies of the packets that arrived
immediately before the loss. It has low computational complexity and performs
reasonably well. Another form of receiver-based recovery is interpolation, which
uses audio before and after the loss to interpolate a suitable packet to cover the
loss. Interpolation performs somewhat better than packet repetition but is significantly
more computationally intensive [Perkins 1998].
7.3.4 Case Study: VoIP with Skype
Skype is an immensely popular VoIP application with over 50 million accounts
active on a daily basis. In addition to providing host-to-host VoIP service, Skype
offers host-to-phone services, phone-to-host services, and multi-party host-to-host
620 CHAPTER 7 • MULTIMEDIA NETWORKING
Original
stream
Interleaved
stream
Received
stream
Reconstructed
stream
1 5 9 13
1 2 4
1 2 3 4
1 5 9 13
2 6 10 14
5 6 8
5 6 7 8
2 6 10 14 loss
3 7 11 15
9 10 12
9 10 11 12
4 8 12 16
13 14 16
13 14 15 16
4 8 12 16
Figure 7.9  Sending interleaved audio
video conferencing services. (Here, a host is again any Internet connected IP device,
including PCs, tablets, and smartphones.) Skype was acquired by Microsoft in 2011
for over $8 billion.
Because the Skype protocol is proprietary, and because all Skype’s control and
media packets are encrypted, it is difficult to precisely determine how Skype operates.
Nevertheless, from the Skype Web site and several measurement studies, researchers
have learned how Skype generally works [Baset 2006; Guha 2006; Chen 2006; Suh
2006; Ren 2006; Zhang X 2012]. For both voice and video, the Skype clients have at
their disposal many different codecs, which are capable of encoding the media at a wide
range of rates and qualities. For example, video rates for Skype have been measured to
be as low as 30 kbps for a low-quality session up to almost 1 Mbps for a high quality
session [Zhang X 2012]. Typically, Skype’s audio quality is better than the “POTS”
(Plain Old Telephone Service) quality provided by the wire-line phone system. (Skype
codecs typically sample voice at 16,000 samples/sec or higher, which provides richer
tones than POTS, which samples at 8,000/sec.) By default, Skype sends audio and
video packets over UDP. However, control packets are sent over TCP, and media packets
are also sent over TCP when firewalls block UDP streams. Skype uses FEC for loss
recovery for both voice and video streams sent over UDP. The Skype client also adapts
the audio and video streams it sends to current network conditions, by changing video
quality and FEC overhead [Zhang X 2012].
Skype uses P2P techniques in a number of innovative ways, nicely illustrating
how P2P can be used in applications that go beyond content distribution and file
sharing. As with instant messaging, host-to-host Internet telephony is inherently P2P
since, at the heart of the application, pairs of users (that is, peers) communicate with
each other in real time. But Skype also employs P2P techniques for two other important
functions, namely, for user location and for NAT traversal.
As shown in Figure 7.10, the peers (hosts) in Skype are organized into a hierarchical
overlay network, with each peer classified as a super peer or an ordinary peer.
Skype maintains an index that maps Skype usernames to current IP addresses (and
port numbers). This index is distributed over the super peers. When Alice wants to
call Bob, her Skype client searches the distributed index to determine Bob’s current
IP address. Because the Skype protocol is proprietary, it is currently not known how
the index mappings are organized across the super peers, although some form of
DHT organization is very possible.
P2P techniques are also used in Skype relays, which are useful for establishing
calls between hosts in home networks. Many home network configurations
provide access to the Internet through NATs, as discussed in Chapter 4. Recall that
a NAT prevents a host from outside the home network from initiating a connection
to a host within the home network. If both Skype callers have NATs, then
there is a problem—neither can accept a call initiated by the other, making a call
seemingly impossible. The clever use of super peers and relays nicely solves this
problem. Suppose that when Alice signs in, she is assigned to a non-NATed super
peer and initiates a session to that super peer. (Since Alice is initiating the session,
her NAT permits this session.) This session allows Alice and her super peer to
7.3 • VOICE-OVER-IP 621
exchange control messages. The same happens for Bob when he signs in. Now,
when Alice wants to call Bob, she informs her super peer, who in turn informs
Bob’s super peer, who in turn informs Bob of Alice’s incoming call. If Bob
accepts the call, the two super peers select a third non-NATed super peer—the
relay peer—whose job will be to relay data between Alice and Bob. Alice’s and
Bob’s super peers then instruct Alice and Bob respectively to initiate a session
with the relay. As shown in Figure 7.10, Alice then sends voice packets to the
relay over the Alice-to-relay connection (which was initiated by Alice), and the
relay then forwards these packets over the relay-to-Bob connection (which was
initiated by Bob); packets from Bob to Alice flow over these same two relay connections
in reverse. And voila!—Bob and Alice have an end-to-end connection
even though neither can accept a session originating from outside.
Up to now, our discussion on Skype has focused on calls involving two persons.
Now let’s examine multi-party audio conference calls. With N > 2 participants, if each
user were to send a copy of its audio stream to each of the N 1 other users, then a
total of N(N 1) audio streams would need to be sent into the network to support the
audio conference. To reduce this bandwidth usage, Skype employs a clever distribution
-
-
622 CHAPTER 7 • MULTIMEDIA NETWORKING
Callee
peer
Caller
peer
Relay
peer
Super
peer
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Skype
Figure 7.10  Skype peers
technique. Specifically, each user sends its audio stream to the conference initiator. The
conference initiator combines the audio streams into one stream (basically by adding
all the audio signals together) and then sends a copy of each combined stream to each
of the other N 1 participants. In this manner, the number of streams is reduced to
2(N 1). For ordinary two-person video conversations, Skype routes the call peer-topeer,
unless NAT traversal is required, in which case the call is relayed through a non-
NATed peer, as described earlier. For a video conference call involving N > 2
participants, due to the nature of the video medium, Skype does not combine the call
into one stream at one location and then redistribute the stream to all the participants,
as it does for voice calls. Instead, each participant's video stream is routed to a server
cluster (located in Estonia as of 2011), which in turn relays to each participant the
N 1 streams of the N 1 other participants [Zhang X 2012]. You may be wondering
why each participant sends a copy to a server rather than directly sending a copy of
its video stream to each of the other N 1 participants? Indeed, for both approaches,
N(N 1) video streams are being collectively received by the N participants in the
conference. The reason is, because upstream link bandwidths are significantly lower
than downstream link bandwidths in most access links, the upstream links may not be
able to support the N 1 streams with the P2P approach.
VoIP systems such as Skype, QQ, and Google Talk introduce new privacy
concerns. Specifically, when Alice and Bob communicate over VoIP, Alice can sniff
Bob’s IP address and then use geo-location services [MaxMind 2012; Quova 2012]
to determine Bob’s current location and ISP (for example, his work or home ISP). In
fact, with Skype it is possible for Alice to block the transmission of certain packets
during call establishment so that she obtains Bob’s current IP address, say every hour,
without Bob knowing that he is being tracked and without being on Bob’s contact
list. Furthermore, the IP address discovered from Skype can be correlated with IP
addresses found in BitTorrent, so that Alice can determine the files that Bob is downloading
[LeBlond 2011]. Moreover, it is possible to partially decrypt a Skype call by
doing a traffic analysis of the packet sizes in a stream [White 2011].
7.4 Protocols for Real-Time Conversational
Applications
Real-time conversational applications, including VoIP and video conferencing, are
compelling and very popular. It is therefore not surprising that standards bodies,
such as the IETF and ITU, have been busy for many years (and continue to be
busy!) at hammering out standards for this class of applications. With the appropriate
standards in place for real-time conversational applications, independent companies
are creating new products that interoperate with each other. In this section we
examine RTP and SIP for real-time conversational applications. Both standards are
enjoying widespread implementation in industry products.
-
-
-
- -
-
-
7.4 • PROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS 623
7.4.1 RTP
In the previous section, we learned that the sender side of a VoIP application appends
header fields to the audio chunks before passing them to the transport layer. These
header fields include sequence numbers and timestamps. Since most multimedia
networking applications can make use of sequence numbers and timestamps, it is convenient
to have a standardized packet structure that includes fields for audio/video data,
sequence number, and timestamp, as well as other potentially useful fields. RTP,
defined in RFC 3550, is such a standard. RTP can be used for transporting common
formats such as PCM, ACC, and MP3 for sound and MPEG and H.263 for video. It can
also be used for transporting proprietary sound and video formats. Today, RTP enjoys
widespread implementation in many products and research prototypes. It is also complementary
to other important real-time interactive protocols, such as SIP.
In this section, we provide an introduction to RTP. We also encourage you to
visit Henning Schulzrinne’s RTP site [Schulzrinne-RTP 2012], which provides a
wealth of information on the subject. Also, you may want to visit the RAT site
[RAT 2012], which documents VoIP application that uses RTP.
RTP Basics
RTP typically runs on top of UDP. The sending side encapsulates a media chunk
within an RTP packet, then encapsulates the packet in a UDP segment, and then
hands the segment to IP. The receiving side extracts the RTP packet from the UDP
segment, then extracts the media chunk from the RTP packet, and then passes the
chunk to the media player for decoding and rendering.
As an example, consider the use of RTP to transport voice. Suppose the voice
source is PCM-encoded (that is, sampled, quantized, and digitized) at 64 kbps. Further
suppose that the application collects the encoded data in 20-msec chunks, that is,
160 bytes in a chunk. The sending side precedes each chunk of the audio data with
an RTP header that includes the type of audio encoding, a sequence number, and a
timestamp. The RTP header is normally 12 bytes. The audio chunk along with the
RTP header form the RTP packet. The RTP packet is then sent into the UDP socket
interface. At the receiver side, the application receives the RTP packet from its socket
interface. The application extracts the audio chunk from the RTP packet and uses the
header fields of the RTP packet to properly decode and play back the audio chunk.
If an application incorporates RTP—instead of a proprietary scheme to provide
payload type, sequence numbers, or timestamps—then the application will more easily
interoperate with other networked multimedia applications. For example, if two different
companies develop VoIP software and they both incorporate RTP into their product,
there may be some hope that a user using one of the VoIP products will be able to communicate
with a user using the other VoIP product. In Section 7.4.2, we’ll see that RTP
is often used in conjunction with SIP, an important standard for Internet telephony.
It should be emphasized that RTP does not provide any mechanism to ensure
timely delivery of data or provide other quality-of-service (QoS) guarantees; it
624 CHAPTER 7 • MULTIMEDIA NETWORKING
does not even guarantee delivery of packets or prevent out-of-order delivery of
packets. Indeed, RTP encapsulation is seen only at the end systems. Routers do
not distinguish between IP datagrams that carry RTP packets and IP datagrams
that don’t.
RTP allows each source (for example, a camera or a microphone) to be assigned
its own independent RTP stream of packets. For example, for a video conference
between two participants, four RTP streams could be opened—two streams for
transmitting the audio (one in each direction) and two streams for transmitting the
video (again, one in each direction). However, many popular encoding techniques—
including MPEG 1 and MPEG 2—bundle the audio and video into a single stream
during the encoding process. When the audio and video are bundled by the encoder,
then only one RTP stream is generated in each direction.
RTP packets are not limited to unicast applications. They can also be sent over
one-to-many and many-to-many multicast trees. For a many-to-many multicast
session, all of the session’s senders and sources typically use the same multicast
group for sending their RTP streams. RTP multicast streams belonging together,
such as audio and video streams emanating from multiple senders in a video conference
application, belong to an RTP session.
RTP Packet Header Fields
As shown in Figure 7.11, the four main RTP packet header fields are the payload
type, sequence number, timestamp, and source identifier fields.
The payload type field in the RTP packet is 7 bits long. For an audio stream, the
payload type field is used to indicate the type of audio encoding (for example, PCM,
adaptive delta modulation, linear predictive encoding) that is being used. If a sender
decides to change the encoding in the middle of a session, the sender can inform the
receiver of the change through this payload type field. The sender may want to change
the encoding in order to increase the audio quality or to decrease the RTP stream bit
rate. Table 7.2 lists some of the audio payload types currently supported by RTP.
For a video stream, the payload type is used to indicate the type of video encoding
(for example, motion JPEG, MPEG 1, MPEG 2, H.261). Again, the sender can change
video encoding on the fly during a session. Table 7.3 lists some of the video payload
types currently supported by RTP. The other important fields are the following:
• Sequence number field. The sequence number field is 16 bits long. The sequence
number increments by one for each RTP packet sent, and may be used by the
Payload
type
Sequence
number
Synchronization
source identifier
Miscellaneous
Timestamp fields
Figure 7.11  RTP header fields
7.4 • PROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS 625
receiver to detect packet loss and to restore packet sequence. For example, if the
receiver side of the application receives a stream of RTP packets with a gap
between sequence numbers 86 and 89, then the receiver knows that packets 87
and 88 are missing. The receiver can then attempt to conceal the lost data.
• Timestamp field. The timestamp field is 32 bits long. It reflects the sampling
instant of the first byte in the RTP data packet. As we saw in the preceding
section, the receiver can use timestamps to remove packet jitter introduced in
the network and to provide synchronous playout at the receiver. The timestamp
is derived from a sampling clock at the sender. As an example, for
audio the timestamp clock increments by one for each sampling period (for
example, each 125 sec for an 8 kHz sampling clock); if the audio application
generates chunks consisting of 160 encoded samples, then the timestamp
increases by 160 for each RTP packet when the source is active. The
626 CHAPTER 7 • MULTIMEDIA NETWORKING
Payload-Type Number Audio Format Sampling Rate Rate
0 PCM -law 8 kHz 64 kbps
1 1016 8 kHz 4.8 kbps
3 GSM 8 kHz 13 kbps
7 LPC 8 kHz 2.4 kbps
9 G.722 16 kHz 48–64 kbps
14 MPEG Audio 90 kHz —
15 G.728 8 kHz 16 kbps
Table 7.2  Audio payload types supported by RTP
Payload-Type Number Video Format
26 Motion JPEG
31 H.261
32 MPEG 1 video
33 MPEG 2 video
Table 7.3  Some video payload types supported by RTP
timestamp clock continues to increase at a constant rate even if the source is
inactive.
• Synchronization source identifier (SSRC). The SSRC field is 32 bits long. It identifies
the source of the RTP stream. Typically, each stream in an RTP session has
a distinct SSRC. The SSRC is not the IP address of the sender, but instead is a
number that the source assigns randomly when the new stream is started. The
probability that two streams get assigned the same SSRC is very small. Should
this happen, the two sources pick a new SSRC value.
7.4.2 SIP
The Session Initiation Protocol (SIP), defined in [RFC 3261; RFC 5411], is an
open and lightweight protocol that does the following:
• It provides mechanisms for establishing calls between a caller and a callee over
an IP network. It allows the caller to notify the callee that it wants to start a call.
It allows the participants to agree on media encodings. It also allows participants
to end calls.
• It provides mechanisms for the caller to determine the current IP address of the
callee. Users do not have a single, fixed IP address because they may be assigned
addresses dynamically (using DHCP) and because they may have multiple IP
devices, each with a different IP address.
• It provides mechanisms for call management, such as adding new media streams
during the call, changing the encoding during the call, inviting new participants
during the call, call transfer, and call holding.
Setting Up a Call to a Known IP Address
To understand the essence of SIP, it is best to take a look at a concrete example. In
this example, Alice is at her PC and she wants to call Bob, who is also working at
his PC. Alice’s and Bob’s PCs are both equipped with SIP-based software for making
and receiving phone calls. In this initial example, we’ll assume that Alice knows
the IP address of Bob’s PC. Figure 7.12 illustrates the SIP call-establishment
process.
In Figure 7.12, we see that an SIP session begins when Alice sends Bob an
INVITE message, which resembles an HTTP request message. This INVITE message
is sent over UDP to the well-known port 5060 for SIP. (SIP messages can also
be sent over TCP.) The INVITE message includes an identifier for Bob
(bob@193.64.210.89), an indication of Alice’s current IP address, an indication that
Alice desires to receive audio, which is to be encoded in format AVP 0 (PCM
encoded -law) and encapsulated in RTP, and an indication that she wants to receive
7.4 • PROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS 627
the RTP packets on port 38060. After receiving Alice’s INVITE message, Bob sends
an SIP response message, which resembles an HTTP response message. This
response SIP message is also sent to the SIP port 5060. Bob’s response includes a
200 OK as well as an indication of his IP address, his desired encoding and packetization
for reception, and his port number to which the audio packets should be sent.
Note that in this example Alice and Bob are going to use different audio-encoding
mechanisms: Alice is asked to encode her audio with GSM whereas Bob is asked to
encode his audio with PCM -law. After receiving Bob’s response, Alice sends Bob
an SIP acknowledgment message. After this SIP transaction, Bob and Alice can talk.
(For visual convenience, Figure 7.12 shows Alice talking after Bob, but in truth they
628 CHAPTER 7 • MULTIMEDIA NETWORKING
Time Time
167.180.112.24
INVITE bob@193.64.210.89
c=IN IP4 167.180.112.24
m=audio 38060 RTP/AVP 0
200 OK
c=In IP4 193.64.210.89
m=audio 48753 RTP/AVP 3
Bob’s
terminal rings
193.64.210.89
µ Law audio
port 5060
port 5060
port 38060
Alice Bob
port 5060
port 48753
ACK
GSM
Figure 7.12  SIP call establishment when Alice knows Bob’s IP address
would normally talk at the same time.) Bob will encode and packetize the audio as
requested and send the audio packets to port number 38060 at IP address
167.180.112.24. Alice will also encode and packetize the audio as requested and
send the audio packets to port number 48753 at IP address 193.64.210.89.
From this simple example, we have learned a number of key characteristics
of SIP. First, SIP is an out-of-band protocol: The SIP messages are sent and
received in sockets that are different from those used for sending and receiving
the media data. Second, the SIP messages themselves are ASCII-readable and
resemble HTTP messages. Third, SIP requires all messages to be acknowledged,
so it can run over UDP or TCP.
In this example, let’s consider what would happen if Bob does not have a
PCM -law codec for encoding audio. In this case, instead of responding with 200
OK, Bob would likely respond with a 600 Not Acceptable and list in the message
all the codecs he can use. Alice would then choose one of the listed codecs and
send another INVITE message, this time advertising the chosen codec. Bob could
also simply reject the call by sending one of many possible rejection reply codes.
(There are many such codes, including “busy,” “gone,” “payment required,” and
“forbidden.”)
SIP Addresses
In the previous example, Bob’s SIP address is sip:bob@193.64.210.89. However,
we expect many—if not most—SIP addresses to resemble e-mail addresses. For
example, Bob’s address might be sip:bob@domain.com. When Alice’s SIP device
sends an INVITE message, the message would include this e-mail-like address;
the SIP infrastructure would then route the message to the IP device that Bob is
currently using (as we’ll discuss below). Other possible forms for the SIP address
could be Bob’s legacy phone number or simply Bob’s first/middle/last name
(assuming it is unique).
An interesting feature of SIP addresses is that they can be included in Web
pages, just as people’s e-mail addresses are included in Web pages with the mailto
URL. For example, suppose Bob has a personal homepage, and he wants to provide
a means for visitors to the homepage to call him. He could then simply
include the URL sip:bob@domain.com. When the visitor clicks on the URL, the
SIP application in the visitor’s device is launched and an INVITE message is sent
to Bob.
SIP Messages
In this short introduction to SIP, we’ll not cover all SIP message types and headers.
Instead, we’ll take a brief look at the SIP INVITE message, along with a few common
header lines. Let us again suppose that Alice wants to initiate a VoIP call to
Bob, and this time Alice knows only Bob’s SIP address, bob@domain.com, and
7.4 • PROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS 629
does not know the IP address of the device that Bob is currently using. Then her
message might look something like this:
INVITE sip:bob@domain.com SIP/2.0
Via: SIP/2.0/UDP 167.180.112.24
From: sip:alice@hereway.com
To: sip:bob@domain.com
Call-ID: a2e3a@pigeon.hereway.com
Content-Type: application/sdp
Content-Length: 885
c=IN IP4 167.180.112.24
m=audio 38060 RTP/AVP 0
The INVITE line includes the SIP version, as does an HTTP request message.
Whenever an SIP message passes through an SIP device (including the device that originates
the message), it attaches a Via header, which indicates the IP address of the
device. (We’ll see soon that the typical INVITE message passes through many SIP
devices before reaching the callee’s SIP application.) Similar to an e-mail message, the
SIP message includes a From header line and a To header line. The message includes a
Call-ID, which uniquely identifies the call (similar to the message-ID in e-mail). It
includes a Content-Type header line, which defines the format used to describe the content
contained in the SIP message. It also includes a Content-Length header line, which
provides the length in bytes of the content in the message. Finally, after a carriage return
and line feed, the message contains the content. In this case, the content provides information
about Alice’s IP address and how Alice wants to receive the audio.
Name Translation and User Location
In the example in Figure 7.12, we assumed that Alice’s SIP device knew the IP
address where Bob could be contacted. But this assumption is quite unrealistic, not
only because IP addresses are often dynamically assigned with DHCP, but also
because Bob may have multiple IP devices (for example, different devices for his
home, work, and car). So now let us suppose that Alice knows only Bob’s e-mail
address, bob@domain.com, and that this same address is used for SIP-based calls.
In this case, Alice needs to obtain the IP address of the device that the user
bob@domain.com is currently using. To find this out, Alice creates an INVITE message
that begins with INVITE bob@domain.com SIP/2.0 and sends this message to
an SIP proxy. The proxy will respond with an SIP reply that might include the IP
address of the device that bob@domain.com is currently using. Alternatively, the
reply might include the IP address of Bob’s voicemail box, or it might include a
URL of a Web page (that says “Bob is sleeping. Leave me alone!”). Also, the result
returned by the proxy might depend on the caller: If the call is from Bob’s wife, he
630 CHAPTER 7 • MULTIMEDIA NETWORKING
might accept the call and supply his IP address; if the call is from Bob’s mother-inlaw,
he might respond with the URL that points to the I-am-sleeping Web page!
Now, you are probably wondering, how can the proxy server determine the current
IP address for bob@domain.com? To answer this question, we need to say a few
words about another SIP device, the SIP registrar. Every SIP user has an associated
registrar. Whenever a user launches an SIP application on a device, the application
sends an SIP register message to the registrar, informing the registrar of its current
IP address. For example, when Bob launches his SIP application on his PDA, the
application would send a message along the lines of:
REGISTER sip:domain.com SIP/2.0
Via: SIP/2.0/UDP 193.64.210.89
From: sip:bob@domain.com
To: sip:bob@domain.com
Expires: 3600
Bob’s registrar keeps track of Bob’s current IP address. Whenever Bob switches
to a new SIP device, the new device sends a new register message, indicating the
new IP address. Also, if Bob remains at the same device for an extended period of
time, the device will send refresh register messages, indicating that the most
recently sent IP address is still valid. (In the example above, refresh messages need
to be sent every 3600 seconds to maintain the address at the registrar server.) It is
worth noting that the registrar is analogous to a DNS authoritative name server: The
DNS server translates fixed host names to fixed IP addresses; the SIP registrar translates
fixed human identifiers (for example, bob@domain.com) to dynamic IP
addresses. Often SIP registrars and SIP proxies are run on the same host.
Now let’s examine how Alice’s SIP proxy server obtains Bob’s current IP
address. From the preceding discussion we see that the proxy server simply needs to
forward Alice’s INVITE message to Bob’s registrar/proxy. The registrar/proxy
could then forward the message to Bob’s current SIP device. Finally, Bob, having
now received Alice’s INVITE message, could send an SIP response to Alice.
As an example, consider Figure 7.13, in which jim@umass.edu, currently
working on 217.123.56.89, wants to initiate a Voice-over-IP (VoIP) session with
keith@upenn.edu, currently working on 197.87.54.21. The following steps are
taken: (1) Jim sends an INVITE message to the umass SIP proxy. (2) The proxy
does a DNS lookup on the SIP registrar upenn.edu (not shown in diagram) and then
forwards the message to the registrar server. (3) Because keith@upenn.edu is no
longer registered at the upenn registrar, the upenn registrar sends a redirect response,
indicating that it should try keith@eurecom.fr. (4) The umass proxy sends an
INVITE message to the eurecom SIP registrar. (5) The eurecom registrar knows the
IP address of keith@eurecom.fr and forwards the INVITE message to the host
197.87.54.21, which is running Keith’s SIP client. (6–8) An SIP response is sent back
through registrars/proxies to the SIP client on 217.123.56.89. (9) Media is sent
7.4 • PROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS 631
directly between the two clients. (There is also an SIP acknowledgment message,
which is not shown.)
Our discussion of SIP has focused on call initiation for voice calls. SIP, being a
signaling protocol for initiating and ending calls in general, can be used for video
conference calls as well as for text-based sessions. In fact, SIP has become a fundamental
component in many instant messaging applications. Readers desiring to
learn more about SIP are encouraged to visit Henning Schulzrinne’s SIPWeb site
[Schulzrinne-SIP 2012]. In particular, on this site you will find open source software
for SIP clients and servers [SIP Software 2012].
7.5 Network Support for Multimedia
In Sections 7.2 through 7.4, we learned how application-level mechanisms such as
client buffering, prefetching, adapting media quality to available bandwidth, adaptive
playout, and loss mitigation techniques can be used by multimedia applications
632 CHAPTER 7 • MULTIMEDIA NETWORKING
9
5
6
4
7
2
3
1
8
SIP registrar
upenn.edu
SIP proxy
umass.edu
SIP client
217.123.56.89
SIP client
197.87.54.21
SIP registrar
eurcom.fr
Figure 7.13  Session initiation, involving SIP proxies and registrars
to improve a multimedia application’s performance. We also learned how content
distribution networks and P2P overlay networks can be used to provide a systemlevel
approach for delivering multimedia content. These techniques and approaches
are all designed to be used in today’s best-effort Internet. Indeed, they are in use
today precisely because the Internet provides only a single, best-effort class of service.
But as designers of computer networks, we can’t help but ask whether the
network (rather than the applications or application-level infrastructure alone) might
provide mechanisms to support multimedia content delivery. As we’ll see shortly,
the answer is, of course, “yes”! But we’ll also see that a number of these new network-
level mechanisms have yet to be widely deployed. This may be due to their
complexity and to the fact that application-level techniques together with best-effort
service and properly dimensioned network resources (for example, bandwidth) can
indeed provide a “good-enough” (even if not-always-perfect) end-to-end multimedia
delivery service.
Table 7.4 summarizes three broad approaches towards providing network-level
support for multimedia applications.
• Making the best of best-effort service. The application-level mechanisms and
infrastructure that we studied in Sections 7.2 through 7.4 can be successfully
used in a well-dimensioned network where packet loss and excessive end-to-end
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 633
Approach Granularity Guarantee Mechanisms Complexity Deployment to date
Making the all traffic none, or application- minimal everywhere
best of best- treated soft layer support,
effort service. equally CDNs, overlays,
network-level
resource
provisioning
Differentiated different none, packet marking, medium some
service classes of or soft policing,
traffic scheduling
treated
differently
Per-connection each soft or hard, packet marking, light little
Quality-of- source- once flow policing, scheduling;
Service (QoS) destination is admitted call admission and
Guarantees flows treated signaling
differently
Table 7.4  Three network-level approaches to supporting multimedia
applications
delay rarely occur. When demand increases are forecasted, the ISPs deploy additional
bandwidth and switching capacity to continue to ensure satisfactory delay
and packet-loss performance [Huang 2005]. We’ll discuss such network dimensioning
further in Section 7.5.1.
• Differentiated service. Since the early days of the Internet, it’s been envisioned
that different types of traffic (for example, as indicated in the Type-of-Service
field in the IP4v packet header) could be provided with different classes of service,
rather than a single one-size-fits-all best-effort service. With differentiated
service, one type of traffic might be given strict priority over another class of
traffic when both types of traffic are queued at a router. For example, packets
belonging to a real-time conversational application might be given priority over
other packets due to their stringent delay constraints. Introducing differentiated
service into the network will require new mechanisms for packet marking (indicating
a packet’s class of service), packet scheduling, and more. We’ll cover differentiated
service, and new network mechanisms needed to implement this
service, in Section 7.5.2.
• Per-connection Quality-of-Service (QoS) Guarantees. With per-connection
QoS guarantees, each instance of an application explicitly reserves end-to-end
bandwidth and thus has a guaranteed end-to-end performance. A hard guarantee
means the application will receive its requested quality of service (QoS) with
certainty. A soft guarantee means the application will receive its requested
quality of service with high probability. For example, if a user wants to make a
VoIP call from Host A to Host B, the user’s VoIP application reserves bandwidth
explicitly in each link along a route between the two hosts. But permitting
applications to make reservations and requiring the network to honor the
reservations requires some big changes. First, we need a protocol that, on
behalf of the applications, reserves link bandwidth on the paths from the
senders to their receivers. Second, we’ll need new scheduling policies in the
router queues so that per-connection bandwidth reservations can be honored.
Finally, in order to make a reservation, the applications must give the network
a description of the traffic that they intend to send into the network and the network
will need to police each application’s traffic to make sure that it abides
by that description. These mechanisms, when combined, require new and complex
software in hosts and routers. Because per-connection QoS guaranteed
service has not seen significant deployment, we’ll cover these mechanisms
only briefly in Section 7.5.3.
7.5.1 Dimensioning Best-Effort Networks
Fundamentally, the difficulty in supporting multimedia applications arises from
their stringent performance requirements––low end-to-end packet delay, delay
634 CHAPTER 7 • MULTIMEDIA NETWORKING
jitter, and loss—and the fact that packet delay, delay jitter, and loss occur whenever
the network becomes congested. A first approach to improving the quality
of multimedia applications—an approach that can often be used to solve just
about any problem where resources are constrained—is simply to “throw money
at the problem” and thus simply avoid resource contention. In the case of networked
multimedia, this means providing enough link capacity throughout the
network so that network congestion, and its consequent packet delay and loss,
never (or only very rarely) occurs. With enough link capacity, packets could zip
through today’s Internet without queuing delay or loss. From many perspectives
this is an ideal situation—multimedia applications would perform perfectly, users
would be happy, and this could all be achieved with no changes to Internet’s besteffort
architecture.
The question, of course, is how much capacity is “enough” to achieve this
nirvana, and whether the costs of providing “enough” bandwidth are practical
from a business standpoint to the ISPs. The question of how much capacity to
provide at network links in a given topology to achieve a given level of performance
is often known as bandwidth provisioning. The even more complicated
problem of how to design a network topology (where to place routers, how to
interconnect routers with links, and what capacity to assign to links) to achieve a
given level of end-to-end performance is a network design problem often referred
to as network dimensioning. Both bandwidth provisioning and network dimensioning
are complex topics, well beyond the scope of this textbook. We note here,
however, that the following issues must be addressed in order to predict application-
level performance between two network end points, and thus provision
enough capacity to meet an application’s performance requirements.
• Models of traffic demand between network end points. Models may need to be
specified at both the call level (for example, users “arriving” to the network and
starting up end-to-end applications) and at the packet level (for example, packets
being generated by ongoing applications). Note that workload may change over
time.
• Well-defined performance requirements. For example, a performance requirement
for supporting delay-sensitive traffic, such as a conversational multimedia
application, might be that the probability that the end-to-end delay of the packet
is greater than a maximum tolerable delay be less than some small value
[Fraleigh 2003].
• Models to predict end-to-end performance for a given workload model, and techniques
to find a minimal cost bandwidth allocation that will result in all user
requirements being met. Here, researchers are busy developing performance
models that can quantify performance for a given workload, and optimization
techniques to find minimal-cost bandwidth allocations meeting performance
requirements.
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 635
Given that today’s best-effort Internet could (from a technology standpoint)
support multimedia traffic at an appropriate performance level if it were dimensioned
to do so, the natural question is why today’s Internet doesn’t do so. The
answers are primarily economic and organizational. From an economic standpoint,
would users be willing to pay their ISPs enough for the ISPs to install sufficient
bandwidth to support multimedia applications over a best-effort Internet? The organizational
issues are perhaps even more daunting. Note that an end-to-end path
between two multimedia end points will pass through the networks of multiple ISPs.
From an organizational standpoint, would these ISPs be willing to cooperate
(perhaps with revenue sharing) to ensure that the end-to-end path is properly dimensioned
to support multimedia applications? For a perspective on these economic and
organizational issues, see [Davies 2005]. For a perspective on provisioning tier-1
backbone networks to support delay-sensitive traffic, see [Fraleigh 2003].
7.5.2 Providing Multiple Classes of Service
Perhaps the simplest enhancement to the one-size-fits-all best-effort service in
today’s Internet is to divide traffic into classes, and provide different levels of service
to these different classes of traffic. For example, an ISP might well want to provide
a higher class of service to delay-sensitive Voice-over-IP or teleconferencing
traffic (and charge more for this service!) than to elastic traffic such as email or
HTTP. Alternatively, an ISP may simply want to provide a higher quality of service
to customers willing to pay more for this improved service. A number of residential
wired-access ISPs and cellular wireless-access ISPs have adopted such tiered levels
of service—with platinum-service subscribers receiving better performance than
gold- or silver-service subscribers.
We’re all familiar with different classes of service from our everyday lives—
first-class airline passengers get better service than business-class passengers, who
in turn get better service than those of us who fly economy class; VIPs are provided
immediate entry to events while everyone else waits in line; elders are revered in
some countries and provided seats of honor and the finest food at a table. It’s important
to note that such differential service is provided among aggregates of traffic,
that is, among classes of traffic, not among individual connections. For example, all
first-class passengers are handled the same (with no first-class passenger receiving
any better treatment than any other first-class passenger), just as all VoIP packets
would receive the same treatment within the network, independent of the particular
end-to-end connection to which they belong. As we will see, by dealing with a small
number of traffic aggregates, rather than a large number of individual connections,
the new network mechanisms required to provide better-than-best service can be
kept relatively simple.
The early Internet designers clearly had this notion of multiple classes of service
in mind. Recall the type-of-service (ToS) field in the IPv4 header in Figure 4.13.
636 CHAPTER 7 • MULTIMEDIA NETWORKING
IEN123 [ISI 1979] describes the ToS field also present in an ancestor of the IPv4
datagram as follows: “The Type of Service [field] provides an indication of the
abstract parameters of the quality of service desired. These parameters are to be used
to guide the selection of the actual service parameters when transmitting a datagram
through a particular network. Several networks offer service precedence, which
somehow treats high precedence traffic as more important that other traffic.” More
than four decades ago, the vision of providing different levels of service to different
classes of traffic was clear! However, it’s taken us an equally long period of time to
realize this vision.
Motivating Scenarios
Let’s begin our discussion of network mechanisms for providing multiple classes of
service with a few motivating scenarios.
Figure 7.14 shows a simple network scenario in which two application packet
flows originate on Hosts H1 and H2 on one LAN and are destined for Hosts H3 and
H4 on another LAN. The routers on the two LANs are connected by a 1.5 Mbps
link. Let’s assume the LAN speeds are significantly higher than 1.5 Mbps, and focus
on the output queue of router R1; it is here that packet delay and packet loss will
occur if the aggregate sending rate of H1 and H2 exceeds 1.5 Mbps. Let’s further
suppose that a 1 Mbps audio application (for example, a CD-quality audio call)
shares the 1.5 Mbps link between R1 and R2 with an HTTPWeb-browsing application
that is downloading a Web page from H2 to H4.
R1
1.5 Mbps link R2
H2
H1
H4
H3
Figure 7.14  Competing audio and HTTP applications
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 637
In the best-effort Internet, the audio and HTTP packets are mixed in the output
queue at R1 and (typically) transmitted in a first-in-first-out (FIFO) order. In
this scenario, a burst of packets from the Web server could potentially fill up the
queue, causing IP audio packets to be excessively delayed or lost due to buffer
overflow at R1. How should we solve this potential problem? Given that the
HTTP Web-browsing application does not have time constraints, our intuition
might be to give strict priority to audio packets at R1. Under a strict priority
scheduling discipline, an audio packet in the R1 output buffer would always be
transmitted before any HTTP packet in the R1 output buffer. The link from R1 to
R2 would look like a dedicated link of 1.5 Mbps to the audio traffic, with HTTP
traffic using the R1-to-R2 link only when no audio traffic is queued. In order for
R1 to distinguish between the audio and HTTP packets in its queue, each packet
must be marked as belonging to one of these two classes of traffic. This was the
original goal of the type-of-service (ToS) field in IPv4. As obvious as this might
seem, this then is our first insight into mechanisms needed to provide multiple
classes of traffic:
Insight 1: Packet marking allows a router to distinguish among packets
belonging to different classes of traffic.
Note that although our example considers a competing multimedia and elastic
flow, the same insight applies to the case that platinum, gold, and silver classes of
service are implemented—a packet-marking mechanism is still needed to indicate
that class of service to which a packet belongs.
Now suppose that the router is configured to give priority to packets marked as
belonging to the 1 Mbps audio application. Since the outgoing link speed is
1.5 Mbps, even though the HTTP packets receive lower priority, they can still, on
average, receive 0.5 Mbps of transmission service. But what happens if the audio
application starts sending packets at a rate of 1.5 Mbps or higher (either maliciously
or due to an error in the application)? In this case, the HTTP packets will starve, that
is, they will not receive any service on the R1-to-R2 link. Similar problems would
occur if multiple applications (for example, multiple audio calls), all with the same
class of service as the audio application, were sharing the link’s bandwidth; they too
could collectively starve the FTP session. Ideally, one wants a degree of isolation
among classes of traffic so that one class of traffic can be protected from the other.
This protection could be implemented at different places in the network—at each
and every router, at first entry to the network, or at inter-domain network boundaries.
This then is our second insight:
Insight 2: It is desirable to provide a degree of traffic isolation among classes
so that one class is not adversely affected by another class of traffic that misbehaves.
638 CHAPTER 7 • MULTIMEDIA NETWORKING
We’ll examine several specific mechanisms for providing such isolation
among traffic classes. We note here that two broad approaches can be taken.
First, it is possible to perform traffic policing, as shown in Figure 7.15. If a traffic
class or flow must meet certain criteria (for example, that the audio flow not
exceed a peak rate of 1 Mbps), then a policing mechanism can be put into place
to ensure that these criteria are indeed observed. If the policed application misbehaves,
the policing mechanism will take some action (for example, drop or
delay packets that are in violation of the criteria) so that the traffic actually entering
the network conforms to the criteria. The leaky bucket mechanism that we’ll
examine shortly is perhaps the most widely used policing mechanism. In Figure
7.15, the packet classification and marking mechanism (Insight 1) and the policing
mechanism (Insight 2) are both implemented together at the network’s edge,
either in the end system or at an edge router.
A complementary approach for providing isolation among traffic classes is
for the link-level packet-scheduling mechanism to explicitly allocate a fixed
R1
1.5 Mbps link
Packet marking
and policing
Metering and policing Marks
R2
H2
H1
Key:
H4
H3
Figure 7.15  Policing (and marking) the audio and HTTP traffic classes
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 639
amount of link bandwidth to each class. For example, the audio class could be
allocated 1 Mbps at R1, and the HTTP class could be allocated 0.5 Mbps. In this
case, the audio and HTTP flows see a logical link with capacity 1.0 and 0.5
Mbps, respectively, as shown in Figure 7.16. With strict enforcement of the linklevel
allocation of bandwidth, a class can use only the amount of bandwidth that
has been allocated; in particular, it cannot utilize bandwidth that is not currently
being used by others. For example, if the audio flow goes silent (for example, if
the speaker pauses and generates no audio packets), the HTTP flow would still
not be able to transmit more than 0.5 Mbps over the R1-to-R2 link, even though
the audio flow’s 1 Mbps bandwidth allocation is not being used at that moment.
Since bandwidth is a “use-it-or-lose-it” resource, there is no reason to prevent
HTTP traffic from using bandwidth not used by the audio traffic. We’d like to use
bandwidth as efficiently as possible, never wasting it when it could be otherwise
used. This gives rise to our third insight:
Insight 3: While providing isolation among classes or flows, it is desirable
to use resources (for example, link bandwidth and buffers) as efficiently as
possible.
Scheduling Mechanisms
Recall from our discussion in Section 1.3 and Section 4.3 that packets belonging
to various network flows are multiplexed and queued for transmission at the
640 CHAPTER 7 • MULTIMEDIA NETWORKING
R1
1.5 Mbps link
1.0 Mbps
logical link
0.5 Mbps
logical link
R2
H2
H1
H4
H3
Figure 7.16  Logical isolation of audio and HTTP traffic classes
output buffers associated with a link. The manner in which queued packets are
selected for transmission on the link is known as the link-scheduling discipline.
Let us now consider several of the most important link-scheduling disciplines in
more detail.
First-In-First-Out (FIFO)
Figure 7.17 shows the queuing model abstractions for the FIFO link-scheduling discipline.
Packets arriving at the link output queue wait for transmission if the link is
currently busy transmitting another packet. If there is not sufficient buffering space
to hold the arriving packet, the queue’s packet-discarding policy then determines
whether the packet will be dropped (lost) or whether other packets will be removed
from the queue to make space for the arriving packet. In our discussion below, we
will ignore packet discard. When a packet is completely transmitted over the outgoing
link (that is, receives service) it is removed from the queue.
The FIFO (also known as first-come-first-served, or FCFS) scheduling discipline
selects packets for link transmission in the same order in which they arrived at
the output link queue. We’re all familiar with FIFO queuing from bus stops (particularly
in England, where queuing seems to have been perfected) or other service
centers, where arriving customers join the back of the single waiting line, remain in
order, and are then served when they reach the front of the line.
Figure 7.18 shows the FIFO queue in operation. Packet arrivals are indicated
by numbered arrows above the upper timeline, with the number indicating the order
R1
1.5 Mbps link
R1 output
interface queue
R2
H2
H1
H4
H3
Figure 7.17  FIFO queuing abstraction
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 641
in which the packet arrived. Individual packet departures are shown below the lower
timeline. The time that a packet spends in service (being transmitted) is indicated by
the shaded rectangle between the two timelines. Because of the FIFO discipline,
packets leave in the same order in which they arrived. Note that after the departure
of packet 4, the link remains idle (since packets 1 through 4 have been transmitted
and removed from the queue) until the arrival of packet 5.
Priority Queuing
Under priority queuing, packets arriving at the output link are classified into priority
classes at the output queue, as shown in Figure 7.19. As discussed in the previous section,
a packet’s priority class may depend on an explicit marking that it carries in its
packet header (for example, the value of the ToS bits in an IPv4 packet), its source or
destination IP address, its destination port number, or other criteria. Each priority class
typically has its own queue. When choosing a packet to transmit, the priority queuing
discipline will transmit a packet from the highest priority class that has a nonempty
queue (that is, has packets waiting for transmission). The choice among packets in the
same priority class is typically done in a FIFO manner.
Figure 7.20 illustrates the operation of a priority queue with two priority
classes. Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5
belong to the low-priority class. Packet 1 arrives and, finding the link idle, begins
transmission. During the transmission of packet 1, packets 2 and 3 arrive and are
queued in the low- and high-priority queues, respectively. After the transmission
of packet 1, packet 3 (a high-priority packet) is selected for transmission over
packet 2 (which, even though it arrived earlier, is a low-priority packet). At the end
of the transmission of packet 3, packet 2 then begins transmission. Packet 4 (a
high-priority packet) arrives during the transmission of packet 2 (a low-priority
packet). Under a nonpreemptive priority queuing discipline, the transmission of
642 CHAPTER 7 • MULTIMEDIA NETWORKING
Time
Arrivals
Departures
Packet
in service
Time
1
1 2 3 4 5
2 3
1
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
2 3 4 5
4 5
Figure 7.18  The FIFO queue in operation
a packet is not interrupted once it has begun. In this case, packet 4 queues for
transmission and begins being transmitted after the transmission of packet 2 is
completed.
Round Robin and Weighted Fair Queuing (WFQ)
Under the round robin queuing discipline, packets are sorted into classes as
with priority queuing. However, rather than there being a strict priority of service
among classes, a round robin scheduler alternates service among the classes. In
the simplest form of round robin scheduling, a class 1 packet is transmitted, followed
by a class 2 packet, followed by a class 1 packet, followed by a class 2
packet, and so on. A so-called work-conserving queuing discipline will never
allow the link to remain idle whenever there are packets (of any class) queued for
Arrivals
Departures
Packet
in service
1
1 3 2 4 5
2 3
1 3 2 4 5
4 5
Time
Time
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
Figure 7.20  Operation of the priority queue
Arrivals Departures
Low-priority queue
(waiting area)
Classify
High-priority queue
(waiting area)
Link
(server)
Figure 7.19  Priority queuing model
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 643
transmission. A work-conserving round robin discipline that looks for a packet
of a given class but finds none will immediately check the next class in the round
robin sequence.
Figure 7.21 illustrates the operation of a two-class round robin queue. In
this example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5 belong to the
second class. Packet 1 begins transmission immediately upon arrival at the output
queue. Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for
transmission. After the transmission of packet 1, the link scheduler looks for a class
2 packet and thus transmits packet 3. After the transmission of packet 3, the scheduler
looks for a class 1 packet and thus transmits packet 2. After the transmission of
packet 2, packet 4 is the only queued packet; it is thus transmitted immediately after
packet 2.
A generalized abstraction of round robin queuing that has found considerable
use in QoS architectures is the so-called weighted fair queuing (WFQ) discipline
[Demers 1990; Parekh 1993]. WFQ is illustrated in Figure 7.22. Arriving packets
are classified and queued in the appropriate per-class waiting area. As in round robin
scheduling, a WFQ scheduler will serve classes in a circular manner—first serving
class 1, then serving class 2, then serving class 3, and then (assuming there are three
classes) repeating the service pattern. WFQ is also a work-conserving queuing
discipline and thus will immediately move on to the next class in the service
sequence when it finds an empty class queue.
WFQ differs from round robin in that each class may receive a differential
amount of service in any interval of time. Specifically, each class, i, is assigned a
weight, wi. Under WFQ, during any interval of time during which there are class i
packets to send, class i will then be guaranteed to receive a fraction of service equal
to wi/(Swj), where the sum in the denominator is taken over all classes that also have
packets queued for transmission. In the worst case, even if all classes have queued
packets, class i will still be guaranteed to receive a fraction wi /(Swj) of the
644 CHAPTER 7 • MULTIMEDIA NETWORKING
Arrivals
Packet
in service
1
1 3 2 4 5
2 3
1 3 2 4 5
4 5
Departures
Time
Time
t = 0 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
Figure 7.21  Operation of the two-class round robin queue
bandwidth. Thus, for a link with transmission rate R, class i will always achieve a
throughput of at least R · wi /(Swj). Our description of WFQ has been an idealized
one, as we have not considered the fact that packets are discrete units of data and a
packet’s transmission will not be interrupted to begin transmission of another
packet; [Demers 1990] and [Parekh 1993] discuss this packetization issue. As we
will see in the following sections, WFQ plays a central role in QoS architectures. It
is also available in today’s router products [Cisco QoS 2012].
Policing: The Leaky Bucket
One of our earlier insights was that policing, the regulation of the rate at which a
class or flow (we will assume the unit of policing is a flow in our discussion below)
is allowed to inject packets into the network, is an important QoS mechanism. But
what aspects of a flow’s packet rate should be policed? We can identify three important
policing criteria, each differing from the other according to the time scale over
which the packet flow is policed:
• Average rate. The network may wish to limit the long-term average rate (packets
per time interval) at which a flow’s packets can be sent into the network. A
crucial issue here is the interval of time over which the average rate will be
policed. A flow whose average rate is limited to 100 packets per second is
more constrained than a source that is limited to 6,000 packets per minute, even
though both have the same average rate over a long enough interval of time. For
example, the latter constraint would allow a flow to send 1,000 packets in a given
second-long interval of time, while the former constraint would disallow this
sending behavior.
Classify
Arrivals Departures
w1
w2
w3 Link
Figure 7.22  Weighted fair queuing (WFQ)
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 645
• Peak rate. While the average-rate constraint limits the amount of traffic that can
be sent into the network over a relatively long period of time, a peak-rate constraint
limits the maximum number of packets that can be sent over a shorter
period of time. Using our example above, the network may police a flow at an
average rate of 6,000 packets per minute, while limiting the flow’s peak rate to
1,500 packets per second.
• Burst size. The network may also wish to limit the maximum number of packets
(the “burst” of packets) that can be sent into the network over an extremely short
interval of time. In the limit, as the interval length approaches zero, the burst size
limits the number of packets that can be instantaneously sent into the network.
Even though it is physically impossible to instantaneously send multiple packets
into the network (after all, every link has a physical transmission rate that cannot
be exceeded!), the abstraction of a maximum burst size is a useful one.
The leaky bucket mechanism is an abstraction that can be used to characterize
these policing limits. As shown in Figure 7.23, a leaky bucket consists of a bucket
that can hold up to b tokens. Tokens are added to this bucket as follows. New tokens,
which may potentially be added to the bucket, are always being generated at a rate
of r tokens per second. (We assume here for simplicity that the unit of time is a second.)
If the bucket is filled with less than b tokens when a token is generated, the
newly generated token is added to the bucket; otherwise the newly generated token
is ignored, and the token bucket remains full with b tokens.
Let us now consider how the leaky bucket can be used to police a packet flow.
Suppose that before a packet is transmitted into the network, it must first remove a
646 CHAPTER 7 • MULTIMEDIA NETWORKING
To network
Packets
Remove
token
Token
wait area
Bucket holds
up to
b tokens
r tokens/sec
Figure 7.23  The leaky bucket policer
token from the token bucket. If the token bucket is empty, the packet must wait for
a token. (An alternative is for the packet to be dropped, although we will not consider
that option here.) Let us now consider how this behavior polices a traffic flow. Because
there can be at most b tokens in the bucket, the maximum burst size for a leaky-bucketpoliced
flow is b packets. Furthermore, because the token generation rate is r, the maximum
number of packets that can enter the network of any interval of time of length t
is rt + b. Thus, the token-generation rate, r, serves to limit the long-term average rate
at which packets can enter the network. It is also possible to use leaky buckets (specifically,
two leaky buckets in series) to police a flow’s peak rate in addition to the longterm
average rate; see the homework problems at the end of this chapter.
Leaky Bucket + Weighted Fair Queuing = Provable Maximum Delay in a
Queue
Let’s close our discussion of scheduling and policing by showing how the two can
be combined to provide a bound on the delay through a router’s queue. Let’s consider
a router’s output link that multiplexes n flows, each policed by a leaky bucket
with parameters bi and ri, i = 1, . . . , n, using WFQ scheduling. We use the term flow
here loosely to refer to the set of packets that are not distinguished from each other
by the scheduler. In practice, a flow might be comprised of traffic from a single endto-
end connection or a collection of many such connections, see Figure 7.24.
Recall from our discussion of WFQ that each flow, i, is guaranteed to receive a
share of the link bandwidth equal to at least R · wi/(Swj), where R is the transmission
b1
r1
w1
wn
bn
rn
Figure 7.24  n multiplexed leaky bucket flows with WFQ scheduling
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 647
rate of the link in packets/sec. What then is the maximum delay that a packet will
experience while waiting for service in the WFQ (that is, after passing through the
leaky bucket)? Let us focus on flow 1. Suppose that flow 1’s token bucket is initially
full. A burst of b1 packets then arrives to the leaky bucket policer for flow 1. These
packets remove all of the tokens (without wait) from the leaky bucket and then join
the WFQ waiting area for flow 1. Since these b1 packets are served at a rate of at least
R · wi /(Swj) packet/sec, the last of these packets will then have a maximum delay,
dmax, until its transmission is completed, where
The rationale behind this formula is that if there are b1 packets in the queue and
packets are being serviced (removed) from the queue at a rate of at least R · w1/
(Swj) packets per second, then the amount of time until the last bit of the last packet
is transmitted cannot be more than b1/(R · w1/(Swj)). A homework problem asks you
to prove that as long as r1 < R · w1/(Swj), then dmax is indeed the maximum delay
that any packet in flow 1 will ever experience in the WFQ queue.
7.5.3 Diffserv
Having seen the motivation, insights, and specific mechanisms for providing multiple
classes of service, let’s wrap up our study of approaches toward proving multiple
classes of service with an example—the Internet Diffserv architecture [RFC
2475; RFC Kilkki 1999]. Diffserv provides service differentiation—that is, the ability
to handle different classes of traffic in different ways within the Internet in a scalable
manner. The need for scalability arises from the fact that millions of
simultaneous source-destination traffic flows may be present at a backbone router.
We’ll see shortly that this need is met by placing only simple functionality within
the network core, with more complex control operations being implemented at the
network’s edge.
Let’s begin with the simple network shown in Figure 7.25. We’ll describe one
possible use of Diffserv here; other variations are possible, as described in RFC
2475. The Diffserv architecture consists of two sets of functional elements:
• Edge functions: packet classification and traffic conditioning. At the incoming
edge of the network (that is, at either a Diffserv-capable host that generates
traffic or at the first Diffserv-capable router that the traffic passes through), arriving
packets are marked. More specifically, the differentiated service (DS) field in
the IPv4 or IPv6 packet header is set to some value [RFC 3260]. The definition
of the DS field is intended to supersede the earlier definitions of the IPv4 typeof-
service field and the IPv6 traffic class fields that we discussed in Chapter 4.
For example, in Figure 7.25, packets being sent from H1 to H3 might be marked
dmax =
b1
R  w1> gwj
648 CHAPTER 7 • MULTIMEDIA NETWORKING
at R1, while packets being sent from H2 to H4 might be marked at R2. The mark
that a packet receives identifies the class of traffic to which it belongs. Different
classes of traffic will then receive different service within the core network.
• Core function: forwarding. When a DS-marked packet arrives at a Diffservcapable
router, the packet is forwarded onto its next hop according to the so-called
per-hop behavior (PHB) associated with that packet’s class. The per-hop behavior
influences how a router’s buffers and link bandwidth are shared among the competing
classes of traffic. Acrucial tenet of the Diffserv architecture is that a router’s perhop
behavior will be based only on packet markings, that is, the class of traffic to
which a packet belongs. Thus, if packets being sent from H1 to H3 in Figure 7.25
receive the same marking as packets being sent from H2 to H4, then the network
routers treat these packets as an aggregate, without distinguishing whether the packets
originated at H1 or H2. For example, R3 would not distinguish between packets
from H1 and H2 when forwarding these packets on to R4. Thus, the Diffserv architecture
obviates the need to keep router state for individual source-destination
pairs—a critical consideration in making Diffserv scalable.
An analogy might prove useful here. At many large-scale social events (for example, a
large public reception, a large dance club or discothèque, a concert, or a football game),
people entering the event receive a pass of one type or another: VIP passes for Very
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 649
R4
Leaf router
Key:
Core router
R2
R1 R6
R7
R3 R5
H1
H2
H4
H3
R2 R3
Figure 7.25  A simple Diffserv network example
Important People; over-21 passes for people who are 21 years old or older (for example,
if alcoholic drinks are to be served); backstage passes at concerts; press passes for
reporters; even an ordinary pass for the Ordinary Person. These passes are typically distributed
upon entry to the event, that is, at the edge of the event. It is here at the edge
where computationally intensive operations, such as paying for entry, checking for the
appropriate type of invitation, and matching an invitation against a piece of identification,
are performed. Furthermore, there may be a limit on the number of people of a
given type that are allowed into an event. If there is such a limit, people may have to
wait before entering the event. Once inside the event, one’s pass allows one to receive
differentiated service at many locations around the event—a VIP is provided with free
drinks, a better table, free food, entry to exclusive rooms, and fawning service. Conversely,
an ordinary person is excluded from certain areas, pays for drinks, and receives
only basic service. In both cases, the service received within the event depends solely
on the type of one’s pass. Moreover, all people within a class are treated alike.
Figure 7.26 provides a logical view of the classification and marking functions
within the edge router. Packets arriving to the edge router are first classified. The
classifier selects packets based on the values of one or more packet header fields
(for example, source address, destination address, source port, destination port, and
protocol ID) and steers the packet to the appropriate marking function. As noted
above, a packet’s marking is carried in the DS field in the packet header.
In some cases, an end user may have agreed to limit its packet-sending rate to conform
to a declared traffic profile. The traffic profile might contain a limit on the peak
rate, as well as the burstiness of the packet flow, as we saw previously with the leaky
bucket mechanism. As long as the user sends packets into the network in a way that
conforms to the negotiated traffic profile, the packets receive their priority marking and
are forwarded along their route to the destination. On the other hand, if the traffic profile
is violated, out-of-profile packets might be marked differently, might be shaped (for
example, delayed so that a maximum rate constraint would be observed), or might be
dropped at the network edge. The role of the metering function, shown in Figure 7.26,
is to compare the incoming packet flow with the negotiated traffic profile and to determine
whether a packet is within the negotiated traffic profile. The actual decision about
whether to immediately remark, forward, delay, or drop a packet is a policy issue determined
by the network administrator and is not specified in the Diffserv architecture.
So far, we have focused on the marking and policing functions in the Diffserv
architecture. The second key component of the Diffserv architecture involves the
per-hop behavior (PHB) performed by Diffserv-capable routers. PHB is rather cryptically,
but carefully, defined as “a description of the externally observable forwarding
behavior of a Diffserv node applied to a particular Diffserv behavior aggregate”
[RFC 2475]. Digging a little deeper into this definition, we can see several important
considerations embedded within:
• A PHB can result in different classes of traffic receiving different performance
(that is, different externally observable forwarding behaviors).
650 CHAPTER 7 • MULTIMEDIA NETWORKING
• While a PHB defines differences in performance (behavior) among classes, it
does not mandate any particular mechanism for achieving these behaviors. As
long as the externally observable performance criteria are met, any implementation
mechanism and any buffer/bandwidth allocation policy can be used. For
example, a PHB would not require that a particular packet-queuing discipline
(for example, a priority queue versus a WFQ queue versus a FCFS queue) be
used to achieve a particular behavior. The PHB is the end, to which resource allocation
and implementation mechanisms are the means.
• Differences in performance must be observable and hence measurable.
Two PHBs have been defined: an expedited forwarding (EF) PHB [RFC 3246] and
an assured forwarding (AF) PHB [RFC 2597]. The expedited forwarding PHB
specifies that the departure rate of a class of traffic from a router must equal or
exceed a configured rate. The assured forwarding PHB divides traffic into four
classes, where each AF class is guaranteed to be provided with some minimum
amount of bandwidth and buffering.
Let’s close our discussion of Diffserv with a few observations regarding its
service model. First, we have implicitly assumed that Diffserv is deployed within a
single administrative domain, but typically an end-to-end service must be fashioned
from multiple ISPs sitting between communicating end systems. In order to provide
end-to-end Diffserv service, all the ISPs between the end systems must not only provide
this service, but most also cooperate and make settlements in order to offer end
customers true end-to-end service. Without this kind of cooperation, ISPs directly
selling Diffserv service to customers will find themselves repeatedly saying: “Yes,
we know you paid extra, but we don’t have a service agreement with the ISP that
dropped and delayed your traffic. I’m sorry that there were so many gaps in your
Packets Forward
Classifier Marker
Drop
Shaper/
Dropper
Meter
Figure 7.26  A simple Diffserv network example
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 651
VoIP call!” Second, if Diffserv were actually in place and the network ran at only
moderate load, most of the time there would be no perceived difference between a
best-effort service and a Diffserv service. Indeed, end-to-end delay is usually dominated
by access rates and router hops rather than by queuing delays in the routers.
Imagine the unhappy Diffserv customer who has paid more for premium service but
finds that the best-effort service being provided to others almost always has the
same performance as premium service!
7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees:
Resource Reservation and Call Admission
In the previous section, we have seen that packet marking and policing, traffic isolation,
and link-level scheduling can provide one class of service with better performance
than another. Under certain scheduling disciplines, such as priority scheduling,
the lower classes of traffic are essentially “invisible” to the highest-priority class of
traffic. With proper network dimensioning, the highest class of service can indeed
achieve extremely low packet loss and delay—essentially circuit-like performance.
But can the network guarantee that an ongoing flow in a high-priority traffic class
will continue to receive such service throughout the flow’s duration using only the
mechanisms that we have described so far? It cannot. In this section, we’ll see why
yet additional network mechanisms and protocols are required when a hard service
guarantee is provided to individual connections.
Let’s return to our scenario from Section 7.5.2 and consider two 1 Mbps
audio applications transmitting their packets over the 1.5 Mbps link, as shown in
Figure 7.27. The combined data rate of the two flows (2 Mbps) exceeds the link
652 CHAPTER 7 • MULTIMEDIA NETWORKING
R1
1.5 Mbps link
1 Mbps
audio
1 Mbps
audio
R2
H2
H1
H4
H3
Figure 7.27  Two competing audio applications overloading
the R1-to-R2 link
capacity. Even with classification and marking, isolation of flows, and sharing of
unused bandwidth (of which there is none), this is clearly a losing proposition.
There is simply not enough bandwidth to accommodate the needs of both applications
at the same time. If the two applications equally share the bandwidth,
each application would lose 25 percent of its transmitted packets. This is such an
unacceptably low QoS that both audio applications are completely unusable;
there’s no need even to transmit any audio packets in the first place.
Given that the two applications in Figure 7.27 cannot both be satisfied simultaneously,
what should the network do? Allowing both to proceed with an unusable
QoS wastes network resources on application flows that ultimately provide no utility
to the end user. The answer is hopefully clear—one of the application flows
should be blocked (that is, denied access to the network), while the other should be
allowed to proceed on, using the full 1 Mbps needed by the application. The telephone
network is an example of a network that performs such call blocking—if the
required resources (an end-to-end circuit in the case of the telephone network) cannot
be allocated to the call, the call is blocked (prevented from entering the network)
and a busy signal is returned to the user. In our example, there is no gain in allowing
a flow into the network if it will not receive a sufficient QoS to be considered
usable. Indeed, there is a cost to admitting a flow that does not receive its needed
QoS, as network resources are being used to support a flow that provides no utility
to the end user.
By explicitly admitting or blocking flows based on their resource requirements,
and the source requirements of already-admitted flows, the network can guarantee
that admitted flows will be able to receive their requested QoS. Implicit in the need
to provide a guaranteed QoS to a flow is the need for the flow to declare its QoS
requirements. This process of having a flow declare its QoS requirement, and then
having the network either accept the flow (at the required QoS) or block the flow is
referred to as the call admission process. This then is our fourth insight (in addition
to the three earlier insights from Section 7.5.2) into the mechanisms needed to provide
QoS.
Insight 4: If sufficient resources will not always be available, and QoS is to be
guaranteed, a call admission process is needed in which flows declare their
QoS requirements and are then either admitted to the network (at the required
QoS) or blocked from the network (if the required QoS cannot be provided by
the network).
Our motivating example in Figure 7.27 highlights the need for several new network
mechanisms and protocols if a call (an end-to-end flow) is to be guaranteed a given
quality of service once it begins:
• Resource reservation. The only way to guarantee that a call will have the
resources (link bandwidth, buffers) needed to meet its desired QoS is to explicitly
7.5 • NETWORK SUPPORT FOR MULTIMEDIA 653
allocate those resources to the call—a process known in networking parlance as
resource reservation. Once resources are reserved, the call has on-demand access
to these resources throughout its duration, regardless of the demands of all other
calls. If a call reserves and receives a guarantee of x Mbps of link bandwidth, and
never transmits at a rate greater than x, the call will see loss- and delay-free performance.
• Call admission. If resources are to be reserved, then the network must have a
mechanism for calls to request and reserve resources. Since resources are not
infinite, a call making a call admission request will be denied admission, that is,
be blocked, if the requested resources are not available. Such a call admission is
performed by the telephone network—we request resources when we dial a number.
If the circuits (TDMA slots) needed to complete the call are available, the
circuits are allocated and the call is completed. If the circuits are not available,
then the call is blocked, and we receive a busy signal. A blocked call can try
again to gain admission to the network, but it is not allowed to send traffic into
the network until it has successfully completed the call admission process. Of
course, a router that allocates link bandwidth should not allocate more than is
available at that link. Typically, a call may reserve only a fraction of the link’s
bandwidth, and so a router may allocate link bandwidth to more than one call.
However, the sum of the allocated bandwidth to all calls should be less than the
link capacity if hard quality of service guarantees are to be provided.
• Call setup signaling. The call admission process described above requires
that a call be able to reserve sufficient resources at each and every network
router on its source-to-destination path to ensure that its end-to-end QoS
requirement is met. Each router must determine the local resources required by
the session, consider the amounts of its resources that are already committed to
other ongoing sessions, and determine whether it has sufficient resources to
satisfy the per-hop QoS requirement of the session at this router without violating
local QoS guarantees made to an already-admitted session. A signaling
protocol is needed to coordinate these various activities—the per-hop allocation
of local resources, as well as the overall end-to-end decision of whether or
not the call has been able to reserve sufficient resources at each and every
router on the end-to-end path. This is the job of the call setup protocol, as
shown in Figure 7.28. The RSVP protocol [Zhang 1993, RFC 2210] was
proposed for this purpose within an Internet architecture for providing qualityof-
service guarantees. In ATM networks, the Q2931b protocol [Black 1995]
carries this information among the ATM network’s switches and end point.
Despite a tremendous amount of research and development, and even products
that provide for per-connection quality of service guarantees, there has been
almost no extended deployment of such services. There are many possible reasons.
First and foremost, it may well be the case that the simple application-level
mechanisms that we studied in Sections 7.2 through 7.4, combined with proper
654 CHAPTER 7 • MULTIMEDIA NETWORKING
network dimensioning (Section 7.5.1) provide “good enough” best-effort network
service for multimedia applications. In addition, the added complexity and cost of
deploying and managing a network that provides per-connection quality of service
guarantees may be judged by ISPs to be simply too high given predicted customer
revenues for that service.
7.6 Summary
Multimedia networking is one of the most exciting developments in the Internet
today. People throughout the world are spending less time in front of their radios
and televisions, and are instead turning to the Internet to receive audio and video
transmissions, both live and prerecorded. This trend will certainly continue as highspeed
wireless Internet access becomes more and more prevalent. Moreover, with
sites like YouTube, users have become producers as well as consumers of multimedia
Internet content. In addition to video distribution, the Internet is also being used
to transport phone calls. In fact, over the next 10 years, the Internet, along with wireless
Internet access, may make the traditional circuit-switched telephone system a
thing of the past. VoIP not only provides phone service inexpensively, but also provides
numerous value-added services, such as video conferencing, online directory
services, voice messaging, and integration into social networks such as Facebook
and Google+.
7.6 • SUMMARY 655
QoS call signaling setup
Request/reply
Figure 7.28  The call setup process
In Section 7.1, we described the intrinsic characteristics of video and voice, and
then classified multimedia applications into three categories: (i) streaming stored
audio/video, (ii) conversational voice/video-over-IP, and (iii) streaming live audio/
video.
In Section 7.2, we studied streaming stored video in some depth. For streaming
video applications, prerecorded videos are placed on servers, and users send
requests to these servers to view the videos on demand. We saw that streaming video
systems can be classified into three categories: UDP streaming, HTTP streaming,
and adaptive HTTP streaming. Although all three types of systems are used in practice,
the majority of today’s systems employ HTTP streaming and adaptive HTTP
streaming. We observed that the most important performance measure for streaming
video is average throughput. In Section 7.2 we also investigated CDNs, which help
distribute massive amounts of video data to users around the world. We also surveyed
the technology behind three major Internet video-streaming companies: Netflix,
YouTube, and Kankan.
In Section 7.3, we examined how conversational multimedia applications, such as
VoIP, can be designed to run over a best-effort network. For conversational multimedia,
timing considerations are important because conversational applications are highly
delay-sensitive. On the other hand, conversational multimedia applications are losstolerant—
occasional loss only causes occasional glitches in audio/video playback, and
these losses can often be partially or fully concealed. We saw how a combination of
client buffers, packet sequence numbers, and timestamps can greatly alleviate the
effects of network-induced jitter. We also surveyed the technology behind Skype, one
of the leading voice- and video-over-IP companies. In Section 7.4, we examined two of
the most important standardized protocols for VoIP, namely, RTP and SIP.
In Section 7.5, we introduced how several network mechanisms (link-level
scheduling disciplines and traffic policing) can be used to provide differentiated
service among several classes of traffic.
Homework Problems and Questions
Chapter 7 Review Questions
SECTION 7.1
R1. Reconstruct Table 7.1 for when Victor Video is watching a 4 Mbps video,
Facebook Frank is looking at a new 100 Kbyte image every 20 seconds, and
Martha Music is listening to 200 kbps audio stream.
R2. There are two types of redundancy in video. Describe them, and discuss how
they can be exploited for efficient compression.
R3. Suppose an analog audio signal is sampled 16,000 times per second, and each
sample is quantized into one of 1024 levels. What would be the resulting bit
rate of the PCM digital audio signal?
656 CHAPTER 7 • MULTIMEDIA NETWORKING
R4. Multimedia applications can be classified into three categories. Name and
describe each category.
SECTION 7.2
R5. Streaming video systems can be classified into three categories. Name and
briefly describe each of these categories.
R6. List three disadvantages of UDP streaming.
R7. With HTTP streaming, are the TCP receive buffer and the client’s application
buffer the same thing? If not, how do they interact?
R8. Consider the simple model for HTTP streaming. Suppose the server sends
bits at a constant rate of 2 Mbps and playback begins when 8 million bits
have been received. What is the initial buffering delay ?
R9. CDNs typically adopt one of two different server placement philosophies.
Name and briefly describe these two philosophies.
R10. Several cluster selection strategies were described in Section 7.2.4. Which of
these strategies finds a good cluster with respect to the client’s LDNS? Which
of these strategies finds a good cluster with respect to the client itself?
R11. Besides network-related considerations such as delay, loss, and bandwidth
performance, there are many additional important factors that go into designing
a cluster selection strategy. What are they?
SECTION 7.3
R12. What is the difference between end-to-end delay and packet jitter? What are
the causes of packet jitter?
R13. Why is a packet that is received after its scheduled playout time considered
lost?
R14. Section 7.3 describes two FEC schemes. Briefly summarize them. Both
schemes increase the transmission rate of the stream by adding overhead.
Does interleaving also increase the transmission rate?
SECTION 7.4
R15. How are different RTP streams in different sessions identified by a receiver?
How are different streams from within the same session identified?
R16. What is the role of a SIP registrar? How is the role of an SIP registrar different
from that of a home agent in Mobile IP?
SECTION 7.5
R17. In Section 7.5, we discussed non-preemptive priority queuing. What would
be preemptive priority queuing? Does preemptive priority queuing make
sense for computer networks?
R18. Give an example of a scheduling discipline that is not work conserving.
tp
HOMEWORK PROBLEMS AND QUESTIONS 657
R19. Give an example from queues you experience in your everyday life of FIFO,
priority, RR, and WFQ.
Problems
P1. Consider the figure below. Similar to our discussion of Figure 7.1, suppose
that video is encoded at a fixed bit rate, and thus each video block contains
video frames that are to be played out over the same fixed amount of
time, . The server transmits the first video block at , the second block
at , the third block at , and so on. Once the client begins
playout, each block should be played out time units after the previous
block.
a. Suppose that the client begins playout as soon as the first block arrives at
. In the figure below, how many blocks of video (including the first
block) will have arrived at the client in time for their playout? Explain
how you arrived at your answer.
b. Suppose that the client begins playout now at . How many blocks
of video (including the first block) will have arrived at the client in time
for their playout? Explain how you arrived at your answer.
c. In the same scenario at (b) above, what is the largest number of blocks
that is ever stored in the client buffer, awaiting playout? Explain how you
arrived at your answer.
d. What is the smallest playout delay at the client, such that every video
block has arrived in time for its playout? Explain how you arrived at your
answer.
t1 + 
t1

t0 +  t0 + 2
 t0
658 CHAPTER 7 • MULTIMEDIA NETWORKING
Constant bit
rate video
transmission
by server
1 2 3 4 5 6 7 8 9
Time
? ? ? ? ? ? ? ? ? ? ?
Video block number
t0 t1
Video
reception
at client
P2. Recall the simple model for HTTP streaming shown in Figure 7.3. Recall that
B denotes the size of the client’s application buffer, and Q denotes the number
of bits that must be buffered before the client application begins playout.
Also r denotes the video consumption rate. Assume that the server sends bits
at a constant rate x whenever the client buffer is not full.
a. Suppose that x < r. As discussed in the text, in this case playout will alternate
between periods of continuous playout and periods of freezing.
Determine the length of each continuous playout and freezing period as a
function of Q, r, and x.
b. Now suppose that x > r. At what time does the client application
buffer become full?
P3. Recall the simple model for HTTP streaming shown in Figure 7.3. Suppose
the buffer size is infinite but the server sends bits at variable rate x(t). Specifically,
suppose x(t) has the following saw-tooth shape. The rate is initially
zero at time t = 0 and linearly climbs to H at time t = T. It then repeats this
pattern again and again, as shown in the figure below.
a. What is the server’s average send rate?
b. Suppose that Q = 0, so that the client starts playback as soon as it receives
a video frame. What will happen?
c. Now suppose Q > 0. Determine as a function of Q, H, and T the time at
which playback first begins.
d. Suppose H > 2r and Q = HT/2. Prove there will be no freezing after the
initial playout delay.
e. Suppose H > 2r. Find the smallest value of Q such that there will be no
freezing after the initial playback delay.
f. Now suppose that the buffer size B is finite. Suppose H > 2r. As a function
of Q, B, T, and H, determine the time when the client application
buffer first becomes full.
t = tf
t = tf
H
Time
T 2T 3T 4T
Bit rate x(t)
PROBLEMS 659
P4. Recall the simple model for HTTP streaming shown in Figure 7.3. Suppose
the client application buffer is infinite, the server sends at the constant rate x,
and the video consumption rate is r with r < x. Also suppose playback begins
immediately. Suppose that the user terminates the video early at time t = E.
At the time of termination, the server stops sending bits (if it hasn’t already
sent all the bits in the video).
a. Suppose the video is infinitely long. How many bits are wasted (that is,
sent but not viewed)?
b. Suppose the video is T seconds long with T > E. How many bits are
wasted (that is, sent but not viewed)?
P5. Consider a DASH system for which there are N video versions (at N different
rates and qualities) and N audio versions (at N different rates and versions).
Suppose we want to allow the player to choose at any time any of the N video
versions and any of the N audio versions.
a. If we create files so that the audio is mixed in with the video, so server
sends only one media stream at given time, how many files will the server
need to store (each a different URL)?
b. If the server instead sends the audio and video streams separately and has
the client synchronize the streams, how many files will the server need to
store?
P6. In the VoIP example in Section 7.3, let h be the total number of header bytes
added to each chunk, including UDP and IP header.
a. Assuming an IP datagram is emitted every 20 msecs, find the transmission
rate in bits per second for the datagrams generated by one side of this application.
b. What is a typical value of h when RTP is used?
P7. Consider the procedure described in Section 7.3 for estimating average delay
di. Suppose that u = 0.1. Let r1 – t1 be the most recent sample delay, let r2 – t2
be the next most recent sample delay, and so on.
a. For a given audio application suppose four packets have arrived at the
receiver with sample delays r4 – t4, r3 – t3, r2 – t2, and r1 – t1. Express the
estimate of delay d in terms of the four samples.
b. Generalize your formula for n sample delays.
c. For the formula in Part b, let n approach infinity and give the resulting
formula. Comment on why this averaging procedure is called an exponential
moving average.
P8. Repeat Parts a and b in Question P7 for the estimate of average delay deviation.
P9. For the VoIP example in Section 7.3, we introduced an online procedure
(exponential moving average) for estimating delay. In this problem we will
660 CHAPTER 7 • MULTIMEDIA NETWORKING
examine an alternative procedure. Let ti be the timestamp of the ith packet
received; let ri be the time at which the ith packet is received. Let dn be our
estimate of average delay after receiving the nth packet. After the first packet
is received, we set the delay estimate equal to d1 = r1 – t1.
a. Suppose that we would like dn = (r1 – t1 + r2 – t2 + . . . + rn – tn)/n for all
n. Give a recursive formula for dn in terms of dn–1, rn, and tn.
b. Describe why for Internet telephony, the delay estimate described in Section
7.3 is more appropriate than the delay estimate outlined in Part a.
P10. Compare the procedure described in Section 7.3 for estimating average delay
with the procedure in Section 3.5 for estimating round-trip time. What do the
procedures have in common? How are they different?
P11. Consider the figure below (which is similar to Figure 7.7). A sender begins
sending packetized audio periodically at t = 1. The first packet arrives at the
receiver at t = 8.
Packets
generated
Time
Packets
1 8
Packets
received
a. What are the delays (from sender to receiver, ignoring any playout delays)
of packets 2 through 8? Note that each vertical and horizontal line segment
in the figure has a length of 1, 2, or 3 time units.
b. If audio playout begins as soon as the first packet arrives at the receiver at
t = 8, which of the first eight packets sent will not arrive in time for playout?
c. If audio playout begins at t = 9, which of the first eight packets sent will
not arrive in time for playout?
d. What is the minimum playout delay at the receiver that results in all of the
first eight packets arriving in time for their playout?
PROBLEMS 661
P12. Consider again the figure in P11, showing packet audio transmission and
reception times.
a. Compute the estimated delay for packets 2 through 8, using the formula
for di from Section 7.3.2. Use a value of u = 0.1.
b. Compute the estimated deviation of the delay from the estimated average
for packets 2 through 8, using the formula for vi from Section 7.3.2. Use a
value of u = 0.1.
P13. Recall the two FEC schemes for VoIP described in Section 7.3. Suppose the
first scheme generates a redundant chunk for every four original chunks.
Suppose the second scheme uses a low-bit rate encoding whose transmission
rate is 25 percent of the transmission rate of the nominal stream.
a. How much additional bandwidth does each scheme require? How much
playback delay does each scheme add?
b. How do the two schemes perform if the first packet is lost in every group
of five packets? Which scheme will have better audio quality?
c. How do the two schemes perform if the first packet is lost in every group
of two packets? Which scheme will have better audio quality?
P14. a. Consider an audio conference call in Skype with N > 2 participants.
Suppose each participant generates a constant stream of rate r bps. How
many bits per second will the call initiator need to send? How many bits
per second will each of the other N – 1 participants need to send? What
is the total send rate, aggregated over all participants?
b. Repeat part (a) for a Skype video conference call using a central server.
c. Repeat part (b), but now for when each peer sends a copy of its video
stream to each of the N – 1 other peers.
P15. a. Suppose we send into the Internet two IP datagrams, each carrying a different
UDP segment. The first datagram has source IP address A1, destination
IP address B, source port P1, and destination port T. The second datagram
has source IP address A2, destination IP address B, source port P2, and destination
port T. Suppose that A1 is different from A2 and that P1 is different
from P2. Assuming that both datagrams reach their final destination, will
the two UDP datagrams be received by the same socket? Why or why not?
b. Suppose Alice, Bob, and Claire want to have an audio conference call
using SIP and RTP. For Alice to send and receive RTP packets to and from
Bob and Claire, is only one UDP socket sufficient (in addition to the
socket needed for the SIP messages)? If yes, then how does Alice’s SIP
client distinguish between the RTP packets received from Bob and Claire?
P16. True or false:
a. If stored video is streamed directly from a Web server to a media player, then
the application is using TCP as the underlying transport protocol.
662 CHAPTER 7 • MULTIMEDIA NETWORKING
b. When using RTP, it is possible for a sender to change encoding in the middle
of a session.
c. All applications that use RTP must use port 87.
d. If an RTP session has a separate audio and video stream for each sender,
then the audio and video streams use the same SSRC.
e. In differentiated services, while per-hop behavior defines differences in
performance among classes, it does not mandate any particular mechanism
for achieving these performances.
f. Suppose Alice wants to establish an SIP session with Bob. In her INVITE
message she includes the line: m=audio 48753 RTP/AVP 3 (AVP 3
denotes GSM audio). Alice has therefore indicated in this message that
she wishes to send GSM audio.
g. Referring to the preceding statement, Alice has indicated in her INVITE
message that she will send audio to port 48753.
h. SIP messages are typically sent between SIP entities using a default SIP
port number.
i. In order to maintain registration, SIP clients must periodically send
REGISTER messages.
j. SIP mandates that all SIP clients support G.711 audio encoding.
P17. Suppose that the WFQ scheduling policy is applied to a buffer that supports three
classes, and suppose the weights are 0.5, 0.25, and 0.25 for the three classes.
a. Suppose that each class has a large number of packets in the buffer. In what
sequence might the three classes be served in order to achieve the WFQ
weights? (For round robin scheduling, a natural sequence is 123123123 . . .).
b. Suppose that classes 1 and 2 have a large number of packets in the buffer,
and there are no class 3 packets in the buffer. In what sequence might the
three classes be served in to achieve the WFQ weights?
P18. Consider the figure below. Answer the following questions:
Time
Arrivals
Departures
Packet
in service
Time
1 6
2 3 5 9
8 10
11
4 7 12
t = 0
1
t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14
1
PROBLEMS 663
a. Assuming FIFO service, indicate the time at which packets 2 through 12
each leave the queue. For each packet, what is the delay between its
arrival and the beginning of the slot in which it is transmitted? What is the
average of this delay over all 12 packets?
b. Now assume a priority service, and assume that odd-numbered packets
are high priority, and even-numbered packets are low priority. Indicate
the time at which packets 2 through 12 each leave the queue. For each
packet, what is the delay between its arrival and the beginning of the
slot in which it is transmitted? What is the average of this delay over all
12 packets?
c. Now assume round robin service. Assume that packets 1, 2, 3, 6, 11, and
12 are from class 1, and packets 4, 5, 7, 8, 9, and 10 are from class 2. Indicate
the time at which packets 2 through 12 each leave the queue. For
each packet, what is the delay between its arrival and its departure? What
is the average delay over all 12 packets?
d. Now assume weighted fair queueing (WFQ) service. Assume that oddnumbered
packets are from class 1, and even-numbered packets are from
class 2. Class 1 has a WFQ weight of 2, while class 2 has a WFQ weight
of 1. Note that it may not be possible to achieve an idealized WFQ schedule
as described in the text, so indicate why you have chosen the particular
packet to go into service at each time slot. For each packet what is the
delay between its arrival and its departure? What is the average delay over
all 12 packets?
e. What do you notice about the average delay in all four cases (FIFO, RR,
priority, and WFQ)?
P19. Consider again the figure for P18.
a. Assume a priority service, with packets 1, 4, 5, 6, and 11 being highpriority
packets. The remaining packets are low priority. Indicate the slots
in which packets 2 through 12 each leave the queue.
b. Now suppose that round robin service is used, with packets 1, 4, 5, 6, and
11 belonging to one class of traffic, and the remaining packets belonging
to the second class of traffic. Indicate the slots in which packets 2 through
12 each leave the queue.
c. Now suppose that WFQ service is used, with packets 1, 4, 5, 6, and 11
belonging to one class of traffic, and the remaining packets belonging to
the second class of traffic. Class 1 has a WFQ weight of 1, while class 2
has a WFQ weight of 2 (note that these weights are different than in the
previous question). Indicate the slots in which packets 2 through 12 each
leave the queue. See also the caveat in the question above regarding WFQ
service.
664 CHAPTER 7 • MULTIMEDIA NETWORKING
P20. Consider the figure below, which shows a leaky bucket policer being fed by a
stream of packets. The token buffer can hold at most two tokens, and is initially
full at t = 0. New tokens arrive at a rate of one token per slot. The output
link speed is such that if two packets obtain tokens at the beginning of a
time slot, they can both go to the output link in the same slot. The timing
details of the system are as follows:
PROBLEMS 665
Arrivals
Packet queue
(wait for tokens)
9
10
7 6 4
8 5
1
3
2
t = 8 t = 6 t = 4 t = 2 t = 0 t = 4 t = 2 t = 0
r = 1 token/slot
b = 2 tokens
1. Packets (if any) arrive at the beginning of the slot. Thus in the figure,
packets 1, 2, and 3 arrive in slot 0. If there are already packets in the
queue, then the arriving packets join the end of the queue. Packets
proceed towards the front of the queue in a FIFO manner.
2. After the arrivals have been added to the queue, if there are any queued
packets, one or two of those packets (depending on the number of available
tokens) will each remove a token from the token buffer and go to the
output link during that slot. Thus, packets 1 and 2 each remove a token
from the buffer (since there are initially two tokens) and go to the output
link during slot 0.
3. A new token is added to the token buffer if it is not full, since the token
generation rate is r = 1 token/slot.
4. Time then advances to the next time slot, and these steps repeat.
Answer the following questions:
a. For each time slot, identify the packets that are in the queue and the number
of tokens in the bucket, immediately after the arrivals have been
processed (step 1 above) but before any of the packets have passed
through the queue and removed a token. Thus, for the t = 0 time slot in the
example above, packets 1, 2 and 3 are in the queue, and there are two
tokens in the buffer.
b. For each time slot indicate which packets appear on the output after the
token(s) have been removed from the queue. Thus, for the t = 0 time
slot in the example above, packets 1 and 2 appear on the output link
from the leaky buffer during slot 0.
P21. Repeat P20 but assume that r = 2. Assume again that the bucket is initially
full.
P22. Consider P21 and suppose now that r = 3, and that b = 2 as before. Will your
answer to the question above change?
P23. Consider the leaky-bucket policer that polices the average rate and burst size
of a packet flow. We now want to police the peak rate, p, as well. Show how
the output of this leaky-bucket policer can be fed into a second leaky bucket
policer so that the two leaky buckets in series police the average rate, peak
rate, and burst size. Be sure to give the bucket size and token generation rate
for the second policer.
P24. A packet flow is said to conform to a leaky-bucket specification (r,b) with
burst size b and average rate r if the number of packets that arrive to the leaky
bucket is less than rt + b packets in every interval of time of length t for all t.
Will a packet flow that conforms to a leaky-bucket specification (r,b) ever
have to wait at a leaky bucket policer with parameters r and b? Justify your
answer.
P25. Show that as long as r1 < R w1/(S wj), then dmax is indeed the maximum delay
that any packet in flow 1 will ever experience in the WFQ queue.
Programming Assignment
In this lab, you will implement a streaming video server and client. The client will
use the real-time streaming protocol (RTSP) to control the actions of the server. The
server will use the real-time protocol (RTP) to packetize the video for transport over
UDP. You will be given Python code that partially implements RTSP and RTP at the
client and server. Your job will be to complete both the client and server code. When
you are finished, you will have created a client-server application that does the following:
• The client sends SETUP, PLAY, PAUSE, and TEARDOWN RTSP commands,
and the server responds to the commands.
• When the server is in the playing state, it periodically grabs a stored JPEG frame,
packetizes the frame with RTP, and sends the RTP packet into a UDP socket.
• The client receives the RTP packets, removes the JPEG frames, decompresses
the frames, and renders the frames on the client’s monitor.
666 CHAPTER 7 • MULTIMEDIA NETWORKING
The code you will be given implements the RTSP protocol in the server and the
RTP depacketization in the client. The code also takes care of displaying the transmitted
video. You will need to implement RTSP in the client and RTP server. This
programming assignment will significantly enhance the student’s understanding of
RTP, RTSP, and streaming video. It is highly recommended. The assignment also
suggests a number of optional exercises, including implementing the RTSP
DESCRIBE command at both client and server. You can find full details of
the assignment, as well as an overview of the RTSP protocol, at the Web site
http://www.awl.com/kurose-ross.
PROGRAMMING ASSIGNMENT 667
What made you decide to specialize in multimedia networking?
This happened almost by accident. As a PhD student, I got involved with DARTnet, an
experimental network spanning the United States with T1 lines. DARTnet was used as
a proving ground for multicast and Internet real-time tools. That led me to write my first
audio tool, NeVoT. Through some of the DARTnet participants, I became involved in the
IETF, in the then-nascent Audio Video Transport working group. This group later ended up
standardizing RTP.
What was your first job in the computer industry? What did it entail?
My first job in the computer industry was soldering together an Altair computer kit when I
was a high school student in Livermore, California. Back in Germany, I started a little consulting
company that devised an address management program for a travel agency—storing
data on cassette tapes for our TRS-80 and using an IBM Selectric typewriter with a homebrew
hardware interface as a printer.
My first real job was with AT&T Bell Laboratories, developing a network emulator for
constructing experimental networks in a lab environment.
What are the goals of the Internet Real-Time Lab?
Our goal is to provide components and building blocks for the Internet as the single future
communications infrastructure. This includes developing new protocols, such as GIST
(for network-layer signaling) and LoST (for finding resources by location), or enhancing
protocols that we have worked on earlier, such as SIP, through work on rich presence,
peer-to-peer systems, next-generation emergency calling, and service creation tools.
Recently, we have also looked extensively at wireless systems for VoIP, as 802.11b and
802.11n networks and maybe WiMax networks are likely to become important last-mile
technologies for telephony. We are also trying to greatly improve the ability of users to
diagnose faults in the complicated tangle of providers and equipment, using a peer-to-peer
fault diagnosis system called DYSWIS (Do You See What I See).
668
Henning Schulzrinne
Henning Schulzrinne is a professor, chair of the Department of
Computer Science, and head of the Internet Real-Time Laboratory at
Columbia University. He is the co-author of RTP, RTSP, SIP, and
GIST—key protocols for audio and video communications over the
Internet. Henning received his BS in electrical and industrial engineering
at TU Darmstadt in Germany, his MS in electrical and computer
engineering at the University of Cincinnati, and his PhD in electrical
engineering at the University of Massachusetts, Amherst.
AN INTERVIEW WITH...
We try to do practically relevant work, by building prototypes and open source systems,
by measuring performance of real systems, and by contributing to IETF standards.
What is your vision for the future of multimedia networking?
We are now in a transition phase; just a few years shy of when IP will be the universal platform
for multimedia services, from IPTV to VoIP. We expect radio, telephone, and TV to be
available even during snowstorms and earthquakes, so when the Internet takes over the role
of these dedicated networks, users will expect the same level of reliability.
We will have to learn to design network technologies for an ecosystem of competing
carriers, service and content providers, serving lots of technically untrained users and
defending them against a small, but destructive, set of malicious and criminal users.
Changing protocols is becoming increasingly hard. They are also becoming more complex,
as they need to take into account competing business interests, security, privacy, and the
lack of transparency of networks caused by firewalls and network address translators.
Since multimedia networking is becoming the foundation for almost all of consumer
entertainment, there will be an emphasis on managing very large networks, at low cost.
Users will expect ease of use, such as finding the same content on all of their devices.
Why does SIP have a promising future?
As the current wireless network upgrade to 3G networks proceeds, there is the hope of a
single multimedia signaling mechanism spanning all types of networks, from cable
modems, to corporate telephone networks and public wireless networks. Together with
software radios, this will make it possible in the future that a single device can be used on
a home network, as a cordless BlueTooth phone, in a corporate network via 802.11 and in
the wide area via 3G networks. Even before we have such a single universal wireless
device, the personal mobility mechanisms make it possible to hide the differences between
networks. One identifier becomes the universal means of reaching a person, rather than
remembering or passing around half a dozen technology- or location-specific telephone
numbers.
SIP also breaks apart the provision of voice (bit) transport from voice services. It now
becomes technically possible to break apart the local telephone monopoly, where one
company provides neutral bit transport, while others provide IP “dial tone” and the classical
telephone services, such as gateways, call forwarding, and caller ID.
Beyond multimedia signaling, SIP offers a new service that has been missing in the
Internet: event notification. We have approximated such services with HTTP kludges and
e-mail, but this was never very satisfactory. Since events are a common abstraction for
distributed systems, this may simplify the construction of new services.
669
Do you have any advice for students entering the networking field?
Networking bridges disciplines. It draws from electrical engineering, all aspects of computer
science, operations research, statistics, economics, and other disciplines. Thus,
networking researchers have to be familiar with subjects well beyond protocols and routing
algorithms.
Given that networks are becoming such an important part of everyday life, students
wanting to make a difference in the field should think of the new resource constraints in
networks: human time and effort, rather than just bandwidth or storage.
Work in networking research can be immensely satisfying since it is about allowing
people to communicate and exchange ideas, one of the essentials of being human. The
Internet has become the third major global infrastructure, next to the transportation system
and energy distribution. Almost no part of the economy can work without high-performance
networks, so there should be plenty of opportunities for the foreseeable future.
670
CHAPTER 8
Security in
Computer
Networks
671
Way back in Section 1.6 we described some of the more prevalent and damaging
classes of Internet attacks, including malware attacks, denial of service, sniffing,
source masquerading, and message modification and deletion. Although we have
since learned a tremendous amount about computer networks, we still haven’t
examined how to secure networks from those attacks. Equipped with our newly
acquired expertise in computer networking and Internet protocols, we’ll now study
in-depth secure communication and, in particular, how computer networks can be
defended from those nasty bad guys.
Let us introduce Alice and Bob, two people who want to communicate and
wish to do so “securely.” This being a networking text, we should remark that Alice
and Bob could be two routers that want to exchange routing tables securely, a client
and server that want to establish a secure transport connection, or two e-mail applications
that want to exchange secure e-mail—all case studies that we will consider
later in this chapter. Alice and Bob are well-known fixtures in the security community,
perhaps because their names are more fun than a generic entity named “A”
that wants to communicate securely with a generic entity named “B.” Love affairs,
wartime communication, and business transactions are the commonly cited human
needs for secure communications; preferring the first to the latter two, we’re happy
to use Alice and Bob as our sender and receiver, and imagine them in this first
scenario.
We said that Alice and Bob want to communicate and wish to do so “securely,”
but what precisely does this mean? As we will see, security (like love) is a manysplendored
thing; that is, there are many facets to security. Certainly, Alice and
Bob would like for the contents of their communication to remain secret from an
eavesdropper. They probably would also like to make sure that when they are
communicating, they are indeed communicating with each other, and that if their
communication is tampered with by an eavesdropper, that this tampering is
detected. In the first part of this chapter, we’ll cover the fundamental cryptography
techniques that allow for encrypting communication, authenticating the party with
whom one is communicating, and ensuring message integrity.
In the second part of this chapter, we’ll examine how the fundamental cryptography
principles can be used to create secure networking protocols. Once again
taking a top-down approach, we’ll examine secure protocols in each of the (top
four) layers, beginning with the application layer. We’ll examine how to secure email,
how to secure a TCP connection, how to provide blanket security at the network
layer, and how to secure a wireless LAN. In the third part of this chapter we’ll
consider operational security, which is about protecting organizational networks
from attacks. In particular, we’ll take a careful look at how firewalls and intrusion
detection systems can enhance the security of an organizational network.
8.1 What Is Network Security?
Let’s begin our study of network security by returning to our lovers, Alice and Bob,
who want to communicate “securely.” What precisely does this mean? Certainly,
Alice wants only Bob to be able to understand a message that she has sent, even
though they are communicating over an insecure medium where an intruder
(Trudy, the intruder) may intercept whatever is transmitted from Alice to Bob. Bob
also wants to be sure that the message he receives from Alice was indeed sent by
Alice, and Alice wants to make sure that the person with whom she is communicating
is indeed Bob. Alice and Bob also want to make sure that the contents of their
messages have not been altered in transit. They also want to be assured that they
can communicate in the first place (i.e., that no one denies them access to the
resources needed to communicate). Given these considerations, we can identify the
following desirable properties of secure communication.
• Confidentiality. Only the sender and intended receiver should be able to understand
the contents of the transmitted message. Because eavesdroppers may intercept
the message, this necessarily requires that the message be somehow
encrypted so that an intercepted message cannot be understood by an interceptor.
This aspect of confidentiality is probably the most commonly perceived
672 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
meaning of the term secure communication. We’ll study cryptographic techniques
for encrypting and decrypting data in Section 8.2.
• Message integrity. Alice and Bob want to ensure that the content of their communication
is not altered, either maliciously or by accident, in transit. Extensions
to the checksumming techniques that we encountered in reliable transport and
data link protocols can be used to provide such message integrity. We will study
message integrity in Section 8.3.
• End-point authentication. Both the sender and receiver should be able to
confirm the identity of the other party involved in the communication—
to confirm that the other party is indeed who or what they claim to be.
Face-to-face human communication solves this problem easily by visual
recognition. When communicating entities exchange messages over a
medium where they cannot see the other party, authentication is not so
simple. When a user wants to access an inbox, how does the mail server verify
that the user is the person he or she claims to be? We study end-point
authentication in Section 8.4.
• Operational security. Almost all organizations (companies, universities, and
so on) today have networks that are attached to the public Internet. These networks
therefore can potentially be compromised. Attackers can attempt
to deposit worms into the hosts in the network, obtain corporate secrets, map
the internal network configurations, and launch DoS attacks. We’ll see in
Section 8.9 that operational devices such as firewalls and intrusion detection
systems are used to counter attacks against an organization’s network. A
firewall sits between the organization’s network and the public network,
controlling packet access to and from the network. An intrusion detection system
performs “deep packet inspection,” alerting the network administrators
about suspicious activity.
Having established what we mean by network security, let’s next consider
exactly what information an intruder may have access to, and what actions can be
taken by the intruder. Figure 8.1 illustrates the scenario. Alice, the sender, wants to
send data to Bob, the receiver. In order to exchange data securely, while meeting the
requirements of confidentiality, end-point authentication, and message integrity,
Alice and Bob will exchange control messages and data messages (in much the
same way that TCP senders and receivers exchange control segments and data segments).
All or some of these messages will typically be encrypted. As discussed in
Section 1.6, an intruder can potentially perform
• eavesdropping—sniffing and recording control and data messages on the
channel.
• modification, insertion, or deletion of messages or message content.
8.1 • WHAT IS NETWORK SECURITY? 673
As we’ll see, unless appropriate countermeasures are taken, these capabilities
allow an intruder to mount a wide variety of security attacks: snooping on communication
(possibly stealing passwords and data), impersonating another entitity,
hijacking an ongoing session, denying service to legitimate network users by overloading
system resources, and so on. A summary of reported attacks is maintained at
the CERT Coordination Center [CERT 2012].
Having established that there are indeed real threats loose in the Internet,
what are the Internet equivalents of Alice and Bob, our friends who need to communicate
securely? Certainly, Bob and Alice might be human users at two end
systems, for example, a real Alice and a real Bob who really do want to exchange
secure e-mail. They might also be participants in an electronic commerce transaction.
For example, a real Bob might want to transfer his credit card number
securely to a Web server to purchase an item online. Similarly, a real Alice might
want to interact with her bank online. The parties needing secure communication
might themselves also be part of the network infrastructure. Recall that the
domain name system (DNS, see Section 2.5) or routing daemons that exchange
routing information (see Section 4.6) require secure communication between two
parties. The same is true for network management applications, a topic we examine
in Chapter 9. An intruder that could actively interfere with DNS lookups (as
discussed in Section 2.5), routing computations [RFC 4272], or network management
functions [RFC 3414] could wreak havoc in the Internet.
Having now established the framework, a few of the most important definitions,
and the need for network security, let us next delve into cryptography.
While the use of cryptography in providing confidentiality is self-evident, we’ll see
shortly that it is also central to providing end-point authentication and message
integrity—making cryptography a cornerstone of network security.
674 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Secure
sender
Alice
Trudy
Channel
Control, data messages
Secure
receiver
Bob
Data Data
Figure 8.1  Sender, receiver, and intruder (Alice, Bob, and Trudy)
8.2 Principles of Cryptography
Although cryptography has a long history dating back at least as far as Julius Caesar,
modern cryptographic techniques, including many of those used in the Internet,
are based on advances made in the past 30 years. Kahn’s book, The Codebreakers
[Kahn 1967], and Singh’s book, The Code Book: The Science of Secrecy from
Ancient Egypt to Quantum Cryptography [Singh 1999], provide a fascinating look
at the long history of cryptography. A complete discussion of cryptography itself
requires a complete book [Kaufman 1995; Schneier 1995] and so we only touch
on the essential aspects of cryptography, particularly as they are practiced on the
Internet. We also note that while our focus in this section will be on the use of
cryptography for confidentiality, we’ll see shortly that cryptographic techniques
are inextricably woven into authentication, message integrity, nonrepudiation,
and more.
Cryptographic techniques allow a sender to disguise data so that an intruder can
gain no information from the intercepted data. The receiver, of course, must be able
to recover the original data from the disguised data. Figure 8.2 illustrates some of
the important terminology.
Suppose now that Alice wants to send a message to Bob. Alice’s message in
its original form (for example, “Bob, I love you. Alice”) is known as
plaintext, or cleartext. Alice encrypts her plaintext message using an encryption
algorithm so that the encrypted message, known as ciphertext, looks unintelligible
to any intruder. Interestingly, in many modern cryptographic systems, including
those used in the Internet, the encryption technique itself is known—published, standardized,
and available to everyone (for example, [RFC 1321; RFC 3447; RFC
8.2 • PRINCIPLES OF CRYPTOGRAPHY 675
Encryption
algorithm
Ciphertext
Channel
Trudy
Alice Bob
Decryption
algorithm
Plaintext
Key:
Key
Plaintext
KA KB
Figure 8.2  Cryptographic components
2420; NIST 2001]), even a potential intruder! Clearly, if everyone knows the
method for encoding data, then there must be some secret information that prevents
an intruder from decrypting the transmitted data. This is where keys come in.
In Figure 8.2, Alice provides a key, KA, a string of numbers or characters, as
input to the encryption algorithm. The encryption algorithm takes the key and the
plaintext message, m, as input and produces ciphertext as output. The notation
KA(m) refers to the ciphertext form (encrypted using the key KA) of the plaintext
message, m. The actual encryption algorithm that uses key KA will be evident from
the context. Similarly, Bob will provide a key, KB, to the decryption algorithm
that takes the ciphertext and Bob’s key as input and produces the original plaintext
as output. That is, if Bob receives an encrypted message KA(m), he decrypts it
by computing KB(KA(m)) = m. In symmetric key systems, Alice’s and Bob’s keys
are identical and are secret. In public key systems, a pair of keys is used. One of
the keys is known to both Bob and Alice (indeed, it is known to the whole world).
The other key is known only by either Bob or Alice (but not both). In the following
two subsections, we consider symmetric key and public key systems in more
detail.
8.2.1 Symmetric Key Cryptography
All cryptographic algorithms involve substituting one thing for another, for example,
taking a piece of plaintext and then computing and substituting the appropriate
ciphertext to create the encrypted message. Before studying a modern key-based
cryptographic system, let us first get our feet wet by studying a very old, very simple
symmetric key algorithm attributed to Julius Caesar, known as the Caesar
cipher (a cipher is a method for encrypting data).
For English text, the Caesar cipher would work by taking each letter in the
plaintext message and substituting the letter that is k letters later (allowing wraparound;
that is, having the letter z followed by the letter a) in the alphabet. For
example if k = 3, then the letter a in plaintext becomes d in ciphertext; b in plaintext
becomes e in ciphertext, and so on. Here, the value of k serves as the key. As an
example, the plaintext message “bob, i love you. alice” becomes “ere,
l oryh brx. dolfh” in ciphertext. While the ciphertext does indeed look like
gibberish, it wouldn’t take long to break the code if you knew that the Caesar cipher
was being used, as there are only 25 possible key values.
An improvement on the Caesar cipher is the monoalphabetic cipher, which
also substitutes one letter of the alphabet with another letter of the alphabet. However,
rather than substituting according to a regular pattern (for example, substitution
with an offset of k for all letters), any letter can be substituted for any other
letter, as long as each letter has a unique substitute letter, and vice versa. The substitution
rule in Figure 8.3 shows one possible rule for encoding plaintext.
The plaintext message “bob, i love you. alice” becomes “nkn, s
gktc wky. mgsbc.” Thus, as in the case of the Caesar cipher, this looks like
676 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
gibberish. A monoalphabetic cipher would also appear to be better than the Caesar
cipher in that there are 26! (on the order of 1026) possible pairings of letters
rather than 25 possible pairings. A brute-force approach of trying all 1026 possible
pairings would require far too much work to be a feasible way of breaking the
encryption algorithm and decoding the message. However, by statistical analysis
of the plaintext language, for example, knowing that the letters e and t are the most
frequently occurring letters in typical English text (accounting for 13 percent and 9
percent of letter occurrences), and knowing that particular two- and three-letter
occurrences of letters appear quite often together (for example, “in,” “it,” “the,”
“ion,” “ing,” and so forth) make it relatively easy to break this code. If the intruder
has some knowledge about the possible contents of the message, then it is even easier
to break the code. For example, if Trudy the intruder is Bob’s wife and suspects
Bob of having an affair with Alice, then she might suspect that the names “bob”
and “alice” appear in the text. If Trudy knew for certain that those two names
appeared in the ciphertext and had a copy of the example ciphertext message
above, then she could immediately determine seven of the 26 letter pairings,
requiring 109 fewer possibilities to be checked by a brute-force method. Indeed, if
Trudy suspected Bob of having an affair, she might well expect to find some other
choice words in the message as well.
When considering how easy it might be for Trudy to break Bob and Alice’s
encryption scheme, one can distinguish three different scenarios, depending on what
information the intruder has.
• Ciphertext-only attack. In some cases, the intruder may have access only to the
intercepted ciphertext, with no certain information about the contents of the
plaintext message. We have seen how statistical analysis can help in a ciphertext-
only attack on an encryption scheme.
• Known-plaintext attack.We saw above that if Trudy somehow knew for sure that
“bob” and “alice” appeared in the ciphertext message, then she could have determined
the (plaintext, ciphertext) pairings for the letters a, l, i, c, e, b, and o.
Trudy might also have been fortunate enough to have recorded all of the ciphertext
transmissions and then found Bob’s own decrypted version of one of the
transmissions scribbled on a piece of paper. When an intruder knows some of the
(plaintext, ciphertext) pairings, we refer to this as a known-plaintext attack on
the encryption scheme.
8.2 • PRINCIPLES OF CRYPTOGRAPHY 677
Plaintext letter: a b c d e f g h i j k l m n o p q r s t u v w x y z
Ciphertext letter: m n b v c x z a s d f g h j k l p o i u y t r e w q
Figure 8.3  A monoalphabetic cipher
• Chosen-plaintext attack. In a chosen-plaintext attack, the intruder is able to
choose the plaintext message and obtain its corresponding ciphertext form. For
the simple encryption algorithms we’ve seen so far, if Trudy could get Alice to
send the message, “The quick brown fox jumps over the lazy
dog,” she could completely break the encryption scheme. We’ll see shortly that
for more sophisticated encryption techniques, a chosen-plaintext attack does not
necessarily mean that the encryption technique can be broken.
Five hundred years ago, techniques improving on monoalphabetic encryption,
known as polyalphabetic encryption, were invented. The idea behind polyalphabetic
encryption is to use multiple monoalphabetic ciphers, with a specific monoalphabetic
cipher to encode a letter in a specific position in the plaintext message.
Thus, the same letter, appearing in different positions in the plaintext message,
might be encoded differently. An example of a polyalphabetic encryption scheme is
shown in Figure 8.4. It has two Caesar ciphers (with k = 5 and k = 19), shown as
rows. We might choose to use these two Caesar ciphers, C1 and C2, in the repeating
pattern C1, C2, C2, C1, C2. That is, the first letter of plaintext is to be encoded using
C1, the second and third using C2, the fourth using C1, and the fifth using C2. The
pattern then repeats, with the sixth letter being encoded using C1, the seventh with
C2, and so on. The plaintext message “bob, i love you.” is thus encrypted
“ghu, n etox dhz.” Note that the first b in the plaintext message is encrypted
using C1, while the second b is encrypted using C2. In this example, the encryption
and decryption “key” is the knowledge of the two Caesar keys (k = 5, k = 19) and
the pattern C1, C2, C2, C1, C2.
Block Ciphers
Let us now move forward to modern times and examine how symmetric key encryption
is done today. There are two broad classes of symmetric encryption techniques:
stream ciphers and block ciphers. We’ll briefly examine stream ciphers in Section
8.7 when we investigate security for wireless LANs. In this section, we focus on
block ciphers, which are used in many secure Internet protocols, including PGP
(for secure e-mail), SSL (for securing TCP connections), and IPsec (for securing the
network-layer transport).
678 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Plaintext letter: a b c d e f g h i j k l m n o p q r s t u v w x y z
C1(k = 5):
C2(k = 19):
f g h i j k l m n o p q r s t u v w x y z a b c d e
t u v w x y z a b c d e f g h i j k l m n o p q r s
Figure 8.4  A polyalphabetic cipher using two Caesar ciphers
8.2 • PRINCIPLES OF CRYPTOGRAPHY 679
In a block cipher, the message to be encrypted is processed in blocks of k bits.
For example, if k = 64, then the message is broken into 64-bit blocks, and each block
is encrypted independently. To encode a block, the cipher uses a one-to-one mapping
to map the k-bit block of cleartext to a k-bit block of ciphertext. Let’s look at
an example. Suppose that k = 3, so that the block cipher maps 3-bit inputs (cleartext)
to 3-bit outputs (ciphertext). One possible mapping is given in Table 8.1.
Notice that this is a one-to-one mapping; that is, there is a different output for each
input. This block cipher breaks the message up into 3-bit blocks and encrypts each
block according to the above mapping. You should verify that the message
010110001111 gets encrypted into 101000111001.
Continuing with this 3-bit block example, note that the mapping in Table 8.1 is
just one mapping of many possible mappings. How many possible mappings are there?
To answer this question, observe that a mapping is nothing more than a permutation
of all the possible inputs. There are 23 (= 8) possible inputs (listed under the
input columns). These eight inputs can be permuted in 8! = 40,320 different ways.
Since each of these permutations specifies a mapping, there are 40,320 possible
mappings. We can view each of these mappings as a key—if Alice and Bob both
know the mapping (the key), they can encrypt and decrypt the messages sent
between them.
The brute-force attack for this cipher is to try to decrypt ciphtertext by using all
mappings. With only 40,320 mappings (when k = 3), this can quickly be accomplished
on a desktop PC. To thwart brute-force attacks, block ciphers typically use
much larger blocks, consisting of k = 64 bits or even larger. Note that the number of
possible mappings for a general k-block cipher is 2k!, which is astronomical for even
moderate values of k (such as k = 64).
Although full-table block ciphers, as just described, with moderate values of
k can produce robust symmetric key encryption schemes, they are unfortunately
difficult to implement. For k = 64 and for a given mapping, Alice and Bob
would need to maintain a table with 264 input values, which is an infeasible task.
Moreover, if Alice and Bob were to change keys, they would have to each regenerate
Table 8.1  A specific 3-bit block cipher
input output input output
000 110 100 011
001 111 101 010
010 101 110 000
011 100 111 001
the table. Thus, a full-table block cipher, providing predetermined mappings
between all inputs and outputs (as in the example above), is simply out of the
question.
Instead, block ciphers typically use functions that simulate randomly permuted
tables. An example (adapted from [Kaufman 1995]) of such a function for k = 64
bits is shown in Figure 8.5. The function first breaks a 64-bit block into 8 chunks,
with each chunk consisting of 8 bits. Each 8-bit chunk is processed by an 8-bit to 8-
bit table, which is of manageable size. For example, the first chunk is processed by
the table denoted by T1. Next, the 8 output chunks are reassembled into a 64-bit
block. The positions of the 64 bits in the block are then scrambled (permuted) to
produce a 64-bit output. This output is fed back to the 64-bit input, where another
cycle begins. After n such cycles, the function provides a 64-bit block of ciphertext.
The purpose of the rounds is to make each input bit affect most (if not all) of the
final output bits. (If only one round were used, a given input bit would affect only 8
of the 64 output bits.) The key for this block cipher algorithm would be the eight
permutation tables (assuming the scramble function is publicly known).
Today there are a number of popular block ciphers, including DES (standing for
Data Encryption Standard), 3DES, and AES (standing for Advanced Encryption
Standard). Each of these standards uses functions, rather than predetermined tables,
along the lines of Figure 8.5 (albeit more complicated and specific to each cipher).
Each of these algorithms also uses a string of bits for a key. For example, DES uses
64-bit blocks with a 56-bit key. AES uses 128-bit blocks and can operate with keys
that are 128, 192, and 256 bits long. An algorithm’s key determines the specific
680 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
64-bit output
Loop
for n
rounds
8 bits
8 bits
T1
8 bits
8 bits
T2
8 bits
8 bits
T3
8 bits
64-bit input
8 bits
T4
8 bits
8 bits
T5
8 bits
8 bits
T6
8 bits
8 bits
T7
8 bits
8 bits
T8
64-bit scrambler
Figure 8.5  An example of a block cipher
8.2 • PRINCIPLES OF CRYPTOGRAPHY 681
“mini-table” mappings and permutations within the algorithm’s internals. The bruteforce
attack for each of these ciphers is to cycle through all the keys, applying
the decryption algorithm with each key. Observe that with a key length of n, there
are 2n possible keys. NIST [NIST 2001] estimates that a machine that could crack
56-bit DES in one second (that is, try all 256 keys in one second) would take approximately
149 trillion years to crack a 128-bit AES key.
Cipher-Block Chaining
In computer networking applications, we typically need to encrypt long messages
(or long streams of data). If we apply a block cipher as described by simply chopping
up the message into k-bit blocks and independently encrypting each block, a
subtle but important problem occurs. To see this, observe that two or more of the cleartext
blocks can be identical. For example, the cleartext in two or more blocks could
be “HTTP/1.1”. For these identical blocks, a block cipher would, of course, produce
the same ciphertext. An attacker could potentially guess the cleartext when it sees
identical ciphertext blocks and may even be able to decrypt the entire message by
identifying identical ciphtertext blocks and using knowledge about the underlying
protocol structure [Kaufman 1995].
To address this problem, we can mix some randomness into the ciphertext so
that identical plaintext blocks produce different ciphertext blocks. To explain this
idea, let m(i) denote the ith plaintext block, c(i) denote the ith ciphertext block,
and a  b denote the exclusive-or (XOR) of two bit strings, a and b. (Recall that
the 0  0 = 1  1 = 0 and 0  1 = 1  0 = 1, and the XOR of two bit strings is done
on a bit-by-bit basis. So, for example, 10101010  11110000 = 01011010.) Also,
denote the block-cipher encryption algorithm with key S as KS. The basic idea is
as follows. The sender creates a random k-bit number r(i) for the ith block and calculates
c(i) = KS(m(i) r(i)). Note that a new k-bit random number is chosen for
each block. The sender then sends c(1), r(1), c(2), r(2), c(3), r(3), and so on. Since
the receiver receives c(i) and r(i), it can recover each block of the plaintext by
computing m(i) = KS(c(i))  r(i). It is important to note that, although r(i) is sent
in the clear and thus can be sniffed by Trudy, she cannot obtain the plaintext m(i),
since she does not know the key KS. Also note that if two plaintext blocks m(i) and
m( j) are the same, the corresponding ciphertext blocks c(i) and c( j) will be different
(as long as the random numbers r(i) and r( j) are different, which occurs with
very high probability).
As an example, consider the 3-bit block cipher in Table 8.1. Suppose the plaintext
is 010010010. If Alice encrypts this directly, without including the randomness,
the resulting ciphertext becomes 101101101. If Trudy sniffs this ciphertext, because
each of the three cipher blocks is the same, she can correctly surmise that each of
the three plaintext blocks are the same. Now suppose instead Alice generates the
random blocks r(1) = 001, r(2) =111, and r(3) = 100 and uses the above technique
to generate the ciphertext c(1) = 100, c(2) = 010, and c(3) = 000. Note that the three
ciphertext blocks are different even though the plaintext blocks are the same. Alice
then sends c(1), r(1), c(2), and r(2). You should verify that Bob can obtain the original
plaintext using the shared key KS.
The astute reader will note that introducing randomness solves one problem but
creates another: namely, Alice must transmit twice as many bits as before. Indeed,
for each cipher bit, she must now also send a random bit, doubling the required
bandwidth. In order to have our cake and eat it too, block ciphers typically use a
technique called Cipher Block Chaining (CBC). The basic idea is to send only one
random value along with the very first message, and then have the sender and
receiver use the computed coded blocks in place of the subsequent random number.
Specifically, CBC operates as follows:
1. Before encrypting the message (or the stream of data), the sender generates a
random k-bit string, called the Initialization Vector (IV). Denote this initialization
vector by c(0). The sender sends the IV to the receiver in cleartext.
2. For the first block, the sender calculates m(1)  c(0), that is, calculates the
exclusive-or of the first block of cleartext with the IV. It then runs the result
through the block-cipher algorithm to get the corresponding ciphertext block;
that is, c(1) = KS(m(1)  c(0)). The sender sends the encrypted block c(1) to
the receiver.
3. For the ith block, the sender generates the ith ciphertext block from c(i) =
KS(m(i)  c(i  1)).
Let’s now examine some of the consequences of this approach. First, the
receiver will still be able to recover the original message. Indeed, when the receiver
receives c(i), it decrypts it with KS to obtain s(i) = m(i)  c(i – 1); since the receiver
also knows c(i – 1), it then obtains the cleartext block from m(i) = s(i)  c(i – 1). Second,
even if two cleartext blocks are identical, the corresponding ciphtertexts
(almost always) will be different. Third, although the sender sends the IV in the
clear, an intruder will still not be able to decrypt the ciphertext blocks, since the
intruder does not know the secret key, S. Finally, the sender only sends one overhead
block (the IV), thereby negligibly increasing the bandwidth usage for long
messages (consisting of hundreds of blocks).
As an example, let’s now determine the ciphertext for the 3-bit block cipher in
Table 8.1 with plaintext 010010010 and IV = c(0) = 001. The sender first uses the
IV to calculate c(1) = KS(m(1)  c(0)) = 100. The sender then calculates c(2) =
KS(m(2)  c(1)) = KS(010  100) = 000, and c(3) = KS(m(3)  c(2)) = KS(010 
000) = 101. The reader should verify that the receiver, knowing the IV and KS can
recover the original plaintext.
CBC has an important consequence when designing secure network protocols:
we’ll need to provide a mechanism within the protocol to distribute the IV from
sender to receiver. We’ll see how this is done for several protocols later in this
chapter.
682 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
8.2.2 Public Key Encryption
For more than 2,000 years (since the time of the Caesar cipher and up to the
1970s), encrypted communication required that the two communicating parties
share a common secret—the symmetric key used for encryption and decryption.
One difficulty with this approach is that the two parties must somehow agree on
the shared key; but to do so requires (presumably secure) communication! Perhaps
the parties could first meet and agree on the key in person (for example, two of
Caesar’s centurions might meet at the Roman baths) and thereafter communicate
with encryption. In a networked world, however, communicating parties may never
meet and may never converse except over the network. Is it possible for two parties
to communicate with encryption without having a shared secret key that is
known in advance? In 1976, Diffie and Hellman [Diffie 1976] demonstrated an
algorithm (known now as Diffie-Hellman Key Exchange) to do just that—a radically
different and marvelously elegant approach toward secure communication
that has led to the development of today’s public key cryptography systems. We’ll
see shortly that public key cryptography systems also have several wonderful
properties that make them useful not only for encryption, but for authentication
and digital signatures as well. Interestingly, it has recently come to light that
ideas similar to those in [Diffie 1976] and [RSA 1978] had been independently
developed in the early 1970s in a series of secret reports by researchers at the
Communications-Electronics Security Group in the United Kingdom [Ellis
1987]. As is often the case, great ideas can spring up independently in many
places; fortunately, public key advances took place not only in private, but also
in the public view, as well.
The use of public key cryptography is conceptually quite simple. Suppose Alice
wants to communicate with Bob. As shown in Figure 8.6, rather than Bob and Alice
8.2 • PRINCIPLES OF CRYPTOGRAPHY 683
Encryption
algorithm
Ciphertext
Decryption
algorithm
Plaintext
message, m
Plaintext
message, m
Private decryption key
m = KB
–(KB
+(m))
KB
–
KB
+(m)
KB Public encryption key
+
Figure 8.6  Public key cryptography
sharing a single secret key (as in the case of symmetric key systems), Bob (the recipient
of Alice’s messages) instead has two keys—a public key that is available to
everyone in the world (including Trudy the intruder) and a private key that is known
only to Bob. We will use the notation KB
+ and KB
– to refer to Bob’s public and private
keys, respectively. In order to communicate with Bob, Alice first fetches Bob’s public
key. Alice then encrypts her message, m, to Bob using Bob’s public key and a
known (for example, standardized) encryption algorithm; that is, Alice computes
KB
+ (m). Bob receives Alice’s encrypted message and uses his private key and a known
(for example, standardized) decryption algorithm to decrypt Alice’s encrypted message.
That is, Bob computes KB
–(KB
+(m)). We will see below that there are encryption/
decryption algorithms and techniques for choosing public and private keys such
that KB
–(KB
+(m)) = m; that is, applying Bob’s public key, KB
+, to a message, m (to get
KB
+ (m)), and then applying Bob’s private key, KB
– , to the encrypted version of m (that
is, computing KB
–(KB
+ (m))) gives back m. This is a remarkable result! In this manner,
Alice can use Bob’s publicly available key to send a secret message to Bob without
either of them having to distribute any secret keys! We will see shortly that we can
interchange the public key and private key encryption and get the same remarkable
result––that is, KB
– (B
+(m)) = KB
+ (KB
– (m)) = m.
The use of public key cryptography is thus conceptually simple. But two immediate
worries may spring to mind. A first concern is that although an intruder intercepting
Alice’s encrypted message will see only gibberish, the intruder knows both
the key (Bob’s public key, which is available for all the world to see) and the algorithm
that Alice used for encryption. Trudy can thus mount a chosen-plaintext
attack, using the known standardized encryption algorithm and Bob’s publicly available
encryption key to encode any message she chooses! Trudy might well try, for
example, to encode messages, or parts of messages, that she suspects that Alice
might send. Clearly, if public key cryptography is to work, key selection and encryption/
decryption must be done in such a way that it is impossible (or at least so hard
as to be nearly impossible) for an intruder to either determine Bob’s private key or
somehow otherwise decrypt or guess Alice’s message to Bob. A second concern is
that since Bob’s encryption key is public, anyone can send an encrypted message to
Bob, including Alice or someone claiming to be Alice. In the case of a single shared
secret key, the fact that the sender knows the secret key implicitly identifies the
sender to the receiver. In the case of public key cryptography, however, this is no
longer the case since anyone can send an encrypted message to Bob using Bob’s
publicly available key. A digital signature, a topic we will study in Section 8.3, is
needed to bind a sender to a message.
RSA
While there may be many algorithms that address these concerns, the RSA algorithm
(named after its founders, Ron Rivest, Adi Shamir, and Leonard Adleman)
has become almost synonymous with public key cryptography. Let’s first see how
RSA works and then examine why it works.
684 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
RSA makes extensive use of arithmetic operations using modulo-n arithmetic.
So let’s briefly review modular arithmetic. Recall that x mod n simply means the
remainder of x when divided by n; so, for example, 19 mod 5 = 4. In modular arithmetic,
one performs the usual operations of addition, multiplication, and exponentiation.
However, the result of each operation is replaced by the integer remainder that
is left when the result is divided by n. Adding and multiplying with modular arithmetic
is facilitated with the following handy facts:
[(a mod n) + (b mod n)] mod n = (a + b) mod n
[(a mod n) – (b mod n)] mod n = (a – b) mod n
[(a mod n) • (b mod n)] mod n = (a • b) mod n
It follows from the third fact that (a mod n)d mod n = ad mod n, which is an identity
that we will soon find very useful.
Now suppose that Alice wants to send to Bob an RSA-encrypted message, as
shown in Figure 8.6. In our discussion of RSA, let’s always keep in mind that a message
is nothing but a bit pattern, and every bit pattern can be uniquely represented
by an integer number (along with the length of the bit pattern). For example, suppose
a message is the bit pattern 1001; this message can be represented by the decimal
integer 9. Thus, when encrypting a message with RSA, it is equivalent to encrypting
the unique integer number that represents the message.
There are two interrelated components of RSA:
• The choice of the public key and the private key
• The encryption and decryption algorithm
To generate the public and private RSA keys, Bob performs the following steps:
1. Choose two large prime numbers, p and q. How large should p and q be? The
larger the values, the more difficult it is to break RSA, but the longer it takes to
perform the encoding and decoding. RSA Laboratories recommends that the
product of p and q be on the order of 1,024 bits. For a discussion of how to
find large prime numbers, see [Caldwell 2012].
2. Compute n = pq and z = (p – 1)(q – 1).
3. Choose a number, e, less than n, that has no common factors (other than 1)
with z. (In this case, e and z are said to be relatively prime.) The letter e is used
since this value will be used in encryption.
4. Find a number, d, such that ed – 1 is exactly divisible (that is, with no remainder)
by z. The letter d is used because this value will be used in decryption. Put
another way, given e, we choose d such that
ed mod z = 1
5. The public key that Bob makes available to the world, KB
+, is the pair of numbers
(n, e); his private key, KB
–, is the pair of numbers (n, d).
8.2 • PRINCIPLES OF CRYPTOGRAPHY 685
The encryption by Alice and the decryption by Bob are done as follows:
• Suppose Alice wants to send Bob a bit pattern represented by the integer number m
(with m < n). To encode, Alice performs the exponentiation me, and then computes
the integer remainder when me is divided by n. In other words, the encrypted
value, c, of Alice’s plaintext message, m, is
c = me mod n
The bit pattern corresponding to this ciphertext c is sent to Bob.
• To decrypt the received ciphertext message, c, Bob computes
m = cd mod n
which requires the use of his private key (n,d).
As a simple example of RSA, suppose Bob chooses p = 5 and q = 7. (Admittedly,
these values are far too small to be secure.) Then n = 35 and z = 24. Bob
chooses e = 5, since 5 and 24 have no common factors. Finally, Bob chooses d = 29,
since 5  29 – 1 (that is, ed – 1) is exactly divisible by 24. Bob makes the two values,
n = 35 and e = 5, public and keeps the value d = 29 secret. Observing these two
public values, suppose Alice now wants to send the letters l, o, v, and e to Bob. Interpreting
each letter as a number between 1 and 26 (with a being 1, and z being 26),
Alice and Bob perform the encryption and decryption shown in Tables 8.2 and 8.3,
respectively. Note that in this example, we consider each of the four letters as a distinct
message. A more realistic example would be to convert the four letters into
their 8-bit ASCII representations and then encrypt the integer corresponding to the
resulting 32-bit bit pattern. (Such a realistic example generates numbers that are
much too long to print in a textbook!)
Given that the “toy” example in Tables 8.2 and 8.3 has already produced some
extremely large numbers, and given that we saw earlier that p and q should each be
several hundred bits long, several practical issues regarding RSA come to mind.
686 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Table 8.2  Alice’s RSA encryption, e = 5, n = 35
Plaintext Letter m: numeric representation m e Ciphertext c = m e mod n
l 12 248832 17
o 15 759375 15
v 22 5153632 22
e 5 3125 10
How does one choose large prime numbers? How does one then choose e and d?
How does one perform exponentiation with large numbers? A discussion of these
important issues is beyond the scope of this book; see [Kaufman 1995] and the references
therein for details.
Session Keys
We note here that the exponentiation required by RSA is a rather time-consuming
process. By contrast, DES is at least 100 times faster in software and between 1,000
and 10,000 times faster in hardware [RSA Fast 2012]. As a result, RSA is often used
in practice in combination with symmetric key cryptography. For example, if Alice
wants to send Bob a large amount of encrypted data, she could do the following.
First Alice chooses a key that will be used to encode the data itself; this key is
referred to as a session key, and is denoted by KS. Alice must inform Bob of the session
key, since this is the shared symmetric key they will use with a symmetric key
cipher (e.g., with DES or AES). Alice encrypts the session key using Bob’s public
key, that is, computes c = (KS)e mod n. Bob receives the RSA-encrypted session key,
c, and decrypts it to obtain the session key, KS. Bob now knows the session key that
Alice will use for her encrypted data transfer.
Why Does RSA Work?
RSA encryption/decryption appears rather magical. Why should it be that by applying
the encryption algorithm and then the decryption algorithm, one recovers the
original message? In order to understand why RSA works, again denote n = pq,
where p and q are the large prime numbers used in the RSA algorithm.
Recall that, under RSA encryption, a message (uniquely represented by an integer),
m, is exponentiated to the power e using modulo-n arithmetic, that is,
c = me mod n
Decryption is performed by raising this value to the power d, again using modulo-n
arithmetic. The result of an encryption step followed by a decryption step is thus
8.2 • PRINCIPLES OF CRYPTOGRAPHY 687
Table 8.3  Bob’s RSA decryption, d = 29, n = 35
Ciphertext c cd m = c d mod n Plaintext Letter
17 4819685721067509150915091411825223071697 12 l
15 127834039403948858939111232757568359375 15 o
22 851643319086537701956194499721106030592 22 v
10 1000000000000000000000000000000 5 e
(me mod n)d mod n . Let’s now see what we can say about this quantity. As mentioned
earlier, one important property of modulo arithmetic is (a mod n)d mod n = ad mod
n for any values a, n, and d. Thus, using a = me in this property, we have
(me mod n)d mod n = med mod n
It therefore remains to show that med mod n = m. Although we’re trying to remove
some of the magic about why RSA works, to establish this, we’ll need to use a rather
magical result from number theory here. Specifically, we’ll need the result that says if
p and q are prime, n = pq, and z = (p – 1)(q – 1), then xy mod n is the same as x(y mod z)
mod n [Kaufman 1995]. Applying this result with x = m and y = ed we have
med mod n = m(ed mod z) mod n
But remember that we have chosen e and d such that ed mod z = 1. This gives us
med mod n = m1 mod n = m
which is exactly the result we are looking for! By first exponentiating to the power
of e (that is, encrypting) and then exponentiating to the power of d (that is, decrypting),
we obtain the original value, m. Even more wonderful is the fact that if we first
exponentiate to the power of d and then exponentiate to the power of e—that is, we
reverse the order of encryption and decryption, performing the decryption operation
first and then applying the encryption operation—we also obtain the original value,
m. This wonderful result follows immediately from the modular arithmetic:
(md mod n)e mod n = mde mod n = med mod n = (me mod n)d mod n
688 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
The security of RSA relies on the fact that there are no known algorithms for
quickly factoring a number, in this case the public value n, into the primes p and q. If
one knew p and q, then given the public value e, one could easily compute the secret
key, d. On the other hand, it is not known whether or not there exist fast algorithms for
factoring a number, and in this sense, the security of RSA is not guaranteed.
Another popular public-key encryption algorithm is the Diffie-Hellman algorithm,
which we will briefly explore in the homework problems. Diffie-Hellman is
not as versatile as RSA in that it cannot be used to encrypt messages of arbitrary
length; it can be used, however, to establish a symmetric session key, which is in
turn used to encrypt messages.
8.3 Message Integrity and Digital Signatures
In the previous section we saw how encryption can be used to provide confidentiality
to two communicating entities. In this section we turn to the equally important
cryptography topic of providing message integrity (also known as message authentication).
Along with message integrity, we will discuss two related topics in this
section: digital signatures and end-point authentication.
We define the message integrity problem using, once again, Alice and Bob.
Suppose Bob receives a message (which may be encrypted or may be in plaintext)
and he believes this message was sent by Alice. To authenticate this message, Bob
needs to verify:
1. The message indeed originated from Alice.
2. The message was not tampered with on its way to Bob.
We’ll see in Sections 8.4 through 8.7 that this problem of message integrity is a
critical concern in just about all secure networking protocols.
As a specific example, consider a computer network using a link-state routing
algorithm (such as OSPF) for determining routes between each pair of routers in
the network (see Chapter 4). In a link-state algorithm, each router needs to broadcast
a link-state message to all other routers in the network. A router’s link-state
message includes a list of its directly connected neighbors and the direct costs to
these neighbors. Once a router receives link-state messages from all of the other
routers, it can create a complete map of the network, run its least-cost routing
algorithm, and configure its forwarding table. One relatively easy attack on the
routing algorithm is for Trudy to distribute bogus link-state messages with incorrect
link-state information. Thus the need for message integrity—when router B
receives a link-state message from router A, router B should verify that router A
actually created the message and, further, that no one tampered with the message
in transit.
In this section, we describe a popular message integrity technique that is used
by many secure networking protocols. But before doing so, we need to cover
another important topic in cryptography—cryptographic hash functions.
8.3.1 Cryptographic Hash Functions
As shown in Figure 8.7, a hash function takes an input, m, and computes a fixedsize
string H(m) known as a hash. The Internet checksum (Chapter 3) and CRCs
(Chapter 4) meet this definition. A cryptographic hash function is required to have
the following additional property:
• It is computationally infeasible to find any two different messages x and y such
that H(x) = H(y).
Informally, this property means that it is computationally infeasible for an
intruder to substitute one message for another message that is protected by the hash
function. That is, if (m, H(m)) are the message and the hash of the message created
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 689
by the sender, then an intruder cannot forge the contents of another message, y, that
has the same hash value as the original message.
Let’s convince ourselves that a simple checksum, such as the Internet checksum,
would make a poor cryptographic hash function. Rather than performing 1s
complement arithmetic (as in the Internet checksum), let us compute a checksum
by treating each character as a byte and adding the bytes together using 4-byte
chunks at a time. Suppose Bob owes Alice $100.99 and sends an IOU to Alice
consisting of the text string “IOU100.99BOB.” The ASCII representation (in
hexadecimal notation) for these letters is 49, 4F, 55, 31, 30, 30, 2E, 39, 39,
42, 4F, 42.
Figure 8.8 (top) shows that the 4-byte checksum for this message is B2 C1
D2 AC. A slightly different message (and a much more costly one for Bob) is
shown in the bottom half of Figure 8.8. The messages “IOU100.99BOB” and
“IOU900.19BOB” have the same checksum. Thus, this simple checksum algorithm
violates the requirement above. Given the original data, it is simple to find
another set of data with the same checksum. Clearly, for security purposes, we are
going to need a more powerful hash function than a checksum.
The MD5 hash algorithm of Ron Rivest [RFC 1321] is in wide use today. It
computes a 128-bit hash in a four-step process consisting of a padding step
(adding a one followed by enough zeros so that the length of the message satisfies
certain conditions), an append step (appending a 64-bit representation of the message
length before padding), an initialization of an accumulator, and a final looping
step in which the message’s 16-word blocks are processed (mangled) in four
rounds. For a description of MD5 (including a C source code implementation) see
[RFC 1321].
690 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Many-to-one
hash function
Long message: m
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash: H(m)
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Figure 8.7  Hash functions
The second major hash algorithm in use today is the Secure Hash Algorithm
(SHA-1) [FIPS 1995]. This algorithm is based on principles similar to those used in
the design of MD4 [RFC 1320], the predecessor to MD5. SHA-1, a US federal
standard, is required for use whenever a cryptographic hash algorithm is needed for
federal applications. It produces a 160-bit message digest. The longer output length
makes SHA-1 more secure.
8.3.2 Message Authentication Code
Let’s now return to the problem of message integrity. Now that we understand hash
functions, let’s take a first stab at how we might perform message integrity:
1. Alice creates message m and calculates the hash H(m) (for example with
SHA-1).
2. Alice then appends H(m) to the message m, creating an extended message
(m, H(m)), and sends the extended message to Bob.
3. Bob receives an extended message (m, h) and calculates H(m). If H(m) = h,
Bob concludes that everything is fine.
This approach is obviously flawed. Trudy can create a bogus message m´ in which
she says she is Alice, calculate H(m´), and send Bob (m´, H(m´)). When Bob receives
the message, everything checks out in step 3, so Bob doesn’t suspect any funny
business.
Figure 8.8  Initial message and fraudulent message have the same
checksum!
Message
I O U 1
0 0 . 9
9 B O B
ASCII
Representation
49 4F 55 31
30 30 2E 39
39 42 4F 42
B2 C1 D2 AC Checksum
Message
I O U 9
0 0 . 1
9 B O B
ASCII
Representation
49 4F 55 39
30 30 2E 31
39 42 4F 42
B2 C1 D2 AC Checksum
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 691
To perform message integrity, in addition to using cryptographic hash functions,
Alice and Bob will need a shared secret s. This shared secret, which is nothing
more than a string of bits, is called the authentication key. Using this shared secret,
message integrity can be performed as follows:
1. Alice creates message m, concatenates s with m to create m + s, and calculates
the hash H(m + s) (for example with SHA-1). H(m + s) is called the message
authentication code (MAC).
2. Alice then appends the MAC to the message m, creating an extended message
(m, H(m + s)), and sends the extended message to Bob.
3. Bob receives an extended message (m, h) and knowing s, calculates the MAC
H(m + s). If H(m + s) = h, Bob concludes that everything is fine.
A summary of the procedure is shown in Figure 8.9. Readers should note that the
MAC here (standing for “message authentication code”) is not the same MAC used
in link-layer protocols (standing for “medium access control”)!
One nice feature of a MAC is that it does not require an encryption algorithm.
Indeed, in many applications, including the link-state routing algorithm described
earlier, communicating entities are only concerned with message integrity and are
not concerned with message confidentiality. Using a MAC, the entities can authenticate
the messages they send to each other without having to integrate complex
encryption algorithms into the integrity process.
As you might expect, a number of different standards for MACs have been proposed
over the years. The most popular standard today is HMAC, which can be
692 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
H(.)
H(.)
m
m
m
m
s
s
s
+ Internet
Compare
Key:
= Message
= Shared secret
H(m+s)
H(m+s)
Figure 8.9  Message authentication code (MAC)
used either with MD5 or SHA-1. HMAC actually runs data and the authentication
key through the hash function twice [Kaufman 1995; RFC 2104].
There still remains an important issue. How do we distribute the shared authentication
key to the communicating entities? For example, in the link-state routing
algorithm, we would somehow need to distribute the secret authentication key to
each of the routers in the autonomous system. (Note that the routers can all use
the same authentication key.) A network administrator could actually accomplish
this by physically visiting each of the routers. Or, if the network administrator is
a lazy guy, and if each router has its own public key, the network administrator
could distribute the authentication key to any one of the routers by encrypting it
with the router’s public key and then sending the encrypted key over the network
to the router.
8.3.3 Digital Signatures
Think of the number of the times you’ve signed your name to a piece of paper during
the last week. You sign checks, credit card receipts, legal documents, and letters.
Your signature attests to the fact that you (as opposed to someone else) have
acknowledged and/or agreed with the document’s contents. In a digital world, one
often wants to indicate the owner or creator of a document, or to signify one’s agreement
with a document’s content. A digital signature is a cryptographic technique
for achieving these goals in a digital world.
Just as with handwritten signatures, digital signing should be done in a way that
is verifiable and nonforgeable. That is, it must be possible to prove that a document
signed by an individual was indeed signed by that individual (the signature must be
verifiable) and that only that individual could have signed the document (the signature
cannot be forged).
Let’s now consider how we might design a digital signature scheme. Observe
that when Bob signs a message, Bob must put something on the message that is
unique to him. Bob could consider attaching a MAC for the signature, where the
MAC is created by appending his key (unique to him) to the message, and then taking
the hash. But for Alice to verify the signature, she must also have a copy of the key,
in which case the key would not be unique to Bob. Thus, MACs are not going to get
the job done here.
Recall that with public-key cryptography, Bob has both a public and private
key, with both of these keys being unique to Bob. Thus, public-key cryptography is
an excellent candidate for providing digital signatures. Let us now examine how it
is done.
Suppose that Bob wants to digitally sign a document, m. We can think of the
document as a file or a message that Bob is going to sign and send. As shown in
Figure 8.10, to sign this document, Bob simply uses his private key, KB
–, to compute
KB
–(m). At first, it might seem odd that Bob is using his private key (which, as
we saw in Section 8.2, was used to decrypt a message that had been encrypted
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 693
with his public key) to sign a document. But recall that encryption and decryption
are nothing more than mathematical operations (exponentiation to the power of e
or d in RSA; see Section 8.2) and recall that Bob’s goal is not to scramble or
obscure the contents of the document, but rather to sign the document in a manner
that is verifiable and nonforgeable. Bob’s digital signature of the document is
KB
–(m).
Does the digital signature KB
–(m) meet our requirements of being verifiable and
nonforgeable? Suppose Alice has m and KB
–(m). She wants to prove in court (being
litigious) that Bob had indeed signed the document and was the only person who
could have possibly signed the document. Alice takes Bob’s public key, KB
+, and
applies it to the digital signature, KB
–(m), associated with the document, m. That is,
she computes KB
+(KB
–(m)), and voilà, with a dramatic flurry, she produces m, which
exactly matches the original document! Alice then argues that only Bob could have
signed the document, for the following reasons:
• Whoever signed the message must have used the private key, KB
–, in computing
the signature KB
–(m), such that KB
+(KB
–(m)) = m.
• The only person who could have known the private key, KB
–, is Bob. Recall from
our discussion of RSA in Section 8.2 that knowing the public key, KB
+, is of no
help in learning the private key, KB
–. Therefore, the only person who could know
KB
– is the person who generated the pair of keys, (KB
+, KB
–), in the first place, Bob.
(Note that this assumes, though, that Bob has not given KB
– to anyone, nor has
anyone stolen KB
– from Bob.)
694 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Encryption
algorithm
Message: m
Bob’s private
key, KB
–
Dear Alice:
Sorry I have been unable
to write for so long. Since
we.....
..........
..........
Bob
Signed message:
KB
– (m)
fadfg54986fgnzmcnv
T98734ngldskg02j
ser09tugkjdflg
..........
Figure 8.10  Creating a digital signature for a document
It is also important to note that if the original document, m, is ever modified to
some alternate form, m´, the signature that Bob created for m will not be valid for m´,
since KB
+(KB
–(m)) does not equal m´. Thus we see that digital signatures also provide
message integrity, allowing the receiver to verify that the message was unaltered as
well as the source of the message.
One concern with signing data by encryption is that encryption and decryption
are computationally expensive. Given the overheads of encryption and decryption,
signing data via complete encryption/decryption can be overkill. A more efficient
approach is to introduce hash functions into the digital signature. Recall from Section
8.3.2 that a hash algorithm takes a message, m, of arbitrary length and computes
a fixed-length “fingerprint” of the message, denoted by H(m). Using a hash function,
Bob signs the hash of a message rather than the message itself, that is, Bob calculates
KB
–(H(m)). Since H(m) is generally much smaller than the original message
m, the computational effort required to create the digital signature is substantially
reduced.
In the context of Bob sending a message to Alice, Figure 8.11 provides a summary
of the operational procedure of creating a digital signature. Bob puts his original
long message through a hash function. He then digitally signs the resulting hash
Bob’s private
key, KB
–
Many-to-one
hash function
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
Package to send hash
to Alice
Fgkopdgoo69cmxw
54psdterma[asofmz
Encryption
algorithm
Figure 8.11  Sending a digitally signed message
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 695
with his private key. The original message (in cleartext) along with the digitally
signed message digest (henceforth referred to as the digital signature) is then sent to
Alice. Figure 8.12 provides a summary of the operational procedure of the signature.
Alice applies the sender’s public key to the message to obtain a hash result.
Alice also applies the hash function to the cleartext message to obtain a second hash
result. If the two hashes match, then Alice can be sure about the integrity and author
of the message.
Before moving on, let’s briefly compare digital signatures with MACs, since
they have parallels, but also have important subtle differences. Both digital signatures
and MACs start with a message (or a document). To create a MAC out of the message,
we append an authentication key to the message, and then take the hash of the
result. Note that neither public key nor symmetric key encryption is involved in creating
the MAC. To create a digital signature, we first take the hash of the message
and then encrypt the message with our private key (using public key cryptography).
696 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Bob’s public
key, KB
+
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
hash
Fgkopdgoo69cmxw
54psdterma[asofmz
Many-to-one
hash function Compare
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Encryption
algorithm
Figure 8.12  Verifying a signed message
Thus, a digital signature is a “heavier” technique, since it requires an underlying
Public Key Infrastructure (PKI) with certification authorities as described below.
We’ll see in Section 8.4 that PGP—a popular secure e-mail system—uses digital
signatures for message integrity. We’ve seen already that OSPF uses MACs for
message integrity. We’ll see in Sections 8.5 and 8.6 that MACs are also used for popular
transport-layer and network-layer security protocols.
Public Key Certification
An important application of digital signatures is public key certification, that is,
certifying that a public key belongs to a specific entity. Public key certification is
used in many popular secure networking protocols, including IPsec and SSL.
To gain insight into this problem, let’s consider an Internet-commerce version
of the classic “pizza prank.” Alice is in the pizza delivery business and accepts
orders over the Internet. Bob, a pizza lover, sends Alice a plaintext message that
includes his home address and the type of pizza he wants. In this message, Bob also
includes a digital signature (that is, a signed hash of the original plaintext message)
to prove to Alice that he is the true source of the message. To verify the signature,
Alice obtains Bob’s public key (perhaps from a public key server or from the e-mail
message) and checks the digital signature. In this manner she makes sure that Bob,
rather than some adolescent prankster, placed the order.
This all sounds fine until clever Trudy comes along. As shown in Figure 8.13,
Trudy is indulging in a prank. She sends a message to Alice in which she says she is
Bob, gives Bob’s home address, and orders a pizza. In this message she also
includes her (Trudy’s) public key, although Alice naturally assumes it is Bob’s public
key. Trudy also attaches a digital signature, which was created with her own
(Trudy’s) private key. After receiving the message, Alice applies Trudy’s public key
(thinking that it is Bob’s) to the digital signature and concludes that the plaintext
message was indeed created by Bob. Bob will be very surprised when the delivery
person brings a pizza with pepperoni and anchovies to his home!
We see from this example that for public key cryptography to be useful, you
need to be able to verify that you have the actual public key of the entity (person,
router, browser, and so on) with whom you want to communicate. For example, when
Alice wants to communicate with Bob using public key cryptography, she needs to
verify that the public key that is supposed to be Bob’s is indeed Bob’s.
Binding a public key to a particular entity is typically done by a Certification
Authority (CA), whose job is to validate identities and issue certificates. A CA has
the following roles:
1. A CA verifies that an entity (a person, a router, and so on) is who it says it is.
There are no mandated procedures for how certification is done. When dealing
with a CA, one must trust the CA to have performed a suitably rigorous identity
verification. For example, if Trudy were able to walk into the Fly-by-Night CA
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 697
and simply announce “I am Alice” and receive certificates associated with the
identity of Alice, then one shouldn’t put much faith in public keys certified by
the Fly-by-Night CA. On the other hand, one might (or might not!) be more
willing to trust a CA that is part of a federal or state program. You can trust the
identity associated with a public key only to the extent to which you can trust a
CA and its identity verification techniques. What a tangled web of trust we spin!
2. Once the CA verifies the identity of the entity, the CA creates a certificate that
binds the public key of the entity to the identity. The certificate contains the
public key and globally unique identifying information about the owner of the
public key (for example, a human name or an IP address). The certificate is
digitally signed by the CA. These steps are shown in Figure 8.14.
Let us now see how certificates can be used to combat pizza-ordering
pranksters, like Trudy, and other undesirables. When Bob places his order he also
sends his CA-signed certificate. Alice uses the CA’s public key to check the validity
of Bob’s certificate and extract Bob’s public key.
698 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Trudy’s private
key, KT
–
Trudy’s public
key, KT
+
Signed (using
Trudy's private key)
message digest
Fgkopdgoo69cmxw
54psdterma[asofmz
Message
Alice,
Deliver a pizza to me.
Bob
Many-to-one
hash function
Alice uses Trudy’s
public key, thinking
it is Bob’s, and
concludes the
message is from Bob
PIZZA
Encryption
algorithm
Figure 8.13  Trudy masquerades as Bob using public key cryptography
Both the International Telecommunication Union (ITU) and the IETF have
developed standards for CAs. ITU X.509 [ITU 2005a] specifies an authentication
service as well as a specific syntax for certificates. [RFC 1422] describes CA-based
key management for use with secure Internet e-mail. It is compatible with X.509 but
goes beyond X.509 by establishing procedures and conventions for a key management
architecture. Table 8.4 describes some of the important fields in a certificate.
Bob’s CA-signed
certificate containing
his public key, KB
+
Certification
Authority (CA)
(KB
+, B)
CA’s private
key, KCA
–
Encryption
algorithm
Figure 8.14  Bob has his public key certified by the CA
Field Name Description
Version Version number of X.509 specification
Serial number CA-issued unique identifier for a certificate
Signature Specifies the algorithm used by CA to sign this certificate
Issuer name Identity of CA issuing this certificate, in distinguished name (DN)[RFC 4514] format
Validity period Start and end of period of validity for certificate
Subject name Identity of entity whose public key is associated with this certificate, in DN format
Subject public key The subject’s public key as well indication of the public key algorithm (and algorithm
parameters) to be used with this key
Table 8.4  Selected fields in an X.509 and RFC 1422 public key
8.3 • MESSAGE INTEGRITY AND DIGITAL SIGNATURES 699
8.4 End-Point Authentication
End-point authentication is the process of one entity proving its identity to
another entity over a computer network, for example, a user proving its identity
to an email server. As humans, we authenticate each other in many ways: We recognize
each other’s faces when we meet, we recognize each other’s voices on the
telephone, we are authenticated by the customs official who checks us against the
picture on our passport.
In this section, we consider how one party can authenticate another
party when the two are communicating over a network. We focus here on authenticating
a “live” party, at the point in time when communication is actually occurring.
A concrete example is a user authenticating him or herself to an e-mail
server. This is a subtly different problem from proving that a message received at
some point in the past did indeed come from that claimed sender, as studied in
Section 8.3.
When performing authentication over the network, the communicating parties
cannot rely on biometric information, such as a visual appearance or a voiceprint.
Indeed, we will see in our later case studies that it is often network
elements such as routers and client/server processes that must authenticate each
other. Here, authentication must be done solely on the basis of messages and data
exchanged as part of an authentication protocol. Typically, an authentication
protocol would run before the two communicating parties run some other protocol
(for example, a reliable data transfer protocol, a routing information
exchange protocol, or an e-mail protocol). The authentication protocol first
establishes the identities of the parties to each other’s satisfaction; only after
authentication do the parties get down to the work at hand.
As in the case of our development of a reliable data transfer (rdt) protocol in
Chapter 3, we will find it instructive here to develop various versions of an authentication
protocol, which we will call ap (authentication protocol), and poke holes in
each version as we proceed. (If you enjoy this stepwise evolution of a design, you
might also enjoy [Bryant 1988], which recounts a fictitious narrative between
designers of an open-network authentication system, and their discovery of the
many subtle issues involved.)
Let’s assume that Alice needs to authenticate herself to Bob.
8.4.1 Authentication Protocol ap1.0
Perhaps the simplest authentication protocol we can imagine is one where Alice
simply sends a message to Bob saying she is Alice. This protocol is shown in
Figure 8.15. The flaw here is obvious—there is no way for Bob actually to know
700 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Alice
I am Alice
Bob
Trudy
Trudy
Alice
I am Alice
Bob
Figure 8.15  Protocol ap1.0 and a failure scenario
8.4 • END-POINT AUTHENTICATION 701
that the person sending the message “I am Alice” is indeed Alice. For example,
Trudy (the intruder) could just as well send such a message.
8.4.2 Authentication Protocol ap2.0
If Alice has a well-known network address (e.g., an IP address) from which she
always communicates, Bob could attempt to authenticate Alice by verifying that the
source address on the IP datagram carrying the authentication message matches
Alice’s well-known address. In this case, Alice would be authenticated. This might
stop a very network-naive intruder from impersonating Alice, but it wouldn’t stop
the determined student studying this book, or many others!
From our study of the network and data link layers, we know that it is not that
hard (for example, if one had access to the operating system code and could build
one’s own operating system kernel, as is the case with Linux and several other
freely available operating systems) to create an IP datagram, put whatever IP source
address we want (for example, Alice’s well-known IP address) into the IP datagram,
and send the datagram over the link-layer protocol to the first-hop router. From then
on, the incorrectly source-addressed datagram would be dutifully forwarded to Bob.
This approach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can be
avoided if Trudy’s first-hop router is configured to forward only datagrams containing
Trudy’s IP source address [RFC 2827]. However, this capability is not universally
deployed or enforced. Bob would thus be foolish to assume that Trudy’s
network manager (who might be Trudy herself) had configured Trudy’s first-hop
router to forward only appropriately addressed datagrams.
8.4.3 Authentication Protocol ap3.0
One classic approach to authentication is to use a secret password. The password is
a shared secret between the authenticator and the person being authenticated. Gmail,
Facebook, telnet, FTP, and many other services use password authentication. In protocol
ap3.0, Alice thus sends her secret password to Bob, as shown in Figure 8.17.
702 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Alice
I am Alice,
password
OK
Bob
Trudy
Alice
I am Alice,
password
OK
Bob
Trudy
Tape recorder
Key:
Figure 8.17  Protocol ap3.0 and a failure scenario
Alice
I am Alice
Alice’s IP addr.
Bob
Trudy
Alice
I am Alice
Alice’s IP addr.
Bob
Trudy
Figure 8.16  Protocol ap2.0 and a failure scenario
Since passwords are so widely used, we might suspect that protocol ap3.0
is fairly secure. If so, we’d be wrong! The security flaw here is clear. If Trudy
eavesdrops on Alice’s communication, then she can learn Alice’s password. Lest
you think this is unlikely, consider the fact that when you Telnet to another
machine and log in, the login password is sent unencrypted to the Telnet server.
Someone connected to the Telnet client or server’s LAN can possibly sniff
(read and store) all packets transmitted on the LAN and thus steal the login password.
In fact, this is a well-known approach for stealing passwords (see, for
example, [Jimenez 1997]). Such a threat is obviously very real, so ap3.0 clearly
won’t do.
8.4.4 Authentication Protocol ap3.1
Our next idea for fixing ap3.0 is naturally to encrypt the password. By encrypting the
password, we can prevent Trudy from learning Alice’s password. If we assume that
Alice and Bob share a symmetric secret key, then Alice can encrypt
the password and send her identification message, “I am Alice,” and her encrypted
password to Bob. Bob then decrypts the password and, assuming the password is
correct, authenticates Alice. Bob feels comfortable in authenticating Alice since Alice
not only knows the password, but also knows the shared secret key value needed to
encrypt the password. Let’s call this protocol ap3.1.
While it is true that ap3.1 prevents Trudy from learning Alice’s password,
the use of cryptography here does not solve the authentication problem. Bob is
subject to a playback attack: Trudy need only eavesdrop on Alice’s communication,
record the encrypted version of the password, and play back the encrypted
version of the password to Bob to pretend that she is Alice. The use of an
encrypted password in ap3.1 doesn’t make the situation manifestly different from
that of protocol ap3.0 in Figure 8.17.
8.4.5 Authentication Protocol ap4.0
The failure scenario in Figure 8.17 resulted from the fact that Bob could not distinguish
between the original authentication of Alice and the later playback of
Alice’s original authentication. That is, Bob could not tell if Alice was live (that
is, was currently really on the other end of the connection) or whether the messages
he was receiving were a recorded playback of a previous authentication of
Alice. The very (very) observant reader will recall that the three-way TCP handshake
protocol needed to address the same problem—the server side of a TCP
connection did not want to accept a connection if the received SYN segment was
an old copy (retransmission) of a SYN segment from an earlier connection. How
KA-B,
8.4 • END-POINT AUTHENTICATION 703
did the TCP server side solve the problem of determining whether the client was
really live? It chose an initial sequence number that had not been used in a very
long time, sent that number to the client, and then waited for the client to respond
with an ACK segment containing that number. We can adopt the same idea here
for authentication purposes.
A nonce is a number that a protocol will use only once in a lifetime. That is,
once a protocol uses a nonce, it will never use that number again. Our ap4.0
protocol uses a nonce as follows:
1. Alice sends the message “I am Alice” to Bob.
2. Bob chooses a nonce, R, and sends it to Alice.
3. Alice encrypts the nonce using Alice and Bob’s symmetric secret key,
and sends the encrypted nonce, (R), back to Bob. As in protocol ap3.1,
it is the fact that Alice knows and uses it to encrypt a value that lets
Bob know that the message he receives was generated by Alice. The nonce
is used to ensure that Alice is live.
4. Bob decrypts the received message. If the decrypted nonce equals the nonce he
sent Alice, then Alice is authenticated.
Protocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-lifetime
value, R, and then checking the returned value, (R), Bob can be sure
that Alice is both who she says she is (since she knows the secret key value
needed to encrypt R) and live (since she has encrypted the nonce, R, that Bob just
created).
The use of a nonce and symmetric key cryptography forms the basis of ap4.0.
A natural question is whether we can use a nonce and public key cryptography
KA-B
KA-B
KA-B
KA-B,
704 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Alice
R
KA–B(R)
I am Alice
Bob
Figure 8.18  Protocol ap4.0 and a failure scenario
(rather than symmetric key cryptography) to solve the authentication problem. This
issue is explored in the problems at the end of the chapter.
8.5 Securing E-Mail
In previous sections, we examined fundamental issues in network security,
including symmetric key and public key cryptography, end-point authentication,
key distribution, message integrity, and digital signatures. We are now going to
examine how these tools are being used to provide security in the Internet.
Interestingly, it is possible to provide security services in any of the top four
layers of the Internet protocol stack. When security is provided for a specific application-
layer protocol, the application using the protocol will enjoy one or more
security services, such as confidentiality, authentication, or integrity. When security
is provided for a transport-layer protocol, all applications that use that protocol
enjoy the security services of the transport protocol. When security is provided at
the network layer on a host-to-host basis, all transport-layer segments (and hence all
application-layer data) enjoy the security services of the network layer. When security
is provided on a link basis, then the data in all frames traveling over the link
receive the security services of the link.
In Sections 8.5 through 8.8, we examine how security tools are being used in
the application, transport, network, and link layers. Being consistent with the general
structure of this book, we begin at the top of the protocol stack and discuss
security at the application layer. Our approach is to use a specific application,
e-mail, as a case study for application-layer security. We then move down the protocol
stack. We’ll examine the SSL protocol (which provides security at the transport
layer), IPsec (which provides security at the network layer), and the security of the
IEEE 802.11 wireless LAN protocol.
You might be wondering why security functionality is being provided at
more than one layer in the Internet. Wouldn’t it suffice simply to provide the
security functionality at the network layer and be done with it? There are two
answers to this question. First, although security at the network layer can offer
“blanket coverage” by encrypting all the data in the datagrams (that is, all the
transport-layer segments) and by authenticating all the source IP addresses, it
can’t provide user-level security. For example, a commerce site cannot rely on
IP-layer security to authenticate a customer who is purchasing goods at the commerce
site. Thus, there is a need for security functionality at higher layers as well
as blanket coverage at lower layers. Second, it is generally easier to deploy new
Internet services, including security services, at the higher layers of the protocol
stack. While waiting for security to be broadly deployed at the network layer,
which is probably still many years in the future, many application developers
8.5 • SECURING E-MAIL 705
“just do it” and introduce security functionality into their favorite applications. A
classic example is Pretty Good Privacy (PGP), which provides secure e-mail
(discussed later in this section). Requiring only client and server application
code, PGP was one of the first security technologies to be broadly used in the
Internet.
8.5.1 Secure E-Mail
We now use the cryptographic principles of Sections 8.2 through 8.3 to create a
secure e-mail system. We create this high-level design in an incremental manner, at
each step introducing new security services. When designing a secure e-mail system,
let us keep in mind the racy example introduced in Section 8.1—the love affair
between Alice and Bob. Imagine that Alice wants to send an e-mail message to Bob,
and Trudy wants to intrude.
Before plowing ahead and designing a secure e-mail system for Alice and
Bob, we should consider which security features would be most desirable for
them. First and foremost is confidentiality. As discussed in Section 8.1, neither
Alice nor Bob wants Trudy to read Alice’s e-mail message. The second feature
that Alice and Bob would most likely want to see in the secure e-mail system is
sender authentication. In particular, when Bob receives the message “I don’t
love you anymore. I never want to see you again. Formerly
yours, Alice,” he would naturally want to be sure that the message
came from Alice and not from Trudy. Another feature that the two lovers would
appreciate is message integrity, that is, assurance that the message Alice sends is
not modified while en route to Bob. Finally, the e-mail system should provide
receiver authentication; that is, Alice wants to make sure that she is indeed sending
the letter to Bob and not to someone else (for example, Trudy) who is impersonating
Bob.
So let’s begin by addressing the foremost concern, confidentiality. The most
straightforward way to provide confidentiality is for Alice to encrypt the message
with symmetric key technology (such as DES or AES) and for Bob to decrypt the
message on receipt. As discussed in Section 8.2, if the symmetric key is long
enough, and if only Alice and Bob have the key, then it is extremely difficult for
anyone else (including Trudy) to read the message. Although this approach is
straightforward, it has the fundamental difficulty that we discussed in Section
8.2—distributing a symmetric key so that only Alice and Bob have copies of it. So
we naturally consider an alternative approach—public key cryptography (using,
for example, RSA). In the public key approach, Bob makes his public key publicly
available (e.g., in a public key server or on his personal Web page), Alice
encrypts her message with Bob’s public key, and she sends the encrypted message
to Bob’s e-mail address. When Bob receives the message, he simply decrypts it
with his private key. Assuming that Alice knows for sure that the public key is
706 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Bob’s public key, this approach is an excellent means to provide the desired confidentiality.
One problem, however, is that public key encryption is relatively inefficient,
particularly for long messages.
To overcome the efficiency problem, let’s make use of a session key (discussed
in Section 8.2.2). In particular, Alice (1) selects a random symmetric session
key, KS, (2) encrypts her message, m, with the symmetric key, (3) encrypts
the symmetric key with Bob’s public key, KB
+, (4) concatenates the encrypted
message and the encrypted symmetric key to form a “package,” and (5) sends the
package to Bob’s e-mail address. The steps are illustrated in Figure 8.19. (In this
and the subsequent figures, the circled “+” represents concatenation and the circled
“–” represents deconcatenation.) When Bob receives the package, he (1)
uses his private key, KB
–, to obtain the symmetric key, KS, and (2) uses the symmetric
key KS to decrypt the message m.
Having designed a secure e-mail system that provides confidentiality, let’s now
design another system that provides both sender authentication and message
integrity. We’ll suppose, for the moment, that Alice and Bob are no longer concerned
with confidentiality (they want to share their feelings with everyone!), and are
concerned only about sender authentication and message integrity. To accomplish
this task, we use digital signatures and message digests, as described in Section 8.3.
Specifically, Alice (1) applies a hash function, H (for example, MD5), to her
message, m, to obtain a message digest, (2) signs the result of the hash function
with her private key, KA
–, to create a digital signature, (3) concatenates the original
(unencrypted) message with the signature to create a package, and (4) sends the
package to Bob’s e-mail address. When Bob receives the package, he (1) applies
Alice’s public key, KA
+, to the signed message digest and (2) compares the result of
this operation with his own hash, H, of the message. The steps are illustrated in
8.5 • SECURING E-MAIL 707
KS (.) KS (.)
KS (m) KS (m)
KS
KS
KB
+(.)
KB
K +(KS ) B
+(KS )
m m
+ Internet –
KB
–(.)
Alice sends e-mail message m Bob receives e-mail message m
Figure 8.19  Alice used a symmetric session key, KS, to send a secret
e-mail to Bob
Figure 8.20. As discussed in Section 8.3, if the two results are the same, Bob can be
pretty confident that the message came from Alice and is unaltered.
Now let’s consider designing an e-mail system that provides confidentiality,
sender authentication, and message integrity. This can be done by combining the
procedures in Figures 8.19 and 8.20. Alice first creates a preliminary package,
exactly as in Figure 8.20, that consists of her original message along with a digitally
signed hash of the message. She then treats this preliminary package as a
message in itself and sends this new message through the sender steps in Figure 8.19,
creating a new package that is sent to Bob. The steps applied by Alice are shown
in Figure 8.21. When Bob receives the package, he first applies his side of Figure 8.19
and then his side of Figure 8.20. It should be clear that this design achieves the
goal of providing confidentiality, sender authentication, and message integrity.
Note that, in this scheme, Alice uses public key cryptography twice: once
with her own private key and once with Bob’s public key. Similarly, Bob also
uses public key cryptography twice—once with his private key and once with
Alice’s public key.
The secure e-mail design outlined in Figure 8.21 probably provides satisfactory
security for most e-mail users for most occasions. But there is still one
important issue that remains to be addressed. The design in Figure 8.21 requires
Alice to obtain Bob’s public key, and requires Bob to obtain Alice’s public key.
The distribution of these public keys is a nontrivial problem. For example, Trudy
might masquerade as Bob and give Alice her own public key while saying that it
is Bob’s public key, enabling her to receive the message meant for Bob. As we
learned in Section 8.3, a popular approach for securely distributing public keys is
to certify the public keys using a CA.
708 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
H(.) KA
– (.) KA
+(.)
KA
– (H(m)) KA
– (H(m))
m
m
m
+ Internet –
Alice sends e-mail message m Bob receives e-mail message m
H(.)
Compare
Figure 8.20  Using hash functions and digital signatures to provide
sender authentication and message integrity
8.5 • SECURING E-MAIL 709
PHIL ZIMMERMANN AND PGP
Philip R. Zimmermann is the creator of Pretty Good Privacy (PGP). For that, he was
the target of a three-year criminal investigation because the government held that US
export restrictions for cryptographic software were violated when PGP spread all
around the world following its 1991 publication as freeware. After releasing PGP as
shareware, someone else put it on the Internet and foreign citizens downloaded it.
Cryptography programs in the United States are classified as munitions under federal
law and may not be exported.
Despite the lack of funding, the lack of any paid staff, and the lack of a company
to stand behind it, and despite government interventions, PGP nonetheless became
the most widely used e-mail encryption software in the world. Oddly enough, the US
government may have inadvertently contributed to PGP’s spread because of the
Zimmermann case.
The US government dropped the case in early 1996. The announcement was met
with celebration by Internet activists. The Zimmermann case had become the story of
an innocent person fighting for his rights against the abuses of big government. The
government’s giving in was welcome news, in part because of the campaign for
Internet censorship in Congress and the push by the FBI to allow increased government
snooping.
After the government dropped its case, Zimmermann founded PGP Inc., which
was acquired by Network Associates in December 1997. Zimmermann is now an
independent consultant in matters cryptographic.
CASE HISTORY
H(.) KA
– (.)
KS (.)
KS
KA
– (H(m))
m
m
+
+ to Internet
KB
+(.)
Figure 8.21  Alice uses symmetric key cyptography, public key
cryptography, a hash function, and a digital signature to
provide secrecy, sender authentication, and message integrity
8.5.2 PGP
Written by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is an e-mail
encryption scheme that has become a de facto standard. Its Web site serves more than
a million pages a month to users in 166 countries [PGPI 2012]. Versions of PGP are
available in the public domain; for example, you can find the PGP software for your
favorite platform as well as lots of interesting reading at the International PGP
Home Page [PGPI 2012]. (A particularly interesting essay by the author of PGP is
[Zimmermann 2012].) The PGP design is, in essence, the same as the design shown
in Figure 8.21. Depending on the version, the PGP software uses MD5 or SHA for
calculating the message digest; CAST, triple-DES, or IDEA for symmetric key
encryption; and RSA for the public key encryption.
When PGP is installed, the software creates a public key pair for the user. The
public key can be posted on the user’s Web site or placed in a public key server. The
private key is protected by the use of a password. The password has to be entered
every time the user accesses the private key. PGP gives the user the option of digitally
signing the message, encrypting the message, or both digitally signing and
encrypting. Figure 8.22 shows a PGP signed message. This message appears after
the MIME header. The encoded data in the message is KA
– (H(m)), that is, the digitally
signed message digest. As we discussed above, in order for Bob to verify the
integrity of the message, he needs to have access to Alice’s public key.
Figure 8.23 shows a secret PGP message. This message also appears after the
MIME header. Of course, the plaintext message is not included within the secret
e-mail message. When a sender (such as Alice) wants both confidentiality and
integrity, PGP contains a message like that of Figure 8.23 within the message of
Figure 8.22.
PGP also provides a mechanism for public key certification, but the mechanism
is quite different from the more conventional CA. PGP public keys are certified by a
web of trust. Alice herself can certify any key/username pair when she believes the
710 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Figure 8.22  A PGP signed message
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Bob:
Can I see you tonight?
Passionately yours, Alice
-----BEGIN PGP SIGNATURE-----
Version: PGP for Personal Privacy 5.0
Charset: noconv
yhHJRHhGJGhgg/12EpJ+lo8gE4vB3mqJhFEvZP9t6n7G6m5Gw2
-----END PGP SIGNATURE-----
pair really belong together. In addition, PGP permits Alice to say that she trusts
another user to vouch for the authenticity of more keys. Some PGP users sign each
other’s keys by holding key-signing parties. Users physically gather, exchange
public keys, and certify each other’s keys by signing them with their private keys.
8.6 Securing TCP Connections: SSL
In the previous section, we saw how cryptographic techniques can provide confidentiality,
data integrity, and end-point authentication to a specific application,
namely, e-mail. In this section, we’ll drop down a layer in the protocol stack and
examine how cryptography can enhance TCP with security services, including confidentiality,
data integrity, and end-point authentication. This enhanced version of
TCP is commonly known as Secure Sockets Layer (SSL). A slightly modified version
of SSL version 3, called Transport Layer Security (TLS), has been standardized
by the IETF [RFC 4346].
The SSL protocol was originally designed by Netscape, but the basic ideas
behind securing TCP had predated Netscape’s work (for example, see Woo [Woo
1994]). Since its inception, SSL has enjoyed broad deployment. SSL is supported
by all popular Web browsers and Web servers, and it is used by essentially all
Internet commerce sites (including Amazon, eBay, Yahoo!, MSN, and so on). Tens
of billions of dollars are spent over SSL every year. In fact, if you have ever purchased
anything over the Internet with your credit card, the communication
between your browser and the server for this purchase almost certainly went over
SSL. (You can identify that SSL is being used by your browser when the URL
begins with https: rather than http.)
To understand the need for SSL, let’s walk through a typical Internet commerce
scenario. Bob is surfing the Web and arrives at the Alice Incorporated site,
which is selling perfume. The Alice Incorporated site displays a form in which
Bob is supposed to enter the type of perfume and quantity desired, his address,
and his payment card number. Bob enters this information, clicks on Submit, and
8.6 • SECURING TCP CONNECTIONS: SSL 711
Figure 8.23  A secret PGP message
-----BEGIN PGP MESSAGE-----
Version: PGP for Personal Privacy 5.0
u2R4d+/jKmn8Bc5+hgDsqAewsDfrGdszX68liKm5F6Gc4sDfcXyt
RfdS10juHgbcfDssWe7/K=lKhnMikLo0+1/BvcX4t==Ujk9PbcD4
Thdf2awQfgHbnmKlok8iy6gThlp
-----END PGP MESSAGE
expects to receive (via ordinary postal mail) the purchased perfumes; he also
expects to receive a charge for his order in his next payment card statement. This
all sounds good, but if no security measures are taken, Bob could be in for a few
surprises.
• If no confidentiality (encryption) is used, an intruder could intercept Bob’s order
and obtain his payment card information. The intruder could then make purchases
at Bob’s expense.
• If no data integrity is used, an intruder could modify Bob’s order, having him
purchase ten times more bottles of perfume than desired.
• Finally, if no server authentication is used, a server could display Alice Incorporated’s
famous logo when in actuality the site maintained by Trudy, who is masquerading
as Alice Incorporated. After receiving Bob’s order, Trudy could take
Bob’s money and run. Or Trudy could carry out an identity theft by collecting
Bob’s name, address, and credit card number.
SSL addresses these issues by enhancing TCP with confidentiality, data integrity,
server authentication, and client authentication.
SSL is often used to provide security to transactions that take place over HTTP.
However, because SSL secures TCP, it can be employed by any application that
runs over TCP. SSL provides a simple Application Programmer Interface (API)
with sockets, which is similar and analogous to TCP’s API. When an application
wants to employ SSL, the application includes SSL classes/libraries. As shown in
Figure 8.24, although SSL technically resides in the application layer, from the
developer’s perspective it is a transport protocol that provides TCP’s services
enhanced with security services.
712 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Figure 8.24  Although SSL technically resides in the application layer,
from the developer’s perspective it is a transport-layer
protocol
TCP
SSL sublayer
IP
Application
Application
layer
TCP enhanced with SSL
SSL socket
TCP socket
TCP
IP
Application
TCP API
TCP socket
8.6.1 The Big Picture
We begin by describing a simplified version of SSL, one that will allow us to get a
big-picture understanding of the why and how of SSL. We will refer to this simplified
version of SSL as “almost-SSL.” After describing almost-SSL, in the next subsection
we’ll then describe the real SSL, filling in the details. Almost-SSL (and
SSL) has three phases: handshake, key derivation, and data transfer. We now
describe these three phases for a communication session between a client (Bob) and
a server (Alice), with Alice having a private/public key pair and a certificate that
binds her identity to her public key.
Handshake
During the handshake phase, Bob needs to (a) establish a TCP connection with
Alice, (b) verify that Alice is really Alice, and (c) send Alice a master secret key,
which will be used by both Alice and Bob to generate all the symmetric keys they
need for the SSL session. These three steps are shown in Figure 8.25. Note that once
the TCP connection is established, Bob sends Alice a hello message. Alice then
responds with her certificate, which contains her public key. As discussed in Section
8.3, because the certificate has been certified by a CA, Bob knows for sure that the
8.6 • SECURING TCP CONNECTIONS: SSL 713
TCP SYN
TCP/SYNACK
Decrypts EMS with
KA
– to get MS
EMS = KA
+ (MS)
TCP ACK
SSL hello
certificate
(b)
(a)
(c)
Create Master
Secret (MS)
Figure 8.25  The almost-SSL handshake, beginning with a TCP
connection
public key in the certificate belongs to Alice. Bob then generates a Master Secret
(MS) (which will only be used for this SSL session), encrypts the MS with Alice’s
public key to create the Encyrpted Master Secret (EMS), and sends the EMS to
Alice. Alice decrypts the EMS with her private key to get the MS. After this phase,
both Bob and Alice (and no one else) know the master secret for this SSL session.
Key Derivation
In principle, the MS, now shared by Bob and Alice, could be used as the symmetric
session key for all subsequent encryption and data integrity checking. It is, however,
generally considered safer for Alice and Bob to each use different cryptographic
keys, and also to use different keys for encryption and integrity checking. Thus, both
Alice and Bob use the MS to generate four keys:
• EB = session encryption key for data sent from Bob to Alice
• MB = session MAC key for data sent from Bob to Alice
• EA = session encryption key for data sent from Alice to Bob
• MA = session MAC key for data sent from Alice to Bob
Alice and Bob each generate the four keys from the MS. This could be done by simply
slicing the MS into four keys. (But in real SSL it is a little more complicated, as
we’ll see.) At the end of the key derivation phase, both Alice and Bob have all four
keys. The two encryption keys will be used to encrypt data; the two MAC keys will
be used to verify the integrity of the data.
Data Transfer
Now that Alice and Bob share the same four session keys (EB, MB, EA, and MA),
they can start to send secured data to each other over the TCP connection. Since
TCP is a byte-stream protocol, a natural approach would be for SSL to encrypt
application data on the fly and then pass the encrypted data on the fly to TCP. But if
we were to do this, where would we put the MAC for the integrity check? We certainly
do not want to wait until the end of the TCP session to verify the integrity of
all of Bob’s data that was sent over the entire session! To address this issue, SSL
breaks the data stream into records, appends a MAC to each record for integrity
checking, and then encrypts the record+MAC. To create the MAC, Bob inputs the
record data along with the key MB into a hash function, as discussed in Section 8.3.
To encrypt the package record+MAC, Bob uses his session encryption key EB. This
encrypted package is then passed to TCP for transport over the Internet.
Although this approach goes a long way, it still isn’t bullet-proof when it comes
to providing data integrity for the entire message stream. In particular, suppose Trudy
is a woman-in-the-middle and has the ability to insert, delete, and replace segments
714 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
in the stream of TCP segments sent between Alice and Bob. Trudy, for example,
could capture two segments sent by Bob, reverse the order of the segments, adjust
the TCP sequence numbers (which are not encrypted), and then send the two reverseordered
segments to Alice. Assuming that each TCP segment encapsulates exactly
one record, let’s now take a look at how Alice would process these segments.
1. TCP running in Alice would think everything is fine and pass the two records
to the SSL sublayer.
2. SSL in Alice would decrypt the two records.
3. SSL in Alice would use the MAC in each record to verify the data integrity of
the two records.
4. SSL would then pass the decrypted byte streams of the two records to the
application layer; but the complete byte stream received by Alice would not be
in the correct order due to reversal of the records!
You are encouraged to walk through similar scenarios for when Trudy removes segments
or when Trudy replays segments.
The solution to this problem, as you probably guessed, is to use sequence numbers.
SSL does this as follows. Bob maintains a sequence number counter, which
begins at zero and is incremented for each SSL record he sends. Bob doesn’t actually
include a sequence number in the record itself, but when he calculates the
MAC, he includes the sequence number in the MAC calculation. Thus, the MAC is
now a hash of the data plus the MAC key MB plus the current sequence number.
Alice tracks Bob’s sequence numbers, allowing her to verify the data integrity of a
record by including the appropriate sequence number in the MAC calculation. This
use of SSL sequence numbers prevents Trudy from carrying out a woman-in-themiddle
attack, such as reordering or replaying segments. (Why?)
SSL Record
The SSL record (as well as the almost-SSL record) is shown in Figure 8.26. The
record consists of a type field, version field, length field, data field, and MAC field.
Note that the first three fields are not encrypted. The type field indicates whether the
record is a handshake message or a message that contains application data. It is also
8.6 • SECURING TCP CONNECTIONS: SSL 715
Figure 8.26  Record format for SSL
Type Version Length Data MAC
Encrypted with EB
used to close the SSL connection, as discussed below. SSL at the receiving end uses
the length field to extract the SSL records out of the incoming TCP byte stream. The
version field is self-explanatory.
8.6.2 A More Complete Picture
The previous subsection covered the almost-SSL protocol; it served to give us a basic
understanding of the why and how of SSL. Now that we have a basic understanding
of SSL, we can dig a little deeper and examine the essentials of the actual SSL protocol.
In parallel to reading this description of the SSL protocol, you are encouraged to
complete the Wireshark SSL lab, available at the textbook’s companion Web site.
SSL Handshake
SSL does not mandate that Alice and Bob use a specific symmetric key algorithm, a
specific public-key algorithm, or a specific MAC. Instead, SSL allows Alice and
Bob to agree on the cryptographic algorithms at the beginning of the SSL session,
during the handshake phase. Additionally, during the handshake phase, Alice and
Bob send nonces to each other, which are used in the creation of the session keys
(EB, MB, EA, and MA). The steps of the real SSL handshake are as follows:
1. The client sends a list of cryptographic algorithms it supports, along with a
client nonce.
2. From the list, the server chooses a symmetric algorithm (for example, AES), a
public key algorithm (for example, RSA with a specific key length), and a
MAC algorithm. It sends back to the client its choices, as well as a certificate
and a server nonce.
3. The client verifies the certificate, extracts the server’s public key, generates a
Pre-Master Secret (PMS), encrypts the PMS with the server’s public key, and
sends the encrypted PMS to the server.
4. Using the same key derivation function (as specified by the SSL standard),
the client and server independently compute the Master Secret (MS) from
the PMS and nonces. The MS is then sliced up to generate the two encryption
and two MAC keys. Furthermore, when the chosen symmetric cipher employs
CBC (such as 3DES or AES), then two Initialization Vectors (IVs)—one for
each side of the connection—are also obtained from the MS. Henceforth, all
messages sent between client and server are encrypted and authenticated
(with the MAC).
5. The client sends a MAC of all the handshake messages.
6. The server sends a MAC of all the handshake messages.
The last two steps protect the handshake from tampering. To see this, observe
that in step 1, the client typically offers a list of algorithms—some strong, some
716 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
weak. This list of algorithms is sent in cleartext, since the encryption algorithms and
keys have not yet been agreed upon. Trudy, as a woman-in-the-middle, could delete
the stronger algorithms from the list, forcing the client to select a weak algorithm.
To prevent such a tampering attack, in step 5 the client sends a MAC of the concatenation
of all the handshake messages it sent and received. The server can compare
this MAC with the MAC of the handshake messages it received and sent. If there is
an inconsistency, the server can terminate the connection. Similarly, the server sends
a MAC of the handshake messages it has seen, allowing the client to check for
inconsistencies.
You may be wondering why there are nonces in steps 1 and 2. Don’t sequence
numbers suffice for preventing the segment replay attack? The answer is yes, but they
don’t alone prevent the “connection replay attack.” Consider the following connection
replay attack. Suppose Trudy sniffs all messages between Alice and Bob. The next
day, Trudy masquerades as Bob and sends to Alice exactly the same sequence of messages
that Bob sent to Alice on the previous day. If Alice doesn’t use nonces, she will
respond with exactly the same sequence of messages she sent the previous day. Alice
will not suspect any funny business, as each message she receives will pass the
integrity check. If Alice is an e-commerce server, she will think that Bob is placing a
second order (for exactly the same thing). On the other hand, by including a nonce in
the protocol, Alice will send different nonces for each TCP session, causing the
encryption keys to be different on the two days. Therefore, when Alice receives
played-back SSL records from Trudy, the records will fail the integrity checks, and the
bogus e-commerce transaction will not succeed. In summary, in SSL, nonces are used
to defend against the “connection replay attack” and sequence numbers are used to
defend against replaying individual packets during an ongoing session.
Connection Closure
At some point, either Bob or Alice will want to end the SSL session. One approach
would be to let Bob end the SSL session by simply terminating the underlying TCP
connection—that is, by having Bob send a TCP FIN segment to Alice. But such a
naive design sets the stage for the truncation attack whereby Trudy once again gets
in the middle of an ongoing SSL session and ends the session early with a TCP FIN.
If Trudy were to do this, Alice would think she received all of Bob’s data when actuality
she only received a portion of it. The solution to this problem is to indicate in
the type field whether the record serves to terminate the SSL session. (Although the
SSL type is sent in the clear, it is authenticated at the receiver using the record’s
MAC.) By including such a field, if Alice were to receive a TCP FIN before receiving
a closure SSL record, she would know that something funny was going on.
This completes our introduction to SSL. We’ve seen that it uses many of the
cryptography principles discussed in Sections 8.2 and 8.3. Readers who want to
explore SSL on yet a deeper level can read Rescorla’s highly readable book on SSL
[Rescorla 2001].
8.6 • SECURING TCP CONNECTIONS: SSL 717
8.7 Network-Layer Security: IPsec and
Virtual Private Networks
The IP security protocol, more commonly known as IPsec, provides security at the
network layer. IPsec secures IP datagrams between any two network-layer entities,
including hosts and routers. As we will soon describe, many institutions (corporations,
government branches, non-profit organizations, and so on) use IPsec to create
virtual private networks (VPNs) that run over the public Internet.
Before getting into the specifics of IPsec, let’s step back and consider what it
means to provide confidentiality at the network layer. With network-layer confidentiality
between a pair of network entities (for example, between two routers, between
two hosts, or between a router and a host), the sending entity encrypts the payloads
of all the datagrams it sends to the receiving entity. The encrypted payload could be a
TCP segment, a UDP segment, an ICMP message, and so on. If such a network-layer
service were in place, all data sent from one entity to the other—including e-mail,
Web pages, TCP handshake messages, and management messages (such as ICMP
and SNMP)—would be hidden from any third party that might be sniffing the network.
For this reason, network-layer security is said to provide “blanket coverage”.
In addition to confidentiality, a network-layer security protocol could potentially
provide other security services. For example, it could provide source authentication, so
that the receiving entity can verify the source of the secured datagram. Anetwork-layer
security protocol could provide data integrity, so that the receiving entity can check for
any tampering of the datagram that may have occurred while the datagram was in transit.
Anetwork-layer security service could also provide replay-attack prevention, meaning
that Bob could detect any duplicate datagrams that an attacker might insert. We will
soon see that IPsec indeed provides mechanisms for all these security services, that is,
for confidentiality, source authentication, data integrity, and replay-attack prevention.
8.7.1 IPsec and Virtual Private Networks (VPNs)
An institution that extends over multiple geographical regions often desires its own
IP network, so that its hosts and servers can send data to each other in a secure and
confidential manner. To achieve this goal, the institution could actually deploy a
stand-alone physical network—including routers, links, and a DNS infrastructure—that
is completely separate from the public Internet. Such a disjoint network, dedicated
to a particular institution, is called a private network. Not surprisingly, a private
network can be very costly, as the institution needs to purchase, install, and maintain
its own physical network infrastructure.
Instead of deploying and maintaining a private network, many institutions today
create VPNs over the existing public Internet. With a VPN, the institution’s interoffice
traffic is sent over the public Internet rather than over a physically independent
network. But to provide confidentiality, the inter-office traffic is encrypted before it
enters the public Internet. A simple example of a VPN is shown in Figure 8.27. Here
718 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
the institution consists of a headquarters, a branch office, and traveling salespersons
that typically access the Internet from their hotel rooms. (There is only one salesperson
shown in the figure.) In this VPN, whenever two hosts within headquarters send
IP datagrams to each other or whenever two hosts within the branch office want to
communicate, they use good-old vanilla IPv4 (that is, without IPsec services). However,
when two of the institution’s hosts communicate over a path that traverses the
public Internet, the traffic is encrypted before it enters the Internet.
To get a feel for how a VPN works, let’s walk through a simple example in the
context of Figure 8.27. When a host in headquarters sends an IP datagram to a salesperson
in a hotel, the gateway router in headquarters converts the vanilla IPv4 datagram
into an IPsec datagram and then forwards this IPsec datagram into the Internet.
This IPsec datagram actually has a traditional IPv4 header, so that the routers in the
public Internet process the datagram as if it were an ordinary IPv4 datagram—to
them, the datagram is a perfectly ordinary datagram. But, as shown Figure 8.27, the
payload of the IPsec datagram includes an IPsec header, which is used for IPsec processing;
furthermore, the payload of the IPsec datagram is encrypted. When the IPsec
datagram arrives at the salesperson’s laptop, the OS in the laptop decrypts the payload
(and provides other security services, such as verifying data integrity) and passes
the unencrypted payload to the upper-layer protocol (for example, to TCP or UDP).
8.7 • NETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS 719
Figure 8.27  Virtual Private Network (VPN)
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header Payload
IP
header Payload
Laptop w/IPsec
Router
w/IPv4 and
IPsec
Router
w/IPv4 and
IPsec
Branch Office
Headquarters
Salesperson
in Hotel
Public
Internet
We have just given a high-level overview of how an institution can employ
IPsec to create a VPN. To see the forest through the trees, we have brushed aside
many important details. Let’s now take a closer look.
8.7.2 The AH and ESP Protocols
IPsec is a rather complex animal—it is defined in more than a dozen RFCs. Two
important RFCs are RFC 4301, which describes the overall IP security architecture,
and RFC 6071, which provides an overview of the IPsec protocol suite. Our goal in
this textbook, as usual, is not simply to re-hash the dry and arcane RFCs, but instead
take a more operational and pedagogic approach to describing the protocols.
In the IPsec protocol suite, there are two principal protocols: the Authentication
Header (AH) protocol and the Encapsulation Security Payload (ESP) protocol.
When a source IPsec entity (typically a host or a router) sends secure datagrams to a
destination entity (also a host or a router), it does so with either the AH protocol or the
ESP protocol. The AH protocol provides source authentication and data integrity but
does not provide confidentiality. The ESP protocol provides source authentication,
data integrity, and confidentiality. Because confidentiality is often critical for VPNs
and other IPsec applications, the ESP protocol is much more widely used than the AH
protocol. In order to de-mystify IPsec and avoid much of its complication, we will
henceforth focus exclusively on the ESP protocol. Readers wanting to learn also about
the AH protocol are encouraged to explore the RFCs and other online resources.
8.7.3 Security Associations
IPsec datagrams are sent between pairs of network entities, such as between two hosts,
between two routers, or between a host and router. Before sending IPsec datagrams
from source entity to destination entity, the source and destination entities create a network-
layer logical connection. This logical connection is called a security association
(SA). An SA is a simplex logical connection; that is, it is unidirectional from source to
destination. If both entities want to send secure datagrams to each other, then two SAs
(that is, two logical connections) need to be established, one in each direction.
For example, consider once again the institutional VPN in Figure 8.27. This institution
consists of a headquarters office, a branch office and, say, n traveling salespersons.
For the sake of example, let’s suppose that there is bi-directional IPsec traffic
between headquarters and the branch office and bi-directional IPsec traffic between
headquarters and the salespersons. In this VPN, how many SAs are there? To answer
this question, note that there are two SAs between the headquarters gateway router and
the branch-office gateway router (one in each direction); for each salesperson’s laptop,
there are two SAs between the headquarters gateway router and the laptop (again, one
in each direction). So, in total, there are (2 + 2n) SAs. Keep in mind, however, that not
all traffic sent into the Internet by the gateway routers or by the laptops will be IPsec
secured. For example, a host in headquarters may want to access a Web server (such as
Amazon or Google) in the public Internet. Thus, the gateway router (and the laptops)
will emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams.
720 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Let’s now take a look “inside” an SA. To make the discussion tangible and concrete,
let’s do this in the context of an SA from router R1 to router R2 in Figure 8.28.
(You can think of Router R1 as the headquarters gateway router and Router R2 as
the branch office gateway router from Figure 8.27.) Router R1 will maintain state
information about this SA, which will include:
• A 32-bit identifier for the SA, called the Security Parameter Index (SPI)
• The origin interface of the SA (in this case 200.168.1.100) and the destination
interface of the SA (in this case 193.68.2.23)
• The type of encryption to be used (for example, 3DES with CBC)
• The encryption key
• The type of integrity check (for example, HMAC with MD5)
• The authentication key
Whenever router R1 needs to construct an IPsec datagram for forwarding over
this SA, it accesses this state information to determine how it should authenticate
and encrypt the datagram. Similarly, router R2 will maintain the same state information
for this SA and will use this information to authenticate and decrypt any
IPsec datagram that arrives from the SA.
An IPsec entity (router or host) often maintains state information for many SAs.
For example, in the VPN example in Figure 8.27 with n salespersons, the headquarters
gateway router maintains state information for (2 + 2n) SAs. An IPsec entity
stores the state information for all of its SAs in its Security Association Database
(SAD), which is a data structure in the entity’s OS kernel.
8.7.4 The IPsec Datagram
Having now described SAs, we can now describe the actual IPsec datagram. IPsec
has two different packet forms, one for the so-called tunnel mode and the other for
the so-called transport mode. The tunnel mode, being more appropriate for VPNs,
is more widely deployed than the transport mode. In order to further de-mystify
8.7 • NETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS 721
Figure 8.28  Security Association (SA) from R1 to R2
Internet
SA
R1
172.16.1/24
Headquarters Branch Office
200.168.1.100 193.68.2.23
172.16.2/24
R2
IPsec and avoid much of its complication, we henceforth focus exclusively on the
tunnel mode. Once you have a solid grip on the tunnel mode, you should be able to
easily learn about the transport mode on your own.
The packet format of the IPsec datagram is shown in Figure 8.29. You might
think that packet formats are boring and insipid, but we will soon see that the IPsec
datagram actually looks and tastes like a popular Tex-Mex delicacy! Let’s examine
the IPsec fields in the context of Figure 8.28. Suppose router R1 receives an ordinary
IPv4 datagram from host 172.16.1.17 (in the headquarters network) which is
destined to host 172.16.2.48 (in the branch-office network). Router R1 uses the following
recipe to convert this “original IPv4 datagram” into an IPsec datagram:
• Appends to the back of the original IPv4 datagram (which includes the original
header fields!) an “ESP trailer” field
• Encrypts the result using the algorithm and key specified by the SA
• Appends to the front of this encrypted quantity a field called “ESP header”; the
resulting package is called the “enchilada”
• Creates an authentication MAC over the whole enchilada using the algorithm
and key specified in the SA
• Appends the MAC to the back of the enchilada forming the payload
• Finally, creates a brand new IP header with all the classic IPv4 header fields
(together normally 20 bytes long), which it appends before the payload
Note that the resulting IPsec datagram is a bona fide IPv4 datagram, with the
traditional IPv4 header fields followed by a payload. But in this case, the payload
contains an ESP header, the original IP datagram, an ESP trailer, and an ESP authentication
field (with the original datagram and ESP trailer encrypted). The original IP
datagram has 172.16.1.17 for the source IP address and 172.16.2.48 for the destination
IP address. Because the IPsec datagram includes the original IP datagram, these
addresses are included (and encrypted) as part of the payload of the IPsec packet.
But what about the source and destination IP addresses that are in the new IP header,
that is, in the left-most header of the IPsec datagram? As you might expect, they are
set to the source and destination router interfaces at the two ends of the tunnels,
namely, 200.168.1.100 and 193.68.2.23. Also, the protocol number in this new IPv4
header field is not set to that of TCP, UDP, or SMTP, but instead to 50, designating
that this is an IPsec datagram using the ESP protocol.
After R1 sends the IPsec datagram into the public Internet, it will pass through
many routers before reaching R2. Each of these routers will process the datagram as
if it were an ordinary datagram—they are completely oblivious to the fact that the
datagram is carrying IPsec-encrypted data. For these public Internet routers, because
the destination IP address in the outer header is R2, the ultimate destination of the
datagram is R2.
Having walked through an example of how an IPsec datagram is constructed, let’s
now take a closer look at the ingredients in the enchilada. We see in Figure 8.29
722 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
that the ESP trailer consists of three fields: padding; pad length; and next header.
Recall that block ciphers require the message to be encrypted to be an integer multiple
of the block length. Padding (consisting of meaningless bytes) is used so that
when added to the original datagram (along with the pad length and next header
fields), the resulting “message” is an integer number of blocks. The pad-length field
indicates to the receiving entity how much padding was inserted (and thus needs to
be removed). The next header identifies the type (e.g., UDP) of data contained in the
payload-data field. The payload data (typically the original IP datagram) and the
ESP trailer are concatenated and then encrypted.
Appended to the front of this encrypted unit is the ESP header, which is sent in
the clear and consists of two fields: the SPI and the sequence number field. The SPI
indicates to the receiving entity the SA to which the datagram belongs; the receiving
entity can then index its SAD with the SPI to determine the appropriate authentication/
decryption algorithms and keys. The sequence number field is used to defend
against replay attacks.
The sending entity also appends an authentication MAC. As stated earlier, the
sending entity calculates a MAC over the whole enchilada (consisting of the ESP
header, the original IP datagram, and the ESP trailer—with the datagram and trailer
being encrypted). Recall that to calculate a MAC, the sender appends a secret MAC
key to the enchilada and then calculates a fixed-length hash of the result.
When R2 receives the IPsec datagram, R2 observes that the destination IP
address of the datagram is R2 itself. R2 therefore processes the datagram. Because
the protocol field (in the left-most IP header) is 50, R2 sees that it should apply
IPsec ESP processing to the datagram. First, peering into the enchilada, R2 uses the
SPI to determine to which SA the datagram belongs. Second, it calculates the MAC
of the enchilada and verifies that the MAC is consistent with the value in the ESP
MAC field. If it is, it knows that the enchilada comes from R1 and has not been tampered
with. Third, it checks the sequence-number field to verify that the datagram is
fresh (and not a replayed datagram). Fourth, it decrypts the encrypted unit using the
8.7 • NETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS 723
Figure 8.29  IPsec datagram format
New IP
header
ESP
header
ESP
trailer
ESP
MAC
Original
IP header
Original IP
datagram payload
Encrypted
“Enchilada” authenticated
Pad
Padding length
Next
SPI Seq # header
decryption algorithm and key associated with the SA. Fifth, it removes padding and
extracts the original, vanilla IP datagram. And finally, sixth, it forwards the original
datagram into the branch office network towards its ultimate destination. Whew,
what a complicated recipe, huh? Well no one ever said that preparing and unraveling
an enchilada was easy!
There is actually another important subtlety that needs to be addressed. It centers
on the following question: When R1 receives an (unsecured) datagram from a
host in the headquarters network, and that datagram is destined to some destination
IP address outside of headquarters, how does R1 know whether it should be
converted to an IPsec datagram? And if it is to be processed by IPsec, how does
R1 know which SA (of many SAs in its SAD) should be used to construct the
IPsec datagram? The problem is solved as follows. Along with a SAD, the IPsec
entity also maintains another data structure called the Security Policy Database
(SPD). The SPD indicates what types of datagrams (as a function of source IP
address, destination IP address, and protocol type) are to be IPsec processed; and
for those that are to be IPsec processed, which SA should be used. In a sense, the
information in a SPD indicates “what” to do with an arriving datagram; the information
in the SAD indicates “how” to do it.
Summary of IPsec Services
So what services does IPsec provide, exactly? Let us examine these services from
the perspective of an attacker, say Trudy, who is a woman-in-the-middle, sitting
somewhere on the path between R1 and R2 in Figure 8.28. Assume throughout this
discussion that Trudy does not know the authentication and encryption keys used by
the SA. What can and cannot Trudy do? First, Trudy cannot see the original datagram.
If fact, not only is the data in the original datagram hidden from Trudy, but so
is the protocol number, the source IP address, and the destination IP address. For
datagrams sent over the SA, Trudy only knows that the datagram originated from
some host in 172.16.1.0/24 and is destined to some host in 172.16.2.0/24. She does
not know if it is carrying TCP, UDP, or ICMP data; she does not know if it is carrying
HTTP, SMTP, or some other type of application data. This confidentiality thus
goes a lot farther than SSL. Second, suppose Trudy tries to tamper with a datagram
in the SA by flipping some of its bits. When this tampered datagram arrives at R2, it
will fail the integrity check (using the MAC), thwarting Trudy’s vicious attempts
once again. Third, suppose Trudy tries to masquerade as R1, creating a IPsec datagram
with source 200.168.1.100 and destination 193.68.2.23. Trudy’s attack will be
futile, as this datagram will again fail the integrity check at R2. Finally, because
IPsec includes sequence numbers, Trudy will not be able create a successful replay
attack. In summary, as claimed at the beginning of this section, IPsec provides—
between any pair of devices that process packets through the network layer—
confidentiality, source authentication, data integrity, and replay-attack prevention.
724 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
8.7.5 IKE: Key Management in IPsec
When a VPN has a small number of end points (for example, just two routers as in
Figure 8.28), the network administrator can manually enter the SA information
(encryption/authentication algorithms and keys, and the SPIs) into the SADs of the
endpoints. Such “manual keying” is clearly impractical for a large VPN, which may
consist of hundreds or even thousands of IPsec routers and hosts. Large, geographically
distributed deployments require an automated mechanism for creating the
SAs. IPsec does this with the Internet Key Exchange (IKE) protocol, specified in
RFC 5996.
IKE has some similarities with the handshake in SSL (see Section 8.6). Each
IPsec entity has a certificate, which includes the entity’s public key. As with SSL, the
IKE protocol has the two entities exchange certificates, negotiate authentication and
encryption algorithms, and securely exchange key material for creating session keys
in the IPsec SAs. Unlike SSL, IKE employs two phases to carry out these tasks.
Let’s investigate these two phases in the context of two routers, R1 and R2, in
Figure 8.28. The first phase consists of two exchanges of message pairs between R1
and R2:
• During the first exchange of messages, the two sides use Diffie-Hellman (see
Homework Problems) to create a bi-directional IKE SA between the routers. To
keep us all confused, this bi-directional IKE SA is entirely different from the
IPsec SAs discussed in Sections 8.6.3 and 8.6.4. The IKE SA provides an authenticated
and encrypted channel between the two routers. During this first message-
pair exchange, keys are established for encryption and authentication for
the IKE SA. Also established is a master secret that will be used to compute
IPSec SA keys later in phase 2. Observe that during this first step, RSA public
and private keys are not used. In particular, neither R1 nor R2 reveals its identity
by signing a message with its private key.
• During the second exchange of messages, both sides reveal their identity to each
other by signing their messages. However, the identities are not revealed to a
passive sniffer, since the messages are sent over the secured IKE SA channel.
Also during this phase, the two sides negotiate the IPsec encryption and authentication
algorithms to be employed by the IPsec SAs.
In phase 2 of IKE, the two sides create an SA in each direction. At the end of
phase 2, the encryption and authentication session keys are established on both sides
for the two SAs. The two sides can then use the SAs to send secured datagrams, as
described in Sections 8.7.3 and 8.7.4. The primary motivation for having two phases
in IKE is computational cost—since the second phase doesn’t involve any publickey
cryptography, IKE can generate a large number of SAs between the two IPsec
entities with relatively little computational cost.
8.7 • NETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS 725
8.8 Securing Wireless LANs
Security is a particularly important concern in wireless networks, where radio waves
carrying frames can propagate far beyond the building containing the wireless base
station and hosts. In this section we present a brief introduction to wireless security.
For a more in-depth treatment, see the highly readable book by Edney and Arbaugh
[Edney 2003].
The issue of security in 802.11 has attracted considerable attention in both technical
circles and in the media. While there has been considerable discussion, there has
been little debate—there seems to be universal agreement that the original 802.11
specification contains a number of serious security flaws. Indeed, public domain software
can now be downloaded that exploits these holes, making those who use the
vanilla 802.11 security mechanisms as open to security attacks as users who use no
security features at all.
In the following section, we discuss the security mechanisms initially standardized
in the 802.11 specification, known collectively as Wired Equivalent Privacy
(WEP). As the name suggests, WEP is meant to provide a level of security similar
to that found in wired networks. We’ll then discuss a few of the security holes in
WEP and discuss the 802.11i standard, a fundamentally more secure version of
802.11 adopted in 2004.
8.8.1 Wired Equivalent Privacy (WEP)
The IEEE 802.11 WEP protocol was designed in 1999 to provide authentication and
data encryption between a host and a wireless access point (that is, base station)
using a symmetric shared key approach. WEP does not specify a key management
algorithm, so it is assumed that the host and wireless access point have somehow
agreed on the key via an out-of-band method. Authentication is carried out as follows:
1. A wireless host requests authentication by an access point.
2. The access point responds to the authentication request with a 128-byte nonce
value.
3. The wireless host encrypts the nonce using the symmetric key that it shares
with the access point.
4. The access point decrypts the host-encrypted nonce.
If the decrypted nonce matches the nonce value originally sent to the host, then the
host is authenticated by the access point.
The WEP data encryption algorithm is illustrated in Figure 8.30. A secret 40-bit
symmetric key, KS, is assumed to be known by both a host and the access point. In
addition, a 24-bit Initialization Vector (IV) is appended to the 40-bit key to create a
64-bit key that will be used to encrypt a single frame. The IV will change from one
726 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
frame to another, and hence each frame will be encrypted with a different 64-bit key.
Encryption is performed as follows. First a 4-byte CRC value (see Section 5.2) is
computed for the data payload. The payload and the four CRC bytes are then
encrypted using the RC4 stream cipher. We will not cover the details of RC4 here
(see [Schneier 1995] and [Edney 2003] for details). For our purposes, it is enough to
know that when presented with a key value (in this case, the 64-bit (KS, IV) key), the
RC4 algorithm produces a stream of key values, k1
IV, k2
IV, k3
IV, . . . that are used to
encrypt the data and CRC value in a frame. For practical purposes, we can think of
these operations being performed a byte at a time. Encryption is performed by
XOR-ing the ith byte of data, di, with the ith key, ki
IV, in the stream of key values
generated by the (KS,IV) pair to produce the ith byte of ciphertext, ci:
ci = di
 ki
IV
The IV value changes from one frame to the next and is included in plaintext in
the header of each WEP-encrypted 802.11 frame, as shown in Figure 8.30. The
receiver takes the secret 40-bit symmetric key that it shares with the sender, appends
the IV, and uses the resulting 64-bit key (which is identical to the key used by the
sender to perform encryption) to decrypt the frame:
di = ci
 ki
IV
Proper use of the RC4 algorithm requires that the same 64-bit key value never
be used more than once. Recall that the WEP key changes on a frame-by-frame
basis. For a given KS (which changes rarely, if ever), this means that there are only
224 unique keys. If these keys are chosen randomly, we can show [Walker 2000;
Edney 2003] that the probability of having chosen the same IV value (and hence
used the same 64-bit key) is more than 99 percent after only 12,000 frames. With 1
Kbyte frame sizes and a data transmission rate of 11 Mbps, only a few seconds are
8.8 • SECURING WIRELESS LANS 727
Figure 8.30  802.11 WEP protocol
Key sequence generator
(for given Ks, IV)
k1
IV
d1
c1
k2
IV k3
IV kN
IV IV kN+1
IV kN+4
Ks: 40-bit secret symmetric
Plaintext frame data plus CRC
IV (per frame)
802.11
header IV
WEP-encrypted data
plus CRC
d2
c2
d3
c3
dN
cN
CRC1
cN+1 cN+4
CRC4
needed before 12,000 frames are transmitted. Furthermore, since the IV is transmitted
in plaintext in the frame, an eavesdropper will know whenever a duplicate IV
value is used.
To see one of the several problems that occur when a duplicate key is used, consider
the following chosen-plaintext attack taken by Trudy against Alice. Suppose
that Trudy (possibly using IP spoofing) sends a request (for example, an HTTP or
FTP request) to Alice to transmit a file with known content, d1, d2, d3, d4,. . . . Trudy
also observes the encrypted data c1, c2, c3, c4. . . . Since di = ci
 ki
IV, if we XOR ci
with each side of this equality we have
di
 ci = ki
IV
With this relationship, Trudy can use the known values of di and ci to compute ki
IV.
The next time Trudy sees the same value of IV being used, she will know the key
sequence k1
IV, k2
IV, k3
IV, . . . and will thus be able to decrypt the encrypted message.
There are several additional security concerns with WEP as well. [Fluhrer
2001] described an attack exploiting a known weakness in RC4 when certain weak
keys are chosen. [Stubblefield 2002] discusses efficient ways to implement and
exploit this attack. Another concern with WEP involves the CRC bits shown in Figure
8.30 and transmitted in the 802.11 frame to detect altered bits in the payload.
However, an attacker who changes the encrypted content (e.g., substituting gibberish
for the original encrypted data), computes a CRC over the substituted gibberish,
and places the CRC into a WEP frame can produce an 802.11 frame that will be
accepted by the receiver. What is needed here are message integrity techniques such
as those we studied in Section 8.3 to detect content tampering or substitution. For
more details of WEP security, see [Edney 2003; Walker 2000; Weatherspoon 2000]
and the references therein.
8.8.2 IEEE 802.11i
Soon after the 1999 release of IEEE 802.11, work began on developing a new and
improved version of 802.11 with stronger security mechanisms. The new standard,
known as 802.11i, underwent final ratification in 2004. As we’ll see, while WEP
provided relatively weak encryption, only a single way to perform authentication,
and no key distribution mechanisms, IEEE 802.11i provides for much stronger
forms of encryption, an extensible set of authentication mechanisms, and a key distribution
mechanism. In the following, we present an overview of 802.11i; an excellent
(streaming audio) technical overview of 802.11i is [TechOnline 2012].
Figure 8.31 overviews the 802.11i framework. In addition to the wireless client
and access point, 802.11i defines an authentication server with which the AP can
communicate. Separating the authentication server from the AP allows one authentication
server to serve many APs, centralizing the (often sensitive) decisions
728 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
regarding authentication and access within the single server, and keeping AP costs
and complexity low. 802.11i operates in four phases:
1. Discovery. In the discovery phase, the AP advertises its presence and the forms
of authentication and encryption that can be provided to the wireless client
node. The client then requests the specific forms of authentication and encryption
that it desires. Although the client and AP are already exchanging messages,
the client has not yet been authenticated nor does it have an encryption
key, and so several more steps will be required before the client can communicate
with an arbitrary remote host over the wireless channel.
2. Mutual authentication and Master Key (MK) generation. Authentication takes
place between the wireless client and the authentication server. In this phase,
the access point acts essentially as a relay, forwarding messages between the
client and the authentication server. The Extensible Authentication Protocol
(EAP) [RFC 3748] defines the end-to-end message formats used in a simple
request/response mode of interaction between the client and authentication
server. As shown in Figure 8.32 EAP messages are encapsulated using
EAPoL (EAP over LAN, [IEEE 802.1X]) and sent over the 802.11 wireless
link. These EAP messages are then decapsulated at the access point, and then
8.8 • SECURING WIRELESS LANS 729
Figure 8.31  802.11i: four phases of operation
STA:
client station
AP:
access point
Wired
network
AS:
authentication
server
1
Discovery of
security capabilities
4
STA, AP use PMK to derive
Temporal Key (TK) used for
message encryption, integrity
AS derives same PMK,
sends to AP
STA derives Pairwise
Master Key (PMK)
2
3 3
STA and AS mutually authenticate, together generate
Master Key (MK). AP serves as “pass through”
re-encapsulated using the RADIUS protocol for transmission over UDP/IP to
the authentication server. While the RADIUS server and protocol [RFC 2865]
are not required by the 802.11i protocol, they are de facto standard components
for 802.11i. The recently standardized DIAMETER protocol [RFC
3588] is likely to replace RADIUS in the near future.
With EAP, the authentication server can choose one of a number of ways to
perform authentication. While 802.11i does not mandate a particular authentication
method, the EAP-TLS authentication scheme [RFC 5216] is often used.
EAP-TLS uses public key techniques (including nonce encryption and message
digests) similar to those we studied in Section 8.3 to allow the client and the
authentication server to mutually authenticate each other, and to derive a
Master Key (MK) that is known to both parties.
3. Pairwise Master Key (PMK) generation. The MK is a shared secret known
only to the client and the authentication server, which they each use to generate
a second key, the Pairwise Master Key (PMK). The authentication server then
sends the PMK to the AP. This is where we wanted to be! The client and AP
now have a shared key (recall that in WEP, the problem of key distribution was
not addressed at all) and have mutually authenticated each other. They’re just
about ready to get down to business.
4. Temporal Key (TK) generation.With the PMK, the wireless client and AP can
now generate additional keys that will be used for communication. Of particular
interest is the Temporal Key (TK), which will be used to perform the link-level
encryption of data sent over the wireless link and to an arbitrary remote host.
730 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Figure 8.32  EAP is an end-to-end protocol. EAP messages are encapsulated
using EAPoL over the wireless link between the client
and the access point, and using RADIUS over UDP/IP
between the access point and the authentication server
STA:
client station
AP:
access point
Wired
network
AS:
authentication
server
EAP TLS
EAP
EAP over LAN (EAPoL) RADIUS
IEEE 802.11 UDP/IP
802.11i provides several forms of encryption, including an AES-based encryption
scheme and a strengthened version of WEP encryption.
8.9 Operational Security: Firewalls and
Intrusion Detection Systems
We’ve seen throughout this chapter that the Internet is not a very safe place—bad
guys are out there, wreaking all sorts of havoc. Given the hostile nature of the
Internet, let’s now consider an organization’s network and the network administrator
who administers it. From a network administrator’s point of view, the world
divides quite neatly into two camps—the good guys (who belong to the organization’s
network, and who should be able to access resources inside the organization’s
network in a relatively unconstrained manner) and the bad guys (everyone
else, whose access to network resources must be carefully scrutinized). In many
organizations, ranging from medieval castles to modern corporate office buildings,
there is a single point of entry/exit where both good guys and bad guys entering
and leaving the organization are security-checked. In a castle, this was done at a
gate at one end of the drawbridge; in a corporate building, this is done at the security
desk. In a computer network, when traffic entering/leaving a network is security-
checked, logged, dropped, or forwarded, it is done by operational devices
known as firewalls, intrusion detection systems (IDSs), and intrusion prevention
systems (IPSs).
8.9.1 Firewalls
A firewall is a combination of hardware and software that isolates an organization’s
internal network from the Internet at large, allowing some packets to pass and blocking
others. A firewall allows a network administrator to control access between the
outside world and resources within the administered network by managing the traffic
flow to and from these resources. A firewall has three goals:
• All traffic from outside to inside, and vice versa, passes through the firewall.
Figure 8.33 shows a firewall, sitting squarely at the boundary between the administered
network and the rest of the Internet. While large organizations may use
multiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a
firewall at a single access point to the network, as shown in Figure 8.33, makes
it easier to manage and enforce a security-access policy.
• Only authorized traffic, as defined by the local security policy, will be allowed to
pass. With all traffic entering and leaving the institutional network passing
through the firewall, the firewall can restrict access to authorized traffic.
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 731
• The firewall itself is immune to penetration. The firewall itself is a device connected
to the network. If not designed or installed properly, it can be compromised,
in which case it provides only a false sense of security (which is worse
than no firewall at all!).
Cisco and Check Point are two of the leading firewall vendors today. You can also
easily create a firewall (packet filter) from a Linux box using iptables (publicdomain
software that is normally shipped with Linux).
Firewalls can be classified in three categories: traditional packet filters,
stateful filters, and application gateways. We’ll cover each of these in turn in the
following subsections.
Traditional Packet Filters
As shown in Figure 8.33, an organization typically has a gateway router connecting
its internal network to its ISP (and hence to the larger public Internet). All traffic leaving
and entering the internal network passes through this router, and it is at this router
where packet filtering occurs. A packet filter examines each datagram in isolation,
determining whether the datagram should be allowed to pass or should be dropped
based on administrator-specific rules. Filtering decisions are typically based on:
732 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Figure 8.33  Firewall placement between the administered network and
the outside world
Administered
network
Firewall
Public
Internet
• IP source or destination address
• Protocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on
• TCP or UDP source and destination port
• TCP flag bits: SYN, ACK, and so on
• ICMP message type
• Different rules for datagrams leaving and entering the network
• Different rules for the different router interfaces
Anetwork administrator configures the firewall based on the policy of the organization.
The policy may take user productivity and bandwidth usage into account as
well as the security concerns of an organization. Table 8.5 lists a number of possible
polices an organization may have, and how they would be addressed with a packet
filter. For example, if the organization doesn’t want any incoming TCP connections
except those for its public Web server, it can block all incoming TCP SYN segments
except TCP SYN segments with destination port 80 and the destination IP address
corresponding to the Web server. If the organization doesn’t want its users to monopolize
access bandwidth with Internet radio applications, it can block all not-critical
UDP traffic (since Internet radio is often sent over UDP). If the organization doesn’t
want its internal network to be mapped (tracerouted) by an outsider, it can block all
ICMP TTL expired messages leaving the organization’s network.
Afiltering policy can be based on a combination of addresses and port numbers.
For example, a filtering router could forward all Telnet datagrams (those with a
port number of 23) except those going to and coming from a list of specific IP
addresses. This policy permits Telnet connections to and from hosts on the allowed
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 733
Policy Firewall Setting
No outside Web access. Drop all outgoing packets to any IP address, port 80
Table 8.5  Policies and corresponding filtering rules for an organization’s
network 130.27/16 with Web server at 130.207.244.203
No incoming TCP connections, except
those for organization’s public Web server only.
Drop all incoming TCP SYN packets to any IP except
130.207.244.203, port 80
Prevent Web-radios from eating up the
available bandwidth.
Drop all incoming UDP packets—except DNS packets.
Prevent your network from being used
for a smurf DoS attack.
Drop all ICMP ping packets going to a “broadcast”
address (eg 130.207.255.255).
Prevent your network from being tracerouted Drop all outgoing ICMP TTL expired traffic
list. Unfortunately, basing the policy on external addresses provides no protection
against datagrams that have had their source addresses spoofed.
Filtering can also be based on whether or not the TCP ACK bit is set. This trick
is quite useful if an organization wants to let its internal clients connect to external
servers but wants to prevent external clients from connecting to internal servers.
Recall from Section 3.5 that the first segment in every TCP connection has the ACK
bit set to 0, whereas all the other segments in the connection have the ACK bit set to
1. Thus, if an organization wants to prevent external clients from initiating connections
to internal servers, it simply filters all incoming segments with the ACK bit set
to 0. This policy kills all TCP connections originating from the outside, but permits
connections originating internally.
Firewall rules are implemented in routers with access control lists, with each
router interface having its own list. An example of an access control list for an
organization 222.22/16 is shown in Table 8.6. This access control list is for an
interface that connects the router to the organization’s external ISPs. Rules are
applied to each datagram that passes through the interface from top to bottom. The
first two rules together allow internal users to surf the Web: The first rule allows any
TCP packet with destination port 80 to leave the organization’s network; the second
rule allows any TCP packet with source port 80 and the ACK bit set to enter
the organization’s network. Note that if an external source attempts to establish a
TCP connection with an internal host, the connection will be blocked, even if the
source or destination port is 80. The second two rules together allow DNS packets
to enter and leave the organization’s network. In summary, this rather restrictive
access control list blocks all traffic except Web traffic initiated from within the
organization and DNS traffic. [CERT Filtering 2012] provides a list of recommended
port/protocol packet filterings to avoid a number of well-known security
holes in existing network applications.
734 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
action source address dest address protocol source port dest port flag bit
Table 8.6  An access control list for a router interface
allow 222.22/16 outside of
222.22/16
TCP > 1023 80 any
allow outside of
222.22/16
222.22/16 TCP 80 > 1023 ACK
allow 222.22/16 outside of
222.22/16
UDP > 1023 53 —
allow outside of
222.22/16
222.22/16 UDP 53 > 1023 —
deny all all all all all all
Stateful Packet Filters
In a traditional packet filter, filtering decisions are made on each packet in isolation.
Stateful filters actually track TCP connections, and use this knowledge to make filtering
decisions.
To understand stateful filters, let’s reexamine the access control list in Table 8.6.
Although rather restrictive, the access control list in Table 8.6 nevertheless allows
any packet arriving from the outside with ACK = 1 and source port 80 to get through
the filter. Such packets could be used by attackers in attempts to crash internal systems
with malformed packets, carry out denial-of-service attacks, or map the internal
network. The naive solution is to block TCP ACK packets as well, but such an
approach would prevent the organization’s internal users from surfing the Web.
Stateful filters solve this problem by tracking all ongoing TCP connections in a
connection table. This is possible because the firewall can observe the beginning of
a new connection by observing a three-way handshake (SYN, SYNACK, and
ACK); and it can observe the end of a connection when it sees a FIN packet for the
connection. The firewall can also (conservatively) assume that the connection is
over when it hasn’t seen any activity over the connection for, say, 60 seconds. An
example connection table for a firewall is shown in Table 8.7. This connection table
indicates that there are currently three ongoing TCP connections, all of which have
been initiated from within the organization. Additionally, the stateful filter includes
a new column, “check connection,” in its access control list, as shown in Table 8.8.
Note that Table 8.8 is identical to the access control list in Table 8.6, except now it
indicates that the connection should be checked for two of the rules.
Let’s walk through some examples to see how the connection table and the
extended access control list work hand-in-hand. Suppose an attacker attempts to
send a malformed packet into the organization’s network by sending a datagram
with TCP source port 80 and with the ACK flag set. Further suppose that this packet
has source port number 12543 and source IP address 150.23.23.155. When this
packet reaches the firewall, the firewall checks the access control list in Table 8.7,
which indicates that the connection table must also be checked before permitting
this packet to enter the organization’s network. The firewall duly checks the connection
table, sees that this packet is not part of an ongoing TCP connection, and rejects
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 735
source address dest address source port dest port
222.22.1.7 37.96.87.123 12699 80
222.22.93.2 199.1.205.23 37654 80
222.22.65.143 203.77.240.43 48712 80
Table 8.7  Connection table for stateful filter
the packet. As a second example, suppose that an internal user wants to surf an
external Web site. Because this user first sends a TCP SYN segment, the user’s TCP
connection gets recorded in the connection table. When the Web server sends back
packets (with the ACK bit necessarily set), the firewall checks the table and sees that
a corresponding connection is in progress. The firewall will thus let these packets
pass, thereby not interfering with the internal user’s Web surfing activity.
Application Gateway
In the examples above, we have seen that packet-level filtering allows an organization
to perform coarse-grain filtering on the basis of the contents of IP and
TCP/UDP headers, including IP addresses, port numbers, and acknowledgment bits.
But what if an organization wants to provide a Telnet service to a restricted set of
internal users (as opposed to IP addresses)? And what if the organization wants such
privileged users to authenticate themselves first before being allowed to create Telnet
sessions to the outside world? Such tasks are beyond the capabilities of traditional
and stateful filters. Indeed, information about the identity of the internal users is
application-layer data and is not included in the IP/TCP/UDP headers.
To have finer-level security, firewalls must combine packet filters with application
gateways. Application gateways look beyond the IP/TCP/UDP headers and
make policy decisions based on application data. An application gateway is an
application-specific server through which all application data (inbound and outbound)
must pass. Multiple application gateways can run on the same host, but each
gateway is a separate server with its own processes.
736 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Table 8.8  Access control list for stateful filter
action source
address
dest
address
protocol source port dest port flag bit
allow 222.22/16 outside of
222.22/16
TCP >1023 80 any
allow outside of
222.22/16
222.22/16 TCP 80 >1023 ACK
allow 222.22/16 outside of
222.22/16
UDP >1023 53 —
allow outside of
222.22/16
222.22/16 UDP 53 >1023 —
deny all all all all all all
check
conxion
X
X
To get some insight into application gateways, let’s design a firewall that allows
only a restricted set of internal users to Telnet outside and prevents all external
clients from Telneting inside. Such a policy can be accomplished by implementing a
combination of a packet filter (in a router) and a Telnet application gateway, as
shown in Figure 8.34. The router’s filter is configured to block all Telnet connections
except those that originate from the IP address of the application gateway.
Such a filter configuration forces all outbound Telnet connections to pass through
the application gateway. Consider now an internal user who wants to Telnet to the
outside world. The user must first set up a Telnet session with the application gateway.
An application running in the gateway, which listens for incoming Telnet sessions,
prompts the user for a user ID and password. When the user supplies this
information, the application gateway checks to see if the user has permission to
Telnet to the outside world. If not, the Telnet connection from the internal user to the
gateway is terminated by the gateway. If the user has permission, then the gateway
(1) prompts the user for the host name of the external host to which the user wants
to connect, (2) sets up a Telnet session between the gateway and the external host,
and (3) relays to the external host all data arriving from the user, and relays to the
user all data arriving from the external host. Thus, the Telnet application gateway
not only performs user authorization but also acts as a Telnet server and a Telnet
client, relaying information between the user and the remote Telnet server. Note that
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 737
Figure 8.34  Firewall consisting of an application gateway and a filter
Application
gateway
Host-to-gateway
Telnet session
Gateway-to-remote
host Telnet session
Router
and filter
the filter will permit step 2 because the gateway initiates the Telnet connection to
the outside world.
738 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
ANONYMITY AND PRIVACY
Suppose you want to visit a controversial Web site (for example, a political activist
site) and you (1) don’t want to reveal your IP address to the Web site, (2) don’t want
your local ISP (which may be your home or office ISP) to know that you are visiting
the site, and (3) you don’t want your local ISP to see the data you are exchanging
with the site. If you use the traditional approach of connecting directly to the Web
site without any encryption, you fail on all three counts. Even if you use SSL, you fail
on the first two counts: Your source IP address is presented to the Web site in every
datagram you send; and the destination address of every packet you send can easily
be sniffed by your local ISP.
To obtain privacy and anonymity, you can instead use a combination of a trusted
proxy server and SSL, as shown in Figure 8.35. With this approach, you first make
an SSL connection to the trusted proxy. You then send, into this SSL connection, an
HTTP request for a page at the desired site. When the proxy receives the SSL-encrypted
HTTP request, it decrypts the request and forwards the cleartext HTTP request to
the Web site. The Web site then responds to the proxy, which in turn forwards the
response to you over SSL. Because the Web site only sees the IP address of the
proxy, and not of your client’s address, you are indeed obtaining anonymous access
to the Web site. And because all traffic between you and the proxy is encrypted,
your local ISP cannot invade your privacy by logging the site you visited or recording
the data you are exchanging. Many companies today (such as proxify.com) make
available such proxy services.
Of course, in this solution, your proxy knows everything: It knows your IP address
and the IP address of the site you’re surfing; and it can see all the traffic in cleartext
exchanged between you and the Web site. Such a solution, therefore, is only as
good as the trustworthiness of the proxy. A more robust approach, taken by the TOR
anonymizing and privacy service, is to route your traffic through a series of noncolluding
proxy servers [TOR 2012]. In particular, TOR allows independent individuals
to contribute proxies to its proxy pool. When a user connects to a server using
TOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and
routes all traffic between client and server over the chain. In this manner, assuming
the proxies do not collude, no one knows that communication took place between
your IP address and the target Web site. Furthermore, although cleartext is sent
between the last proxy and the server, the last proxy doesn’t know what IP address is
sending and receiving the cleartext.
CASE HISTORY
Internal networks often have multiple application gateways, for example, gateways
for Telnet, HTTP, FTP, and e-mail. In fact, an organization’s mail server (see
Section 2.4) and Web cache are application gateways.
Application gateways do not come without their disadvantages. First, a different
application gateway is needed for each application. Second, there is a performance
penalty to be paid, since all data will be relayed via the gateway. This becomes
a concern particularly when multiple users or applications are using the same gateway
machine. Finally, the client software must know how to contact the gateway
when the user makes a request, and must know how to tell the application gateway
what external server to connect to.
8.9.2 Intrusion Detection Systems
We’ve just seen that a packet filter (traditional and stateful) inspects IP, TCP, UDP,
and ICMP header fields when deciding which packets to let pass through the firewall.
However, to detect many attack types, we need to perform deep packet inspection,
that is, look beyond the header fields and into the actual application data that the
packets carry. As we saw in Section 8.9.1, application gateways often do deep packet
inspection. But an application gateway only does this for a specific application.
Clearly, there is a niche for yet another device—a device that not only examines
the headers of all packets passing through it (like a packet filter), but also performs
deep packet inspection (unlike a packet filter). When such a device observes a
suspicious packet, or a suspicious series of packets, it could prevent those packets
from entering the organizational network. Or, because the activity is only deemed as
suspicious, the device could let the packets pass, but send alerts to a network
administrator, who can then take a closer look at the traffic and take appropriate
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 739
Figure 8.35  Providing anonymity and privacy with a proxy
Alice
Anonymizing
Proxy
SSL
Cleartext
actions. A device that generates alerts when it observes potentially malicious traffic
is called an intrusion detection system (IDS). A device that filters out suspicious
traffic is called an intrusion prevention system (IPS). In this section we study both
systems—IDS and IPS—together, since the most interesting technical aspect of
these systems is how they detect suspicious traffic (and not whether they send alerts or
drop packets). We will henceforth collectively refer to IDS systems and IPS systems
as IDS systems.
An IDS can be used to detect a wide range of attacks, including network mapping
(emanating, for example, from nmap), port scans, TCP stack scans, DoS bandwidth-
flooding attacks, worms and viruses, OS vulnerability attacks, and application
vulnerability attacks. (See Section 1.6 for a survey of network attacks.) Today, thousands
of organizations employ IDS systems. Many of these deployed systems are
proprietary, marketed by Cisco, Check Point, and other security equipment vendors.
But many of the deployed IDS systems are public-domain systems, such as the
immensely popular Snort IDS system (which we’ll discuss shortly).
740 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
Figure 8.36  An organization deploying a filter, an application gateway,
and IDS sensors
Internet
Web
server
FTP
server
DNS
server
Internal
network
Application
gateway
Demilitarized zone
Filter
Key:
= IDS sensors
An organization may deploy one or more IDS sensors in its organizational network.
Figure 8.36 shows an organization that has three IDS sensors. When multiple
sensors are deployed, they typically work in concert, sending information about suspicious
traffic activity to a central IDS processor, which collects and integrates the
information and sends alarms to network administrators when deemed appropriate.
In Figure 8.36, the organization has partitioned its network into two regions: a highsecurity
region, protected by a packet filter and an application gateway and monitored
by IDS sensors; and a lower-security region—referred to as the demilitarized
zone (DMZ)—which is protected only by the packet filter, but also monitored by
IDS sensors. Note that the DMZ includes the organization’s servers that need to communicate
with the outside world, such as its public Web server and its authoritative
DNS server.
You may be wondering at this stage, why multiple IDS sensors? Why not just
place one IDS sensor just behind the packet filter (or even integrated with the packet
filter) in Figure 8.36? We will soon see that an IDS not only needs to do deep packet
inspection, but must also compare each passing packet with tens of thousands of
“signatures”; this can be a significant amount of processing, particularly if the
organization receives gigabits/sec of traffic from the Internet. By placing the IDS
sensors further downstream, each sensor sees only a fraction of the organization’s
traffic, and can more easily keep up. Nevertheless, high-performance IDS and IPS
systems are available today, and many organizations can actually get by with just
one sensor located near its access router.
IDS systems are broadly classified as either signature-based systems or
anomaly-based systems. A signature-based IDS maintains an extensive database
of attack signatures. Each signature is a set of rules pertaining to an intrusion activity.
A signature may simply be a list of characteristics about a single packet (e.g.,
source and destination port numbers, protocol type, and a specific string of bits in
the packet payload), or may relate to a series of packets. The signatures are normally
created by skilled network security engineers who research known attacks.
An organization’s network administrator can customize the signatures or add its
own to the database.
Operationally, a signature-based IDS sniffs every packet passing by it, comparing
each sniffed packet with the signatures in its database. If a packet (or series of
packets) matches a signature in the database, the IDS generates an alert. The alert
could be sent to the network administrator in an e-mail message, could be sent to the
network management system, or could simply be logged for future inspection.
Signature-based IDS systems, although widely deployed, have a number of limitations.
Most importantly, they require previous knowledge of the attack to generate
an accurate signature. In other words, a signature-based IDS is completely blind to
new attacks that have yet to be recorded. Another disadvantage is that even if a signature
is matched, it may not be the result of an attack, so that a false alarm is generated.
Finally, because every packet must be compared with an extensive collection of
signatures, the IDS can become overwhelmed with processing and actually fail to
detect many malicious packets.
8.9 • OPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS 741
An anomaly-based IDS creates a traffic profile as it observes traffic in normal
operation. It then looks for packet streams that are statistically unusual, for example,
an inordinate percentage of ICMP packets or a sudden exponential growth in
port scans and ping sweeps. The great thing about anomaly-based IDS systems is
that they don’t rely on previous knowledge about existing attacks—that is, they can
potentially detect new, undocumented attacks. On the other hand, it is an extremely
challenging problem to distinguish between normal traffic and statistically unusual
traffic. To date, most IDS deployments are primarily signature-based, although
some include some anomaly-based features.
Snort
Snort is a public-domain, open source IDS with hundreds of thousands of existing
deployments [Snort 2012; Koziol 2003]. It can run on Linux, UNIX, and Windows
platforms. It uses the generic sniffing interface libpcap, which is also used by
Wireshark and many other packet sniffers. It can easily handle 100 Mbps of traffic;
for installations with gibabit/sec traffic rates, multiple Snort sensors may be needed.
To gain some insight into Snort, let’s take a look at an example of a Snort signature:
alert icmp $EXTERNAL_NET any -> $HOME_NET any
(msg:”ICMP PING NMAP”; dsize: 0; itype: 8;)
This signature is matched by any ICMP packet that enters the organization’s network
($HOME_NET) from the outside ($EXTERNAL_NET), is of type 8 (ICMP
ping), and has an empty payload (dsize = 0). Since nmap (see Section 1.6) generates
ping packets with these specific characteristics, this signature is designed to detect
nmap ping sweeps. When a packet matches this signature, Snort generates an alert
that includes the message “ICMP PING NMAP”.
Perhaps what is most impressive about Snort is the vast community of users and
security experts that maintain its signature database. Typically within a few hours of
a new attack, the Snort community writes and releases an attack signature, which is
then downloaded by the hundreds of thousands of Snort deployments distributed
around the world. Moreover, using the Snort signature syntax, network administrators
can tailor the signatures to their own organization’s needs by either modifying
existing signatures or creating entirely new ones.
8.10 Summary
In this chapter, we’ve examined the various mechanisms that our secret lovers, Bob
and Alice, can use to communicate securely. We’ve seen that Bob and Alice are
interested in confidentiality (so they alone are able to understand the contents of a
transmitted message), end-point authentication (so they are sure that they are talking
742 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
with each other), and message integrity (so they are sure that their messages are
not altered in transit). Of course, the need for secure communication is not confined
to secret lovers. Indeed, we saw in Sections 8.5 through 8.8 that security can be used
in various layers in a network architecture to protect against bad guys who have a
large arsenal of possible attacks at hand.
The first part of this chapter presented various principles underlying secure
communication. In Section 8.2, we covered cryptographic techniques for encrypting
and decrypting data, including symmetric key cryptography and public key cryptography.
DES and RSA were examined as specific case studies of these two major
classes of cryptographic techniques in use in today’s networks.
In Section 8.3, we examined two approaches for providing message integrity:
message authentication codes (MACs) and digital signatures. The two approaches
have a number of parallels. Both use cryptographic hash functions and both techniques
enable us to verify the source of the message as well as the integrity of the
message itself. One important difference is that MACs do not rely on encryption
whereas digital signatures require a public key infrastructure. Both techniques are
extensively used in practice, as we saw in Sections 8.5 through 8.8. Furthermore, digital
signatures are used to create digital certificates, which are important for verifying
the validity of public keys. In Section 8.4, we examined endpoint authentication and
introduced nonces to defend against the replay attack.
In Sections 8.5 through 8.8 we examined several security networking protocols
that enjoy extensive use in practice. We saw that symmetric key cryptography
is at the core of PGP, SSL, IPsec, and wireless security. We saw that public
key cryptography is crucial for both PGP and SSL. We saw that PGP uses digital
signatures for message integrity, whereas SSL and IPsec use MACs. Having now
an understanding of the basic principles of cryptography, and having studied how
these principles are actually used, you are now in position to design your own
secure network protocols!
Armed with the techniques covered in Sections 8.2 through 8.8, Bob and Alice
can communicate securely. (One can only hope that they are networking students
who have learned this material and can thus avoid having their tryst uncovered by
Trudy!) But confidentiality is only a small part of the network security picture.
As we learned in Section 8.9, increasingly, the focus in network security has been
on securing the network infrastructure against a potential onslaught by the bad guys.
In the latter part of this chapter, we thus covered firewalls and IDS systems which
inspect packets entering and leaving an organization’s network.
This chapter has covered a lot of ground, while focusing on the most important
topics in modern network security. Readers who desire to dig deeper are
encouraged to investigate the references cited in this chapter. In particular, we recommend
[Skoudis 2006] for attacks and operational security, [Kaufman 1995] for
cryptography and how it applies to network security, [Rescorla 2001] for an indepth
but readable treatment of SSL, and [Edney 2003] for a thorough discussion
of 802.11 security, including an insightful investigation into WEP and its flaws.
8.10 • SUMMARY 743
Homework Problems and Questions
Chapter 8 Review Problems
SECTION 8.1
R1. What are the differences between message confidentiality and message
integrity? Can you have confidentiality without integrity? Can you have
integrity without confidentiality? Justify your answer.
R2. Internet entities (routers, switches, DNS servers, Web servers, user end systems,
and so on) often need to communicate securely. Give three specific
example pairs of Internet entities that may want secure communication.
SECTION 8.2
R3. From a service perspective, what is an important difference between a
symmetric-key system and a public-key system?
R4. Suppose that an intruder has an encrypted message as well as the decrypted
version of that message. Can the intruder mount a ciphertext-only attack, a
known-plaintext attack, or a chosen-plaintext attack?
R5. Consider an 8-block cipher. How many possible input blocks does this cipher
have? How many possible mappings are there? If we view each mapping as a
key, then how many possible keys does this cipher have?
R6. Suppose N people want to communicate with each of N – 1 other people
using symmetric key encryption. All communication between any two people,
i and j, is visible to all other people in this group of N, and no other person
in this group should be able to decode their communication. How many
keys are required in the system as a whole? Now suppose that public key
encryption is used. How many keys are required in this case?
R7. Suppose n = 10,000, a = 10,023, and b = 10,004. Use an identity of modular
arithmetic to calculate in your head (a • b) mod n.
R8. Suppose you want to encrypt the message 10101111 by encrypting the decimal
number that corresponds to the message. What is the decimal number?
SECTIONS 8.3–8.4
R9. In what way does a hash provide a better message integrity check than a
checksum (such as the Internet checksum)?
R10. Can you “decrypt” a hash of a message to get the original message? Explain
your answer.
R11. Consider a variation of the MAC algorithm (Figure 8.9) where the sender
sends (m, H(m) + s), where H(m) + s is the concatenation of H(m) and s. Is
this variation flawed? Why or why not?
744 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
R12. What does it mean for a signed document to be verifiable and non-forgeable?
R13. In what way does the public-key encrypted message hash provide a better
digital signature than the public-key encrypted message?
R14. Suppose certifier.com creates a certificate for foo.com. Typically, the entire
certificate would be encrypted with certifier.com’s public key. True or False?
R15. Suppose Alice has a message that she is ready to send to anyone who asks.
Thousands of people want to obtain Alice’s message, but each wants to be sure
of the integrity of the message. In this context, do you think a MAC-based or a
digital-signature-based integrity scheme is more suitable? Why?
R16. What is the purpose of a nonce in an end-point authentication protocol?
R17. What does it mean to say that a nonce is a once-in-a-lifetime value? In whose
lifetime?
R18. Is the message integrity scheme based on HMAC susceptible to playback
attacks? If so, how can a nonce be incorporated into the scheme to remove
this susceptibility?
SECTIONS 8.5–8.8
R19. Suppose that Bob receives a PGP message from Alice. How does Bob know
for sure that Alice created the message (rather than, say, Trudy)? Does PGP
use a MAC for message integrity?
R20. In the SSL record, there is a field for SSL sequence numbers. True or False?
R21. What is the purpose of the random nonces in the SSL handshake?
R22. Suppose an SSL session employs a block cipher with CBC. True or False:
The server sends to the client the IV in the clear?
R23. Suppose Bob initiates a TCP connection to Trudy who is pretending to be
Alice. During the handshake, Trudy sends Bob Alice’s certificate. In what
step of the SSL handshake algorithm will Bob discover that he is not communicating
with Alice?
R24. Consider sending a stream of packets from Host A to Host B using IPsec.
Typically, a new SA will be established for each packet sent in the stream.
True or False?
R25. Suppose that TCP is being run over IPsec between headquarters and the
branch office in Figure 8.28. If TCP retransmits the same packet, then the two
corresponding packets sent by R1 packets will have the same sequence number
in the ESP header. True or False?
R26. An IKE SA and an IPsec SA are the same thing. True or False?
R27. Consider WEP for 802.11. Suppose that the data is 10101100 and the
keystream is 1111000. What is the resulting ciphertext?
R28. In WEP, an IV is sent in the clear in every frame. True or False?
HOMEWORK PROBLEMS AND QUESTIONS 745
SECTION 8.9
R29. Stateful packet filters maintain two data structures. Name them and briefly
describe what they do.
R30. Consider a traditional (stateless) packet filter. This packet filter may filter
packets based on TCP flag bits as well as other header fields. True or False?
R31. In a traditional packet filter, each interface can have its own access control
list. True or False?
R32. Why must an application gateway work in conjunction with a router filter to
be effective?
R33. Signature-based IDSs and IPSs inspect into the payloads of TCP and UDP
segments. True or False?
Problems
P1. Using the monoalphabetic cipher in Figure 8.3, encode the message “This is
an easy problem.” Decode the message “rmij’u uamu xyj.”
P2. Show that Trudy’s known-plaintext attack, in which she knows the
(ciphertext, plaintext) translation pairs for seven letters, reduces the number
of possible substitutions to be checked in the example in Section 8.2.1
by approximately 109.
P3. Consider the polyalphabetic system shown in Figure 8.4. Will a chosenplaintext
attack that is able to get the plaintext encoding of the message “The
quick brown fox jumps over the lazy dog.” be sufficient to decode all messages?
Why or why not?
P4. Consider the block cipher in Figure 8.5. Suppose that each block cipher Ti
simply reverses the order of the eight input bits (so that, for example,
11110000 becomes 00001111). Further suppose that the 64-bit scrambler
does not modify any bits (so that the output value of the mth bit is equal to
the input value of the mth bit). (a) With n = 3 and the original 64-bit input
equal to 10100000 repeated eight times, what is the value of the output?
(b) Repeat part (a) but now change the last bit of the original 64-bit input
from a 0 to a 1. (c) Repeat parts (a) and (b) but now suppose that the 64-bit
scrambler inverses the order of the 64 bits.
P5. Consider the block cipher in Figure 8.5. For a given “key” Alice and Bob
would need to keep eight tables, each 8 bits by 8 bits. For Alice (or Bob)
to store all eight tables, how many bits of storage are necessary? How does
this number compare with the number of bits required for a full-table 64-
bit block cipher?
P6. Consider the 3-bit block cipher in Table 8.1. Suppose the plaintext is
100100100. (a) Initially assume that CBC is not used. What is the resulting
746 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
ciphertext? (b) Suppose Trudy sniffs the ciphertext. Assuming she knows that
a 3-bit block cipher without CBC is being employed (but doesn’t know the
specific cipher), what can she surmise? (c) Now suppose that CBC is used
with IV = 111. What is the resulting ciphertext?
P7. (a) Using RSA, choose p = 3 and q = 11, and encode the word “dog” by
encrypting each letter separately. Apply the decryption algorithm to the
encrypted version to recover the original plaintext message. (b) Repeat
part (a) but now encrypt “dog” as one message m.
P8. Consider RSA with p = 5 and q = 11.
a. What are n and z?
b. Let e be 3. Why is this an acceptable choice for e?
c. Find d such that de = 1 (mod z) and d < 160.
d. Encrypt the message m = 8 using the key (n, e). Let c denote the corresponding
ciphertext. Show all work. Hint: To simplify the calculations,
use the fact:
[(a mod n) • (b mod n)] mod n = (a • b) mod n
P9. In this problem, we explore the Diffie-Hellman (DH) public-key encryption
algorithm, which allows two entities to agree on a shared key. The
DH algorithm makes use of a large prime number p and another large number
g less than p. Both p and g are made public (so that an attacker would know
them). In DH, Alice and Bob each independently choose secret keys, SA and
SB, respectively. Alice then computes her public key, TA, by raising g to SA and
then taking mod p. Bob similarly computes his own public key TB by raising g
to SB and then taking mod p. Alice and Bob then exchange their public keys
over the Internet. Alice then calculates the shared secret key S by raising TB to
SA and then taking mod p. Similarly, Bob calculates the shared key S´ by raising
TA to SB and then taking mod p.
a. Prove that, in general, Alice and Bob obtain the same symmetric key, that
is, prove S = S´.
b. With p = 11 and g = 2, suppose Alice and Bob choose private keys SA = 5
and SB = 12, respectively. Calculate Alice’s and Bob’s public keys, TA and
TB . Show all work.
c. Following up on part (b), now calculate S as the shared symmetric key. Show
all work.
d. Provide a timing diagram that shows how Diffie-Hellman can be attacked by
a man-in-the-middle. The timing diagram should have three vertical lines, one
for Alice, one for Bob, and one for the attacker Trudy.
P10. Suppose Alice wants to communicate with Bob using symmetric key cryptography
using a session key KS. In Section 8.2, we learned how public-key
PROBLEMS 747
cryptography can be used to distribute the session key from Alice to Bob. In
this problem, we explore how the session key can be distributed—without
public key cryptography—using a key distribution center (KDC). The KDC
is a server that shares a unique secret symmetric key with each registered
user. For Alice and Bob, denote these keys by KA-KDC and KB-KDC. Design a
scheme that uses the KDC to distribute KS to Alice and Bob. Your scheme
should use three messages to distribute the session key: a message from Alice
to the KDC; a message from the KDC to Alice; and finally a message from
Alice to Bob. The first message is KA-KDC (A, B). Using the notation, KA-KDC,
KB-KDC, S, A, and B answer the following questions.
a. What is the second message?
b. What is the third message?
P11. Compute a third message, different from the two messages in Figure 8.8, that
has the same checksum as the messages in Figure 8.8.
P12. Suppose Alice and Bob share two secret keys: an authentication key S1 and a
symmetric encryption key S2. Augment Figure 8.9 so that both integrity and
confidentiality are provided.
P13. In the BitTorrent P2P file distribution protocol (see Chapter 2), the seed
breaks the file into blocks, and the peers redistribute the blocks to each other.
Without any protection, an attacker can easily wreak havoc in a torrent by
masquerading as a benevolent peer and sending bogus blocks to a small subset
of peers in the torrent. These unsuspecting peers then redistribute the
bogus blocks to other peers, which in turn redistribute the bogus blocks to
even more peers. Thus, it is critical for BitTorrent to have a mechanism that
allows a peer to verify the integrity of a block, so that it doesn’t redistribute
bogus blocks. Assume that when a peer joins a torrent, it initially gets a
.torrent file from a fully trusted source. Describe a simple scheme that
allows peers to verify the integrity of blocks.
P14. The OSPF routing protocol uses a MAC rather than digital signatures to
provide message integrity. Why do you think a MAC was chosen over digital
signatures?
P15. Consider our authentication protocol in Figure 8.18 in which Alice authenticates
herself to Bob, which we saw works well (i.e., we found no flaws in it).
Now suppose that while Alice is authenticating herself to Bob, Bob must
authenticate himself to Alice. Give a scenario by which Trudy, pretending to be
Alice, can now authenticate herself to Bob as Alice. (Hint: Consider that the
sequence of operations of the protocol, one with Trudy initiating and one with
Bob initiating, can be arbitrarily interleaved. Pay particular attention to the fact
that both Bob and Alice will use a nonce, and that if care is not taken, the same
nonce can be used maliciously.)
P16. A natural question is whether we can use a nonce and public key cryptography
to solve the end-point authentication problem in Section 8.4. Consider
748 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
the following natural protocol: (1) Alice sends the message “I am
Alice” to Bob. (2) Bob chooses a nonce, R, and sends it to Alice. (3) Alice
uses her private key to encrypt the nonce and sends the resulting value to
Bob. (4) Bob applies Alice's public key to the received message. Thus, Bob
computes R and authenticates Alice.
a. Diagram this protocol, using the notation for public and private keys
employed in the textbook.
b. Suppose that certificates are not used. Describe how Trudy can become a
“woman-in-the-middle” by intercepting Alice’s messages and then pretending
to be Alice to Bob.
P17. Figure 8.19 shows the operations that Alice must perform with PGP to provide
confidentiality, authentication, and integrity. Diagram the corresponding
operations that Bob must perform on the package received from Alice.
P18. Suppose Alice wants to send an e-mail to Bob. Bob has a public-private key
pair (KB
+,KB
–), and Alice has Bob’s certificate. But Alice does not have a
public, private key pair. Alice and Bob (and the entire world) share the same
hash function H().
a. In this situation, is it possible to design a scheme so that Bob can verify
that Alice created the message? If so, show how with a block diagram for
Alice and Bob.
b. Is it possible to design a scheme that provides confidentiality for sending
the message from Alice to Bob? If so, show how with a block diagram for
Alice and Bob.
P19. Consider the Wireshark output below for a portion of an SSL session.
a. Is Wireshark packet 112 sent by the client or server?
b. What is the server’s IP address and port number?
c. Assuming no loss and no retransmissions, what will be the sequence number
of the next TCP segment sent by the client?
d. How many SSL records does Wireshark packet 112 contain?
e. Does packet 112 contain a Master Secret or an Encrypted Master Secret or
neither?
f. Assuming that the handshake type field is 1 byte and each length field is
3 bytes, what are the values of the first and last bytes of the Master Secret
(or Encrypted Master Secret)?
g. The client encrypted handshake message takes into account how many
SSL records?
h. The server encrypted handshake message takes into account how many
SSL records?
P20. In Section 8.6.1, it is shown that without sequence numbers, Trudy (a womanin-
the middle) can wreak havoc in an SSL session by interchanging TCP
PROBLEMS 749
segments. Can Trudy do something similar by deleting a TCP segment? What
does she need to do to succeed at the deletion attack? What effect will it have?
P21. Suppose Alice and Bob are communicating over an SSL session. Suppose an
attacker, who does not have any of the shared keys, inserts a bogus TCP segment
into a packet stream with correct TCP checksum and sequence numbers (and correct
IP addresses and port numbers). Will SSL at the receiving side accept the
bogus packet and pass the payload to the receiving application? Why or why not?
P22. The following True/False questions pertain to Figure 8.28.
a. When a host in 172.16.1/24 sends a datagram to an Amazon.com server,
the router R1 will encrypt the datagram using IPsec.
b. When a host in 172.16.1/24 sends a datagram to a host in 172.16.2/24, the
router R1 will change the source and destination address of the IP datagram.
c. Suppose a host in 172.16.1/24 initiates a TCP connection to a Web server
in 172.16.2/24. As part of this connection, all datagrams sent by R1 will
have protocol number 50 in the left-most IPv4 header field.
750 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
(Wireshark screenshot reprinted by permission of the Wireshark Foundation.)
d. Consider sending a TCP segment from a host in 172.16.1/24 to a host in
172.16.2/24. Suppose the acknowledgment for this segment gets lost, so
that TCP resends the segment. Because IPsec uses sequence numbers, R1
will not resend the TCP segment.
P23. Consider the example in Figure 8.28. Suppose Trudy is a woman-in-themiddle,
who can insert datagrams into the stream of datagrams going from
R1 and R2. As part of a replay attack, Trudy sends a duplicate copy of one of
the datagrams sent from R1 to R2. Will R2 decrypt the duplicate datagram
and forward it into the branch-office network? If not, describe in detail how
R2 detects the duplicate datagram.
P24. Consider the following pseudo-WEP protocol. The key is 4 bits and the IV
is 2 bits. The IV is appended to the end of the key when generating the
keystream. Suppose that the shared secret key is 1010. The keystreams for
the four possible inputs are as follows:
101000: 0010101101010101001011010100100 . . .
101001: 1010011011001010110100100101101 . . .
101010: 0001101000111100010100101001111 . . .
101011: 1111101010000000101010100010111 . . .
Suppose all messages are 8-bits long. Suppose the ICV (integrity check) is
4-bits long, and is calculated by XOR-ing the first 4 bits of data with the last
4 bits of data. Suppose the pseudo-WEP packet consists of three fields: first
the IV field, then the message field, and last the ICV field, with some of
these fields encrypted.
a. We want to send the message m = 10100000 using the IV = 11 and using
WEP. What will be the values in the three WEP fields?
b. Show that when the receiver decrypts the WEP packet, it recovers the
message and the ICV.
c. Suppose Trudy intercepts a WEP packet (not necessarily with the IV = 11)
and wants to modify it before forwarding it to the receiver. Suppose Trudy
flips the first ICV bit. Assuming that Trudy does not know the keystreams
for any of the IVs, what other bit(s) must Trudy also flip so that the
received packet passes the ICV check?
d. Justify your answer by modifying the bits in the WEP packet in part (a),
decrypting the resulting packet, and verifying the integrity check.
P25. Provide a filter table and a connection table for a stateful firewall that is as
restrictive as possible but accomplishes the following:
a. Allows all internal users to establish Telnet sessions with external
hosts.
b. Allows external users to surf the company Web site at 222.22.0.12.
c. But otherwise blocks all inbound and outbound traffic.
PROBLEMS 751
The internal network is 222.22/16. In your solution, suppose that the connection
table is currently caching three connections, all from inside to outside.
You’ll need to invent appropriate IP addresses and port numbers.
P26. Suppose Alice wants to visit the Web site activist.com using a TOR-like
service. This service uses two non-colluding proxy servers, Proxy1 and
Proxy2. Alice first obtains the certificates (each containing a public key) for
Proxy1 and Proxy2 from some central server. Denote K1
+( ), K2
+( ), K1
–( ), and
K2
–( ) for the encryption/decryption with public and private RSA keys.
a. Using a timing diagram, provide a protocol (as simple as possible) that
enables Alice to establish a shared session key S1 with Proxy1. Denote
S1(m) for encryption/decryption of data m with the shared key S1.
b. Using a timing diagram, provide a protocol (as simple as possible) that
allows Alice to establish a shared session key S2 with Proxy2 without
revealing her IP address to Proxy2.
c. Assume now that shared keys S1 and S2 are now established. Using a timing
diagram, provide a protocol (as simple as possible and not using
public-key cryptography) that allows Alice to request an html page from
activist.com without revealing her IP address to Proxy2 and without
revealing to Proxy1 which site she is visiting. Your diagram should end
with an HTTP request arriving at activist.com.
Wireshark Lab
In this lab (available from the companion Web site), we investigate the Secure Sockets
Layer (SSL) protocol. Recall from Section 8.6 that SSL is used for securing a TCP
connection, and that it is extensively used in practice for secure Internet transactions.
In this lab, we will focus on the SSL records sent over the TCP connection. We will
attempt to delineate and classify each of the records, with a goal of understanding
the why and how for each record. We investigate the various SSL record types as
well as the fields in the SSL messages. We do so by analyzing a trace of the SSL
records sent between your host and an e-commerce server.
IPsec Lab
In this lab (available from the companion Web site), we will explore how to create
IPsec SAs between linux boxes. You can do the first part of the lab with two ordinary
linux boxes, each with one Ethernet adapter. But for the second part of the lab,
you will need four linux boxes, two of which having two Ethernet adapters. In the
second half of the lab, you will create IPsec SAs using the ESP protocol in the tunnel
mode. You will do this by first manually creating the SAs, and then by having
IKE create the SAs.
752 CHAPTER 8 • SECURITY IN COMPUTER NETWORKS
What led you to specialize in the networking security area?
This is going to sound odd, but the answer is simple: It was fun. My background was in systems
programming and systems administration, which leads fairly naturally to security. And
I’ve always been interested in communications, ranging back to part-time systems programming
jobs when I was in college.
My work on security continues to be motivated by two things—a desire to keep computers
useful, which means that their function can’t be corrupted by attackers, and a desire
to protect privacy.
What was your vision for Usenet at the time that you were developing it? And now?
We originally viewed it as a way to talk about computer science and computer programming
around the country, with a lot of local use for administrative matters, for-sale ads, and so on.
In fact, my original prediction was one to two messages per day, from 50–100 sites at the
most—ever. But the real growth was in people-related topics, including—but not limited
to—human interactions with computers. My favorite newsgroups, over the years, have been
things like rec.woodworking, as well as sci.crypt.
To some extent, netnews has been displaced by the Web. Were I to start designing it
today, it would look very different. But it still excels as a way to reach a very broad audience
that is interested in the topic, without having to rely on particular Web sites.
Has anyone inspired you professionally? In what ways?
Professor Fred Brooks—the founder and original chair of the computer science department
at the University of North Carolina at Chapel Hill, the manager of the team that developed
the IBM S/360 and OS/360, and the author of The Mythical Man-Month—was a tremendous
influence on my career. More than anything else, he taught outlook and trade-offs—how to
753
Steven M. Bellovin joined the faculty at Columbia University after
many years at the Network Services Research Lab at AT&T Labs
Research in Florham Park, New Jersey. His focus is on networks,
security, and why the two are incompatible. In 1995, he was
awarded the Usenix Lifetime Achievement Award for his work in the
creation of Usenet, the first newsgroup exchange network that linked
two or more computers and allowed users to share information and
join in discussions. Steve is also an elected member of the National
Academy of Engineering. He received his BA from Columbia
University and his PhD from the University of North Carolina at
Chapel Hill.
AN INTERVIEW WITH . . .
Steven M. Bellovin
look at problems in the context of the real world (and how much messier the real world is
than a theorist would like), and how to balance competing interests in designing a solution.
Most computer work is engineering—the art of making the right trade-offs to satisfy many
contradictory objectives.
What is your vision for the future of networking and security?
Thus far, much of the security we have has come from isolation. A firewall, for example,
works by cutting off access to certain machines and services. But we’re in an era of increasing
connectivity—it’s gotten harder to isolate things. Worse yet, our production systems
require far more separate pieces, interconnected by networks. Securing all that is one of our
biggest challenges.
What would you say have been the greatest advances in security? How much further do
we have to go?
At least scientifically, we know how to do cryptography. That’s been a big help. But most
security problems are due to buggy code, and that’s a much harder problem. In fact, it’s the
oldest unsolved problem in computer science, and I think it will remain that way. The challenge
is figuring out how to secure systems when we have to build them out of insecure
components. We can already do that for reliability in the face of hardware failures; can we
do the same for security?
Do you have any advice for students about the Internet and networking security?
Learning the mechanisms is the easy part. Learning how to “think paranoid” is harder. You
have to remember that probability distributions don’t apply—the attackers can and will find
improbable conditions. And the details matter—a lot.
754
CHAPTER 9
Network
Management
755
Having made our way through the first eight chapters of this text, we’re now well
aware that a network consists of many complex, interacting pieces of hardware and
software—from the links, switches, routers, hosts, and other devices that comprise
the physical components of the network to the many protocols (in both hardware
and software) that control and coordinate these devices. When hundreds or thousands
of such components are cobbled together by an organization to form a network,
it is not surprising that components will occasionally malfunction, that
network elements will be misconfigured, that network resources will be overutilized,
or that network components will simply “break” (for example, a cable will be
cut or a can of soda will be spilled on top of a router). The network administrator,
whose job it is to keep the network “up and running,” must be able to respond to
(and better yet, avoid) such mishaps. With potentially thousands of network components
spread out over a wide area, the network administrator in a network operations
center (NOC) clearly needs tools to help monitor, manage, and control the network.
In this chapter, we’ll examine the architecture, protocols, and information base used
by a network administrator in this task.
9.1 What Is Network Management?
Before diving in to network management itself, let’s first consider a few illustrative
“real-world” non-networking scenarios in which a complex system with many interacting
components must be monitored, managed, and controlled by an administrator.
Electrical power-generation plants have a control room where dials, gauges, and
lights monitor the status (temperature, pressure, flow) of remote valves, pipes, vessels,
and other plant components. These devices allow the operator to monitor the
plant’s many components, and may alert the operator (with the famous flashing red
warning light) when trouble is imminent. Actions are taken by the plant operator to
control these components. Similarly, an airplane cockpit is instrumented to allow a
pilot to monitor and control the many components that make up an airplane. In these
two examples, the “administrator” monitors remote devices and analyzes their data
to ensure that they are operational and operating within prescribed limits (for example,
that a core meltdown of a nuclear power plant is not imminent, or that the plane
is not about to run out of fuel), reactively controls the system by making adjustments
in response to the changes within the system or its environment, and proactively
manages the system (for example, by detecting trends or anomalous behavior,
allowing action to be taken before serious problems arise). In a similar sense, the
network administrator will actively monitor, manage, and control the system with
which she or he is entrusted.
In the early days of networking, when computer networks were research artifacts
rather than a critical infrastructure used by hundreds of millions of people a
day, “network management” was unheard of. If one encountered a network problem,
one might run a few pings to locate the source of the problem and then modify system
settings, reboot hardware or software, or call a remote colleague to do so. (A
very readable discussion of the first major “crash” of the ARPAnet on October 27,
1980, long before network management tools were available, and the efforts taken
to recover from and understand the crash is [RFC 789].) As the public Internet and
private intranets have grown from small networks into a large global infrastructure,
the need to manage the huge number of hardware and software components within
these networks more systematically has grown more important as well.
In order to motivate our study of network management, let’s begin with a simple
example. Figure 9.1 illustrates a small network consisting of three routers and a
number of hosts and servers. Even in such a simple network, there are many scenarios
in which a network administrator might benefit tremendously from having
appropriate network management tools:
• Detecting failure of an interface card at a host or a router. With appropriate network
management tools, a network entity (for example, router A) may report to
the network administrator that one of its interfaces has gone down. (This is
certainly preferable to a phone call to the NOC from an irate user who says the
network connection is down!) A network administrator who actively monitors
756 CHAPTER 9 • NETWORK MANAGEMENT
and analyzes network traffic may be able to really impress the would-be irate
user by detecting problems in the interface ahead of time and replacing the interface
card before it fails. This might be done, for example, if the administrator
noted an increase in checksum errors in frames being sent by the soon-to-die
interface.
• Host monitoring. Here, the network administrator might periodically check to
see if all network hosts are up and operational. Once again, the network administrator
may really be able to impress a network user by proactively responding to
a problem (host down) before it is reported by a user.
• Monitoring traffic to aid in resource deployment. A network administrator might
monitor source-to-destination traffic patterns and notice, for example, that by
switching servers between LAN segments, the amount of traffic that crosses
multiple LANs could be significantly decreased. Imagine the happiness all
around when better performance is achieved with no new equipment costs. Similarly,
by monitoring link utilization, a network administrator might determine
that a LAN segment or the external link to the outside world is overloaded and
9.1 • WHAT IS NETWORK MANAGEMENT? 757
Host
H1
A
B
Host C
Link to
external
network
Server
Figure 9.1  A simple scenario illustrating the uses of network management
that a higher-bandwidth link should thus be provisioned (alas, at an increased
cost). The network administrator might also want to be notified automatically
when congestion levels on a link exceed a given threshold value, in order to provision
a higher-bandwidth link before congestion becomes serious.
• Detecting rapid changes in routing tables. Route flapping—frequent changes in the
routing tables—may indicate instabilities in the routing or a misconfigured router.
Certainly, the network administrator who has improperly configured a router would
prefer to discover the error him- or herself, before the network goes down.
• Monitoring for SLAs. Service Level Agreements (SLAs) are contracts that
define specific performance metrics and acceptable levels of network-provider
performance with respect to these metrics [Huston 1999a]. Verizon and Sprint
are just two of the many network providers that guarantee SLAs [AT&T SLA
2012; Verizon SLA 2012] to their customers. These SLAs include service availability
(outage), latency, throughput, and outage notification requirements.
Clearly, if performance criteria are to be part of a service agreement between a
network provider and its users, then measuring and managing performance will
be of great importance to the network administrator.
• Intrusion detection. A network administrator may want to be notified when network
traffic arrives from, or is destined for, a suspicious source (for example,
host or port number). Similarly, a network administrator may want to detect (and
in many cases filter) the existence of certain types of traffic (for example, sourcerouted
packets, or a large number of SYN packets directed to a given host) that
are known to be characteristic of the types of security attacks that we considered
in Chapter 8.
The International Organization for Standardization (ISO) has created a network
management model that is useful for placing the anecdotal scenarios above in a
more structured framework. Five areas of network management are defined:
• Performance management. The goal of performance management is to quantify,
measure, report, analyze, and control the performance (for example, utilization
and throughput) of different network components. These components
include individual devices (for example, links, routers, and hosts) as well as
end-to-end abstractions such as a path through the network. We will see
shortly that protocol standards such as the Simple Network Management Protocol
(SNMP) [RFC 3410] play a central role in Internet performance
management.
• Fault management. The goal of fault management is to log, detect, and respond
to fault conditions in the network. The line between fault management and performance
management is rather blurred. We can think of fault management as
the immediate handling of transient network failures (for example, link, host,
or router hardware or software outages), while performance management takes
758 CHAPTER 9 • NETWORK MANAGEMENT
the longer-term view of providing acceptable levels of performance in the face
of varying traffic demands and occasional network device failures. As with
performance management, the SNMP protocol plays a central role in fault
management.
• Configuration management. Configuration management allows a network manager
to track which devices are on the managed network and the hardware and
software configurations of these devices. An overview of configuration management
and requirements for IP-based networks can be found in [RFC 3139].
• Accounting management. Accounting management allows the network manager
to specify, log, and control user and device access to network resources. Usage
quotas, usage-based charging, and the allocation of resource-access privileges all
fall under accounting management.
• Security management. The goal of security management is to control access to
network resources according to some well-defined policy. The key distribution
centers that we studied in Section 8.3 are components of security management.
The use of firewalls to monitor and control external access points to one’s network,
a topic we studied in Section 8.9, is another crucial component.
In this chapter, we’ll cover only the rudiments of network management. Our
focus will be purposefully narrow—we’ll examine only the infrastructure for
network management—the overall architecture, network management protocols,
and information base through which a network administrator keeps the network
up and running. We’ll not cover the decision-making processes of the network
administrator, who must plan, analyze, and respond to the management information
that is conveyed to the NOC. In this area, topics such as fault identification
and management [Katzela 1995; Medhi 1997; Labovitz 1997; Steinder 2002;
Feamster 2005; Wu 2005; Teixeira 2006], anomaly detection [Lakhina 2004;
Lakhina 2005; Barford 2009], and more come into consideration. Nor will we
cover the broader topic of service management [Saydam 1996; RFC 3052]—the
provisioning of resources such as bandwidth, server capacity, and the other computational/
communication resources needed to meet the mission-specific service
requirements of an enterprise.
An often-asked question is “What is network management?” Our discussion
above has motivated the need for, and illustrated a few of the uses of, network management.
We’ll conclude this section with a single-sentence (albeit a rather long runon
sentence) definition of network management from [Saydam 1996]:
“Network management includes the deployment, integration, and coordination
of the hardware, software, and human elements to monitor, test, poll, configure,
analyze, evaluate, and control the network and element resources to meet the
real-time, operational performance, and Quality of Service requirements at a
reasonable cost.”
9.1 • WHAT IS NETWORK MANAGEMENT? 759
It’s a mouthful, but it’s a good workable definition. In the following sections, we’ll
add some meat to this rather bare-bones definition of network management.
9.2 The Infrastructure for
Network Management
We’ve seen in the preceding section that network management requires the ability
to “monitor, test, poll, configure, . . . and control” the hardware and software components
in a network. Because the network devices are distributed, this will, at a
minimum, require that the network administrator be able to gather data (for example,
for monitoring purposes) from a remote entity and effect changes at that remote
entity (for example, control it). A human analogy will prove useful here for understanding
the infrastructure needed for network management.
Imagine that you’re the head of a large organization that has branch offices
around the world. It’s your job to make sure that the pieces of your organization are
operating smoothly. How will you do so? At a minimum, you’ll periodically gather
data from your branch offices in the form of reports and various quantitative measures
of activity, productivity, and budget. You’ll occasionally (but not always) be
explicitly notified when there’s a problem in one of the branch offices; the branch
manager who wants to climb the corporate ladder (perhaps to get your job) may
send you unsolicited reports indicating how smoothly things are running at his or
her branch. You’ll sift through the reports you receive, hoping to find smooth operations
everywhere but no doubt finding problems in need of your attention. You
might initiate a one-on-one dialogue with one of your problem branch offices,
gather more data in order to understand the problem, and then pass down an executive
order (“Make this change!”) to the branch office manager.
Implicit in this very common human scenario is an infrastructure for controlling
the organization—the boss (you), the remote sites being controlled (the branch
offices), your remote agents (the branch office managers), communication protocols
(for transmitting standard reports and data, and for one-on-one dialogues), and data
(the report contents and the quantitative measures of activity, productivity, and
budget). Each of these components in human organizational management has a
counterpart in network management.
The architecture of a network management system is conceptually identical to
this simple human organizational analogy. The network management field has its
own specific terminology for the various components of a network management
architecture, and so we adopt that terminology here. As shown in Figure 9.2, there
are three principal components of a network management architecture: a managing
entity (the boss in our analogy above—you), the managed devices (the branch
office), and a network management protocol.
760 CHAPTER 9 • NETWORK MANAGEMENT
The managing entity is an application, typically with a human in the loop,
running in a centralized network management station in the NOC. The managing
entity is the locus of activity for network management; it controls the collection,
processing, analysis, and/or display of network management information. It is
here that actions are initiated to control network behavior and here that the
human network administrator interacts with the network devices.
A managed device is a piece of network equipment (including its software)
that resides on a managed network. This is the branch office in our human analogy.
A managed device might be a host, router, bridge, hub, printer, or modem.
Within a managed device, there may be several so-called managed objects. These
managed objects are the actual pieces of hardware within the managed device (for
example, a network interface card), and the sets of configuration parameters for
the pieces of hardware and software (for example, an intradomain routing protocol
such as RIP). In our human analogy, the managed objects might be the departments
within the branch office. These managed objects have pieces of information
associated with them that are collected into a Management Information Base
9.2 • THE INFRASTRUCTURE FOR NETWORK MANAGEMENT 761
Figure 9.2  Principal components of a network management architecture
Agent Data Agent Data
Agent Data
Agent Data
Managed
device
Managed
device
Managed
device
Managed
device
Managing Agent Data
entity Data
(MIB); we’ll see that the values of these pieces of information are available to
(and in many cases able to be set by) the managing entity. In our human analogy,
the MIB corresponds to quantitative data (measures of activity, productivity, and
budget, with the latter being settable by the managing entity!) exchanged between
the branch office and the main office. We’ll study MIBs in detail in Section 9.3.
Finally, also resident in each managed device is a network management agent, a
process running in the managed device that communicates with the managing
entity, taking local actions at the managed device under the command and control
of the managing entity. The network management agent is the branch manager in
our human analogy.
The third piece of a network management architecture is the network management
protocol. The protocol runs between the managing entity and the managed
devices, allowing the managing entity to query the status of managed devices and
indirectly take actions at these devices via its agents. Agents can use the network
management protocol to inform the managing entity of exceptional events (for
example, component failures or violation of performance thresholds). It’s important
to note that the network management protocol does not itself manage the network.
Instead, it provides capabilities that a network administrator can use to manage
(“monitor, test, poll, configure, analyze, evaluate, and control”) the network. This is
a subtle, but important, distinction.
Although the infrastructure for network management is conceptually simple,
one can often get bogged down with the network-management-speak vocabulary of
“managing entity,” “managed device,” “managing agent,” and “Management Information
Base.” For example, in network-management-speak, in our simple hostmonitoring
scenario, “managing agents” located at “managed devices” are
periodically queried by the “managing entity”—a simple idea, but a linguistic
mouthful! With any luck, keeping in mind the human organizational analogy and its
obvious parallels with network management will be of help as we continue through
this chapter.
Our discussion of network management architecture above has been generic,
and broadly applies to a number of the network management standards and
efforts that have been proposed over the years. Network management standards
began maturing in the late 1980s, with OSI CMISE/CMIP (the Common Management
Information Services Element/Common Management Information
Protocol) [Piscatello 1993; Stallings 1993; Glitho 1998] and the Internet SNMP
(Simple Network Management Protocol) [RFC 3410; Stallings 1999; Rose
1996] emerging as the two most important standards [Subramanian 2000]. Both
are designed to be independent of vendor-specific products or networks. Because
SNMP was quickly designed and deployed at a time when the need for network
management was becoming painfully clear, SNMP found widespread use and
acceptance. Today, SNMP has emerged as the most widely used and deployed
network management framework. We’ll cover SNMP in detail in the following
section.
762 CHAPTER 9 • NETWORK MANAGEMENT
9.2 • THE INFRASTRUCTURE FOR NETWORK MANAGEMENT 763
COMCAST’S NETWORK OPERATIONS CENTER
Comcast’s world-class fiber-based IP network delivers converged products and services to
49 million combined video, data and voice customers. Comcast’s network includes more
than 618,000 plant route miles, 138,000 fiber route miles, 30,000 backbone miles,
122,000 optical nodes, and massive storage for the Comcast Content Delivery Network,
which delivers a Video on Demand product of more than 134 Terabytes. Each part of
Comcast’s network, up to and including the customers’ homes or businesses, is monitored
by one of the company’s Operations Centers.
Comcast operates two National Network Operations Centers that manage the national
backbone, regional area networks, national applications and specific platforms supporting
voice, data and video infrastructure for residential, commercial and wholesale customers.
In addition, Comcast has three Divisional Operations Centers that manage the
local infrastructure that supports all of their customers. Both the National and Divisional
Operations Centers are accountable for proactively monitoring all aspects of their network
and product performance on a 7 x 24 x 365 basis, utilizing common processes and
systems. For example, various network events at the national and local levels have
common pre-defined severity levels, recovery processes, and expected Mean Time to
Restore objectives. The national and divisional centers can back up each other if a local
issue impacts a site’s operation. In addition, the National and Divisional Operations
Centers have an extensive Virtual Private Network that allows engineers to securely access
the network to remotely perform proactive or reactive network management activities.
Comcast’s approach to network management involves five key areas: Performance
Management, Fault Management, Configuration Management, Accounting Management
and Security Management. Performance Management is focused on understanding
These screens show tools supporting correlation, threshold management, ticketing
used by Comcast technicians (Courtesy of Comcast.)
(continues)
PRINCIPLES IN PRACTICE
how the network/systems and applications (collectively referred to as the ecosystem) are
performing with respect to pre-defined measures specific to time of day, day of week, or
special events (e.g., storm surges or pay events, such as a boxing match). These predefined
performance measures exist throughout the service path, from the customer’s residence
or business through the entire network, as well as the interface points to partners
and peers. In addition, synthetic transactions are run to ensure the health of the ecosystem
on a continual basis. Fault Management is defined as the ability to detect, log and
understand anomalies that may impact customers. Comcast utilizes correlation engines to
properly determine an event’s severity and act appropriately, eliminating or remediating
potential issues before they affect customers. Configuration Management makes sure
appropriate versions of hardware and software are in place across all elements of the
ecosystem. Keeping these elements at their peak “golden” levels helps them avoid unintended
consequences. Accounting Management ensures that the operations centers
have a clear understanding of the provisioning and utilization of the ecosystem. This is
especially important to ensure that at all times the operations centers have the ability to reroute
traffic effectively. Security Management ensures that the proper controls exist to
ensure the ecosystem is effectively protected against inappropriate access.
Network Operations Centers and the ecosystem they support are not static. Engineering
and Operations personnel are constantly re-evaluating the pre-defined performance
measures and tools to ensure that the customers’ expectations for operational excellence
are met.
9.3 The Internet-Standard Management
Framework
Contrary to what the name SNMP (Simple Network Management Protocol) might
suggest, network management in the Internet is much more than just a protocol for
moving management data between a management entity and its agents, and has
grown to be much more complex than the word “simple” might suggest. The current
Internet-Standard Management Framework traces its roots back to the Simple Gateway
Monitoring Protocol, SGMP [RFC 1028]. SGMP was designed by a group of
university network researchers, users, and managers, whose experience with SGMP
allowed them to design, implement, and deploy SNMP in just a few months [Lynch
1993]—a far cry from today’s rather drawn-out standardization process. Since then,
SNMP has evolved from SNMPv1 through SNMPv2 to the most recent version,
SNMPv3 [RFC 3410], released in April 1999 and updated in December 2002.
When describing any framework for network management, certain questions
must inevitably be addressed:
764 CHAPTER 9 • NETWORK MANAGEMENT
• What (from a semantic viewpoint) is being monitored? And what form of control
can be exercised by the network administrator?
• What is the specific form of the information that will be reported and/or exchanged?
• What is the communication protocol for exchanging this information?
Recall our human organizational analogy from the previous section. The boss
and the branch managers will need to agree on the measures of activity, productivity,
and budget used to report the branch office’s status. Similarly, they’ll need to
agree on the actions the boss can take (for example, cut the budget, order the branch
manager to change some aspect of the office’s operation, or fire the staff and shut
down the branch office). At a lower level of detail, they’ll need to agree on the form
in which this data is reported. For example, in what currency (dollars, euros?) will
the budget be reported? In what units will productivity be measured? While these
may seem like trivial details, they must be agreed upon, nonetheless. Finally, the
manner in which information is conveyed between the main office and the branch
offices (that is, their communication protocol) must be specified.
The Internet-Standard Management Framework addresses the questions posed
above. The framework consists of four parts:
• Definitions of network management objects, known as MIB objects. In the Internet-
Standard Management Framework, management information is represented
as a collection of managed objects that together form a virtual information store,
known as the Management Information Base (MIB). An MIB object might be a
counter, such as the number of IP datagrams discarded at a router due to errors in
an IP datagram header, or the number of carrier sense errors in an Ethernet interface
card; descriptive information such as the version of the software running on
a DNS server; status information such as whether a particular device is functioning
correctly; or protocol-specific information such as a routing path to a
destination. MIB objects thus define the management information maintained by
a managed device. Related MIB objects are gathered into MIB modules. In our
human organizational analogy, the MIB defines the information conveyed
between the branch office and the main office.
• A data definition language, known as SMI (Structure of Management Information).
SMI defines the data types, an object model, and rules for writing and
revising management information. MIB objects are specified in this data definition
language. In our human organizational analogy, the SMI is used to define
the details of the format of the information to be exchanged.
• A protocol, SNMP. SNMP is used for conveying information and commands
between a managing entity and an agent executing on behalf of that entity within
a managed network device.
• Security and administration capabilities. The addition of these capabilities represents
the major enhancement in SNMPv3 over SNMPv2.
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 765
The Internet network management architecture is thus modular by design, with a
protocol-independent data definition language and MIB, and an MIB-independent
protocol. Interestingly, this modular architecture was first put in place to ease the transition
from an SNMP-based network management to a network management framework
being developed by ISO, the competing network management architecture when
SNMP was first conceived—a transition that never occurred. Over time, however,
SNMP’s design modularity has allowed it to evolve through three major revisions,
with each of the four major parts of SNMP discussed above evolving independently.
Clearly, the right decision about modularity was made, even if for the wrong reason!
In the following subsections, we cover the four major components of the Internet-
Standard Management Framework in more detail.
9.3.1 Structure of Management Information: SMI
The Structure of Management Information, SMI (a rather oddly named component
of the network management framework whose name gives no hint of its functionality),
is the language used to define the management information residing in a managed-
network entity. Such a definition language is needed to ensure that the syntax
and semantics of the network management data are well defined and unambiguous.
Note that the SMI does not define a specific instance of the data in a managed-network
entity, but rather the language in which such information is specified. The documents
describing the SMI for SNMPv3 (which rather confusingly, is called SMIv2) are [RFC
2578; RFC 2579; RFC 2580]. Let’s examine the SMI in a bottom-up manner, starting
with the base data types in the SMI. We’ll then look at how managed objects are
described in SMI, then how related managed objects are grouped into modules.
SMI Base Data Types
RFC 2578 specifies the basic data types in the SMI MIB module-definition language.
Although the SMI is based on the ASN.1 (Abstract Syntax Notation One) [ISO X.680
2002] object-definition language (see Section 9.4), enough SMI-specific data types
have been added that SMI should be considered a data definition language in its own
right. The 11 basic data types defined in RFC 2578 are shown in Table 9.1. In addition
to these scalar objects, it is also possible to impose a tabular structure on an ordered
collection of MIB objects using the SEQUENCE OF construct; see RFC 2578 for
details. Most of the data types in Table 9.1 will be familiar (or self-explanatory) to
most readers. The one data type we will discuss in more detail shortly is the OBJECT
IDENTIFIER data type, which is used to name an object.
SMI Higher-Level Constructs
In addition to the basic data types, the SMI data definition language also provides
higher-level language constructs.
766 CHAPTER 9 • NETWORK MANAGEMENT
The OBJECT-TYPE construct is used to specify the data type, status, and
semantics of a managed object. Collectively, these managed objects contain the
management data that lies at the heart of network management. There are more than
10,000 defined objects in various Internet RFCs [RFC 3410]. The OBJECT-TYPE
construct has four clauses. The SYNTAX clause of an OBJECT-TYPE definition
specifies the basic data type associated with the object. The MAX-ACCESS clause
specifies whether the managed object can be read, be written, be created, or have its
value included in a notification. The STATUS clause indicates whether the object
definition is current and valid, obsolete (in which case it should not be implemented,
as its definition is included for historical purposes only), or deprecated (obsolete,
but implementable for interoperability with older implementations). The DESCRIPTION
clause contains a human-readable textual definition of the object; this “documents”
the purpose of the managed object and should provide all the semantic
information needed to implement the managed object.
As an example of the OBJECT-TYPE construct, consider the ipSystem-
StatsInDelivers object-type definition from [RFC 4293]. This object defines
a 32-bit counter that keeps track of the number of IP datagrams that were received
at the managed device and were successfully delivered to an upper-layer protocol.
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 767
Data Type Description
INTEGER 32-bit integer, as defined in ASN.1, with a value between 231 and 231  1
inclusive, or a value from a list of possible named constant values.
Integer32 32-bit integer with a value between 231 and 231  1 inclusive.
Unsigned32 Unsigned 32-bit integer in the range 0 to 232  1 inclusive.
OCTET STRING ASN.1-format byte string representing arbitrary binary or textual data, up to 65,535
bytes long.
OBJECT IDENTIFIER ASN.1-format administratively assigned (structured name); see Section 9.3.2.
IPaddress 32-bit Internet address, in network-byte order.
Counter32 32-bit counter that increases from 0 to 232  1 and then wraps around to 0.
Counter64 64-bit counter.
Gauge32 32-bit integer that will not count above 232  1 nor decrease beyond 0 when
increased or decreased.
TimeTicks Time, measured in 1/100ths of a second since some event.
Opaque Uninterpreted ASN.1 string, needed for backward compatibility.
Table 9.1  Basic data types of the SMI
The final line of this definition is concerned with the name of this object, a topic
we’ll consider in the following subsection.
ipSystemStatsInDelivers OBJECT-TYPE
SYNTAX Counter32
MAX-ACCESS read-only
STATUS current
DESCRIPTION
"The total number of datagrams successfully
delivered to IPuser-protocols (including ICMP).
When tracking interface statistics, the counter
of the interface to which these datagrams were
addressed is incremented. This interface might
not be the same as the input interface for
some of the datagrams.
Discontinuities in the value of this counter can
occur at re-initialization of the management
system, and at other times as indicated by the
value of ipSystemStatsDiscontinuityTime."
::= { ipSystemStatsEntry 18 }
The MODULE-IDENTITY construct allows related objects to be grouped
together within a “module.” For example, [RFC 4293] specifies the MIB module that
defines managed objects (including ipSystemStatsInDelivers) for managing
implementations of the Internet Protocol (IP) and its associated Internet Control
Message Protocol (ICMP). [RFC 4022] specifies the MIB module for TCP, and [RFC
4113] specifies the MIB module for UDP. [RFC 4502] defines the MIB module for
RMON remote monitoring. In addition to containing the OBJECT-TYPE definitions
of the managed objects within the module, the MODULE-IDENTITY construct
contains clauses to document contact information of the author of the module, the
date of the last update, a revision history, and a textual description of the module. As
an example, consider the module definition for management of the IP protocol:
ipMIB MODULE-IDENTITY
LAST-UPDATED "200602020000Z"
ORGANIZATION "IETF IPv6 MIB Revision Team"
CONTACT-INFO
"Editor:
Shawn A. Routhier
Interworking Labs
108 Whispering Pines Dr. Suite 235
768 CHAPTER 9 • NETWORK MANAGEMENT
Scotts Valley, CA 95066
USA
EMail: <sar@iwl.com>"
DESCRIPTION
"The MIB module for managing IP and ICMP
implementations, but excluding their
management of IP routes.
Copyright (C) The Internet Society (2006).
This version of this MIB module is part of
RFC 4293; see the RFC itself for full legal
notices."
REVISION "200602020000Z"
DESCRIPTION
"The IP version neutral revision with added
IPv6 objects for ND, default routers, and
router advertisements. As well as being the
successor to RFC 2011, this MIB is also the
successor to RFCs 2465 and 2466. Published
as RFC 4293."
REVISION "199411010000Z"
DESCRIPTION
"A separate MIB module (IP-MIB) for IP and
ICMP management objects. Published as RFC
2011."
REVISION "199103310000Z"
DESCRIPTION
"The initial revision of this MIB module was
part of MIB-II, which was published as RFC
1213."
::= { mib-2 48}
The NOTIFICATION-TYPE construct is used to specify information regarding
SNMPv2-Trap and InformationRequest messages generated by an agent, or a managing
entity; see Section 9.3.3. This information includes a textual DESCRIPTION
of when such messages are to be sent, as well as a list of values to be included in the
message generated; see [RFC 2578] for details. The MODULE-COMPLIANCE
construct defines the set of managed objects within a module that an agent must
implement. The AGENT-CAPABILITIES construct specifies the capabilities of
agents with respect to object- and event-notification definitions.
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 769
770 CHAPTER 9 • NETWORK MANAGEMENT
9.3.2 Management Information Base: MIB
As noted previously, the Management Information Base, MIB, can be thought of
as a virtual information store, holding managed objects whose values collectively
reflect the current “state” of the network. These values may be queried and/or set by
a managing entity by sending SNMP messages to the agent that is executing in a
managed device on behalf of the managing entity. Managed objects are specified
using the OBJECT-TYPE SMI construct discussed above and gathered into MIB
modules using the MODULE-IDENTITY construct.
The IETF has been busy standardizing the MIB modules associated with routers,
hosts, and other network equipment. This includes basic identification data about a
particular piece of hardware, and management information about the device’s network
interfaces and protocols. As of 2006 there were more than 200 standards-based
MIB modules and an even larger number of vendor-specific (private) MIB modules.
With all of these standards, the IETF needed a way to identify and name the standardized
modules as well as the specific managed objects within a module. Rather than
start from scratch, the IETF adopted a standardized object identification (naming)
framework that had already been put in place by the International Organization for
Standardization (ISO). As is the case with many standards bodies, the ISO had
“grand plans” for its standardized object identification framework—to identify every
possible standardized object (for example, data format, protocol, or piece of information)
in any network, regardless of the network standards organization (for example,
Internet IETF, ISO, IEEE, or ANSI), equipment manufacturer, or network owner. A
lofty goal indeed! The object identification framework adopted by ISO is part of the
ASN.1 (Abstract Syntax Notation One) [ISO X.680 2002] object definition language
that we’ll discuss in Section 9.4. Standardized MIB modules have their own cozy
corner in this all-encompassing naming framework, as discussed below.
As shown in Figure 9.3, objects are named in the ISO naming framework in a
hierarchical manner. Note that each branch point in the tree has both a name and a
number (shown in parentheses); any point in the tree is thus identifiable by the
sequence of names or numbers that specify the path from the root to that point in the
identifier tree. A fun, but incomplete and unofficial, Web-based utility for traversing
part of the object identifier tree (using branch information contributed by volunteers)
may be found in [OID Repository 2012].
At the top of the hierarchy are the ISO and the Telecommunication Standardization
Sector of the International Telecommunication Union (ITU-T), the two
main standards organizations dealing with ASN.1, as well as a branch for joint
efforts by these two organizations. Under the ISO branch of the tree, we find
entries for all ISO standards (1.0) and for standards issued by standards bodies of
various ISO-member countries (1.2). Although not shown in Figure 9.3, under (ISO
member body, a.k.a. 1.2) we would find USA (1.2.840), under which we would
find a number of IEEE, ANSI, and company-specific standards. These include RSA
(1.2.840.11359) and Microsoft (1.2.840.113556), under which we find the
Microsoft File Formats (1.2.840.113556.4) for various Microsoft products, such as
Word (1.2.840.113556.4.2). But we are interested here in networking (not
Microsoft Word files), so let us turn our attention to the branch labeled 1.3, the
standards issued by bodies recognized by the ISO. These include the U.S. Department
of Defense (6) (under which we will find the Internet standards), the Open
Software Foundation (22), the airline association SITA (69), NATO-identified bodies
(57), as well as many other organizations.
Under the Internet branch of the tree (1.3.6.1), there are seven categories.
Under the private (1.3.6.1.4) branch, we find a list [IANA 2009b] of the names
and private enterprise codes of many thousands of private companies that have registered
with the Internet Assigned Numbers Authority (IANA) [IANA 2009a]. Under
the management (1.3.6.1.2) and MIB-2 branches (1.3.6.1.2.1) of the object identifier
tree, we find the definitions of the standardized MIB modules. Whew—it’s a
long journey down to our corner of the ISO name space!
Standardized MIB Modules
The lowest level of the tree in Figure 9.3 shows some of the important hardwareoriented
MIB modules (system and interface) as well as modules associated
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 771
ITU-T (0) ISO (1) Joint ISO/ITU-T (2)
Standard (0) ISO member
body (2)
ISO identified
organization (3)
NATO
identified (57)
Open Software
Foundation (22)
US
DoD (6)
Internet (1)
directory
(1)
experimental
(3)
security
(5)
mail
(7)
private
(4)
snmpv2
(6)
management
(2)
MIB-2 (1)
system
(1)
address
translation
(3)
icmp
(5)
udp
(7)
cmot
(9)
interface
(2)
ip
(4)
tcp
(6)
egp
(8)
transmission
(10)
rmon
(16)
snmp
(11)
Figure 9.3  ASN.1 object identifier tree
772 CHAPTER 9 • NETWORK MANAGEMENT
with some of the most important Internet protocols. [RFC 5000] lists all of the standardized
MIB modules as of 2008. While MIB-related RFCs make for rather tedious
and dry reading, it is instructive (that is, like eating vegetables, it is “good for you”)
to consider a few MIB module definitions to get a flavor for the type of information
in a module.
The managed objects falling under system contain general information about
the device being managed; all managed devices must support the system MIB
objects. Table 9.2 defines the objects in the system group, as defined in [RFC 1213].
Table 9.3 defines the managed objects in the MIB module for the UDP protocol at a
managed entity.
9.3.3 SNMP Protocol Operations and Transport Mappings
The Simple Network Management Protocol version 2 (SNMPv2) [RFC 3416] is used
to convey MIB information among managing entities and agents executing on behalf
of managing entities. The most common usage of SNMP is in a request-response
mode in which an SNMPv2 managing entity sends a request to an SNMPv2 agent,
who receives the request, performs some action, and sends a reply to the request. Typically,
a request will be used to query (retrieve) or modify (set) MIB object values
Object Identifier Name Type Description (from RFC 1213)
1.3.6.1.2.1.1.1 sysDescr OCTET STRING “Full name and version identification of the system’s hardware
type, software operating-system, and networking software.”
1.3.6.1.2.1.1.2 sysObjectID OBJECT IDENTIFIER Vendor-assigned object ID that “provides an easy and
unambiguous means for determining ‘what kind of box’ is
being managed.”
1.3.6.1.2.1.1.3 sysUpTime TimeTicks “The time (in hundredths of a second) since the network
management portion of the system was last re-initialized.”
1.3.6.1.2.1.1.4 sysContact OCTET STRING “The contact person for this managed node, together with information
on how to contact this person.”
1.3.6.1.2.1.1.5 sysName OCTET STRING “An administratively assigned name for this managed node. By
convention, this is the node’s fully qualified domain name.”
1.3.6.1.2.1.1.6 sysLocation OCTET STRING “The physical location of this node.”
1.3.6.1.2.1.1.7 sysServices Integer32 A coded value that indicates the set of services available
at this node: physical (for example, a repeater), data
link/subnet (for example, bridge), Internet (for example,
IP gateway), end-to-end (for example, host), applications.
Table 9.2  Managed objects in the MIB-2 system group
associated with a managed device. A second common usage of SNMP is for an agent
to send an unsolicited message, known as a trap message, to a managing entity. Trap
messages are used to notify a managing entity of an exceptional situation that has
resulted in changes to MIB object values. We saw earlier in Section 9.1 that the network
administrator might want to receive a trap message, for example, when an interface
goes down, congestion reaches a predefined level on a link, or some other
noteworthy event occurs. Note that there are a number of important trade-offs between
polling (request-response interaction) and trapping; see the homework problems.
SNMPv2 defines seven types of messages, known generically as protocol data
units—PDUs—as shown in Table 9.4 and described next. The format of the PDU is
shown in Figure 9.4.
• The GetRequest, GetNextRequest, and GetBulkRequest PDUs are
all sent from a managing entity to an agent to request the value of one or more
MIB objects at the agent’s managed device. The object identifiers of the MIB
objects whose values are being requested are specified in the variable binding
portion of the PDU. GetRequest, GetNextRequest, and GetBulkRequest
differ in the granularity of their data requests. GetRequest can request
an arbitrary set of MIB values; multiple GetNextRequests can be used to
sequence through a list or table of MIB objects; GetBulkRequest allows a
large block of data to be returned, avoiding the overhead incurred if multiple
GetRequest or GetNextRequest messages were to be sent. In all three
cases, the agent responds with a Response PDU containing the object identifiers
and their associated values.
• The SetRequest PDU is used by a managing entity to set the value of one or
more MIB objects in a managed device. An agent replies with a Response PDU
with the “noError” error status to confirm that the value has indeed been set.
Object Identifier Name Type Description (from RFC 4113)
1.3.6.1.2.1.7.1 udpInDatagrams Counter32 “total number of UDP datagrams delivered to UDP users”
1.3.6.1.2.1.7.2 udpNoPorts Counter32 “total number of received UDP datagrams for which there
was no application at the destination port”
1.3.6.1.2.1.7.3 udpInErrors Counter32 “number of received UDP datagrams that could not be
delivered for reasons other than the lack of an application
at the destination port”
1.3.6.1.2.1.7.4 udpOutDatagrams Counter32 “total number of UDP datagrams sent from this entity”
Table 9.3  Selected managed objects in the MIB-2 UDP module
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 773
774 CHAPTER 9 • NETWORK MANAGEMENT
PDU
type
(0–3)
Request
Id
Error
Status
(0–5)
Error
Index
Name
Name Value
Name Value
PDU
Type
(4)
Enterprise Agent
Addr
Trap
Type
(0–7)
Specific
code
Time
stamp
Value
Get/set header
Trap header Trap information
SNMP PDU
Variables to get/set
Figure 9.4  SNMP PDU format
SNMPv2 PDU Type Sender-receiver Description
GetRequest manager-to-agent get value of one or more MIB object instances
GetNextRequest manager-to-agent get value of next MIB object instance in list or table
GetBulkRequest manager-to-agent get values in large block of data, for example, values in a
large table
InformRequest manager-to-manager inform remote managing entity of MIB values
remote to its access
SetRequest manager-to-agent set value of one or more MIB object instances
Response agent-to-manager or generated in response to
manager-to-manager GetRequest,
GetNextRequest,
GetBulkRequest,
SetRequest PDU, or
InformRequest
SNMPv2-Trap agent-to-manager inform manager of an exceptional event
Table 9.4  SNMPv2 PDU types
• The InformRequest PDU is used by a managing entity to notify another
managing entity of MIB information that is remote to the receiving entity. The
receiving entity replies with a Response PDU with the “noError” error status
to acknowledge receipt of the InformRequest PDU.
• The final type of SNMPv2 PDU is the trap message. Trap messages are generated
asynchronously; that is, they are not generated in response to a received request but
rather in response to an event for which the managing entity requires notification.
RFC 3418 defines well-known trap types that include a cold or warm start by a
device, a link going up or down, the loss of a neighbor, or an authentication failure
event. Areceived trap request has no required response from a managing entity.
Given the request-response nature of SNMPv2, it is worth noting here that
although SNMP PDUs can be carried via many different transport protocols, the
SNMP PDU is typically carried in the payload of a UDP datagram. Indeed, RFC
3417 states that UDP is “the preferred transport mapping.” Since UDP is an unreliable
transport protocol, there is no guarantee that a request, or its response, will be
received at the intended destination. The request ID field of the PDU is used by the
managing entity to number its requests to an agent; an agent’s response takes its
request ID from that of the received request. Thus, the request ID field can be used
by the managing entity to detect lost requests or replies. It is up to the managing
entity to decide whether to retransmit a request if no corresponding response is
received after a given amount of time. In particular, the SNMP standard does not
mandate any particular procedure for retransmission, or even if retransmission is to
be done in the first place. It only requires that the managing entity “needs to act
responsibly in respect to the frequency and duration of retransmissions.” This, of
course, leads one to wonder how a “responsible” protocol should act!
9.3.4 Security and Administration
The designers of SNMPv3 have said that “SNMPv3 can be thought of as SNMPv2
with additional security and administration capabilities” [RFC 3410]. Certainly,
there are changes in SNMPv3 over SNMPv2, but nowhere are those changes more
evident than in the area of administration and security. The central role of security
in SNMPv3 was particularly important, since the lack of adequate security resulted
in SNMP being used primarily for monitoring rather than control (for example,
SetRequest is rarely used in SNMPv1).
As SNMP has matured through three versions, its functionality has grown but
so too, alas, has the number of SNMP-related standards documents. This is evidenced
by the fact that there is even now an RFC [RFC 3411] that “describes an
architecture for describing SNMP Management Frameworks”! While the notion of
an “architecture” for “describing a framework” might be a bit much to wrap one’s
mind around, the goal of RFC 3411 is an admirable one—to introduce a common
language for describing the functionality and actions taken by an SNMPv3 agent or
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 775
776 CHAPTER 9 • NETWORK MANAGEMENT
Command
generator
SNMP
applications
SNMP
engine
Notification
receiver
Proxy
forwarder
Dispatching
Messageprocessing
system
Timeliness,
authentication,
privacy
Security
Transport layer
Access
control
Command
generator
Notification
originator Other
PDU
Security/message header PDU
Figure 9.5  SNMPv3 engine and applications
managing entity. The architecture of an SNMPv3 entity is straightforward, and a
tour through the architecture will serve to solidify our understanding of SNMP.
So-called SNMP applications consist of a command generator, notification
receiver, and proxy forwarder (all of which are typically found in a managing
entity); a command responder and notification originator (both of which are typically
found in an agent); and the possibility of other applications. The command
generator generates the GetRequest, GetNextRequest, GetBulkRequest,
and SetRequest PDUs that we examined in Section 9.3.3 and handles the
received responses to these PDUs. The command responder executes in an agent
and receives, processes, and replies (using the Response message) to received
GetRequest, GetNextRequest, GetBulkRequest, and SetRequest
PDUs. The notification originator application in an agent generates Trap PDUs;
these PDUs are eventually received and processed in a notification receiver application
at a managing entity. The proxy forwarder application forwards request, notification,
and response PDUs.
A PDU sent by an SNMP application next passes through the SNMP “engine”
before it is sent via the appropriate transport protocol. Figure 9.5 shows how a PDU
generated by the command generator application first enters the dispatch module,
where the SNMP version is determined. The PDU is then processed in the messageprocessing
system, where the PDU is wrapped in a message header containing the
SNMP version number, a message ID, and message size information. If encryption or
authentication is needed, the appropriate header fields for this information are
included as well; see [RFC 3411] for details. Finally, the SNMP message (the application-
generated PDU plus the message header information) is passed to the appropriate
transport protocol. The preferred transport protocol for carrying SNMP messages is
UDP (that is, SNMP messages are carried as the payload in a UDP datagram), and the
preferred port number for the SNMP is port 161. Port 162 is used for trap messages.
We have seen above that SNMP messages are used not just to monitor, but also
to control (for example, through the SetRequest command) network elements.
Clearly, an intruder that could intercept SNMP messages and/or generate its own
SNMP packets into the management infrastructure could wreak havoc in the network.
Thus, it is crucial that SNMP messages be transmitted securely. Surprisingly,
it is only in the most recent version of SNMP that security has received the attention
that it deserves. SNMPv3 security is known as user-based security [RFC 3414] in
that there is the traditional concept of a user, identified by a username, with which
security information such as a password, key value, or access privileges are associated.
SNMPv3 provides for encryption, authentication, protection against playback
attacks (see Section 8.3), and access control.
• Encryption. SNMP PDUs can be encrypted using the Data Encryption Standard
(DES) in Cipher Block Chaining (CBC) mode. Note that since DES is a sharedkey
system, the secret key of the user encrypting data must be known by the
receiving entity that must decrypt the data.
• Authentication. SNMP uses the Message Authentication Code (MAC) technique
that we studied in Section 8.3.1 to provide both authentication and protection
against tampering [RFC 4301]. Recall that a MAC requires the sender and
receiver both to know a common secret key.
• Protection against playback. Recall from our discussion in Chapter 8 that nonces
can be used to guard against playback attacks. SNMPv3 adopts a related
approach. In order to ensure that a received message is not a replay of some
earlier message, the receiver requires that the sender include a value in each message
that is based on a counter in the receiver. This counter, which functions as a
nonce, reflects the amount of time since the last reboot of the receiver’s network
management software and the total number of reboots since the receiver’s network
management software was last configured. As long as the counter in a
received message is within some margin of error of the receiver’s actual value,
the message is accepted as a nonreplay message, at which point it may be authenticated
and/or decrypted. See [RFC 3414] for details.
• Access control. SNMPv3 provides a view-based access control [RFC 3415] that
controls which network management information can be queried and/or set
by which users. An SNMP entity retains information about access rights and
9.3 • THE INTERNET-STANDARD MANAGEMENT FRAMEWORK 777
778 CHAPTER 9 • NETWORK MANAGEMENT
policies in a Local Configuration Datastore (LCD). Portions of the LCD are
themselves accessible as managed objects, defined in the View-Based Access
Control Model Configuration MIB [RFC 3415], and thus can be managed and
manipulated remotely via SNMP.
9.4 ASN.1
In this book, we have covered a number of interesting topics in computer networking.
This section on ASN.1, however, may not make the top-ten list of interesting topics.
Like vegetables, knowledge about ASN.1 and the broader issue of presentation services
is something that is “good for you.” ASN.1 is an ISO-originated standard that is
used in a number of Internet-related protocols, particularly in the area of network management.
For example, we saw in Section 9.3 that MIB variables in SNMP were inextricably
tied to ASN.1. So while the material on ASN.1 in this section may be rather
dry, we hope the reader will take it on faith that the material is important.
In order to motivate our discussion here, consider the following thought experiment.
Suppose one could reliably copy data from one computer’s memory directly
into a remote computer’s memory. If one could do this, would the communication
problem be “solved?” The answer to the question depends on one’s definition of
“the communication problem.” Certainly, a perfect memory-to-memory copy would
exactly communicate the bits and bytes from one machine to another. But does such
an exact copy of the bits and bytes mean that when software running on the receiving
computer accesses this data, it will see the same values that were stored into the
sending computer’s memory? The answer to this question is “not necessarily!” The
crux of the problem is that different computer architectures, different operating systems,
and different compilers have different conventions for storing and representing
data. If data is to be communicated and stored among multiple computers (as it
is in every communication network), this problem of data representation must
clearly be solved.
As an example of this problem, consider the simple C code fragment below.
How might this structure be laid out in memory?
struct {
char code;
int x;
} test;
test.x = 259;
test.code = ‘a’;
The left side of Figure 9.6 shows a possible layout of this data on one hypothetical
architecture: there is a single byte of memory containing the character a,
followed by a 16-bit word containing the integer value 259, stored with the most
significant byte first. The layout in memory on another computer is shown in the
right half of Figure 9.6. The character a is followed by the integer value stored
with the least significant byte stored first and with the 16-bit integer aligned to
start on a 16-bit word boundary. Certainly, if one were to perform a verbatim
copy between these two computers’ memories and use the same structure definition
to access the stored values, one would see very different results on the two
computers!
The fact that different architectures have different internal data formats is a real
and pervasive problem. The particular problem of integer storage in different formats
is so common that it has a name. “Big-endian” order for storing integers has
the most significant bytes of the integer stored first (at the lowest storage address).
“Little-endian” order stores the least significant bytes first. Sun SPARC and
Motorola processors are big-endian, while Intel processors are little-endian. As an
aside, the terms “big-endian” and “little-endian” come from the book, Gulliver’s
Travels, by Jonathan Swift, in which two groups of people dogmatically insist
on doing a simple thing in two different ways (hopefully, the analogy to the computer
architecture community is clear). One group in the land of Lilliput insists on
breaking their eggs at the larger end (“the big-endians”), while the other insists
on breaking them at the smaller end. The difference was the cause of great civil
strife and rebellion.
Given that different computers store and represent data in different ways,
how should networking protocols deal with this? For example, if an SNMP agent
is about to send a Response message containing the integer count of the number
of received UDP datagrams, how should it represent the integer value to be sent
to the managing entity—in big-endian or little-endian order? One option would
be for the agent to send the bytes of the integer in the same order in which they
would be stored in the managing entity. Another option would be for the agent to
send in its own storage order and have the receiving entity reorder the bytes, as
needed. Either option would require the sender or receiver to learn the other’s
format for integer representation.
9.4 • ASN.1 779
a
00000001
00000011
test.code
test.x
test.code
test.x
a
00000011
00000001
Figure 9.6  Two different data layouts on two different architectures
780 CHAPTER 9 • NETWORK MANAGEMENT
Aging
60’s hippie
Groovy
Grandma 2012 Teenager
Groovy
Figure 9.7  The presentation problem
A third option is to have a machine-independent, OS-independent, languageindependent
method for describing integers and other data types (that is, a datadefinition
language) and rules that state the manner in which each of the data types
is to be transmitted over the network. When data of a given type is received, it is
received in a known format and can then be stored in whatever machine-specific
format is required. Both the SMI that we studied in Section 9.3 and ASN.1 adopt
this third option. In ISO parlance, these two standards describe a presentation
service—the service of transmitting and translating information from one machinespecific
format to another. Figure 9.7 illustrates a real-world presentation problem;
neither receiver understands the essential idea being communicated—that the
speaker likes something. As shown in Figure 9.8, a presentation service can solve
this problem by translating the idea into a commonly understood (by the presentation
service), person-independent language, sending that information to the receiver,
and then translating into a language understood by the receiver.
Table 9.5 shows a few of the ASN.1-defined data types. Recall that we
encountered the INTEGER, OCTET STRING, and OBJECT IDENTIFIER data
types in our earlier study of the SMI. Since our goal here is (mercifully) not to
provide a complete introduction to ASN.1, we refer the reader to the standards or
to the printed and online book [Larmouth 1996] for a description of ASN.1 types
and constructors, such as SEQUENCE and SET, that allow for the definition of
structured data types.
In addition to providing a data definition language, ASN.1 also provides Basic
Encoding Rules (BER) that specify how instances of objects that have been defined
using the ASN.1 data definition language are to be sent over the network. The BER
adopts a so-called TLV (Type, Length, Value) approach to encoding data for
transmission. For each data item to be sent, the data type, the length of the data item,
and then the actual value of the data item are sent, in that order. With this simple
convention, the received data is essentially self-identifying.
Figure 9.9 shows how the two data items in a simple example would be sent.
In this example, the sender wants to send the character string “smith” followed by
the value 259 decimal (which equals 00000001 00000011 in binary, or a byte value
of 1 followed by a byte value of 3), assuming big-endian order. The first byte in the
9.4 • ASN.1 781
Aging
60’s hippie
Presentation
service
Grandma 2012 Teenager
Presentation
service
It is pleasing
Cat’s pajamas Awesome
Groovy
It is pleasing Presentation
service
Figure 9.8  The presentation problem solved
Tag Type Description
1 BOOLEAN value is “true” or “false”
2 INTEGER can be arbitrarily large
3 BITSTRING list of one or more bits
4 OCTET STRING list of one or more bytes
5 NULL no value
6 OBJECT IDENTIFIER name, in the ASN.1 standard naming
tree; see Section 9.2.2
9 REAL floating point
Table 9.5  Selected ASN.1 data types
782 CHAPTER 9 • NETWORK MANAGEMENT
lastname ::= OCTET STRING
weight ::= INTEGER
{weight, 259}
{lastname, "smith"}
Module of data type
declarations written
in ASN.1
Instances of data type
specified in module
Basic Encoding Rules
(BER)
Transmitted
byte stream
3
1
2
2
h
t
i
m
s
5
4
Figure 9.9  BER encoding example
transmitted stream has the value 4, indicating that the type of the following data
item is an OCTET STRING; this is the “T” in the TLV encoding. The second byte
in the stream contains the length of the OCTET STRING, in this case 5. The third
byte in the transmitted stream begins the OCTET STRING of length 5; it contains
the ASCII representation of the letter s. The T, L, and V values of the next data item
are 2 (the INTEGER type tag value), 2 (that is, an integer of length 2 bytes), and
the 2-byte big-endian representation of the value 259 decimal.
In our previous discussion, we have only touched on a small and simple subset
of ASN.1. Resources for learning more about ASN.1 include the ASN.1 standards
document [ISO X.680 2002], the online OSI-related book [Larmouth 2012], and the
ASN.1-related Web sites, [OSS 2012] and [OID Repository 2012].
9.5 Conclusion
Our study of network management, and indeed of all of networking, is now complete!
In this final chapter on network management, we began by motivating the need
for providing appropriate tools for the network administrator—the person whose job
it is to keep the network “up and running”—for monitoring, testing, polling,
configuring, analyzing, evaluating, and controlling the operation of the network.
Our analogies with the management of complex systems such as power plants,
airplanes, and human organization helped motivate this need. We saw that the
architecture of network management systems revolves around five key components:
(1) a network manager, (2) a set of managed remote (from the network manager)
devices, (3) the Management Information Bases (MIBs) at these devices, containing
data about the devices’ status and operation, (4) remote agents that report MIB information
and take action under the control of the network manager, and (5) a protocol
for communication between the network manager and the remote devices.
We then delved into the details of the Internet-Standard Management Framework,
and the SNMP protocol in particular. We saw how SNMP instantiates the five
key components of a network management architecture, and we spent considerable
time examining MIB objects, the SMI—the data definition language for specifying
MIBs, and the SNMP protocol itself. Noting that the SMI and ASN.1 are inextricably
tied together, and that ASN.1 plays a key role in the presentation layer in the
ISO/OSI seven-layer reference model, we then briefly examined ASN.1. Perhaps
more important than the details of ASN.1 itself was the noted need to provide for
translation between machine-specific data formats in a network. While some network
architectures explicitly acknowledge the importance of this service by having
a presentation layer, this layer is absent in the Internet protocol stack.
It is also worth noting that there are many topics in network management that
we chose not to cover—topics such as fault identification and management, proactive
anomaly detection, alarm correlation, and the larger issues of service management
(for example, as opposed to network management). While important, these
topics would form a text in their own right, and we refer the reader to the references
noted in Section 9.1.
Homework Problems and Questions
Chapter 9 Review Questions
SECTION 9.1
R1. Why would a network manager benefit from having network management
tools? Describe five scenarios.
R2. What are the five areas of network management defined by the ISO?
HOMEWORK PROBLEMS AND QUESTIONS 783
784 CHAPTER 9 • NETWORK MANAGEMENT
R3. What is the difference between network management and service
management?
SECTION 9.2
R4. Define the following terms: managing entity, managed device, management
agent, MIB, network management protocol.
SECTION 9.3
R5. What is the role of the SMI in network management?
R6. What is an important difference between a request-response message and a
trap message in SNMP?
R7. What are the seven message types used in SNMP?
R8. What is meant by an “SNMP engine”?
SECTION 9.4
R9. What is the purpose of the ASN.1 object identifier tree?
R10. What is the role of ASN.1 in the ISO/OSI reference model’s presentation layer?
R11. Does the Internet have a presentation layer? If not, how are concerns about
differences in machine architectures—for example, the different representation
of integers on different machines—addressed?
R12. What is meant by TLV encoding?
Problems
P1. Consider the two ways in which communication occurs between a managing
entity and a managed device: request-response mode and trapping. What
are the pros and cons of these two approaches, in terms of (1) overhead,
(2) notification time when exceptional events occur, and (3) robustness
with respect to lost messages between the managing entity and the
device?
P2. In Section 9.3 we saw that it was preferable to transport SNMP messages in
unreliable UDP datagrams. Why do you think the designers of SNMP chose
UDP rather than TCP as the transport protocol of choice for SNMP?
P3. What is the ASN.1 object identifier for the ICMP protocol (see Figure 9.3)?
P4. Suppose you worked for a US-based company that wanted to develop its
own MIB for managing a product line. Where in the object identifier tree
(Figure 9.3) would it be registered? (Hint: You’ll have to do some digging
through RFCs or other documents to answer this question.)
P5. Recall from Section 9.3.2 that a private company (enterprise) can create its
own MIB variables under the private branch 1.3.6.4. Suppose that IBM
wanted to create a MIB for its Web server software. What would be the next
OID qualifier after 1.3.6.1.4? (In order to answer this question, you will need
to consult [IANA 2009b]). Search the Web and see if you can find out
whether such a MIB exists for an IBM server.
P6. Why do you think the length precedes the value in a TLV encoding (rather
than the length following the value)?
P7. Consider Figure 9.9. What would be the BER encoding of {weight, 165}
{lastname, “Michael”}?
P8. Consider Figure 9.9. What would be the BER encoding of {weight, 145}
{lastname, “Sridhar”}?
PROBLEMS 785
Please describe one or two of the most exciting projects you have worked on during your
career. What were the biggest challenges?
When I was a researcher at AT&T, a group of us designed a new way to manage routing in
Internet Service Provider backbone networks. Traditionally, network operators configure
each router individually, and these routers run distributed protocols to compute paths
through the network. We believed that network management would be simpler and more
flexible if network operators could exercise direct control over how routers forward traffic
based on a network-wide view of the topology and traffic. The Routing Control Platform
(RCP) we designed and built could compute the routes for all of AT&T’s backbone on a
single commodity computer, and could control legacy routers without modification. To me,
this project was exciting because we had a provocative idea, a working system, and
ultimately a real deployment in an operational network.
What changes and innovations do you see happening in network management in the
future?
Rather than simply “bolting on” network management on top of existing networks,
researchers and practitioners alike are starting to design networks that are fundamentally
easier to manage. Like our early work on the RCP, the main idea in so-called Software
Defined Networking (SDN) is to run a controller that can install low-level packet-handling
rules in the underlying switches using a standard protocol. This controller can run various
786
Jennifer Rexford
Jennifer Rexford is a Professor in the Computer Science department at
Princeton University. Her research has the broad goal of making computer
networks easier to design and manage, with particular emphasis
on routing protocols. From 1996–2004, she was a member of the
Network Management and Performance department at AT&T Labs--
Research. While at AT&T, she designed techniques and tools for network
measurement, traffic engineering, and router configuration that were deployed in AT&T’s
backbone network. Jennifer is co-author of the book ”Web Protocols and Practice:
Networking Protocols, Caching, and Traffic Measurement,” published by Addison-Wesley in
May 2001. She served as the chair of ACM SIGCOMM from 2003 to 2007. She received
her BSE degree in electrical engineering from Princeton University in 1991, and her MSE
and PhD degrees in electrical engineering and computer science from the University of
Michigan in 1993 and 1996, respectively. In 2004, Jennifer was the winner of ACM’s
Grace Murray Hopper Award for outstanding young computer professional and appeared on
the MIT TR-100 list of top innovators under the age of 35.
AN INTERVIEW WITH...
network-management applications, such as dynamic access control, seamless user mobility,
traffic engineering, server load balancing, energy-efficient networking, and so on. I believe
SDN is a great opportunity to get network management right, by rethinking the relationship
between the network devices and the software that manages them.
Where do you see the future of networking and the Internet?
Networking is an exciting field because the applications and the underlying technologies
change all the time. We are always reinventing ourselves! Who would have predicted even
five or ten years ago the dominance of smart phones, allowing mobile users to access
existing applications as well as new location-based services? The emergence of cloud computing
is fundamentally changing the relationship between users and the applications they
run, and networked sensors are enabling a wealth of new applications. The pace of innovation
is truly inspiring.
The underlying network is a crucial component in all of these innovations. Yet, the
network is notoriously “in the way”—limiting performance, compromising reliability,
constraining applications, and complicating the deployment and management of services.
We should strive to make the network of the future as invisible as the air we breathe, so it
never stands in the way of new ideas and valuable services. To do this, we need to raise the
level of abstraction above individual network devices and protocols (and their attendant
acronyms!), so we can reason about the network as a whole.
What people inspired you professionally?
I’ve long been inspired by Sally Floyd at the International Computer Science Institute.
Her research is always purposeful, focusing on the important challenges facing the Internet.
She digs deeply into hard questions until she understands the problem and the space of
solutions completely, and she devotes serious energy into “making things happen,” such as
pushing her ideas into protocol standards and network equipment. Also, she gives back to
the community, through professional service in numerous standards and research organizations
and by creating tools (such as the widely used ns-2 and ns-3 simulators) that enable
other researchers to succeed. She retired in 2009 but her influence on the field will be felt
for years to come.
What are your recommendations for students who want careers in computer science and
networking?
Networking is an inherently interdisciplinary field. Applying techniques from other disciplines
to networking problems is a great way to move the field forward. We’ve seen tremendous
787
breakthroughs in networking come from such diverse areas as queuing theory, game
theory, control theory, distributed systems, network optimization, programming languages,
machine learning, algorithms, data structures, and so on. I think that becoming conversant
in a related field, or collaborating closely with experts in those fields, is a wonderful
way to put networking on a stronger foundation, so we can learn how to build networks
that are worthy of society’s trust. Beyond the theoretical disciplines, networking is exciting
because we create real artifacts that real people use. Mastering how to design and build
systems—by gaining experience in operating systems, computer architecture, and so
on—is another fantastic way to amplify your knowledge of networking to help change
the world.
788
References
789
A note on URLs. In the references below, we have provided URLs for Web pages, Web-only
documents, and other material that has not been published in a conference or journal (when
we have been able to locate a URL for such material). We have not provided URLs for
conference and journal publications, as these documents can usually be located via a search
engine, from the conference Web site (e.g., papers in all ACM SIGCOMM conferences and
workshops can be located via http://www.acm.org/sigcomm), or via a digital library
subscription. While all URLs provided below were valid (and tested) in Jan. 2012, URLs
can become out of date. Please consult the online version of this book (http://www.awl.com/
kurose-ross) for an up-to-date bibliography.
A note on Internet Request for Comments (RFCs): Copies of Internet RFCs are available
at many sites. The RFC Editor of the Internet Society (the body that oversees the RFCs)
maintains the site, http://www.rfc-editor.org. This site allows you to search for a specific
RFC by title, number, or authors, and will show updates to any RFCs listed. Internet RFCs
can be updated or obsoleted by later RFCs. Our favorite site for getting RFCs is the original
source—http://www.rfc-editor.org.
[3Com Addressing 2012] 3Com Corp., “White paper: Understanding IP addressing:
Everything you ever wanted to know,” http://www.3com.com/other/pdfs/infra/corpinfo/
en_US/501302.pdf
[3GPP 2012] Third Generation Partnership Project homepage, http://www.3gpp.org/
[3GPP Network Architecture 2012] 3GPP, “TS 23.002: Network Architecture: Digital
Cellular Telecommunications System (Phase 2+); Universal Mobile Telecommunications
System (UMTS); LTE,” http://www.3gpp.org/ftp/Specs/html-info/23002.htm
[Albitz 1993] P. Albitz and C. Liu, DNS and BIND, O’Reilly & Associates, Petaluma, CA, 1993.
[Abramson 1970] N. Abramson, “The Aloha System—Another Alternative for Computer
Communications,” Proc. 1970 Fall Joint Computer Conference, AFIPS Conference, p. 37, 1970.
[Abramson 1985] N. Abramson, “Development of the Alohanet,” IEEE Transactions on
Information Theory, Vol. IT-31, No. 3 (Mar. 1985), pp. 119–123.
[Abramson 2009] N. Abramson, “The Alohanet – Surfing for Wireless Data,” IEEE
Communications Magazine, Vol. 47, No. 12, pp. 21–25.
[Abu-Libdeh 2010] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, A. Donnelly,
“Symbiotic Routing in Future Data Centers,” Proc. 2010 ACM SIGCOMM.
[Adhikari 2011a] V. K. Adhikari, S. Jain, Y. Chen, Z. L. Zhang, “Vivisecting YouTube: An
Active Measurement Study,” Technical Report, University of Minnesota, 2011.
[Adhikari 2012] V. K. Adhikari, Y. Gao, F. Hao, M. Varvello, V. Hilt, M. Steiner, Z. L.
Zhang, “Unreeling Netflix: Understanding and Improving Multi-CDN Movie Delivery,”
Technical Report, University of Minnesota, 2012.
[Afanasyev 2010] A. Afanasyev, N. Tilley, P. Reiher, L. Kleinrock, “Host-to-Host Congestion
Control for TCP,” IEEE Communications Surveys & Tutorials, Vol. 12, No. 3, pp. 304–342.
[Agarwal 2009] S. Agarwal, J. Lorch, “Matchmaking for Online Games and Other Latencysensitive
P2P Systems,” Proc. 2009 ACM SIGCOMM.
[Ahn 1995] J. S. Ahn, P. B. Danzig, Z. Liu, and Y. Yan, “Experience with TCPVegas: Emulation
and Experiment,” Proc. 1995 ACM SIGCOMM (Boston, MA, Aug. 1995), pp. 185–195.
[Akamai 2012] Akamai homepage, http://www.akamai.com
[Akella 2003] A. Akella, S. Seshan, A. Shaikh, “An Empirical Evaluation of Wide-Area
Internet Bottlenecks,” Proc. 2003 ACM Internet Measurement Conference (Miami, FL,
Nov. 2003).
[Akhshabi 2011] S. Akhshabi, A. C. Begen, C. Dovrolis, “An Experimental Evaluation of
Rate-Adaptation Algorithms in Adaptive Streaming over HTTP,” Proc. 2011 ACM
Multimedia Systems Conf.
[Akyildiz 2010] I. Akyildiz, D. Gutierrex-Estevez, E. Reyes, “The Evolution to 4G Cellular
Systems, LTE Advanced,” Physical Communication, Elsevier, 3 (2010), 217–244.
[Alcatel-Lucent 2009] Alcatel-Lucent, “Introduction to Evolved Packet Core,”
http://downloads.lightreading.com/wplib/alcatellucent/ALU_WP_Intro_to_EPC.pdf
[Al-Fares 2008] M. Al-Fares, A. Loukissas, A. Vahdat, “A Scalable, Commodity Data
Center Network Architecture,” Proc. 2008 ACM SIGCOMM.
[Alizadeh 2010] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel, B. Prabhakar, S.
Sengupta, M. Sridharan, “Data Center TCP (DCTCP),” Proc. 2010 ACM SIGCOMM.
[Allman 2011] E. Allman, “The Robustness Principle Reconsidered: Seeking a Middle
Ground,” Communications of the ACM, Vol. 54, No. 8 (Aug. 2011), pp. 40–45.
[Anderson 1995] J. B. Andersen, T. S. Rappaport, S. Yoshida, “Propagation Measurements
and Models for Wireless Communications Channels,” IEEE Communications Magazine,
(Jan. 1995), pp. 42–49.
[Andrews 2002] M. Andrews, M. Shepherd, A. Srinivasan, P. Winkler, F. Zane, “Clustering
and Server Election Using Passive Monitoring,” Proc. 2002 IEEE INFOCOM.
[Androutsellis-Theotokis 2004] S. Androutsellis-Theotokis, D. Spinellis, “A Survey of
Peer-to-Peer Content Distribution Technologies,” ACM Computing Surveys, Vol. 36, No. 4
(Dec. 2004), pp. 335–371.
[Aperjis 2008] C. Aperjis, M.J. Freedman, R. Johari, “Peer-Assisted Content Distribution
with Prices,” Proc. ACM CoNEXT’08 (Madrid, Dec. 2008).
[Appenzeller 2004] G. Appenzeller, I. Keslassy, N. McKeown, “Sizing Router Buffers,”
Proc. 2004 ACM SIGCOMM (Portland, OR, Aug. 2004).
[Ash 1998] G. R. Ash, Dynamic Routing in Telecommunications Networks, McGraw Hill,
New York, NY, 1998.
[ASO-ICANN 2012] The Address Supporting Organization home page, http://www.aso
.icann.org
[AT&T SLA 2012] AT&T, “AT&T High Speed Internet Business Edition Service Level
Agreements,” http://www.att.com/gen/general?pid=6622
[Atheros 2012] Atheros Communications Inc. “Atheros AR5006 WLAN Chipset Product
Bulletins,” http://www.atheros.com/pt/AR5006Bulletins.htm
[Augustin 2009] B. Augustin, B. Krishnamurthy, W. Willinger, “IXPs: Mapped?” Proc.
Internet Measurement Conference (IMC), November 2009.
790 REFERENCES
[Ayanoglu 1995] E. Ayanoglu, S. Paul, T. F. La Porta, K. K. Sabnani, R. D. Gitlin,
“AIRMAIL: A Link-Layer Protocol for Wireless Networks,” ACM ACM/Baltzer Wireless
Networks Journal, 1: 47–60, Feb. 1995.
[Bakre 1995] A. Bakre, B. R. Badrinath, “I-TCP: Indirect TCP for Mobile Hosts,” Proc.
1995 Int. Conf. on Distributed Computing Systems (ICDCS) (May 1995), pp. 136–143.
[Balakrishnan 1997] H. Balakrishnan, V. Padmanabhan, S. Seshan, R. Katz, “A Comparison
of Mechanisms for Improving TCP Performance Over Wireless Links,” IEEE/ACM
Transactions on Networking Vol. 5, No. 6 (Dec. 1997).
[Balakrishnan 2003] H. Balakrishnan, F. Kaashoek, D. Karger, R. Morris, I. Stoica,
“Looking Up Data in P2P Systems,” Communications of the ACM, Vol. 46, No. 2 (Feb. 2003),
pp. 43–48.
[Baldauf 2007] M. Baldauf, S. Dustdar, F. Rosenberg, “A Survey on Context-Aware
Systems,” Int. J. Ad Hoc and Ubiquitous Computing, Vol. 2, No. 4 (2007), pp. 263–277.
[Ballani 2006] H. Ballani, P. Francis, S. Ratnasamy, “A Measurement-based Deployment
Proposal for IP Anycast,” Proc. 2006 ACM Internet Measurement Conf.
[Ballani 2011] H. Ballani, P. Costa, T. Karagiannis, Ant Rowstron, “Towards Predictable
Datacenter Networks,” Proc. 2011 ACM SIGCOMM.
[Baran 1964] P. Baran, “On Distributed Communication Networks,” IEEE Transactions on
Communication Systems, Mar. 1964. Rand Corporation Technical report with the same title
(Memorandum RM-3420-PR, 1964). http://www.rand.org/publications/RM/RM3420/
[Bardwell 2004] J. Bardwell, “You Believe You Understand What You Think I Said . . . The
Truth About 802.11 Signal And Noise Metrics: A Discussion Clarifying Often-Misused
802.11 WLAN Terminologies,” http://www.connect802.com/download/techpubs/2004/
you_believe_D100201.pdf
[Barford 2009] P. Barford, N. Duffield, A. Ron, J. Sommers, “ Network Performance
Anomaly Detection and Localization,” Proc. 2009 IEEE INFOCOM (Apr. 2009).
[Baronti 2007] P. Baronti, P. Pillai, V. Chook, S. Chessa, A. Gotta, Y. Hu, “Wireless Sensor
Networks: A Survey on the State of the Art and the 802.15.4 and ZigBee Standards,”
Computer Communications, Vol. 30, No. 7 (2007), pp. 1655–1695.
[Baset 2006] S. A. Basset and H. Schulzrinne, “An analysis of the Skype peer-to-peer
Internet Telephony Protocol,” Proc. 2006 IEEE INFOCOM (Barcelona, Spain, Apr. 2006).
[BBC 2001] BBC news online “A Small Slice of Design,” Apr. 2001, http://news.bbc.co.uk/
2/hi/science/nature/1264205.stm
[BBC 2012] BBC, “Multicast,” http://www.bbc.co.uk/multicast/
[Beheshti 2008] N. Beheshti, Y. Ganjali, M. Ghobadi, N. McKeown, G. Salmon,
“Experimental Study of Router Buffer Sizing,” Proc. ACM Internet Measurement
Conference (October 2008, Vouliagmeni, Greece).
[Bender 2000] P. Bender, P. Black, M. Grob, R. Padovani, N. Sindhushayana, A. Viterbi,
“CDMA/HDR: A bandwidth-efficient high-speed wireless data service for nomadic users,”
IEEE Commun. Mag., Vol. 38, No. 7 (July 2000) pp. 70–77.
[Berners-Lee 1989] T. Berners-Lee, CERN, “Information Management: A Proposal,”
Mar. 1989, May 1990. http://www.w3.org/History/1989/proposal.html
[Berners-Lee 1994] T. Berners-Lee, R. Cailliau, A. Luotonen, H. Frystyk Nielsen, A. Secret,
“The World-Wide Web,” Communications of the ACM, Vol. 37, No. 8 (Aug. 1994), pp. 76–82.
REFERENCES 791
[Bertsekas 1991] D. Bertsekas, R. Gallagher, Data Networks, 2nd Ed., Prentice Hall,
Englewood Cliffs, NJ, 1991.
[Biddle 2003] P. Biddle, P. England, M. Peinado, B. Willman, “The Darknet and the Future
of Content Distribution,” 2002 ACM Workshop on Digital Rights Management, (Nov. 2002,
Washington, D.C.) http://crypto.stanford.edu/DRM2002/darknet5.doc
[Biersack 1992] E. W. Biersack, “Performance evaluation of forward error correction in
ATM networks,” Proc. 1999 ACM SIGCOMM (Baltimore, MD, Aug. 1992), pp. 248–257.
[BIND 2012] Internet Software Consortium page on BIND, http://www.isc.org/bind.html
[Bisdikian 2001] C. Bisdikian, “An Overview of the Bluetooth Wireless Technology,” IEEE
Communications Magazine, No. 12 (Dec. 2001), pp. 86–94.
[Bishop 2003] M. Bishop, Computer Security: Art and Science, Boston: Addison Wesley,
Boston MA, 2003.
[Black 1995] U. Black, ATM Volume I: Foundation for Broadband Networks, Prentice Hall,
1995.
[Black 1997] U. Black, ATM Volume II: Signaling in Broadband Networks, Prentice Hall,
1997.
[Blumenthal 2001] M. Blumenthal, D. Clark, “Rethinking the Design of the Internet: the
End-to-end Arguments vs. the Brave New World,” ACM Transactions on Internet
Technology, Vol. 1, No. 1 (Aug. 2001), pp. 70–109.
[Bochman 1984] G. V. Bochmann, C. A. Sunshine, “Formal methods in communication
protocol design,” IEEE Transactions on Communications, Vol. 28, No. 4 (Apr. 1980)
pp. 624–631.
[Bolot 1994] J-C. Bolot, T. Turletti, “A rate control scheme for packet video in the Internet,”
Proc. 1994 IEEE INFOCOM, pp. 1216–1223.
[Bolot 1996] J-C. Bolot, A. Vega-Garcia, “Control Mechanisms for Packet Audio in the
Internet,” Proc. 1996 IEEE INFOCOM, pp. 232–239.
[Bradner 1996] S. Bradner, A. Mankin, IPng: Internet Protocol Next Generation, Addison-
Wesley, Reading, MA, 1996.
[Brakmo 1995] L. Brakmo, L. Peterson, “TCP Vegas: End to End Congestion Avoidance on
a Global Internet,” IEEE Journal of Selected Areas in Communications, Vol. 13, No. 8
(Oct. 1995), pp. 1465–1480.
[Breslau 2000] L. Breslau, E. Knightly, S. Shenker, I. Stoica, H. Zhang, “Endpoint
Admission Control: Architectural Issues and Performance,” Proc. 2000 ACM SIGCOMM
(Stockholm, Sweden, Aug. 2000).
[Bryant 1988] B. Bryant, “Designing an Authentication System: A Dialogue in Four
Scenes,” http://web.mit.edu/kerberos/www/dialogue.html
[Bush 1945] V. Bush, “As We May Think,” The Atlantic Monthly, July 1945. http://www
.theatlantic.com/unbound/flashbks/computer/bushf.htm
[Byers 1998] J. Byers, M. Luby, M. Mitzenmacher, A. Rege, “A digital fountain approach to
reliable distribution of bulk data,” Proc. 1998 ACM SIGCOMM (Vancouver, Canada, Aug. 1998),
pp. 56–67.
[Cablelabs 2012] CableLabs homepage, http://www.cablelabs.com
[CacheLogic 2012] CacheLogic homepage, http://www.cachelogic.com
792 REFERENCES
[Caesar 2005a] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, J. van der
Merwe, “Design and implementation of a Routing Control Platform,” Proc. Networked
Systems Design and Implementation (May 2005).
[Caesar 2005b] M. Caesar, J. Rexford, “BGP Routing Policies in ISP Networks,” IEEE
Network Magazine, Vol. 19, No. 6 (Nov. 2005).
[Casado 2009] M. Casado, M. Freedman, J. Pettit, J. Luo, N. Gude, N. McKeown, S.
Shenker, “Rethinking Enterprise Network Control,” IEEE/ACM Transactions on Networking
(ToN), Vol. 17, No. 4 (Aug. 2009), pp. 1270–1283.
[Caldwell 2012] C. Caldwell, “The Prime Pages,” http://www.utm.edu/research/primes/prove
[Cardwell 2000] N. Cardwell, S. Savage, T. Anderson, “Modeling TCP Latency,” Proc.
2000 IEEE INFOCOM (Tel-Aviv, Israel, Mar. 2000).
[CASA 2012] Center for Collaborative Adaptive Sensing of the Atmosphere, http://www
.casa.umass.edu
[Casado 2007] M. Casado, M. Freedman, J. Pettit, J. Luo, N. McKeown, S. Shenker,
“Ethane: Taking Control of the Enterprise,” Proc. 2007 ACM SIGCOMM (Kyoto, Japan,
Aug. 2007).
[Casner 1992] S. Casner, S. Deering, “First IETF Internet Audiocast,” ACM SIGCOMM
Computer Communications Review, Vol. 22, No. 3 (July 1992), pp. 92–97.
[Ceiva 2012] Ceiva homepage, http://www.ceiva.com/
[CENS 2012] Center for Embedded Network Sensing, http://www.cens.ucla.edu/
[Cerf 1974] V. Cerf, R. Kahn, “A Protocol for Packet Network Interconnection,” IEEE
Transactions on Communications Technology, Vol. COM-22, No. 5, pp. 627–641.
[CERT 2001–09] CERT, “Advisory 2001–09: Statistical Weaknesses in TCP/IP Initial
Sequence Numbers,” http://www.cert.org/advisories/CA-2001-09.html
[CERT 2003–04] CERT, “CERT Advisory CA-2003-04 MS-SQL Server Worm,” http://
www.cert.org/advisories/CA-2003-04.html
[CERT 2012] CERT Coordination Center, http://www.cert.org/advisories
[CERT Filtering 2012] CERT, “Packet Filtering for Firewall Systems,” http://www.cert.org/
tech_tips/packet_filtering.html
[Cert SYN 1996] CERT, “Advisory CA-96.21: TCP SYN Flooding and IP Spoofing
Attacks,” http://www.cert.org/advisories/CA-1998-01.html
[Chao 2001] H. J. Chao, C. Lam, E. Oki, Broadband Packet Switching Technologies—
A Practical Guide to ATM Switches and IP Routers, John Wiley & Sons, 2001.
[Chao 2011] C. Zhang, P. Dunghel, D. Wu, K. W. Ross, “Unraveling the BitTorrent
Ecosystem,” IEEE Transactions on Parallel and Distributed Systems, Vol. 22, No. 7 (July 2011).
[Chen 2000] G. Chen, D. Kotz, “A Survey of Context-Aware Mobile Computing Research,”
Technical Report TR2000-381, Dept. of Computer Science, Dartmouth College, Nov. 2000.
http://www.cs.dartmouth.edu/reports/TR2000-381.pdf
[Chen 2006] K.-T. Chen, C.-Y. Huang, P. Huang, C.-L. Lei, “Quantifying Skype User
Satisfaction,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006).
[Chen 2010] K. Chen, C. Guo, H. Wu, J. Yuan, Z. Feng, Y. Chen, S. Lu, W. Wu, “Generic
and Automatic Address Configuration for Data Center Networks,” Proc. 2010 ACM
SIGCOMM.
REFERENCES 793
[Chen 2011] Y. Chen, S. Jain, V. K. Adhikari, Z. Zhang, “Characterizing Roles of Front-End
Servers in End-to-End Performance of Dynamic Content Distribution,” Proc. 2011 ACM
Internet Measurement Conference (Berlin, Germany, Nov. 2011).
[Chenoweth 2010] T. Chenoweth, R. Minch, S. Tabor, “Wireless Insecurity: Examining
User Security Behavior on Public Networks,” Communications of the ACM, Vol. 53, No. 2
(Feb. 2010), pp. 134–138.
[Cheswick 2000] B. Cheswick, H. Burch, S. Branigan, “Mapping and Visualizing the
Internet,” Proc. 2000 Usenix Conference (San Diego, CA, June 2000).
[Chiu 1989] D. Chiu, R. Jain, “Analysis of the Increase and Decrease Algorithms for Congestion
Avoidance in Computer Networks,” Computer Networks and ISDN Systems, Vol. 17, No. 1,
pp. 1–14. http://www.cs.wustl.edu/~jain/papers/cong_av.htm
[Christiansen 2001] M. Christiansen, K. Jeffay, D. Ott, F. D. Smith, “Tuning Red for Web
Traffic,” IEEE/ACM Transactions on Networking, Vol. 9, No. 3 (June 2001), pp. 249–264.
[Chu 2002] Y. Chu, S. Rao, S. Seshan, H Zhang, “A Case for End System Multicast,” IEEE
J. Selected Areas in Communications, Vol 20, No. 8 (Oct. 2002), pp. 1456–1471.
[Chuang 2005] S. Chuang, S. Iyer, N. McKeown, “Practical Algorithms for Performance
Guarantees in Buffered Crossbars,” Proc. 2005 IEEE INFOCOM.
[Cicconetti 2006] C. Cicconetti, L. Lenzini, A. Mingozi, K. Eklund, “Quality of Service
Support in 802.16 Networks,” IEEE Network Magazine (Mar./Apr. 2006), pp. 50–55.
[Cisco 12000 2012] Cisco Systems Inc., “Cisco XR 12000 Series and Cisco 12000 Series
Routers,” http://www.cisco.com/en/US/products/ps6342/index.html
[Cisco 8500 2012] Cisco Systems Inc., “Catalyst 8500 Campus Switch Router Architecture,”
http://www.cisco.com/univercd/cc/td/doc/product/l3sw/8540/rel_12_0/w5_6f/softcnfg/
1cfg8500.pdf
[Cisco 2011] Cisco Visual Networking Index: Forecast and Methodology, 2010–2015, White
Paper, 2011.
[Cisco 2012] Cisco 2012, Data Centers, http://www.cisco.com/go/dce
[Cisco NAT 2012] Cisco Systems Inc., “How NATWorks,” http://www.cisco.com/en/US/
tech/tk648/tk361/technologies_tech_note09186a0080094831.shtml
[Cisco QoS 2012] Cisco Systems Inc., “Advanced QoS Services for the Intelligent Internet,”
http://www.cisco.com/warp/public/cc/pd/iosw/ioft/ioqo/tech/qos_wp.htm
[Cisco Queue 2012] Cisco Systems Inc., “Congestion Management Overview,” http://www
.cisco.com/en/US/docs/ios/12_2/qos/configuration/guide/qcfconmg.html
[Cisco Switches 2012] Cisco Systems Inc, “Multiservice Switches,” http://www.cisco.com/
warp/public/cc/pd/si/index.shtml
[Cisco SYN 2012] Cisco Systems Inc., “Defining Strategies to Protect Against TCP SYN
Denial of Service Attacks,” http://www.cisco.com/en/US/tech/tk828/technologies_tech_
note09186a00800f67d5.shtml
[Cisco VNI 2011] Cisco, “Visual Networking Index,” http://www.cisco.com/web/solutions/
sp/vni/vni_forecast_highlights/index.html
[Clark 1988] D. Clark, “The Design Philosophy of the DARPA Internet Protocols,” Proc.
1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).
[Clarke 2002] I. Clarke, T. W. Hong, S. G. Miller, O. Sandberg, B. Wiley, “Protecting
Free Expression Online with Freenet,” IEEE Internet Computing (Jan.–Feb. 2002),
pp. 40–49.
794 REFERENCES
[Cohen 1977] D. Cohen, “Issues in Transnet Packetized Voice Communication,” Proc. Fifth
Data Communications Symposium (Snowbird, UT, Sept. 1977), pp. 6–13.
[Cohen 2003] B. Cohen, “Incentives to Build Robustness in BitTorrent,” First Workshop on
the Economics of Peer-to-Peer Systems (Berkeley, CA, June 2003).
[Cookie Central 2012] Cookie Central homepage, http://www.cookiecentral.com/
n_cookie_faq.htm
[CoolStreaming 2005] X. Zhang, J. Liu, J., B. Li, and T.-S. P. Yum, “CoolStreamingDONet/:
A Data-driven Overlay Network for Peer-to-Peer Live Media Streaming,” Proc. 2005 IEEE
INFOCOM (Miami, FL, Mar. 2005).
[Cormen 2001] T. H. Cormen, Introduction to Algorithms, 2nd Ed., MIT Press, Cambridge,
MA, 2001.
[Crow 1997] B. Crow, I. Widjaja, J. Kim, P. Sakai, “IEEE 802.11 Wireless Local Area
Networks,” IEEE Communications Magazine (Sept. 1997), pp. 116–126.
[Crowcroft 1995] J. Crowcroft, Z. Wang, A. Smith, J. Adams, “A Comparison of the IETF
and ATM Service Models,” IEEE Communications Magazine (Nov./Dec. 1995), pp. 12–16.
[Crowcroft 1999] J. Crowcroft, M. Handley, I. Wakeman, Internetworking Multimedia,
Morgan-Kaufman, San Francisco, 1999.
[Curtis 2011] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, S. Banerjee,
“DevoFlow: Scaling Flow Management for High-Performance Networks,” Proc. 2011 ACM
SIGCOMM.
[Cusumano 1998] M. A. Cusumano, D. B. Yoffie, Competing on Internet Time: Lessons
from Netscape and its Battle with Microsoft, Free Press, New York, NY, 1998.
[Dahlman 1998] E. Dahlman, B. Gudmundson, M. Nilsson, J. Sköld, “UMTS/IMT-2000 Based
on Wideband CDMA,” IEEE Communications Magazine (Sept. 1998), pp. 70–80.
[Daigle 1991] J. N. Daigle, Queuing Theory for Telecommunications, Addison-Wesley,
Reading, MA, 1991.
[Dalal 1978] Y. Dalal, R. Metcalfe, “Reverse Path Forwarding of Broadcast Packets,”
Communications of the ACM, Vol. 21, No. 12 (Dec. 1978), pp. 1040–1048.
[Davie 2000] B. Davie and Y. Rekhter, MPLS: Technology and Applications, Morgan
Kaufmann Series in Networking, 2000.
[Davies 2005] G. Davies, F. Kelly, “Network Dimensioning, Service Costing, and Pricing in
a Packet-Switched Environment,” Telecommunications Policy, Vol. 28, No. 4, pp. 391–412.
[DEC 1990] Digital Equipment Corporation, “In Memoriam: J. C. R. Licklider 1915–1990,”
SRC Research Report 61, Aug. 1990. http://www.memex.org/licklider.pdf
[DeClercq 2002] J. DeClercq, O. Paridaens, “Scalability Implications of Virtual Private
Networks,” IEEE Communications Magazine, Vol. 40, No. 5 (May 2002), pp. 151–157.
[Demers 1990] A. Demers, S. Keshav, S. Shenker, “Analysis and Simulation of a Fair Queuing
Algorithm,” Internetworking: Research and Experience, Vol. 1, No. 1 (1990), pp. 3–26.
[Denning 1997] D. Denning (Editor), P. Denning (Preface), Internet Besieged: Countering
Cyberspace Scofflaws, Addison-Wesley, Reading, MA, 1997.
[dhc 2012] IETF Dynamic Host Configuration working group homepage, http://www.ietf.
org/html.charters/dhc-charter.html
[Dhungel 2012] P. Dhungel, K. W. Ross, M. Steiner., Y. Tian, X. Hei, “Xunlei: Peer-Assisted
Download Acceleration on a Massive Scale,” Passive and Active Measurement Conference
(PAM) 2012, Vienna, 2012.
REFERENCES 795
[Diffie 1976]W. Diffie, M. E. Hellman, “New Directions in Cryptography,” IEEE
Transactions on Information Theory, Vol IT-22 (1976), pp. 644–654.
[Diggavi 2004] S. N. Diggavi, N. Al-Dhahir, A. Stamoulis, R. Calderbank, “Great
Expectations: The Value of Spatial Diversity in Wireless Networks,” Proceedings of the
IEEE, Vol. 92, No. 2 (Feb. 2004).
[Dilley 2002] J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman, B. Weihl, “Globally
Distributed Content Delivert,” IEEE Internet Computing (Sept.–Oct. 2002).
[Ding 2011] Y. Ding, Y. Du, Y. Hu, Z. Liu, L. Wang, K. W. Ross, A. Ghose, “Broadcast
Yourself: Understanding YouTube Uploaders,” Proc. 2011 ACM Internet Measurement
Conference (Berlin).
[Diot 2000] C. Diot, B. N. Levine, B. Lyles, H. Kassem, D. Balensiefen, “Deployment
Issues for the IP Multicast Service and Architecture,” IEEE Network, Vol. 14, No. 1
(Jan./Feb. 2000) pp. 78–88.
[Dischinger 2007] M. Dischinger, A. Haeberlen, K. Gummadi, S. Saroiu, “Characterizing
residential broadband networks,” Proc. 2007 ACM Internet Measurement Conference, pp. 24–26.
[Dmitiropoulos 2007] X. Dmitiropoulos, D. Krioukov, M. Fomenkov, B. Huffaker, Y. Hyun,
KC Claffy, G. Riley, “AS Relationships: Inference and Validation,” ACM Computer
Communication Review (Jan. 2007).
[DOCSIS 2004] Data-over-cable service interface specifications: Radio-frequency interface
specification. ITU-T J.112, 2004.
[DOCSIS 2011] Data-Over-Cable Service Interface Specifications, DOCSIS 3.0: MAC and
Upper Layer Protocols Interface Specification, CM-SP-MULPIv3.0-I16-110623, 2011.
[Dodge 2012] M. Dodge, “An Atlas of Cyberspaces,” http://www.cybergeography.org/atlas/
isp_maps.html
[Donahoo 2001] M. Donahoo, K. Calvert, TCP/IP Sockets in C: Practical Guide for
Programmers, Morgan Kaufman, 2001.
[Doucer 2002] J. R. Douceur, “The Sybil Attack,” First International Workshop on Peer-to-
Peer Systems (IPTPS ’02) (Cambridge, MA, Mar. 2002).
[DSL 2012] DSL Forum homepage, http://www.dslforum.org/
[Dhunghel 2008] P. Dhungel, D. Wu, B. Schonhorst, K.W. Ross, “A Measurement Study of
Attacks on BitTorrent Leechers,” 7th International Workshop on Peer-to-Peer Systems
(IPTPS 2008) (Tampa Bay, FL, Feb. 2008).
[Droms 2002] R. Droms, T. Lemon, The DHCP Handbook (2nd Edition), SAMS Publishing,
2002.
[Edney 2003] J. Edney and W. A. Arbaugh, Real 802.11 Security: Wi-Fi Protected Access
and 802.11i, Addison-Wesley Professional, 2003.
[Edwards 2011]W. K. Edwards, R. Grinter, R. Mahajan, D. Wetherall, “Advancing the State
of Home Networking,” Communications of the ACM, Vol. 54, No. 6 (June 2011), pp. 62–71.
[Eklund 2002] K. Eklund, R. Marks, K. Stanswood, S. Wang, “IEEE Standard 802.16: A
Technical Overview of the Wireless MAN Air Interface for Broadband Wireless Access,”
IEEE Communications Magazine (June 2002), pp. 98–107.
[Ellis 1987] H. Ellis, “The Story of Non-Secret Encryption,” http://jya.com/ellisdoc.htm
[Ericsson 2011] Ericsson, “LTE—An Introduction,” www.ericsson.com/res/docs/2011/
lte_an_introduction.pdf
796 REFERENCES
[Ericsson 2012] Ericsson, “The Evolution of Edge,” http://www.ericsson.com/technology/
whitepapers/broadband/evolution_of_EDGE.shtml
[Estrin 1997] D. Estrin, M. Handley, A. Helmy, P. Huang, D. Thaler, “A Dynamic Bootstrap
Mechanism for Rendezvous-Based Multicast Routing,” Proc. 1998 IEEE INFOCOM
(New York, NY, Apr. 1998).
[Falkner 2007] J. Falkner, M. Piatek, J.P. John, A. Krishnamurthy, T. Anderson, “Profiling a
Million Sser DHT,” Proc. 2007 ACM Internet Measurement Conference.
[Faloutsos 1999] C. Faloutsos, M. Faloutsos, P. Faloutsos, “What Does the Internet Look
Like? Empirical Laws of the Internet Topology,” Proc. 1999 ACM SIGCOMM (Boston, MA,
Aug. 1999).
[Farrington 2010] N. Farrington, G. Porter, S. Radhakrishnan, H. Bazzaz, V. Subramanya,
Y. Fainman, G. Papen, A. Vahdat, “Helios: A Hybrid Electrical/Optical Switch Architecture
for Modular Data Centers,” Proc. 2010 ACM SIGCOMM.
[Feamster 2004] N. Feamster, J. Winick, J. Rexford, “A Model for BGP Routing for
Network Engineering,” Proc. 2004 ACM SIGMETRICS (New York, NY, June 2004).
[Feamster 2005] N. Feamste, H. Balakrishnan, “Detecting BGP Configuration Faults with
Static Analysis,” NSDI (May 2005).
[Feldman 2005] M. Feldman J. Chuang, “Overcoming Free-Riding Behavior in Peer-to-peer
Systems,” ACM SIGecom Exchanges (July 2005).
[Feldmeier 1995] D. Feldmeier, “Fast Software Implementation of Error Detection Codes,”
IEEE/ACM Transactions on Networking, Vol. 3, No. 6 (Dec. 1995), pp. 640–652.
[FIPS 1995] Federal Information Processing Standard, “Secure Hash Standard,” FIPS
Publication 180-1. http://www.itl.nist.gov/fipspubs/fip180-1.htm
[Floyd 1999] S. Floyd, K. Fall, “Promoting the Use of End-to-End Congestion Control
in the Internet,” IEEE/ACM Transactions on Networking, Vol. 6, No. 5 (Oct. 1998),
pp. 458–472.
[Floyd 2000] S. Floyd, M. Handley, J. Padhye, J. Widmer, “Equation-Based Congestion Control
for Unicast Applications,” Proc. 2000 ACM SIGCOMM (Stockholm, Sweden, Aug. 2000).
[Floyd 2001] S. Floyd, “A Report on Some Recent Developments in TCP Congestion
Control,” IEEE Communications Magazine (Apr. 2001).
[Floyd 2012] S. Floyd, “References on RED (Random Early Detection) Queue
Management,” http://www.icir.org/floyd/red.html
[Floyd Synchronization 1994] S. Floyd, V. Jacobson, “Synchronization of Periodic
Routing Messages,” IEEE/ACM Transactions on Networking, Vol. 2, No. 2 (Apr. 1997)
pp. 122–136.
[Floyd TCP 1994] S. Floyd, “TCP and Explicit Congestion Notification,” ACM SIGCOMM
Computer Communications Review, Vol. 24, No. 5 (Oct. 1994), pp. 10–23.
[Fluhrer 2001] S. Fluhrer, I. Mantin, A. Shamir, “Weaknesses in the Key Scheduling
Algorithm of RC4,” Eighth Annual Workshop on Selected Areas in Cryptography, (Toronto,
Canada, Aug. 2002).
[Fortz 2000] B. Fortz, M. Thorup, “Internet Traffic Engineering by Optimizing OSPF
Weights,” Proc. 2000 IEEE INFOCOM (Tel Aviv, Israel, Apr. 2000).
[Fortz 2002] B. Fortz, J. Rexford, M. Thorup, “Traffic Engineering with Traditional IP
Routing Protocols,” IEEE Communication Magazine (Oct. 2002).
REFERENCES 797
[Fraleigh 2003] C. Fraleigh, F. Tobagi, C. Diot, “Provisioning IP Backbone Networks to Support
Latency Sensitive Traffic,” Proc. 2003 IEEE INFOCOM (San Francisco, CA, Mar. 2003).
[Freedman 2004] M. J. Freedman, E. Freudenthal, D. Mazires, “Democratizing Content
Publication with Coral,” USENIX NSDI, 2004.
[Friedman 1999] T. Friedman, D. Towsley “Multicast Session Membership Size
Estimation,” Proc. 1999 IEEE INFOCOM (New York, NY, Mar. 1999).
[Frost 1994] J. Frost, “BSD Sockets: A Quick and Dirty Primer,” http://world.std.com/~jimf/
papers/sockets/sockets.html
[FTTH Council 2011a] FTTH Council, “NORTH AMERICAN FTTH STATUS—MARCH
31, 2011” (March 2011), www.ftthcouncil.org
[FTTH Council 2011b] FTTH Council, “2011 Broadband Consumer Research” (June 2011),
www.ftthcouncil.org
[Gallagher 1983] R. G. Gallagher, P. A. Humblet, P. M. Spira, “A Distributed Algorithm for
Minimum Weight-Spanning Trees,” ACM Trans. on Programming Languages and Systems,
Vol. 1, No. 5 (Jan. 1983), pp. 66–77.
[Gao 2001] L. Gao, J. Rexford, “Stable Internet Routing Without Global Coordination,”
IEEE/ACM Transactions on Networking, Vol. 9, No. 6 (Dec. 2001), pp. 681–692.
[Garces-Erce 2003] L. Garces-Erce, K. W. Ross, E. Biersack, P. Felber, G. Urvoy-Keller,
“TOPLUS: Topology Centric Lookup Service,” Fifth Int. Workshop on Networked Group
Communications (NGC 2003) (Munich, Sept. 2003) http://cis.poly.edu/~ross/papers/TOPLUS.pdf
[Gartner 2003] F. C. Gartner, “A Survey of Self-Stabilizing Spanning-Tree Construction
Algorithms,” Technical Report IC/2003/38, Swiss Federal Institute of Technology (EPFL),
School of Computer and Communication Sciences, June 10, 2003. http://ic2.epfl.ch/
publications/documents/IC_TECH_REPORT_200338.pdf
[Gauthier 1999] L. Gauthier, C. Diot, and J. Kurose, “End-to-end Transmission Control
Mechanisms for Multiparty Interactive Applications on the Internet,” Proc. 1999 IEEE
INFOCOM (New York, NY, Apr. 1999).
[Girard 1990] A. Girard, Routing and Dimensioning in Circuit-Switched Networks,
Addison-Wesley, Reading, MA, 1990.
[Glitho 1998] R. Glitho, “Contrasting OSI Systems Management to SNMP and TMN,”
Journal of Network and Systems Management, Vol. 6, No. 2 (June 1998), pp. 113–131.
[Gnutella 2009] “The Gnutella Protocol Specification, v0.4” http://www9.limewire.com/
developer/gnutella_protocol_0.4.pdf
[Goodman 1997] David J. Goodman, Wireless Personal Communications Systems, Prentice-
Hall, 1997.
[Google Locations 2012] Google data centers. http://www.google.com/corporate/
datacenter/locations.html
[Goralski 1999]W. Goralski, Frame Relay for High-Speed Networks, John Wiley, New
York, 1999.
[Goralski 2001]W. Goralski, Optical Networking and WDM, Osborne/McGraw-Hill,
Berkeley, CA, 2001.
[Greenberg 2009a] A. Greenberg, J. Hamilton, D. Maltz, P. Patel, “The Cost of a Cloud:
Research Problems in Data Center Networks,” ACM Computer Communications Review
(Jan. 2009).
798 REFERENCES
[Greenberg 2009b] A. Greenberg, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz,
P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” Proc. 2009
ACM SIGCOMM.
[Greenberg 2011] A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D.
Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,”
Communications of the ACM, Vol. 54, No. 3 (Mar. 2011), pp. 95–104.
[Griffin 2012] T. Griffin, “Interdomain Routing Links,” http://www.cl.cam.ac.uk/~tgg22/
interdomain/
[Guha 2006] S. Guha, N. Daswani, R. Jain, “An Experimental Study of the Skype Peer-to-
Peer VoIP System,” Proc. Fifth Int. Workshop on P2P Systems (Santa Barbara, CA, 2006).
[Guo 2005] L. Guo, S. Chen, Z. Xiao, E. Tan, X. Ding, X. Zhang, “Measurement, Analysis,
and Modeling of BitTorrent-Like Systems,” Proc. 2005 ACM Internet Measurement
Conference.
[Guo 2009] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, S. Lu,
“BCube: A High Performance, Server-centric Network Architecture for Modular Data
Centers,” Proc. 2009 ACM SIGCOMM.
[Gupta 2001] P. Gupta, N. McKeown, “Algorithms for Packet Classification,” IEEE
Network Magazine, Vol. 15, No. 2 (Mar./Apr. 2001), pp. 24–32.
[Ha 2008] Ha, S., Rhee, I., L. Xu, “CUBIC: A New TCP-Friendly High-Speed TCP Variant,”
ACM SIGOPS Operating System Review, 2008.
[Halabi 2000] S. Halabi, Internet Routing Architectures, 2nd Ed., Cisco Press, 2000.
[Halperin 2008] D. Halperin, T. Heydt-Benjamin, B. Ransford, S. Clark, B. Defend, W.
Morgan, K. Fu, T. Kohno, W. Maisel, “Pacemakers and implantable cardiac defibrillators:
Software radio attacks and zero-power defenses,” Proc. 29th Annual IEEE Symposium on
Security and Privacy (May 2008).
[Halperin 2011] D. Halperin, S. Kandula, J. Padhye, P. Bahl, D. Wetherall, “Augmenting
Data Center Networks with Multi-Gigabit Wireless Links,” Proc. 2011 ACM SIGCOMM.
[Hanabali 2005] A. A. Hanbali, E. Altman, P. Nain, “A Survey of TCP over Ad Hoc
Networks,” IEEE Commun. Surveys and Tutorials, Vol. 7, No. 3 (2005), pp. 22–36.
[Hei 2007] X. Hei, C. Liang, J. Liang, Y. Liu, K. W. Ross, “A Measurement Study of a
Large-scale P2P IPTV System,” IEEE Trans. on Multimedia (Dec. 2007).
[Heidemann 1997] J. Heidemann, K. Obraczka, J. Touch, “Modeling the Performance of
HTTP over Several Transport Protocols,” IEEE/ACM Transactions on Networking, Vol. 5,
No. 5 (Oct. 1997), pp. 616–630.
[Held 2001] G. Held, Data Over Wireless Networks: Bluetooth, WAP, and Wireless LANs,
McGraw-Hill, 2001.
[Hersent 2000] O. Hersent, D. Gurle, J-P. Petit, IP Telephony: Packet-Based Multimedia
Communication Systems, Pearson Education Limited, Edinburgh, 2000.
[Holland 2001] G. Holland, N. Vaidya, V. Bahl, “A Rate-Adaptive MAC Protocol for Multi-
Hop Wireless Networks,” Proc. 2001 ACM Int. Conference of Mobile Computing and
Networking (Mobicom01) (Rome, Italy, July 2001).
[Hollot 2002] C.V. Hollot, V. Misra, D. Towsley, W. Gong, “Analysis and design of
controllers for AQM routers supporting TCP flows,” IEEE Transactions on Automatic
Control, Vol. 47, No. 6 (June 2002), pp. 945–959.
REFERENCES 799
[Huang 2002] C. Haung, V. Sharma, K. Owens, V. Makam, “Building Reliable MPLS
Networks Using a Path Protection Mechanism,” IEEE Communications Magazine, Vol. 40,
No. 3 (Mar. 2002), pp. 156–162.
[Huang 2005] Y. Huang, R. Guerin, “Does Over-Provisioning Become More or Less
Efficient as Networks Grow Larger?,” Proc. IEEE Int. Conf. Network Protocols (ICNP)
(Boston MA, November 2005).
[Huang 2007] C. Huang, Jin Li, K.W. Ross, “Can Internet VoD Be Profitable?,” Proc 2007
ACM SIGCOMM (Kyoto, Aug. 2007).
[Huang 2008] C. Huang, J. Li, A. Wang, K. W. Ross, “Understanding Hybrid CDN-P2P:
Why Limelight Needs its Own Red Swoosh,” Proc. 2008 NOSSDAV, Braunschweig,
Germany.
[Huang 2010] C. Huang, N. Holt, Y. A. Wang, A. Greenberg, J. Li, K. W. Ross, “A DNS
Reflection Method for Global Traffic Management,” Proc. 2010 USENIX, Boston.
[Huitema 1998] C. Huitema, IPv6: The New Internet Protocol, 2nd Ed., Prentice Hall,
Englewood Cliffs, NJ, 1998.
[Huston 1999a] G. Huston, “Interconnection, Peering, and Settlements—Part I,” The
Internet Protocol Journal, Vol. 2, No. 1 (Mar. 1999).
[Huston 2004] G. Huston, “NAT Anatomy: A Look Inside Network Address Translators,”
The Internet Protocol Journal, Vol. 7, No. 3 (Sept. 2004).
[Huston 2008a] G. Huston, “Confronting IPv4 Address Exhaustion,” http://www.potaroo
.net/ispcol/2008-10/v4depletion.html
[Huston 2008b] G. Huston, G. Michaelson, “IPv6 Deployment: Just where are we?” http://
www.potaroo.net/ispcol/2008-04/ipv6.html
[Huston 2011a] G. Huston, “A Rough Guide to Address Exhaustion,” The Internet Protocol
Journal, Vol. 14, No. 1 (Mar. 2011).
[Huston 2011b] G. Huston, “Transitioning Protocols,” The Internet Protocol Journal, Vol.
14, No. 1 (Mar. 2011).
[IAB 2012] Internet Architecture Board homepage, http://www.iab.org/
[IANA 2012a] Internet Assigned Number Authority homepage, http://www.iana.org/
[IANA 2012b] Internet Assigned Number Authority, “Private Enterprise Numbers” http://
www.iana.org/assignments/enterprise-numbers
[IANA Protocol Numbers 2012] Internet Assigned Numbers Authority, Protocol Numbers,
http://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
[IANA TLD 2012] IANA Root Zone Database, http://www.iana.org/domains/root/db/
[ICANN 2012] The Internet Corporation for Assigned Names and Numbers homepage,
http://www.icann.org
[IEC Optical 2012] IEC Online Education, “Optical Access,” http://www.iec.org/online/
tutorials/opt_acc/
[IEEE 802 2012] IEEE 802 LAN/MAN Standards Committee homepage, http://www
.ieee802.org/
[IEEE 802.11 1999] IEEE 802.11, “1999 Edition (ISO/IEC 8802-11: 1999) IEEE Standards
for Information Technology—Telecommunications and Information Exchange Between
Systems—Local and Metropolitan Area Network—Specific Requirements—Part 11:
800 REFERENCES
Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specification,”
http://standards.ieee.org/getieee802/download/802.11-1999.pdf
[IEEE 802.11n 2012] IEEE, “IEEE P802.11—Task Group N—Meeting Update: Status of
802.11n,” http://grouper.ieee.org/groups/802/11/Reports/tgn_update.htm
[IEEE 802.15 2012] IEEE 802.15 Working Group for WPAN homepage, http://grouper.ieee.
org/groups/802/15/.
[IEEE 802.15.4 2012] IEEE 802.15 WPAN Task Group 4, http://www.ieee802.org/15/
pub/TG4.html
[IEEE 802.16d 2004] IEEE, “IEEE Standard for Local and Metropolitan Area Networks,
Part 16: Air Interface for Fixed Broadband Wireless Access Systems,” http://standards.ieee.
org/getieee802/download/802.16-2004.pdf
[IEEE 802.16e 2005] IEEE, “IEEE Standard for Local and Metropolitan Area Networks, Part 16:
Air Interface for Fixed and Mobile Broadband Wireless Access Systems, Amendment 2: Physical
and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed
Bands and Corrigendum 1,” http://standards.ieee.org/getieee802/download/802 .16e-2005.pdf
[IEEE 802.1q 2005] IEEE, “IEEE Standard for Local and Metropolitan Area Networks:
Virtual Bridged Local Area Networks,” http://standards.ieee.org/getieee802/download/802
.1Q-2005.pdf
[IEEE 802.1X] IEEE Std 802.1X-2001 Port-Based Network Access Control, http://
standards.ieee.org/reading/ieee/std_public/description/lanman/802.1x-2001_desc.html
[IEEE 802.3 2012] IEEE, “IEEE 802.3 CSMA/CD (Ethernet),” http://grouper.ieee.org/
groups/802/3/
[IEEE 802.5 2012] IEEE, IEEE 802.5 homepage, http://www.ieee802.org/5/www8025org/
[IETF 2012] Internet Engineering Task Force homepage, http://www.ietf.org
[Ihm 2011] S. Ihm, V. S. Pai, “Towards Understanding Modern Web Traffic,” Proc. 2011
ACM Internet Measurement Conference (Berlin).
[IMAP 2012] The IMAP Connection, http://www.imap.org/
[Intel 2012] Intel Corp, “Intel® 82544 Gigabit Ethernet Controller,” http://www.intel.com/
design/network/products/lan/docs/82544_docs.htm
[Intel WiMax 2012] Intel Corp., “WiMax Technology,” http://www.intel.com/technology/
wimax/index.htm
[Internet2 Multicast 2012] Internet2 Multicast Working Group homepage, http://www
.internet2.edu/multicast/
[IPv6 2012] IPv6.com homepage, http://www.ipv6.com/
[ISC 2012] Internet Systems Consortium homepage, http://www.isc.org
[ISI 1979] Information Sciences Institute, “DoD Standard Internet Protocol,” Internet
Engineering Note 123 (Dec. 1979), http://www.isi.edu/in-notes/ien/ien123.txt
[ISO 2012] International Organization for Standardization homepage, International
Organization for Standardization, http://www.iso.org/
[ISO X.680 2002] International Organization for Standardization, “X.680: ITU-T
Recommendation X.680 (2002) Information Technology—Abstract Syntax Notation One
(ASN.1): Specification of Basic Notation.” http://www.itu.int/ITU-T/studygroups/com17/
languages/X.680-0207.pdf
REFERENCES 801
[ITU 1999] Asymmetric Digital Subscriber Line (ADSL) Transceivers. ITU-T G.992.1, 1999.
[ITU 2003] Asymmetric Digital Subscriber Line (ADSL) Transceivers—Extended
Bandwidth ADSL2 (ADSL2Plus). ITU-T G.992.5, 2003.
[ITU 2005a] International Telecommunication Union, “ITU-T X.509, The Directory: Publickey
and attribute certificate frameworks” (August 2005).
[ITU 2005b] International Telecommunication Union, The Internet of Things, 2005, http://
www.itu.int/osg/spu/publications/internetofthings/InternetofThings_summary.pdf
[ITU 2012] The ITU homepage, http://www.itu.int/
[ITU Statistics 2012] International Telecommunications Union, “ICT Statistics,” http://
www.itu.int/ITU-D/icteye/Reports.aspx
[ITU 2011] ITU, “Measuring the Information Society, 2011,” http://www.itu.int/ITUD/
ict/publications/idi/2011/index.html
[ITU 2011] ITU, “The World in 2010: ICT Facts and Figures,” http://www.itu.int/ITUD/
ict/material/Telecom09_flyer.pdf
[ITU-T Q.2931 1995] International Telecommunication Union, “Recommendation Q.2931
(02/95) - Broadband Integrated Services Digital Network (B-ISDN)—Digital subscriber
signalling system no. 2 (DSS 2)—User-network interface (UNI)—Layer 3 specification for
basic call/connection control.”
[Iyer 2002] S. Iyer, R. Zhang, N. McKeown, “Routers with a Single Stage of Buffering,”
Proc. 2002 ACM SIGCOMM (Pittsburgh, PA, Aug. 2002).
[Iyer 2008] S. Iyer, R. R. Kompella, N. McKeown, “Designing Packet Buffers for Router
Line Cards,” IEEE Transactions on Networking, Vol. 16, No. 3 (June 2008), pp. 705–717.
[Jacobson 1988] V. Jacobson, “Congestion Avoidance and Control,” Proc. 1988 ACM
SIGCOMM (Stanford, CA, Aug. 1988), pp. 314–329.
[Jain 1986] R. Jain, “A timeout-based congestion control scheme for window flow-controlled
networks,” IEEE Journal on Selected Areas in Communications SAC-4, 7 (Oct. 1986).
[Jain 1989] R. Jain, “A Delay-Based Approach for Congestion Avoidance in Interconnected
Heterogeneous Computer Networks,” ACM SIGCOMM Computer Communications Review,
Vol. 19, No. 5 (1989), pp. 56–71.
[Jain 1994] R. Jain, FDDI Handbook: High-Speed Networking Using Fiber and Other
Media, Addison-Wesley, Reading, MA, 1994.
[Jain 1996] R. Jain. S. Kalyanaraman, S. Fahmy, R. Goyal, S. Kim, “Tutorial Paper on ABR
Source Behavior,” ATM Forum/96-1270, Oct. 1996. http://www.cse.wustl.edu/~jain/atmf/ftp/
atm96-1270.pdf
[Jaiswal 2003] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, D. Towsley, “Measurement and
Classification of Out-of-Sequence Packets in a Tier-1 IP backbone,” Proc. 2003 IEEE INFOCOM.
[Ji 2003] P. Ji, Z. Ge, J. Kurose, D. Towsley, “A Comparison of Hard-State and Soft-State
Signaling Protocols,” Proc. 2003 ACM SIGCOMM (Karlsruhe, Germany, Aug. 2003).
[Jiang 2001] W. Jiang, J. Lennox, H. Schulzrinne, K. Singh, “Towards Junking the PBX:
Deploying IP Telephony,” NOSSDAV’01 (Port Jefferson, NY, June 2001).
[Jimenez 1997] D. Jimenez, “Outside Hackers Infiltrate MIT Network, Compromise Security,”
The Tech, Vol. 117, No 49 (Oct. 1997), p. 1, http://www-tech.mit.edu/V117/N49/hackers.49n.html
[Jin 2004] C. Jin, D. X. We, S. Low, “FAST TCP: Motivation, architecture, algorithms,
performance,” Proc. 2004 IEEE INFOCOM (Hong Kong, March 2004).
802 REFERENCES
[Kaaranen 2001] H. Kaaranen, S. Naghian, L. Laitinen, A. Ahtiainen, V. Niemi, Networks:
Architecture, Mobility and Services, New York: John Wiley & Sons, 2001.
[Kahn 1967] D. Kahn, The Codebreakers: The Story of Secret Writing, The Macmillan
Company, 1967.
[Kahn 1978] R. E. Kahn, S. Gronemeyer, J. Burchfiel, R. Kunzelman, “Advances in Packet
Radio Technology,” Proc. 1978 IEEE INFOCOM, 66, 11 (Nov. 1978).
[Kamerman 1997] A. Kamerman, L. Monteban, “WaveLAN-II: AHigh–Performance Wireless
LAN for the Unlicensed Band,” Bell Labs Technical Journal (Summer 1997), pp. 118–133.
[Kangasharju 2000] J. Kangasharju, K. W. Ross, J. W. Roberts, “Performance Evaluation of
Redirection Schemes in Content Distribution Networks,” Proc. 5th Web Caching and
Content Distribution Workshop (Lisbon, Portugal, May 2000).
[Kar 2000] K. Kar, M. Kodialam, T. V. Lakshman, “Minimum Interference Routing of
Bandwidth Guaranteed Tunnels with MPLS Traffic Engineering Applications,” IEEE J.
Selected Areas in Communications (Dec. 2000).
[Karn 1987] P. Karn, C. Partridge, “Improving Round-Trip Time Estimates in Reliable
Transport Protocols,” Proc. 1987 ACM SIGCOMM.
[Karol 1987] M. Karol, M. Hluchyj, A. Morgan, “Input Versus Output Queuing on a Space-
Division Packet Switch,” IEEE Transactions on Communications, Vol. 35, No. 12 (Dec.
1987), pp. 1347–1356.
[Katabi 2002] D. Katabi, M. Handley, C. Rohrs, “Internet Congestion Control for Future
High Bandwidth-Delay Product Environments,” Proc. 2002 ACM SIGCOMM (Pittsburgh,
PA, Aug. 2002).
[Katzela 1995] I. Katzela, M. Schwartz. “Schemes for Fault Identification in
Communication Networks,” IEEE/ACM Transactions on Networking, Vol. 3, No. 6
(Dec. 1995), pp. 753–764.
[Kaufman 1995] C. Kaufman, R. Perlman, M. Speciner, Network Security, Private
Communication in a Public World, Prentice Hall, Englewood Cliffs, NJ, 1995.
[Kelly 1998] F. P. Kelly, A. Maulloo, D. Tan, “Rate control for communication networks:
Shadow prices, proportional fairness and stability,” J. Operations Res. Soc., Vol. 49, No. 3
(Mar. 1998), pp. 237–252.
[Kelly 2003] T. Kelly, “Scalable TCP: improving performance in high speed wide area
networks,” ACM SIGCOMM Computer Communications Review, Volume 33, No. 2
(Apr. 2003), pp 83–91.
[Kilkki 1999] K. Kilkki, Differentiated Services for the Internet, Macmillan Technical
Publishing, Indianapolis, IN, 1999.
[Kim 2005] H. Kim, S. Rixner, V. Pai, “Network Interface Data Caching,” IEEE
Transactions on Computers, Vol. 54, No. 11 (Nov. 2005), pp. 1394–1408.
[Kim 2008] C. Kim, M. Caesar, J. Rexford, “Floodless in SEATTLE: AScalable Ethernet
Architecture for Large Enterprises,” Proc. 2008 ACM SIGCOMM (Seattle, WA, Aug. 2008).
[Kleinrock 1961] L. Kleinrock, “Information Flow in Large Communication Networks,”
RLE Quarterly Progress Report, July 1961.
[Kleinrock 1964] L. Kleinrock, 1964 Communication Nets: Stochastic Message Flow and
Delay, McGraw-Hill, New York, NY, 1964.
[Kleinrock 1975] L. Kleinrock, Queuing Systems, Vol. 1, John Wiley, New York, 1975.
REFERENCES 803
[Kleinrock 1975b] L. Kleinrock, F. A. Tobagi, “Packet Switching in Radio Channels: Part
I—Carrier Sense Multiple-Access Modes and Their Throughput-Delay Characteristics,”
IEEE Transactions on Communications, Vol. 23, No. 12 (Dec. 1975), pp. 1400–1416.
[Kleinrock 1976] L. Kleinrock, Queuing Systems, Vol. 2, John Wiley, New York, 1976.
[Kleinrock 2004] L. Kleinrock, “The Birth of the Internet,” http://www.lk.cs.ucla.edu/LK/
Inet/birth.html
[Kohler 2006] E. Kohler, M. Handley, S. Floyd, “DDCP: Designing DCCP: Congestion
Control Without Reliability,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006).
[Kolding 2003] T. Kolding, K. Pedersen, J. Wigard, F. Frederiksen, P.Mogensen, “High
Speed Downlink Packet Access: WCDMAEvolution,” IEEE Vehicular Technology Society
News (Feb. 2003), pp. 4–10.
[Koponen 2011] T. Koponen, S. Shenker, H. Balakrishnan, N. Feamster, I. Ganichev, A.
Ghodsi, P. B. Godfrey, N. McKeown, G. Parulkar, B. Raghavan, J. Rexford, S. Arianfar, D.
Kuptsov, “Architecting for Innovation,” ACM Computer Communications Review, 2011.
[Korhonen 2003] J. Korhonen, Introduction to 3G Mobile Communications, 2nd ed., Artech
House, 2003.
[Koziol 2003] J. Koziol, Intrusion Detection with Snort, Sams Publishing, 2003.
[Krishnamurthy 2001] B. Krishnamurthy, and J. Rexford, Web Protocols and Practice: HTTP/
1.1, Networking Protocols, and Traffic Measurement, Addison-Wesley, Boston, MA, 2001.
[Krishnamurthy 2001b] B. Krishnamurthy, C. Wills, Y. Zhang, “On the Use and Performance
of Content Distribution Networks,” Proc. 2001 ACM Internet Measurement Conference.
[Krishnan 2009] R. Krishnan, H. Madhyastha, S. Srinivasan, S. Jain, A. Krishnamurthy, T.
Anderson, J. Gao, “Moving Beyond End-to-end Path Information to Optimize CDN
Performance,” Proc. 2009 ACM Internet Measurement Conference.
[Kulkarni 2005] S. Kulkarni, C. Rosenberg, “Opportunistic Scheduling: Generalizations to
Include Multiple Constraints, Multiple Interfaces, and Short Term Fairness,” Wireless
Networks, 11 (2005), 557–569.
[Kumar 2006] R. Kumar, K.W. Ross, “Optimal Peer-Assisted File Distribution: Single and
Multi-Class Problems,” IEEE Workshop on Hot Topics in Web Systems and Technologies
(Boston, MA, 2006).
[Labovitz 1997] C. Labovitz, G. R. Malan, F. Jahanian, “Internet Routing Instability,” Proc.
1997 ACM SIGCOMM (Cannes, France, Sept. 1997), pp. 115–126.
[Labovitz 2010] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, F. Jahanian,
“Internet Inter-Domain Traffic,” Proc. 2010 ACM SIGCOMM.
[Labrador 1999] M. Labrador, S. Banerjee, “Packet Dropping Policies for ATM and IP
Networks,” IEEE Communications Surveys, Vol. 2, No. 3 (Third Quarter 1999), pp. 2–14.
[Lacage 2004] M. Lacage, M.H. Manshaei, T. Turletti, “IEEE 802.11 Rate Adaptation: A
Practical Approach,” ACM Int. Symposium on Modeling, Analysis, and Simulation of
Wireless and Mobile Systems (MSWiM) (Venice, Italy, Oct. 2004).
[Lakhina 2004] A. Lakhina, M. Crovella, C. Diot, “Diagnosing Network-Wide Traffic
Anomalies,” Proc. 2004 ACM SIGCOMM.
[Lakhina 2005] A. Lakhina, M. Crovella, C. Diot, “Mining Anomalies Using Traffic Feature
Distributions,” Proc. 2005 ACM SIGCOMM.
804 REFERENCES
[Lakshman 1997] T. V. Lakshman, U. Madhow, “The Performance of TCP/IP for Networks
with High Bandwidth-Delay Products and Random Loss,” IEEE/ACM Transactions on
Networking, Vol. 5, No. 3 (1997), pp. 336–350.
[Lam 1980] S. Lam, “A Carrier Sense Multiple Access Protocol for Local Networks,”
Computer Networks, Vol. 4 (1980), pp. 21–32.
[Larmouth 1996] J. Larmouth, Understanding OSI, International Thomson Computer Press
1996. Chapter 8 of this book deals with ASN.1 and is available online at http://www.salford
.ac.uk/iti/books/osi/all.html#head8.
[Larmouth 2012] J. Larmouth, Understanding OSI, http://www.business.salford.ac.uk/
legacy/isi/books/osi/osi.html
[Lawton 2001] G. Lawton, “Is IPv6 Finally Gaining Ground?” IEEE Computer Magazine
(Aug. 2001), pp. 11–15.
[LeBlond 2011] S. LeBlond, C. Zhang, A. Legout, K. W. Ross, W. Dabbous, “Exploring the
Privacy Limits of Real-Time Communication Applications,” Proc. 2011 ACM Internet
Measurement Conference (Berlin, 2011).
[LeBlond 2011] S. LeBlond, C. Zhang, A. Legout, K. W. Ross, W. Dabbous, “I Know Where
You and What You Are Sharing: Exploiting P2P Communications to Invade Users Privacy,”
Proc. 2011 ACM Internet Measurement Conference (Berlin).
[Leighton 2009] T. Leighton, “Improving Performance on the Internet,” Communications of
the ACM, Vol. 52, No. 2 (Feb. 2009), pp. 44–51.
[Leiner 1998] B. Leiner, V. Cerf, D. Clark, R. Kahn, L. Kleinrock, D. Lynch, J. Postel, L.
Roberts, S. Woolf, “A Brief History of the Internet,” http://www.isoc.org/internet/history/
brief.html
[Leung 2006] K. Leung, V. O.K. Li, “TCP in Wireless Networks: Issues, Approaches, and
Challenges,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 4 (2006), pp. 64–79.
[Li 2004] L. Li, D. Alderson, W. Willinger, J. Doyle, “A First-Principles Approach to
Understanding the Internet’s Router-Level Topology,” Proc. 2004 ACM SIGCOMM
(Portland, OR, Aug. 2004).
[Li 2007] J. Li, M. Guidero, Z. Wu, E. Purpus, T. Ehrenkranz, “BGP Routing Dynamics
Revisited.” ACM Computer Communication Review (April 2007).
[Liang 2006] J. Liang, N. Naoumov, K.W. Ross, “The Index Poisoning Attack in P2P File-
Sharing Systems,” Proc. 2006 IEEE INFOCOM (Barcelona, Spain, April 2006).
[Lin 2001] Y. Lin, I. Chlamtac, Wireless and Mobile Network Architectures, John Wiley and
Sons, New York, NY, 2001.
[Liogkas 2006] N. Liogkas, R. Nelson, E. Kohler, L. Zhang, “Exploiting BitTorrent For Fun
(But Not Profit),” 6th International Workshop on Peer-to-Peer Systems (IPTPS 2006).
[Liu 2002] B. Liu, D. Goeckel, D. Towsley, “TCP-Cognizant Adaptive Forward Error
Correction in Wireless Networks,” Proc. 2002 Global Internet.
[Liu 2003] J. Liu, I. Matta, M. Crovella, “End-to-End Inference of Loss Nature in a Hybrid
Wired/Wireless Environment,” Proc. WiOpt’03: Modeling and Optimization in Mobile, Ad
Hoc and Wireless Networks.
[Liu 2010] Z. Liu, P. Dhungel, Di Wu, C. Zhang, K. W. Ross, “Understanding and
Improving Incentives in Private P2P Communities,” ICDCS (Genoa, Italy, 2010).
REFERENCES 805
[Locher 2006] T. Locher, P. Moor, S. Schmid, R. Wattenhofer, “Free Riding in BitTorrent is
Cheap,” Proc. ACM HotNets 2006 (Irvine CA, Nov. 2006).
[Lui 2004] J. Lui, V. Misra, D. Rubenstein, “On the Robustness of Soft State Protocols,”
Proc. IEEE Int. Conference on Network Protocols (ICNP ’04), pp. 50–60.
[Luotonen 1998] A. Luotonen, Web Proxy Servers, Prentice Hall, Englewood Cliffs, NJ, 1998.
[Lynch 1993] D. Lynch, M. Rose, Internet System Handbook, Addison-Wesley, Reading,
MA, 1993.
[Macedonia 1994] M. Macedonia, D. Brutzman, “MBone Provides Audio and Video Across
the Internet,” IEEE Computer Magazine, Vol. 27, No. 4 (Apr. 1994), pp. 30–36.
[Mahdavi 1997] J. Mahdavi, S. Floyd, “TCP-Friendly Unicast Rate-Based Flow Control,”
unpublished note (Jan. 1997).
[Malware 2006] Computer Economics, “2005 Malware Report: The Impact of Malicious
Code Attacks,” http://www.computereconomics.com
[manet 2012] IETF Mobile Ad-hoc Networks (manet) Working Group, http://www.ietf.org/
html.charters/manet-charter.html
[Mao 2002] Z. M. Mao, C. Cranor, F. Boudlis, M. Rabinovich, O. Spatscheck, J. Wang, “A
Precise and Efficient Evaluation of the Proximity Between Web Clients and Their Local
DNS Servers,” Proc. 2002 USENIX ATC.
[MaxMind 2012] http://www.maxmind.com/app/ip-location
[Maymounkov 2002] P. Maymounkov, D. Mazières. “Kademlia: A Peer-to-Peer Information
System Based on the XOR Metric.” Proceedings of the 1st International Workshop on
Peerto-Peer Systems (IPTPS ’02) (Mar. 2002), pp. 53–65.
[McKeown 1997a] N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, M. Horowitz,
“The Tiny Tera: A Packet Switch Core,” IEEE Micro Magazine (Jan.–Feb. 1997).
[McKeown 1997b] N. McKeown, “A Fast Switched Backplane for a Gigabit Switched
Router,” Business Communications Review, Vol. 27, No. 12. http://tiny-tera.stanford.edu/
~nickm/papers/cisco_fasts_wp.pdf
[McKeown 2008] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J.
Rexford, S. Shenker, J. Turner, “OpenFlow: Enabling Innovation in Campus Networks,”
ACM SIGCOMM Computer Communication Review, Vol. 38, No. 2 (Apr. 2008).
[McQuillan 1980] J. McQuillan, I. Richer, E. Rosen, “The New Routing Algorithm for the
Arpanet,” IEEE Transactions on Communications, Vol. 28, No. 5 (May 1980), pp. 711–719.
[Medhi 1997] D. Medhi, D. Tipper (eds.), Special Issue: Fault Management in Communication
Networks, Journal of Network and Systems Management, Vol. 5. No. 2 (June 1997).
[Metcalfe 1976] R. M. Metcalfe, D. R. Boggs. “Ethernet: Distributed Packet Switching for
Local Computer Networks,” Communications of the Association for Computing Machinery,
Vol. 19, No. 7 (July 1976), pp. 395–404.
[Meyers 2004] A. Myers, T. Ng, H. Zhang, “Rethinking the Service Model: Scaling Ethernet
to a Million Nodes,” ACM Hotnets Conference, 2004.
[MFA Forum 2012] IP/MPLS Forum homepage, http://www.ipmplsforum.org/
[Mirkovic 2005] J. Mirkovic, S. Dietrich, D. Dittrich. P. Reiher, Internet Denial of Service:
Attack and Defense Mechanisms, Prentice Hall, 2005.
[Mockapetris 1988] P. V. Mockapetris, K. J. Dunlap, “Development of the Domain Name
System,” Proc. 1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).
806 REFERENCES
[Mockapetris 2005] P. Mockapetris, Sigcomm Award Lecture, video available at http://
www.postel.org/sigcomm
[Mogul 2003] J. Mogul, “TCP offload is a dumb idea whose time has come,” Proc. HotOS
IX: The 9th Workshop on Hot Topics in Operating Systems (2003), USENIX Association.
[Molinero-Fernandez 2002] P. Molinaro-Fernandez, N. McKeown, H. Zhang, “Is IP Going
to Take Over the World (of Communications)?,” Proc. 2002 ACM Hotnets.
[Molle 1987] M. L. Molle, K. Sohraby, A. N. Venetsanopoulos, “Space-Time Models of
Asynchronous CSMA Protocols for Local Area Networks,” IEEE Journal on Selected Areas
in Communications, Vol. 5, No. 6 (1987), pp. 956–968.
[Moore 2001] D. Moore, G. Voelker, S. Savage, “Inferring Internet Denial of Service
Activity,” Proc. 2001 USENIX Security Symposium (Washington, DC, Aug. 2001).
[Moore 2003] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, N. Weaver, “Inside
the Slammer Worm,” 2003 IEEE Security and Privacy Conference.
[Moshchuck 2006] A. Moshchuk, T. Bragin, S. Gribble, H. Levy, “A Crawler-based Study
of Spyware on the Web,” Proc. 13th Annual Network and Distributed Systems Security
Symposium (NDSS 2006) (San Diego, CA, Feb. 2006).
[Motorola 2007] Motorola, “Long Term Evolution (LTE): A Technical Overview,”
http://www.motorola.com/staticfiles/Business/Solutions/Industry%20Solutions/Service%20P
roviders/Wireless%20Operators/LTE/_Document/Static%20Files/6834_MotDoc_New.pdf
[Mouly 1992] M. Mouly, M. Pautet, The GSM System for Mobile Communications, Cell and
Sys, Palaiseau, France, 1992.
[Moy 1998] J. Moy, OSPF: Anatomy of An Internet Routing Protocol, Addison-Wesley,
Reading, MA, 1998.
[Mudigonda 2011] J. Mudigonda, P. Yalagandula, J. C. Mogul, B. Stiekes, Y. Pouffary,
“NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters,” Proc.
2011 ACM SIGCOMM.
[Mukherjee 1997] B. Mukherjee, Optical Communication Networks, McGraw-Hill, 1997.
[Mukherjee 2006] B. Mukherjee, Optical WDM Networks, Springer, 2006.
[Mydotr 2009] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S.
Radhakrishnan, V. Subramanya, A. Vahdat, “PortLand: A Scalable Fault-Tolerant Layer 2
Data Center Network Fabric,” Proc. 2009 ACM SIGCOMM.
[Nadel 2011] B. Nadel, “4G shootout: Verizon LTE vs. Sprint WiMax,” Computerworld,
February 3, 2011.
[Nahum 2002] E. Nahum, T. Barzilai, D. Kandlur, “Performance Issues in WWWServers,”
IEEE/ACM Transactions on Networking, Vol 10, No. 1 (Feb. 2002).
[Naoumov 2006] N. Naoumov, K.W. Ross, “Exploiting P2P Systems for DDoS Attacks,”
Intl Workshop on Peer-to-Peer Information Management (Hong Kong, May 2006),
[Neglia 2007] G. Neglia, G. Reina, H. Zhang, D. Towsley, A. Venkataramani, J. Danaher,
“Availability in BitTorrent Systems,” Proc. 2007 IEEE INFOCOM.
[Neumann 1997] R. Neumann, “Internet Routing Black Hole,” The Risks Digest: Forum on
Risks to the Public in Computers and Related Systems, Vol. 19, No. 12 (May 1997). http://
catless.ncl.ac.uk/Risks/19.12.html#subj1.1
[Neville-Neil 2009] G. Neville-Neil, “Whither Sockets?” Communications of the ACM,
Vol. 52, No. 6 (June 2009), pp. 51–55.
REFERENCES 807
[Nicholson 2006]ANicholson, Y. Chawathe, M. Chen, B. Noble, D. Wetherall, “Improved
Access Point Selection,” Proc. 2006 ACM Mobisys Conference (Uppsala Sweden, 2006).
[Nielsen 1997] H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud’hommeaux, H. W. Lie, C.
Lilley, “Network Performance Effects of HTTP/1.1, CSS1, and PNG,” W3C Document,
1997 (also appears in Proc. 1997 ACM SIGCOM (Cannes, France, Sept 1997),
pp. 155–166.
[NIST 2001] National Institute of Standards and Technology, “Advanced Encryption
Standard (AES),” Federal Information Processing Standards 197, Nov. 2001, http://csrc.nist
.gov/publications/fips/fips197/fips-197.pdf
[NIST IPv6 2012] National Institute of Standards, “Estimating IPv6 & DNSSEC
Deployment SnapShots,” http://usgv6-deploymon.antd.nist.gov/snap-all.html
[Nmap 2012] Nmap homepage, http://www.insecure.com/nmap
[Nonnenmacher 1998] J. Nonnenmacher, E. Biersak, D. Towsley, “Parity-Based Loss
Recovery for Reliable Multicast Transmission,” IEEE/ACM Transactions on Networking,
Vol. 6, No. 4 (Aug. 1998), pp. 349–361.
[NTIA 1998] National Telecommunications and Information Administration (NTIA), US
Department of Commerce, “Management of Internet names and addresses,” Docket Number:
980212036-8146-02. http://www.ntia.doc.gov/ntiahome/domainname/6_5_98dns.htm
[O’Dell 2009] M. O’Dell, “Network Front-End Processors, Yet Again,” Communications of
the ACM, Vol. 52, No. 6 (June 2009), pp. 46–50.
[OID Repository 2012] OID Repository, http://www.oid-info.com/
[OSI 2012] International Organization for Standardization homepage, http://www.iso.org/
iso/en/ISOOnline.frontpage
[OSS 2012] OSS Nokalva, “ASN.1 Resources,” http://www.oss.com/asn1/
[Padhye 2000] J. Padhye, V. Firoiu, D. Towsley, J. Kurose, “Modeling TCP Reno
Performance: A Simple Model and its Empirical Validation,” IEEE/ACM Transactions on
Networking, Vol. 8 No. 2 (Apr. 2000), pp. 133–145.
[Padhye 2001] J. Padhye, S. Floyd, “On Inferring TCP Behavior,” Proc. 2001 ACM
SIGCOMM (San Diego, CA, Aug. 2001).
[Pan 1997] P. Pan, H. Schulzrinne, “Staged Refresh Timers for RSVP,” Proc. 2nd Global
Internet Conference (Phoenix, AZ, Dec. 1997).
[Parekh 1993] A. Parekh, R. Gallagher, “A generalized processor sharing approach to flow
control in integrated services networks: the single-node case,” IEEE/ACM Transactions on
Networking, Vol. 1, No. 3 (June 1993), pp. 344–357.
[Partridge 1992] C. Partridge, S. Pink, “An Implementation of the Revised Internet Stream
Protocol (ST-2),” Journal of Internetworking: Research and Experience, Vol. 3, No. 1
(Mar. 1992).
[Partridge 1998] C. Partridge, et al. “A Fifty Gigabit per second IP Router,” IEEE/ACM
Transactions on Networking, Vol. 6, No. 3 (Jun. 1998), pp. 237–248.
[Pathak 2010] A. Pathak, Y. A. Wang, C. Huang, A. Greenberg, Y. C. Hu, J. Li, K. W. Ross,
“Measuring and Evaluating TCP Splitting for Cloud Services,” Passive and Active
Measurement (PAM) Conference (Zurich, 2010).
[Paxson 1997] V. Paxson, “End-to-End Internet Packet Dynamics,” Proc. 1997 ACM
SIGCOMM (Cannes, France, Sept. 1997).
808 REFERENCES
[Perkins 1994] A. Perkins, “Networking with Bob Metcalfe,” The Red Herring Magazine
(Nov. 1994).
[Perkins 1998] C. Perkins, O. Hodson, V. Hardman, “A Survey of Packet Loss Recovery
Techniques for Streaming Audio,” IEEE Network Magazine (Sept./Oct. 1998), pp. 40–47.
[Perkins 1998b] C. Perkins, Mobile IP: Design Principles and Practice, Addison-Wesley,
Reading, MA, 1998.
[Perkins 2000] C. Perkins, Ad Hoc Networking, Addison-Wesley, Reading, MA, 2000.
[Perlman 1999] R. Perlman, Interconnections: Bridges, Routers, Switches, and
Internetworking Protocols, 2nd ed., Addison-Wesley Professional Computing Series,
Reading, MA, 1999.
[PGPI 2012] The International PGP Home Page, http://www.pgpi.org
[Phifer 2000] L. Phifer, “The Trouble with NAT,” The Internet Protocol Journal, Vol. 3, No. 4
(Dec. 2000), http://www.cisco.com/warp/public/759/ipj_3-4/ipj_3-4_nat.html
[Piatek 2007] M. Piatek, T. Isdal, T. Anderson, A. Krishnamurthy, A. Venkataramani, “Do
Incentives Build Robustness in Bittorrent?,” Proc. NSDI (2007).
[Piatek 2008] M. Piatek, T. Isdal, A. Krishnamurthy, T. Anderson, “One hop Reputations for
Peer-to-peer File Sharing Workloads,” Proc. NSDI (2008).
[Pickholtz 1982] R. Pickholtz, D. Schilling, L. Milstein, “Theory of Spread Spectrum
Communication—a Tutorial,” IEEE Transactions on Communications, Vol. 30, No. 5 (May
1982), pp. 855–884.
[PingPlotter 2012] PingPlotter homepage, http://www.pingplotter.com
[Piscatello 1993] D. Piscatello, A. Lyman Chapin, Open Systems Networking, Addison-
Wesley, Reading, MA, 1993.
[Point Topic 2006] Point Topic Ltd., World Broadband Statistics Q1 2006, http://www
.pointtopic.com
[Potaroo 2012] “Growth of the BGP Table–1994 to Present,” http://bgp.potaroo.net/
[PPLive 2012] PPLive homepage, http://www.pplive.com
[Quagga 2012] Quagga, “Quagga Routing Suite,” http://www.quagga.net/
[Quittner 1998] J. Quittner, M. Slatalla, Speeding the Net: The Inside Story of Netscape and
How it Challenged Microsoft, Atlantic Monthly Press, 1998.
[Quova 2012] www.quova.com
[Raiciu 2011] C. Raiciu , S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, M. Handley,
“Improving Datacenter Performance and Robustness with Multipath TCP,” Proc. 2011 ACM
SIGCOMM.
[Ramakrishnan 1990] K. K. Ramakrishnan, R. Jain, “A Binary Feedback Scheme for
Congestion Avoidance in Computer Networks,” ACM Transactions on Computer Systems,
Vol. 8, No. 2 (May 1990), pp. 158–181.
[Raman 1999] S. Raman, S. McCanne, “A Model, Analysis, and Protocol Framework
for Soft State-based Communication,” Proc. 1999 ACM SIGCOMM (Boston, MA, Aug.
1999).
[Raman 2007] B. Raman, K. Chebrolu, “Experiences in using WiFi for Rural Internet in
India,” IEEE Communications Magazine, Special Issue on New Directions in Networking
Technologies in Emerging Economies (Jan. 2007).
REFERENCES 809
[Ramaswami 2010] R. Ramaswami, K. Sivarajan, G. Sasaki, Optical Networks: A Practical
Perspective, Morgan Kaufman Publishers, 2010.
[Ramjee 1994] R. Ramjee, J. Kurose, D. Towsley, H. Schulzrinne, “Adaptive Playout
Mechanisms for Packetized Audio Applications in Wide-Area Networks,” Proc. 1994 IEEE
INFOCOM.
[Rao 1996] K. R. Rao and J. J. Hwang, Techniques and Standards for Image, Video and
Audio Coding, Prentice Hall, Englewood Cliffs, NJ, 1996.
[Rao 2011] A. S. Rao, Y. S. Lim, C. Barakat, A. Legout, D. Towsley, W. Dabbous, “Network
Characteristics of Video Streaming Traffic,” Proc. 2011 ACM CoNEXT (Tokyo).
[RAT 2012] Robust Audio Tool, http://www-mice.cs.ucl.ac.uk/multimedia/software/rat/
[Ratnasamy 2001] S. Ratnasamy, P. Francis, M. Handley, R. Karp, S. Shenker, “A
Scalable Content-Addressable Network,” Proc. 2001 ACM SIGCOMM (San Diego, CA,
Aug. 2001).
[Ren 2006] S. Ren, L. Guo, and X. Zhang, “ASAP: an AS-aware peer-relay protocol for high
quality VoIP,” Proc. 2006 IEEE ICDCS (Lisboa, Portugal, July 2006).
[Rescorla 2001] E. Rescorla, SSL and TLS: Designing and Building Secure Systems,
Addison-Wesley, Boston, 2001.
[RFC 001] S. Crocker, “Host Software,” RFC 001 (the very first RFC!).
[RFC 768] J. Postel, “User Datagram Protocol,” RFC 768, Aug. 1980.
[RFC 789] E. Rosen, “Vulnerabilities of Network Control Protocols,” RFC 789.
[RFC 791] J. Postel, “Internet Protocol: DARPA Internet Program Protocol Specification,”
RFC 791, Sept. 1981.
[RFC 792] J. Postel, “Internet Control Message Protocol,” RFC 792, Sept. 1981.
[RFC 793] J. Postel, “Transmission Control Protocol,” RFC 793, Sept. 1981.
[RFC 801] J. Postel, “NCP/TCP Transition Plan,” RFC 801, Nov. 1981.
[RFC 826] D. C. Plummer, “An Ethernet Address Resolution Protocol—or—Converting
Network Protocol Addresses to 48 bit Ethernet Address for Transmission on Ethernet
Hardware,” RFC 826, Nov. 1982.
[RFC 829] V. Cerf, “Packet Satellite Technology Reference Sources,” RFC 829, Nov.
1982.
[RFC 854] J. Postel, J. Reynolds, “TELNET Protocol Specification,” RFC 854, May 1993.
[RFC 950] J. Mogul, J. Postel, “Internet Standard Subnetting Procedure,” RFC 950, Aug.
1985.
[RFC 959] J. Postel and J. Reynolds, “File Transfer Protocol (FTP),” RFC 959, Oct. 1985.
[RFC 977] B. Kantor, P. Lapsley, “Network News Transfer Protocol,” RFC 977, Feb. 1986.
[RFC 1028] J. Davin, J.D. Case, M. Fedor, M. Schoffstall, “A Simple Gateway Monitoring
Protocol,” RFC 1028, Nov. 1987.
[RFC 1034] P. V. Mockapetris, “Domain Names—Concepts and Facilities,” RFC 1034,
Nov. 1987.
[RFC 1035] P. Mockapetris, “Domain Names—Implementation and Specification,” RFC
1035, Nov. 1987.
[RFC 1058] C. L. Hendrick, “Routing Information Protocol,” RFC 1058, June 1988.
810 REFERENCES
[RFC 1071] R. Braden, D. Borman, and C. Partridge, “Computing The Internet Checksum,”
RFC 1071, Sept. 1988.
[RFC 1075] D. Waitzman, C. Partridge, S. Deering, “Distance Vector Multicast Routing
Protocol,” RFC 1075, Nov. 1988.
[RFC 1112] S. Deering, “Host Extension for IP Multicasting,” RFC 1112, Aug. 1989.
[RFC 1122] R. Braden, “Requirements for Internet Hosts—Communication Layers,” RFC
1122, Oct. 1989.
[RFC 1123] R. Braden, ed., “Requirements for Internet Hosts—Application and Support,”
RFC-1123, Oct. 1989.
[RFC 1142] D. Oran, “OSI IS-IS Intra-Domain Routing Protocol,” RFC 1142, Feb. 1990.
[RFC 1190] C. Topolcic, “Experimental Internet Stream Protocol: Version 2 (ST-II),” RFC
1190, Oct. 1990.
[RFC 1191] J. Mogul, S. Deering, “Path MTU Discovery,” RFC 1191, Nov. 1990.
[RFC 1213] K. McCloghrie, M. T. Rose, “Management Information Base for Network
Management of TCP/IP-based internets: MIB-II,” RFC 1213, Mar. 1991.
[RFC 1256] S. Deering, “ICMP Router Discovery Messages,” RFC 1256, Sept. 1991.
[RFC 1320] R. Rivest, “The MD4 Message-Digest Algorithm,” RFC 1320, Apr. 1992.
[RFC 1321] R. Rivest, “The MD5 Message-Digest Algorithm,” RFC 1321, Apr. 1992.
[RFC 1323] V. Jacobson, S. Braden, D. Borman, “TCP Extensions for High Performance,”
RFC 1323, May 1992.
[RFC 1422] S. Kent, “Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-
Based Key Management,” RFC 1422.
[RFC 1546] C. Partridge, T. Mendez, W. Milliken, “Host Anycasting Service,” RFC 1546,
1993.
[RFC 1547] D. Perkins, “Requirements for an Internet Standard Point-to-Point Protocol,”
RFC 1547, Dec. 1993.
[RFC 1584] J. Moy, “Multicast Extensions to OSPF,” RFC 1584, Mar. 1994.
[RFC 1633] R. Braden, D. Clark, S. Shenker, “Integrated Services in the Internet
Architecture: an Overview,” RFC 1633, June 1994.
[RFC 1636] R. Braden, D. Clark, S. Crocker, C. Huitema, “Report of IAB Workshop on
Security in the Internet Architecture,” RFC 1636, Nov. 1994.
[RFC 1661] W. Simpson (ed.), “The Point-to-Point Protocol (PPP),” RFC 1661, July 1994.
[RFC 1662] W. Simpson (ed.), “PPP in HDLC-Like Framing,” RFC 1662, July 1994.
[RFC 1700] J. Reynolds and J. Postel, “Assigned Numbers,” RFC 1700, Oct. 1994.
[RFC 1752] S. Bradner, A. Mankin, “The Recommendations for the IP Next Generation
Protocol,” RFC 1752, Jan. 1995.
[RFC 1918] Y. Rekhter, B. Moskowitz, D. Karrenberg, G. J. de Groot, E. Lear, “Address
Allocation for Private Internets,” RFC 1918, Feb. 1996.
[RFC 1930] J. Hawkinson, T. Bates, “Guidelines for Creation, Selection, and Registration of
an Autonomous System (AS),” RFC 1930, Mar. 1996.
[RFC 1938] N. Haller, C. Metz, “A One-Time Password System,” RFC 1938, May 1996.
[RFC 1939] J. Myers and M. Rose, “Post Office Protocol—Version 3,” RFC 1939, May 1996.
REFERENCES 811
[RFC 1945] T. Berners-Lee, R. Fielding, H. Frystyk, “Hypertext Transfer Protocol—
HTTP/1.0,” RFC 1945, May 1996.
[RFC 2003] C. Perkins, “IP Encapsulation within IP,” RFC 2003, Oct. 1996.
[RFC 2004] C. Perkins, “Minimal Encapsulation within IP,” RFC 2004, Oct. 1996.
[RFC 2018] M. Mathis, J. Mahdavi, S. Floyd, A. Romanow, “TCP Selective
Acknowledgment Options,” RFC 2018, Oct. 1996.
[RFC 2050] K. Hubbard, M. Kosters, D. Conrad, D. Karrenberg, J. Postel, “Internet Registry
IP Allocation Guidelines,” RFC 2050, Nov. 1996.
[RFC 2104] H. Krawczyk, M. Bellare, R. Canetti, “HMAC: Keyed-Hashing for Message
Authentication,” RFC 2104, Feb. 1997.
[RFC 2131] R. Droms, “Dynamic Host Configuration Protocol,” RFC 2131, Mar. 1997.
[RFC 2136] P. Vixie, S. Thomson, Y. Rekhter, J. Bound, “Dynamic Updates in the Domain
Name System,” RFC 2136, Apr. 1997.
[RFC 2153] W. Simpson, “PPP Vendor Extensions,” RFC 2153, May 1997.
[RFC 2205] R. Braden, Ed., L. Zhang, S. Berson, S. Herzog, S. Jamin, “Resource
ReSerVation Protocol (RSVP)—Version 1 Functional Specification,” RFC 2205, Sept. 1997.
[RFC 2210] J. Wroclawski, “The Use of RSVP with IETF Integrated Services,” RFC 2210,
Sept. 1997.
[RFC 2211] J. Wroclawski, “Specification of the Controlled-Load Network Element
Service,” RFC 2211, Sept. 1997.
[RFC 2215] S. Shenker, J. Wroclawski, “General Characterization Parameters for Integrated
Service Network Elements,” RFC 2215, Sept. 1997.
[RFC 2326] H. Schulzrinne, A. Rao, R. Lanphier, “Real Time Streaming Protocol (RTSP),”
RFC 2326, Apr. 1998.
[RFC 2328] J. Moy, “OSPF Version 2,” RFC 2328, Apr. 1998.
[RFC 2420] H. Kummert, “The PPP Triple-DES Encryption Protocol (3DESE),” RFC 2420,
Sept. 1998.
[RFC 2453] G. Malkin, “RIP Version 2,” RFC 2453, Nov. 1998.
[RFC 2460] S. Deering, R. Hinden, “Internet Protocol, Version 6 (IPv6) Specification,” RFC
2460, Dec. 1998.
[RFC 2475] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, “An
Architecture for Differentiated Services,” RFC 2475, Dec. 1998.
[RFC 2578] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Structure of Management
Information Version 2 (SMIv2),” RFC 2578, Apr. 1999.
[RFC 2579] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Textual Conventions for
SMIv2,” RFC 2579, Apr. 1999.
[RFC 2580] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Conformance Statements for
SMIv2,” RFC 2580, Apr. 1999.
[RFC 2597] J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, “Assured Forwarding PHB
Group,” RFC 2597, June 1999.
[RFC 2616] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, T. Berners-
Lee, R. Fielding, “Hypertext Transfer Protocol—HTTP/1.1,” RFC 2616, June 1999.
812 REFERENCES
[RFC 2663] P. Srisuresh, M. Holdrege, “IP Network Address Translator (NAT) Terminology
and Considerations,” RFC 2663.
[RFC 2702] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, J. McManus, “Requirements
for Traffic Engineering Over MPLS,” RFC 2702, Sept. 1999.
[RFC 2827] P. Ferguson, D. Senie, “Network Ingress Filtering: Defeating Denial of Service
Attacks which Employ IP Source Address Spoofing,” RFC 2827, May 2000.
[RFC 2865] C. Rigney, S. Willens, A. Rubens, W. Simpson, “Remote Authentication Dial In
User Service (RADIUS),” RFC 2865, June 2000.
[RFC 2961] L. Berger, D. Gan, G. Swallow, P. Pan, F. Tommasi, S. Molendini, “RSVP
Refresh Overhead Reduction Extensions,” RFC 2961, Apr. 2001.
[RFC 3007] B. Wellington, “Secure Domain Name System (DNS) Dynamic Update,” RFC
3007, Nov. 2000.
[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator
(Traditional NAT),” RFC 3022, Jan. 2001.
[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator
(Traditional NAT),” RFC 3022, Jan. 2001.
[RFC 3031] E. Rosen, A. Viswanathan, R. Callon, “Multiprotocol Label Switching
Architecture,” RFC 3031, Jan. 2001.
[RFC 3032] E. Rosen, D. Tappan, G. Fedorkow, Y. Rekhter, D. Farinacci, T. Li, A. Conta,
“MPLS Label Stack Encoding,” RFC 3032, Jan. 2001.
[RFC 3052] M. Eder, S. Nag, “Service Management Architectures Issues and Review,” RFC
3052, Jan. 2001.
[RFC 3139] L. Sanchez, K. McCloghrie, J. Saperia, “Requirements for Configuration
Management of IP-Based Networks,” RFC 3139, June 2001.
[RFC 3168] K. Ramakrishnan, S. Floyd, D. Black, “The Addition of Explicit Congestion
Notification (ECN) to IP,” RFC 3168, Sept. 2001.
[RFC 3209] D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, G. Swallow, “RSVP-TE:
Extensions to RSVP for LSP Tunnels,” RFC 3209, Dec. 2001.
[RFC 3221] G. Huston, “Commentary on Inter-Domain Routing in the Internet,” RFC 3221,
Dec. 2001.
[RFC 3232] J. Reynolds, “Assigned Numbers: RFC 1700 is Replaced by an On-line
Database,” RFC 3232, Jan. 2002.
[RFC 3246] B. Davie, A. Charny, J.C.R. Bennet, K. Benson, J.Y. Le Boudec, W. Courtney,
S. Davari, V. Firoiu, D. Stiliadis, “An Expedited Forwarding PHB (Per-Hop Behavior),”
RFC 3246, Mar. 2002.
[RFC 3260] D. Grossman, “New Terminology and Clarifications for Diffserv,” RFC 3260,
Apr. 2002.
[RFC 3261] J. Rosenberg, H. Schulzrinne, G. Carmarillo, A. Johnston, J. Peterson, R.
Sparks, M. Handley, E. Schooler, “SIP: Session Initiation Protocol,” RFC 3261, July 2002.
[RFC 3272] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W.S. Lai,
“Overview and Principles of Internet Traffic Engineering,” RFC 3272, May 2002.
[RFC 3286] L. Ong, J. Yoakum, “An Introduction to the Stream Control Transmission
Protocol (SCTP),” RFC 3286, May 2002.
REFERENCES 813
[RFC 3346] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W. S. Lai,
“Applicability Statement for Traffic Engineering with MPLS,” RFC 3346, Aug. 2002.
[RFC 3376] B. Cain, S. Deering, I. Kouvelas, B. Fenner, A. Thyagarajan, “Internet Group
Management Protocol, Version 3,” RFC 3376, Oct. 2002.
[RFC 3390] M. Allman, S. Floyd, C. Partridge, “Increasing TCP’s Initial Window,” RFC
3390, Oct. 2002.
[RFC 3410] J. Case, R. Mundy, D. Partain, “Introduction and Applicability Statements for
Internet Standard Management Framework,” RFC 3410, Dec. 2002.
[RFC 3411] D. Harrington, R. Presuhn, B. Wijnen, “An Architecture for Describing
Simple Network Management Protocol (SNMP) Management Frameworks,” RFC 3411,
Dec. 2002.
[RFC 3414] U. Blumenthal and B. Wijnen, “User-based Security Model (USM) for
Version 3 of the Simple Network Management Protocol (SNMPv3),” RFC 3414,
December 2002.
[RFC 3415] B. Wijnen, R. Presuhn, K. McCloghrie, “View-based Access Control Model
(VACM) for the Simple Network Management Protocol (SNMP),” RFC 3415, Dec. 2002.
[RFC 3416] R. Presuhn, J. Case, K. McCloghrie, M. Rose, S. Waldbusser, “Version 2 of the
Protocol Operations for the Simple Network Management Protocol (SNMP),” Dec. 2002.
[RFC 3439] R. Bush and D. Meyer, “Some internet architectural guidelines and philosophy,”
RFC 3439, Dec. 2003.
[RFC 3447] J. Jonsson, B. Kaliski, “Public-Key Cryptography Standards (PKCS) #1: RSA
Cryptography Specifications Version 2.1,” RFC 3447, Feb. 2003.
[RFC 3468] L. Andersson, G. Swallow, “The Multiprotocol Label Switching (MPLS)
Working Group Decision on MPLS Signaling Protocols,” RFC 3468, Feb. 2003.
[RFC 3469] V. Sharma, Ed., F. Hellstrand, Ed, “Framework for Multi-Protocol Label
Switching (MPLS)-based Recovery,” RFC 3469, Feb. 2003. ftp://ftp.rfc-editor.org/innotes/
rfc3469.txt
[RFC 3501] M. Crispin, “Internet Message Access Protocol—Version 4rev1,” RFC 3501,
Mar. 2003.
[RFC 3550] H. Schulzrinne, S. Casner, R. Frederick, V. Jacobson, “RTP: ATransport
Protocol for Real-Time Applications,” RFC 3550, July 2003.
[RFC 3569] S. Bhattacharyya (ed.), “An Overview of Source-Specific Multicast (SSM),”
RFC 3569, July 2003.
[RFC 3588] P. Calhoun, J. Loughney, E. Guttman, G. Zorn, J. Arkko, “Diameter Base
Protocol,” RFC 3588, Sept. 2003.
[RFC 3618] B. Fenner, D. Meyer, Ed., “Multicast Source Discovery Protocol (MSDP),”
RFC 3618, Oct. 2003.
[RFC 3649] S. Floyd, “High Speed TCP for Large Congestion Windows,” RFC 3649, Dec. 2003.
[RFC 3748] B. Aboba, L. Blunk, J. Vollbrecht, J. Carlson, H. Levkowetz, Ed., “Extensible
Authentication Protocol (EAP),” RFC 3748, June 2004.
[RFC 3782] S. Floyd, T. Henderson, A. Gurtov, “The NewReno Modification to TCP’s Fast
Recovery Algorithm,” RFC 3782, Apr. 2004.
814 REFERENCES
[RFC 3973] A. Adams, J. Nicholas, W. Siadak, “Protocol Independent Multicast—Dense
Mode (PIM-DM): Protocol Specification (Revised),” RFC 3973, Jan. 2005.
[RFC 4022] R. Raghunarayan, Ed., “Management Information Base for the Transmission
Control Protocol (TCP),” RFC 4022, Mar. 2005.
[RFC 4113] B. Fenner, J. Flick, “Management Information Base for the User Datagram
Protocol (UDP),” RFC 4113, June 2005.
[RFC 4213] E. Nordmark, R. Gilligan, “Basic Transition Mechanisms for IPv6 Hosts and
Routers,” RFC 4213, Oct. 2005.
[RFC 4271] Y. Rekhter, T. Li, S. Hares, Ed., “A Border Gateway Protocol 4 (BGP-4),” RFC
4271, Jan. 2006.
[RFC 4272] S. Murphy, “BGP Security Vulnerabilities Analysis,” RFC 4274, Jan. 2006.
[RFC 4274] Meyer, D. and K. Patel, “BGP-4 Protocol Analysis”, RFC 4274, January
2006.
[RFC 4291] R. Hinden, S. Deering, “IP Version 6 Addressing Architecture,” RFC 4291,
February 2006.
[RFC 4293] S. Routhier, Ed. “Management Information Base for the Internet Protocol (IP),”
RFC 4293, Apr. 2006.
[RFC 4301] S. Kent, K. Seo, “Security Architecture for the Internet Protocol,” RFC 4301,
Dec. 2005.
[RFC 4302] S. Kent, “IP Authentication Header,” RFC 4302, Dec. 2005.
[RFC 4303] S. Kent, “IP Encapsulating Security Payload (ESP),” RFC 4303, Dec. 2005.
[RFC 4305] D. Eastlake, “Cryptographic Algorithm Implementation Requirements for
Encapsulating Security Payload (ESP) and Authentication Header (AH),” RFC 4305,
Dec. 2005.
[RFC 4340] E. Kohler, M. Handley, S. Floyd, “Datagram Congestion Control Protocol
(DCCP),” RFC 4340, Mar. 2006.
[RFC 4443] A. Conta, S. Deering, M. Gupta, Ed., “Internet Control Message Protocol
(ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,” RFC 4443, Mar.
2006.
[RFC 4346] T. Dierks, E. Rescorla, “The Transport Layer Security (TLS) Protocol Version
1.1,” RFC 4346, Apr. 2006.
[RFC 4502] S. Waldbusser, “Remote Network Monitoring Management Information Base
Version 2,” RFC 4502, May 2006.
[RFC 4514] K. Zeilenga, Ed., “Lightweight Directory Access Protocol (LDAP): String
Representation of Distinguished Names,” RFC 4514, June 2006.
[RFC 4601] B. Fenner, M. Handley, H. Holbrook, I. Kouvelas, “Protocol Independent
Multicast—Sparse Mode (PIM-SM): Protocol Specification (Revised),” RFC 4601, Aug. 2006.
[RFC 4607] H. Holbrook, B. Cain, “Source-Specific Multicast for IP,” RFC 4607, Aug. 2006.
[RFC 4611] M. McBride, J. Meylor, D. Meyer, “Multicast Source Discovery Protocol
(MSDP) Deployment Scenarios,” RFC 4611, Aug. 2006.
[RFC 4632] V. Fuller, T. Li, “Classless Inter-domain Routing (CIDR): The Internet Address
Assignment and Aggregation Plan,” RFC 4632, Aug. 2006.
REFERENCES 815
[RFC 4960] R. Stewart, ed., “Stream Control Transmission Protocol,” RFC 4960, Sept. 2007.
[RFC 4987]W. Eddy, “TCP SYN Flooding Attacks and Common Mitigations,” RFC 4987,
Aug. 2007.
[RFC 5000] RFC editor, “Internet Official Protocol Standards,” RFC 5000, May 2008.
[RFC 5109] A. Li (ed.), “RTP Payload Format for Generic Forward Error Correction,” RFC
5109, Dec. 2007.
[RFC 5110] P. Savola, “Overview of the Internet Multicast Routing Architecture,” RFC
5110, Jan. 2008.
[RFC 5216] D. Simon, B. Aboba, R. Hurst, “The EAP-TLS Authentication Protocol,” RFC
5216, Mar. 2008.
[RFC 5218] D. Thaler, B. Aboba, “What Makes for a Successful Protocol?,” RFC 5218, July
2008.
[RFC 5321] J. Klensin, “Simple Mail Transfer Protocol,” RFC 5321, Oct. 2008.
[RFC 5322] P. Resnick, Ed., “Internet Message Format,” RFC 5322, Oct. 2008.
[RFC 5348] S. Floyd, M. Handley, J. Padhye, J.Widmer, “TCP Friendly Rate Control
(TFRC): Protocol Specification,” RFC 5348, Sept. 2008.
[RFC 5411] J Rosenberg, “A Hitchhiker’s Guide to the Session Initiation Protocol (SIP),”
RFC 5411, Feb. 2009.
[RFC 5681] M. Allman, V. Paxson, E. Blanton, “TCP Congestion Control,” RFC 5681,
Sept. 2009.
[RFC 5944] C. Perkins, Ed., “IP Mobility Support for IPv4, Revised,” RFC 5944, November
2010.
[RFC 5996] C. Kaufman, P. Hoffman, Y. Nir, P. Eronen, “Internet Key Exchange Protocol
Version 2 (IKEv2),” RFC 5996, Sept. 2010.
[RFC 6071] S. Frankel, S. Krishnan, “IP Security (IPsec) and Internet Key Exchange (IKE)
Document Roadmap,” RFC 6071, Feb. 2011.
[RFC 6265] A Barth, “HTTP State Management Mechanism,” RFC 6265, Apr. 2011.
[RFC 6298] V. Paxson, M. Allman, J. Chu, M. Sargent, “Computing TCP’s Retransmission
Timer,” RFC 6298, June 2011.
[Rhee 1998] I. Rhee, “Error Control Techniques for Interactive Low-Bit Rate Video
Transmission over the Internet,” Proc. 1998 ACM SIGCOMM (Vancouver BC, Aug. 1998).
[Roberts 1967] L. Roberts, T. Merril, “Toward a Cooperative Network of Time-Shared
Computers,” AFIPS Fall Conference (Oct. 1966).
[Roberts 2004] J. Roberts, “Internet Traffic, QoS and Pricing,” Proc. 2004 IEEE
INFOCOM, Vol. 92, No. 9 (Sept. 2004), pp. 1389–1399.
[Rodriguez 2010] R. Rodrigues, P. Druschel, “Peer-to-Peer Systems,” Communications of
the ACM, Vol. 53, No. 10 (Oct. 2010), pp. 72–82.
[Rohde 2008] Rohde and Schwarz, “UMTS Long Term Evolution (LTE) Technology
Introduction,” Application Note 1MA111.
[Rom 1990] R. Rom, M. Sidi, Multiple Access Protocols: Performance and Analysis,
Springer-Verlag, New York, 1990.
[Root Servers 2012] Root Servers homepage, http://www.root-servers.org/
816 REFERENCES
[Rose 1996] M. Rose, The Simple Book: An Introduction to Internet Management, Revised
Second Edition, Prentice Hall, Englewood Cliffs, NJ, 1996.
[Ross 1995] K. W. Ross, Multiservice Loss Models for Broadband Telecommunication
Networks, Springer, Berlin, 1995.
[Rowston 2001] A. Rowston, P. Druschel, “Pastry: Scalable, Distributed Object Location
and Routing for Large-Scale Peer-to-Peer Systems,” Proc. 2001 IFIP/ACM Middleware
(Heidelberg, Germany, 2001).
[RSA 1978] R. Rivest, A. Shamir, L. Adelman, “A Method for Obtaining Digital Signatures
and Public-key Cryptosystems,” Communications of the ACM, Vol. 21, No. 2 (Feb. 1978),
pp. 120–126.
[RSA Fast 2012] RSA Laboratories, “How Fast is RSA?” http://www.rsa.com/rsalabs/node
.asp?id=2215
[RSA Key 2012] RSA Laboratories, “How large a key should be used in the RSA Crypto
system?” http://www.rsa.com/rsalabs/node.asp?id=2218
[Rubenstein 1998] D. Rubenstein, J. Kurose, D. Towsley, “Real-Time Reliable Multicast
Using Proactive Forward Error Correction,” Proceedings of NOSSDAV ’98 (Cambridge, UK,
July 1998).
[Rubin 2001] A. Rubin, White-Hat Security Arsenal: Tackling the Threats, Addison-Wesley,
2001.
[Ruiz-Sanchez 2001] M. Ruiz-Sánchez, E. Biersack, W. Dabbous, “Survey and Taxonomy
of IP Address Lookup Algorithms,” IEEE Network Magazine, Vol. 15, No. 2 (Mar./Apr.
2001), pp. 8–23.
[Saltzer 1984] J. Saltzer, D. Reed, D. Clark, “End-to-End Arguments in System Design,”
ACM Transactions on Computer Systems (TOCS), Vol. 2, No. 4 (Nov. 1984).
[Sandvine 2011] “Global Internet Phenomena Report, Spring 2011,” http://www.sandvine.
com/news/global broadband trends.asp, 2011.
[Sardar 2006] B. Sardar, D. Saha, “A Survey of TCP Enhancements for Last-Hop Wireless
Networks,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 3 (2006), pp. 20–34.
[Saroiu 2002] S. Saroiu, P.K. Gummadi, S.D. Gribble, “A Measurement Study of Peer-to-
Peer File Sharing Systems,” Proc. of Multimedia Computing and Networking (MMCN)
(2002).
[Saroiu 2002b] S. Saroiu, K. P. Gummadi, R. J. Dunn, S. D. Gribble, and H. M. Levy, “An
Analysis of Internet Content Delivery Systems,” USENIX OSDI (2002).
[Saydam 1996] T. Saydam, T. Magedanz, “From Networks and Network Management into
Service and Service Management,” Journal of Networks and System Management, Vol. 4,
No. 4 (Dec. 1996), pp. 345–348.
[Schiller 2003] J. Schiller, Mobile Communications 2nd edition, Addison Wesley, 2003.
[Schneier 1995] B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source
Code in C, John Wiley and Sons, 1995.
[Schulzrinne 1997] H. Schulzrinne, “A Comprehensive Multimedia Control Architecture for
the Internet,” NOSSDAV’97 (Network and Operating System Support for Digital Audio and
Video) (St. Louis, MO, May 1997).
[Schulzrinne-RTP 2012] Henning Schulzrinne’s RTP site, http://www.cs.columbia.edu/~hgs/rtp
REFERENCES 817
[Schulzrinne-RTSP 2012] Henning Schulzrinne’s RTSP site, http://www.cs.columbia.edu/
~hgs/rtsp
[Schulzrinne-SIP 2012] Henning Schulzrinne’s SIP site, http://www.cs.columbia.edu/~hgs/sip
[Schwartz 1977] M. Schwartz, Computer-Communication Network Design and Analysis,
Prentice-Hall, Englewood Cliffs, N.J., 1997.
[Schwartz 1980] M. Schwartz, Information, Transmission, Modulation, and Noise, McGraw
Hill, New York, NY 1980.
[Schwartz 1982] M. Schwartz, “Performance Analysis of the SNAVirtual Route Pacing
Control,” IEEE Transactions on Communications, Vol. 30, No. 1 (Jan. 1982), pp. 172–184.
[Scourias 2012] J. Scourias, “Overview of the Global System for Mobile Communications:
GSM.” http://www.privateline.com/PCS/GSM0.html
[Segaller 1998] S. Segaller, Nerds 2.0.1, A Brief History of the Internet, TV Books, New
York, 1998.
[Shacham 1990] N. Shacham, P. McKenney, “Packet Recovery in High-Speed Networks
Using Coding and Buffer Management,” Proc. 1990 IEEE INFOCOM (San Francisco, CA,
Apr. 1990), pp. 124–131.
[Shaikh 2001] A. Shaikh, R. Tewari, M. Agrawal, “On the Effectiveness of DNS-based
Server Selection,” Proc. 2001 IEEE INFOCOM.
[Sharma 2003] P. Sharma, E, Perry, R. Malpani, “IP Multicast Operational Network
management: Design, Challenges, and Experiences,” IEEE Network Magazine (Mar. 2003),
pp. 49–55.
[Singh 1999] S. Singh, The Code Book: The Evolution of Secrecy from Mary, Queen of
Scotsto Quantum Cryptography, Doubleday Press, 1999.
[SIP Software 2012] H. Schulzrinne Software Package site, http://www.cs.columbia.edu/
IRT/software
[Skoudis 2004] E. Skoudis, L. Zeltser, Malware: Fighting Malicious Code, Prentice Hall, 2004.
[Skoudis 2006] E. Skoudis, T. Liston, Counter Hack Reloaded: A Step-by-Step Guide to
Computer Attacks and Effective Defenses (2nd Edition), Prentice Hall, 2006.
[Skype 2012] Skype homepage, www.skype.com
[SMIL 2012] W3C Synchronized Multimedia homepage, http://www.w3.org/AudioVideo
[Smith 2009] J. Smith, “Fighting Physics: ATough Battle,” Communications of the ACM,
Vol. 52, No. 7 (July 2009), pp. 60–65.
[Snort 2012] Sourcefire Inc., Snort homepage, http://http://www.snort.org/
[Solari 1997] S. J. Solari, Digital Video and Audio Compression, McGraw Hill,
New York, NY, 1997.
[Solensky 1996] F. Solensky, “IPv4 Address Lifetime Expectations,” in IPng: Internet
Protocol Next Generation (S. Bradner, A. Mankin, ed.), Addison-Wesley, Reading, MA, 1996.
[Spragins 1991] J. D. Spragins, Telecommunications Protocols and Design, Addison-
Wesley, Reading, MA, 1991.
[Srikant 2004] R. Srikant, The Mathematics of Internet Congestion Control, Birkhauser, 2004
[Sripanidkulchai 2004] K. Sripanidkulchai, B. Maggs, and H. Zhang, “An analysis of live
streaming workloads on the Internet,” Proc. 2004 ACM Internet Measurement Conference
(Taormina, Sicily, Italy), pp. 41–54.
818 REFERENCES
[Stallings 1993]W. Stallings, SNMP, SNMP v2, and CMIP The Practical Guide to Network
Management Standards, Addison-Wesley, Reading, MA, 1993.
[Stallings 1999]W. Stallings, SNMP, SNMPv2, SNMPv3, and RMON 1 and 2, Addison-
Wesley, Reading, MA, 1999.
[Steinder 2002] M. Steinder, A. Sethi, “Increasing robustness of fault localization through
analysis of lost, spurious, and positive symptoms,” Proc. 2002 IEEE INFOCOM.
[Stevens 1990] W. R. Stevens, Unix Network Programming, Prentice-Hall, Englewood
Cliffs, NJ.
[Stevens 1994] W. R. Stevens, TCP/IP Illustrated, Vol. 1: The Protocols, Addison-Wesley,
Reading, MA, 1994.
[Stevens 1997] W.R. Stevens, Unix Network Programming, Volume 1: Networking APIs-
Sockets and XTI, 2nd edition, Prentice-Hall, Englewood Cliffs, NJ, 1997.
[Stewart 1999] J. Stewart, BGP4: Interdomain Routing in the Internet, Addison-Wesley,
1999.
[Stoica 2001] I. Stoica, R. Morris, D. Karger, M.F. Kaashoek, H. Balakrishnan, “Chord: A
Scalable Peer-to-Peer Lookup Service for Internet Applications,” Proc. 2001 ACM
SIGCOMM (San Diego, CA, Aug. 2001).
[Stone 1998] J. Stone, M. Greenwald, C. Partridge, J. Hughes, “Performance of Checksums
and CRC’s Over Real Data,” IEEE/ACM Transactions on Networking, Vol. 6, No. 5 (Oct. 1998),
pp. 529–543.
[Stone 2000] J. Stone, C. Partridge, “When Reality and the Checksum Disagree,” Proc. 2000
ACM SIGCOMM (Stockholm, Sweden, Aug. 2000).
[Strayer 1992]W. T. Strayer, B. Dempsey, A. Weaver, XTP: The Xpress Transfer Protocol,
Addison-Wesley, Reading, MA, 1992.
[Stubblefield 2002] A. Stubblefield, J. Ioannidis, A. Rubin, “Using the Fluhrer, Mantin, and
Shamir Attack to Break WEP,” Proceedings of 2002 Network and Distributed Systems
Security Symposium (2002), pp. 17–22.
[Subramanian 2000] M. Subramanian, Network Management: Principles and Practice,
Addison-Wesley, Reading, MA, 2000.
[Subramanian 2002] L. Subramanian, S. Agarwal, J. Rexford, R. Katz, “Characterizing the
Internet Hierarchy from Multiple Vantage Points,” Proc. 2002 IEEE INFOCOM.
[Sundaresan 2006] K.Sundaresan, K. Papagiannaki, “The Need for Cross-layer Information
in Access Point Selection,” Proc. 2006 ACM Internet Measurement Conference (Rio De
Janeiro, Oct. 2006).
[Su 2006] A.-J. Su, D. Choffnes, A. Kuzmanovic, and F. Bustamante, “Drafting Behind
Akamai” Proc. 2006 ACM SIGCOMM.
[Suh 2006] K. Suh, D. R. Figueiredo, J. Kurose and D. Towsley, “Characterizing and
detecting relayed traffic: A case study using Skype,” Proc. 2006 IEEE INFOCOM
(Barcelona, Spain, Apr. 2006).
[Sunshine 1978] C. Sunshine, Y. Dalal, “Connection Management in Transport Protocols,”
Computer Networks, North-Holland, Amsterdam, 1978.
[Tariq 2008] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, M. Ammar, “Answering
What-If Deployment and Configuration Questions with WISE,” Proc. 2008 ACM
SIGCOMM (Aug. 2008).
REFERENCES 819
[TechnOnLine 2012] TechOnLine, “Protected Wireless Networks,” online webcast tutorial,
http://www.techonline.com/community/tech_topic/internet/21752
[Teixeira 2006] R. Teixeira and J. Rexford, “Managing Routing Disruptions in Internet
Service Provider Networks,” IEEE Communications Magazine (Mar. 2006).
[Thaler 1997] D. Thaler and C. Ravishankar, “Distributed Center-Location Algorithms,”
IEEE Journal on Selected Areas in Communications, Vol. 15, No. 3 (Apr. 1997),
pp. 291–303.
[Think 2012] Technical History of Network Protocols, “Cyclades,” http://www.cs.utexas
.edu/users/chris/think/Cyclades/index.shtml
[Tian 2012] Y. Tian, R. Dey, Y. Liu, K. W. Ross, “China’s Internet: Topology Mapping and
Geolocating,” IEEE INFOCOM Mini-Conference 2012 (Orlando, FL, 2012).
[Tobagi 1990] F. Tobagi, “Fast Packet Switch Architectures for Broadband Integrated
Networks,” Proc. 1990 IEEE INFOCOM, Vol. 78, No. 1 (Jan. 1990), pp. 133–167.
[TOR 2012] Tor: Anonymity Online, http://www.torproject.org
[Torres 2011] R. Torres, A. Finamore, J. R. Kim, M. M. Munafo, S. Rao, “Dissecting Video
Server Selection Strategies in the YouTube CDN,” Proc. 2011 Int. Conf. on Distributed
Computing Systems.
[Turner 1988] J. S. Turner “Design of a Broadcast packet switching network,” IEEE
Transactions on Communications, Vol. 36, No. 6 (June 1988), pp. 734–743.
[Turner 2012] B. Turner, “2G, 3G, 4G Wireless Tutorial,” http://blogs.nmscommunications
.com/communications/2008/10/2g-3g-4g-wireless-tutorial.html
[UPnP Forum 2012] UPnP Forum homepage, http://www.upnp.org/
[van der Berg 2008] R. van der Berg, “How the ‘Net works: an introduction to peering and
transit,” http://arstechnica.com/guides/other/peering-and-transit.ars
[Varghese 1997] G. Varghese, A. Lauck, “Hashed and Hierarchical Timing Wheels: Efficient
Data Structures for Implementing a Timer Facility,” IEEE/ACM Transactions on Networking,
Vol. 5, No. 6 (Dec. 1997), pp. 824–834.
[Vasudevan 2012] S. Vasudevan, C. Diot, J. Kurose, D. Towsley, “Facilitating Access Point
Selection in IEEE 802.11 Wireless Networks,” Proc. 2005 ACM Internet Measurement
Conference, (San Francisco CA, Oct. 2005).
[Verizon FIOS 2012] Verizon, “Verizon FiOS Internet: FAQ,” http://www22.verizon.com/
residential/fiosinternet/faq/faq.htm
[Verizon SLA 2012] Verizon, “Global Latency and Packet Delivery SLA,” http://www.
verizonbusiness.com/terms/global_latency_sla.xml
[Verma 2001] D. C. Verma, Content Distribution Networks: An Engineering Approach, John
Wiley, 2001.
[Villamizar 1994] C. Villamizar, C. Song. “High performance tcp in ansnet,” ACM
SIGCOMM Computer Communications Review, Vol. 24, No. 5 (1994), pp. 45–60.
[Viterbi 1995] A. Viterbi, CDMA: Principles of Spread Spectrum Communication, Addison-
Wesley, Reading, MA, 1995.
[Vixie 2009] P. Vixie, “What DNS Is Not,” Communications of the ACM, Vol. 52, No. 12
(Dec. 2009), pp. 43–47.
[W3C 1995] The World Wide Web Consortium, “A Little History of the World Wide Web”
(1995), http://www.w3.org/History.html
820 REFERENCES
[Wakeman 1992] I. Wakeman, J. Crowcroft, Z. Wang, D. Sirovica, “Layering Considered
Harmful,” IEEE Network (Jan. 1992), pp. 20–24.
[Waldrop 2007] M. Waldrop, “Data Center in a Box,” Scientific American (July 2007).
[Walker 2000] J. Walker, “IEEE P802.11 Wireless LANs, Unsafe at Any Key Size; An Analysis
of the WEP Encapsulation,” Oct. 2000, http://www.drizzle.com/~aboba/IEEE/0-362.zip
[Wall 1980] D. Wall, Mechanisms for Broadcast and Selective Broadcast, Ph.D. thesis,
Stanford University, June 1980.
[Wang 2004] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP:
An Analytic Performance Study,” Proc. 2004 ACM Multimedia Conference (New York, NY,
Oct. 2004).
[Wang 2008] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP:
An Analytic Performance Study,” ACM Transactions on Multimedia Computing
Communications and Applications (TOMCCAP), Vol. 4, No. 2 (Apr. 2008), pp. 16:1–22.
[Wang 2010] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. Ng, M.
Kozuch, M. Ryan, “c-Through: Part-time Optics in Data Centers,” Proc. 2010 ACM
SIGCOMM.
[Weatherspoon 2000] S. Weatherspoon, “Overview of IEEE 802.11b Security,” Intel Technology
Journal (2nd Quarter 2000), http://download.intel.com/technology/itj/q22000/pdf/art_5.pdf
[Wei 2005]W. Wei, B. Wang, C. Zhang, J. Kurose, D. Towsley, “Classification of Access
Network Types: Ethernet, Wireless LAN, ADSL, Cable Modem or Dialup?,” Proc. 2005
IEEE INFOCOM (Apr. 2005).
[Wei 2006]W. Wei, C. Zhang, H. Zang, J. Kurose, D. Towsley, “Inference and Evaluation of
Split-Connection Approaches in Cellular Data Networks,” Proc. Active and Passive
Measurement Workshop (Adelaide, Australia, Mar. 2006).
[Wei 2007] D. X. Wei, C. Jin, S. H. Low, S. Hegde, “FAST TCP: Motivation, Architecture,
Algorithms, Performance,” IEEE/ACM Transactions on Networking (2007).
[Weiser 1991] M. Weiser, “The Computer for the Twenty-First Century,” Scientific
American (Sept. 1991): 94–10. http://www.ubiq.com/hypertext/weiser/SciAmDraft3.html
[White 2011] A. White, K. Snow, A. Matthews, F. Monrose, “Hookt on fon-iks: Phonotactic
Reconstruction of Encrypted VoIP Conversations,” IEEE Symposium on Security and
Privacy, Oakland, CA, 2011.
[Wigle.net 2012]Wireless Geographic Logging Engine, http://www.wigle.net
[Williams 1993] R. Williams, “A Painless Guide to CRC Error Detection Algorithms,”
http://www.ross.net/crc/crcpaper.html
[Wilson 2011] C. Wilson, H. Ballani, T. Karagiannis, A. Rowstron, “Better Never than Late:
Meeting Deadlines in Datacenter Networks,” Proc. 2011 ACM SIGCOMM.
[WiMax Forum 2012] WiMax Forum, http://www.wimaxforum.org
[Wireshark 2012]Wireshark homepage, http://www.wireshark.org
[Wischik 2005] D. Wischik, N. McKeown, “Part I: Buffer Sizes for Core Routers,” ACM
SIGCOMM Computer Communications Review, Vol. 35, No. 3 (July 2005).
[Woo 1994] T. Woo, R. Bindignavle, S. Su, S. Lam, “SNP: an interface for secure network
programming,” Proc. 1994 Summer USENIX (Boston, MA, June 1994), pp. 45–58.
[Wood 2012] L. Wood, “Lloyds Satellites Constellations,” http://www.ee.surrey.ac.uk/
Personal/L.Wood/constellations/iridium.html
REFERENCES 821
[Wu 2005] J. Wu, Z. M. Mao, J. Rexford, J. Wang, “Finding a Needle in a Haystack:
Pinpointing Significant BGP Routing Changes in an IP Network,” Proc. USENIX NSDI (2005).
[Xanadu 2012] Xanadu Project homepage, http://www.xanadu.com/
[Xiao 2000] X. Xiao, A. Hannan, B. Bailey, L. Ni, “Traffic Engineering with MPLS in the
Internet,” IEEE Network (Mar./Apr. 2000).
[Xie 2008] H. Xie, Y.R. Yang, A. Krishnamurthy, Y. Liu, A. Silberschatz, “P4P: Provider
Portal for Applications,” Proc. 2008 ACM SIGCOMM (Seattle, WA, Aug. 2008).
[Yannuzzi 2005] M. Yannuzzi, X. Masip-Bruin, O. Bonaventure, “Open Issues in
Interdomain Routing: A Survey,” IEEE Network Magazine (Nov./Dec. 2005).
[Yavatkar 1994] R. Yavatkar, N. Bhagwat, “Improving End-to-End Performance of TCP
over Mobile Internetworks,” Proc. Mobile 94 Workshop on Mobile Computing Systems and
Applications (Dec. 1994).
[YouTube 2009] YouTube 2009, Google container data center tour, 2009.
[Yu 2006] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman, “SybilGuard: Defending
Against Sybil Attacks via Social Networks,” Proc. 2006 ACM SIGCOMM (Pisa, Italy,
Sept. 2006).
[Zegura 1997] E. Zegura, K. Calvert, M. Donahoo, “A Quantitative Comparison of Graphbased
Models for Internet Topology,” IEEE/ACM Transactions on Networking, Vol. 5, No. 6,
(Dec. 1997). See also http://www.cc.gatech.edu/projects/gtim for a software package that
generates networks with a transit-stub structure.
[Zhang 1993] L. Zhang, S. Deering, D. Estrin, S. Shenker, D. Zappala, “RSVP: A New
Resource Reservation Protocol,” IEEE Network Magazine, Vol. 7, No. 9 (Sept. 1993),
pp. 8–18.
[Zhang 2007] L. Zhang, “A Retrospective View of NAT,” The IETF Journal, Vol. 3, Issue 2
(Oct. 2007).
[Zhang M 2010] M. Zhang, W. John, C. Chen, “Architecture and Download Behavior of
Xunlei: A Measurement-Based Study,” Proc. 2010 Int. Conf. on Educational Technology and
Computers (ICETC).
[Zhang X 2102] X. Zhang, Y. Xu, Y. Liu, Z. Guo, Y. Wang, “Profiling Skype Video Calls:
Rate Control and Video Quality,” IEEE INFOCOM (Mar. 2012).
[Zhao 2004] B. Y. Zhao, L. Huang, J. Stribling, S. C. Rhea, A. D. Joseph, J. Kubiatowicz,
“Tapestry: A Resilient Global-scale Overlay for Service Deployment,” IEEE Journal on
Selected Areas in Communications, Vol. 22, No. 1 (Jan. 2004).
[Zimmerman 1980] H. Zimmerman, “OS1 Reference Model-The ISO Model of
Architecture for Open Systems Interconnection,” IEEE Transactions on Communications,
Vol. 28, No. 4 (Apr. 1980), pp. 425–432.
[Zimmermann 2012] P. Zimmermann, “Why do you need PGP?” http://www.pgpi.org/doc/
whypgp/en/
[Zink 2009] M. Zink, K. Suh, Y. Gu, J. Kurose, “Characteristics of YouTube Network
Traffic at a Campus Network - Measurements, Models, and Implications,” Computer
Networks, Vol. 53, No. 4 (2009), pp. 501–514.
822 REFERENCES
Index
A
AAC (Advanced Audio Coding), 590
Abramson, Norman, 62, 454, 473
ABR ATM network service, 313
ABR (available bit-rate), 259
spare available bandwidth advantage,
267
Abstract Syntax Notation One. See ASN.1
access control and SNMPv3, 777–778
access control lists, 734–736
access delay, 113
access ISP, 32–33
access networks, 12–18
cable Internet access, 14–15, 460–461
dial-up access, 16
DSL (digital subscriber line), 13
enterprises, 16–17
Ethernet, 16–17
FTTH (fiber to the home), 15
home access, 13
link-layer switches, 4
LTE (Long-Term Evolution), 18, 533
optical distribution network, 15
satellite links, 16
2G, 3G, 4G (generation) wide-area
wireless networks, 18, 547–554
wide-area wireless access, 18
wireless LANs, 17
access points. See AP
access routers, 492
Accounting Management, 759, 764
acknowledgments, 210, 234–237
piggybacked, 237
TCP (Transmission Control Protocol),
235–236, 244
Telnet, 237–238
ACK (positive acknowledgments), 207,
240, 208, 212, 217, 235, 257
ACK receipt, 242
active optical networks. See AONs
active queue management algorithms. See
AQM algorithms
adapters, 463–465
MAC addresses, 471
routers, 468
adaptive congestion control, 201
adaptive playout delay, 616–618
adaptive streaming, 600–601
adaptive HTTP streaming, 593
additive-increase, multiplicative-decrease.
See AIMD
address aggregation, 342
address indirection, 406
addressing
Internet, 331–363
processes, 90
Address Resolution Protocol. See ARP
Address Supporting Organization of
ICANN, 345
ad hoc networks, 528
802.15.1 networks, 544
wireless hosts, 517
Adleman, Leonard, 684
Advanced Audio Coding. See AAC
Advanced Encryption Standard. See AES
Advanced Research Projects Agency. See
ARPA
AES (Advanced Encryption Standard),
680
agent advertisement, 566–567
agent discovery, 565–567
agent solicitation, 567
aging time, 478
AH (Authentication Header) protocol, 720
AIMD (additive-increase, multiplicativedecrease)
algorithm, 277–278, 280
Akamai, 114, 133, 609, 603–604,
35, 273
823
alias hostname, 140
ALOHAnet, 62–63, 454, 473
ALOHA protocol, 62–63, 452, 473, 511,
453, 63, 453–455
alternating-bit protocols, 214
Amazon cloud, 608–610
analog audio, 590
anchor foreign agent, 563–564
anchor MSC, 574
Andreessen, Marc, 64
anomaly-based IDSs (intrusion detection
systems), 742
anonymity, 738
anycast, 356
AONs (active optical networks), 15
AP (access points), 517, 528–530,
538–540
Apache Web server, 99, 156
API (Application Programming Interface),
6, 89
application architecture, 86–88
application gateways, 732, 736–738
deep packet inspection, 739–740
application-layer messages, 51, 54–55,
186
application-layer protocols, 49–50
DNS (domain name system), 131, 132
electronic mail, 98
FTP (File Transfer Protocol), 51
HTTP (HyperText Transfer Protocol),
51, 97
Internet e-mail application, 97
proprietary, 97
public domain, 97
security, 705
SMTP (Simple Mail Transfer
Protocol), 51, 97, 121
Telnet, 237
Application Programming Interface. See
API
AQM (active queue management) algorithms,
329
area border routers, 389
ARPA (Advanced Research Projects
Agency), 61, 511
ARP (Address Resolution Protocol),
465–468, 498
ARPAnet, 454, 473, 511
ARP messages 467, 498
ARP tables, 466–467, 481
ARQ (Automatic Repeat reQuest) protocols,
207–208, 576–577
ASN.1 (Abstract Syntax Notation One),
766, 770, 778–782
ASN (autonomous system number),
394
AS-PATH attribute, 394
ASs (autonomous systems), 380–383
association, 529–531
assured forwarding PHB, 651
asynchronous transfer mode. See ATM
ATM ABR (available bit-rate) congestion
control, 266–269, 313
ATM (asynchronous transfer mode), 259,
512
complexity and cost, 470
services, 312–313
multiple service models, 312
Q2931b protocol, 654
audio
AAC (Advanced Audio Coding), 590
glitches, 591
human speech, 590
MP3 (MPEG 1 layer 3), 590
PCM (pulse code modulation), 590
properties, 590–591
quantization, 590
removing jitter at receiver, 614–618
authentication
cryptographic techniques, 675
end-point, 700–705
802.11i, 729–730
MD5, 389
networks, 700–705
secret password, 701–703
SNMPv3, 777
WEP (Wired Equivalent Privacy), 726
802.11 wireless LANs, 530
wireless station, 530
authentication key, 692–693
824 INDEX
authentication protocol examples, 700–705
authoritative DNS servers, 134–137, 499
hostnames, 139
IP addresses, 142
names, 142
Automatic Repeat reQuest protocols. See
ARQ
autonomous system number. See ASN
autonomous systems. See ASs
available bit-rate. See ABR
average throughput, 44
B
backbone area, 389
bandwidth, 29, 281
guaranteed minimal, 311
link-level allocation, 639–640
use-it-or-lose-it resource, 640
video, 588–589, 594
bandwidth flooding, 57
bandwidth provisioning, 635
bandwidth-sensitive applications, 92
Baran, Paul, 60
base HTML file, 99
base station controller. See BSC
base stations, 516–518, 528
handoff between, 572–574
base station system. See BSS
base transceiver station. See BTS
Basic Encoding Rules. See BER
basic service set. See BSS
beacon frames, 529–530
Bellman-Ford equation, 371–372
Bellovin, Steven M., 753–754
BER (Basic Encoding Rules), 780
BER (bit error rate), 520–521
Berners-Lee, Tim, 64
best-effort delivery service, 190
best-effort networks, 634–636
best-effort service, 190, 311–312,
612–614, 633–634
BGP (Border Gateway Protocol),
390–399, 498–499
ASN (autonomous system number), 394
attributes, 394, 395
BGP peers, 391
BGP sessions, 393
complexity, 391, 393
DV (distance-vector) algorithm, 374
eBGP (external BGP) session, 393–395
elimination rules for routes, 396
iBGP (internal BGP) session, 393–395
inter-AS routing protocols, 390–391,
393–399
peers, 391
prefix bits, 342, 344, 393
route advertisement, 391, 396–399
routes, 394–395
route selection, 395–396
routing policy, 397–399
routing tables, 399
session, 393
TCP connections, 391, 393
bidirectional data transfer, 205–206
binary exponential backoff algorithm,
457–458
BIND (Berkeley Internet Name Domain),
131
bit error rate. See BER
bit-level error detection and correction,
438–445
BITNET, 63
bits, 19
propagation delay, 37–38
BitTorrent, 86, 149–151
accepting connections from other
hosts, 352
chunks, 149
developing, 182
Kademlia DHT, 156
P2P (peer-to-peer) protocol, 145, 182
swarming data principles, 183
trading algorithm, 150
blades, 490
block ciphers, 678–681
Bluetooth, 518, 544–545
Boggs, David, 470, 473
Border Gateway Protocol. See BGP
border routers, 491
botnet, 56
INDEX 825
bottleneck link, 45, 279–280
broadcasting (link layer), 433, 464
channels, 433, 446
channel propagation delay, 456
frame, 467
links, 445, 475
broadcasting (network layer)
broadcast routing algorithms, 405
broadcast storm, 401
flooding, 401–403
N-way-unicast, 400–401
sequence number, 401
spanning-tree broadcast, 403–405
browsers, 101–102
client processes, 88
HTTP requests directed to Web cache,
110–111
BSC (base station controller), 550
BSS (base station system), 550
BSS (basic service set), 527–528
MAC address, 539
mobility among, 541–542
BTS (base transceiver station), 548–549
buffered distributors, 475
buffering
packets, 218
streaming video, 592
buffers
packet loss, 25
switch output interfaces, 476
TCP connection, 233
burst size, 646
bursty traffic, 60
bus, switching via, 326
C
cable Internet access, 14–15
DOCSIS protocol, 52, 460
link-layer protocols, 460–461
cable modems, 15, 20, 460–461
cable modem termination system. See
CMTS
CA (Certification Authority), 697–699
certifying public keys, 708
call admission, 653–654
call setup protocol, 654
canonical hostnames, 131–132, 140
care-of-address. See COA
carrier sense multiple access protocol. See
CSMA protocol
carrier sense multiple access with collision
detection. See CSMA/CD
carrier sensing, 454–456
CBC (cipher-block chaining), 681–682
CBR (constant bit rate) ATM network
service, 312
CDMA (code division multiple access),
522–526
CDNs (Content Distribution Networks),
588, 603–608
access networks of ISPs (Internet
Service Providers), 603–604
cluster selection strategy, 606–608
data centers, 604
delay and loss performance
measurements, 606
DNS intercept/redirect, 604–605
IP anycast, 607–608
Netflix, 608
operation, 604–605
replicating content across clusters,
604
cell phones
growth of, 513
Internet access, 65, 546–554
cellular networks
architecture, 547–550
cells, 548–549
handoffs in GSM, 572–574
managing mobility, 570–575
routing calls to cellular user, 571–572
2G, 3G, 4G networks, 547–554
Cerf, Vinton G., 62, 231, 431–432, 511
CERT Coordination Center, 674
certificates, 698–699, 713–714
Certification Authority. See CA
channel numbers, 529
channel partitioning protocols, 447
CDMA (code division multiple access)
protocol, 449, 522–526
826 INDEX
FDM (frequency-division multiplexing),
448
TDM (time-division multiplexing),
448
channels
with bit errors and reliable data transfer,
207–212
802.11 wireless LANs, 529–531
losing packets, 212–215
propagation delay, 456
checksums, 203, 442
ACK/NAK packets, 210
calculation, 203
Internet, 203
poor cryptographic hash function, 690
TCP (Transmission Control Protocol),
334
UDP (User Datagram Protocol),
202–204, 334
chipping rate, 522
choke packet, 266
chosen-plaintext attack, 678
chunks, 149
delaying playout, 615–618
CI (congestion indication) bit, 268–269
CIDR (Classless Interdomain Routing),
342, 344, 496
cipher-block chaining. See CBC
ciphertext, 675, 676
ciphertext-only attack, 677
circuit-switched networks
end-to-end connection, 28
multiplexing, 28–30
versus packet switching, 30–31
reserving time slots, 30–31
sending packets, 28
telephone networks, 27
circuit-switched routing algorithm, 379
Cisco Systems, 65, 323, 732, 740
routers and switches, 325, 326
dominating network core, 323
Clark, Dave, 303, 585
classful addressing, 344
Classless Interdomain Routing. See CIDR
cleartext, 675
Clear to Send control frame. See CTS control
frame
client application buffer, 597–598
client buffering, 594–595
clients, 10, 86, 88, 89, 156
initiating contact with server, 163
HTTP, 198
IP addresses, 162
matching received replies with sent
queries, 140
port number, 162
prefetching video, 597
receiving and processing packets from,
162
TCP socket creation, 163
Web caches as, 111
client-server application
developing, 157–168
TCP, 157
UDP, 157
well-known port number, 157
client side socket interface, 99
client socket, 160, 163, 165
cloud applications and data center
networking, 490–495
cluster selection strategy, 606–608
CMTS (cable modem termination
system), 15, 460
CNAME record, 140
COA (care-of-address), 559, 565
coaxial cable, 20, 474
code division multiple access. See CDMA
collision detection, 454–455
collisions, switch elimination, 479
Comcast, 601, 763–764
commercial ISPs (Internet Service
Providers), 64
communication satellite, 21
computer networking
history of, 60–66
computer networks, 2
conditional GET, 114–116
confidentiality, 672–673, 706–707, 718,
720, 724
Configuration Management, 759, 764
INDEX 827
congestion
causes and costs, 259–265
dropping packets, 265
large queuing delays, 261
transmission capacity wasted, 265
unneeded retransmissions by sender,
263
congestion avoidance, 274–276
congestion control, 190, 240, 250, 596
ABR (available bit-rate) service, 259
AIMD (additive-increase,
multiplicative-decrease), 277–278
approaches, 265–266
ATM (asynchronous transfer mode)
networks, 259
end-to-end, 266, 269
network-assisted, 266, 269
principles, 259–269
source quench message, 353
TCP (Transmission Control Protocol),
269–272, 274–283
UDP (User Datagram Protocol), 282
congestion indication bit. See CI bit
congestion window, 269–270, 272–277,
282
connection-establishment request, 195
connectionless service, 313
connectionless transport and UDP (User
Datagram Protocol), 198–204
connection-oriented service, 94, 313–314
connection-oriented transport and TCP
(Transmission Control Protocol),
230–258
connection replay attack, 717
connections, persistent and nonpersistent,
100–103
connection setup, 310
connection sockets and processes, 198
connection state, 231, 315
constant bit rate ATM network service.
See CBR ATM network service
container-based MDCs (modular data centers),
494
Content Distribution Networks. See CDNs
content provider networks, 34–35
continuous playout, 591–592
control connection, 117
controlled flooding, 401–403
control plane software, 331
conversational applications, 592–593
cookies, 108–110
correspondent, 557, 561, 563
corrupted ACKs or NAKs, 209–210
countdown timer, 214
coverage area, 515
CRC (cyclic redundancy check) codes,
443–445
crossbar switch, 326
cryptographic hash functions, 689–691
cryptographic techniques, 675
cryptography, 675–688
ciphertext, 675–676
cleartext, 675
confidentiality, 675
cryptographic hash functions,
689–691
decryption algorithm, 676
encryption algorithm, 675–676
keys, 676
plaintext, 675–676
public key encryption, 683–688
symmetric key cryptography,
676–682
CSMA (carrier sense multiple access),
453–456
carrier sensing, 455–456
collisions, 455–456
CSMA/CA (carrier sense multiple access
with collision avoidance), 526,
531–537
CSMA/CD (carrier sense multiple access
with collision detection), 456–459
collision detection, 456–459
efficiency, 458–459
Ethernet, 475
CSNET (computer science network), 63
CTS (Clear to Send) control frame,
535–537
cumulative acknowledgments, 222, 236,
243
828 INDEX
customer ISPs, 34
customer-provider relationship, 33
cyclic redundancy check codes. See CRC
codes
D
DARPA (US Department of Defense
Advanced Research Projects
Agency), 62, 431
DASH (Dynamic Adaptive Streaming over
HTTP), 600–601
databases, implementing in P2P network,
151–156
data center networks, 490–495
data centers, 86
border routers, 491
CDNs (Content Distribution
Networks), 604
container-based MDCs (modular data
centers), 494
costs, 490
data center networks, 490–495
hierarchy of routers and switches,
492–493
hosts, 490
interconnection architectures and network
protocols, 493
Internet services, 86
load balancing, 491–492
servers, 11
data definition language, 765
data encryption algorithm and WEP (Wired
Equivalent Privacy), 726–728
Data Encryption Standard. See DES
DATA frame, 535
datagram networks, 313, 317–320
datagrams, 55, 189
detecting bit errors, 334
extending IP header, 335
fragmentation, 335–338
Internet Protocol (IP) v4 format, 332–338
labeling, 487
length, 334
moving between hosts, 51–52
passed to transport layer, 337
reassembling in end systems, 336
sending off subnet, 468–469
source and destination IP addresses,
334–336
TOS (type of service) bits, 333
transport-layer segments, 242
TTL (time-to-live) field, 334
datagram service and network layer, 364
data integrity, 363, 712
data link layer, 50, 53
Data-Over-Cable Service Interface
Specifications. See DOCSIS
data plane and hardware, 331
DATA SMTP command, 123, 124
data transfer, 713
bidirectional, 205–206
reliable, 91, 204–230
SSL (Secure Sockets Layer), 714–715
unidirectional, 205
unreliable, 206
VC (virtual-circuit) networks, 316
DDoS (distributed denial-of-service)
attacks, 56–58, 143–144
DECnet architecture, 266
decentralized routing algorithm, 366
decryption algorithm, 676, 283
deep packet inspection, 739–741
Deering, Steve, 584
default name server, 136
default router, 364
delaying playout, 615–618
delays
comparing transmission and propagation
delays, 38–39
end systems, 43–44
end-to-end, 42–44
nodal processing, 36
packetization, 44
packets, 36
packet-switched networks, 35–39
processing, 36–37
propagation, 36–37
queuing, 36–37, 39–42
total nodal, 36
transmission, 36–37
INDEX 829
delay-sensitive, 592
demilitarized zone. See DMZ
demultiplexing, 191–198
connectionless, 193–194
connection-oriented, 194–197
denial-of-service attacks. See DoS
attacks
DES (Data Encryption Standard), 680
destination port numbers, 192, 234
destination router, 364
DHCP (Dynamic Host Configuration
Protocol), 345–349
IP addresses dynamically assigned,
630
DHCP request message, 348, 495–496
DHCP servers, 346–347, 350, 495, 497
DHTs (distributed hash tables), 145,
151–156
dial-up access, 16
DIAMETER protocol, 530, 730
differentiated service, 634
Diffie-Hellman algorithm, 683, 687, 725
Diffserv, 648–652
DIFS (Distributed Inter-frame Space), 533
digital audio, 590
digital signatures, 707
compared with MAC (message authentication
code), 696–697
hash functions, 695–696
message integrity, 695
overheads of encryption and decryption,
695
PGP (Pretty Good Privacy), 710
public key certification, 697–699
public-key cryptography, 693–694
digital subscriber line. See DSL
digital subscriber line access multiplexer.
See DSLAM
Dijkstra’s least-cost path algorithm,
367–368, 388, 390
OSPF (Open-Shortest Path First),
388
dimensioning best-effort networks,
634–636
directory service, 97
direct routing, 563–564
Direct Sequence Wideband CDMA. See
DS-WCDMA
distance vector, 372
distance-vector algorithm. See DV
algorithm
Distance-Vector Multicast Routing
Protocol. See DVMRP
distributed applications, 5–6
distributed attacks. See DDoS attacks
Distributed Inter-frame Space. See DIFS
DMZ (demilitarized zone), 741
DNS
DNS database, 142, 144
DNS (domain name system), 51, 63, 98,
130–144, 497
additional delay to Internet applications,
131
application-layer protocols, 131–132
attacks and vulnerabilities, 143–144
caching, 138–139
CDN requests, 604–605
client-server paradigm, 132
DDoS attack against targeted host,
143–144
distributed and hierarchical database,
131, 134–138
host aliasing, 131–132
hostname-to-IP-address translation
service, 133–139
improving delay performance, 138
iterative queries, 137–138
load distribution, 132–133
obtaining presence in, 392
operational overview, 133–139
querying and replaying messages, 133
query messages, 136, 140–142, 497
recursive queries, 137–138
reply messages, 136, 140–142, 499
rotation, 132–133
RRs (resource records), 139–140,
498–499
secure communication, 674
servers, 134
services provided by, 131–133
830 INDEX
translating hostnames to IP addresses,
133–139
underlying end-to-end transport
protocol, 132
UPDATE option, 144
DNS servers, 131–135, 495
authoritative DNS servers, 134,
135–136, 140
BIND (Berkeley Internet Name
Domain), 131
DDoS bandwidth-flooding attack, 143
discarding cached information, 139
distant centralized database, 134
hierarchy, 136
local DNS server, 136–137
recursion, 140
root DNS servers, 134, 135
single point of failure, 133
TLD (top-level domain) DNS servers,
134
DOCSIS (Data-Over-Cable Service
Interface Specifications), 15, 52,
460
domain name system. See DNS
DoS (denial-of-service) attacks, 57, 735,
740
fragmentation and, 338
SYN flood attack, 253, 257
dotted-decimal notation, 339
dropping packets, 265
drop-tail, 329
DSLAM (digital subscriber line access
multiplexer), 13–14
DSL (digital subscriber line), 13–14, 20
DS-WCDMA (Direct Sequence Wideband
CDMA), 552
dual-stack approach, 359
duplicate ACKs, 211, 247–248
duplicate data packets, 210, 213–214
DV (distance-vector) algorithm, 366,
371–380
DVMRP (Distance-Vector Multicast
Routing Protocol), 411–412
Dynamic Adaptive Streaming over HTTP.
See DASH
Dynamic Host Configuration Protocol. See
DHCP
dynamic routing algorithm, 366
E
EAP (Extensible Authentication Protocol),
729–730
eavesdropping, 673
eBGP (external BGP), 393–395, 397
EC2, 66
edge router, 12
EFCI (explicit forward congestion
indication) bit, 268
802.15.1, 544–545
802.11i, 728–731
802.11n wireless network, 527
802.1Q frames, 484–486
802.11 wireless LANs, 17, 515, 518,
526–546, 548
address fields, 538–540
APs (access points), 517, 528
architecture, 527–531
association, 529–531
authentication, 530, 728–731
base station, 528
BSS (basic service set), 527–528
channels, 529–531
collision detection, 532
CSMA/CA (CSMA with collision
avoidance), 526, 531–537
data rates, 526–527
frequency band, 526
link-layer frames, 526
MAC addresses, 528
MAC protocols, 531–537
reducing transmission rate, 526
transmitting frames, 532
elastic applications, 92
electromagnetic noise, 519
e-mail, 64, 86, 97, 118–130
access protocols, 125–126
application-layer protocols, 98
security, 705–711
Web-based e-mail system, 120
encapsulation, 53–55, 565–567
INDEX 831
encapsulation/decapsulation and mobile
IP, 565
Encapsulation Security Payload
protocol. See ESP
encryption, 712
cryptography, 675–688
SNMPv3, 777
end-point authentication, 59, 673
end systems, 2, 4–6, 10
API (Application Programming
Interface), 6
applications running on, 6
communication links, 4
connecting, 5
datagram reassembly, 336
delays, 43–44
hosts, 10
IP addresses, 26
processes, 84, 88
TCP protocol, 231
transport-layer protocols, 188
end-to-end congestion control, 266
end-to-end delay, 42–43, 613
Diffserv, 652
Traceroute program, 42–43
end-to-end principle, 203
end-to-end throughput, 44–47
enterprises and access networks, 16–17
EPC (Evolved Packet Core), 553
ER (explicit rate) field, 269
error checking
link-layer protocols, 203
parity checks, 440–442
transport layer, 203
error concealment, 620–621
error detection
ARQ (Automatic Repeat reQuest) protocols,
208
checksumming methods, 203,
442–443
CRC (cyclic redundancy check) codes,
443–445
parity bits, 440–442
two-dimensional parity scheme,
441–442
ESP (Encapsulation Security Payload),
720, 723
ESTABLISHED state, 254
Estrin, Deborah, 303, 584–585
Ethernet, 16–17, 52, 63, 437, 445,
469–476
binary exponential backoff algorithm,
457–458
broadcast link, 475
carrier-sensing random access,
531–532
collision-detection algorithm, 532
CSMA/CD protocol, 474–475
frame format, 474
home networks, 17
link-layer and physical-layer specification,
474
local area networking, 470
MSS (maximum segment size), 233
MTU (maximum transmission unit),
471
multiplexing network-layer protocols,
471–472
physical-layer protocols, 52, 473–474
standardized, 474
store-and-forward packet switching, 475
switch-based star topology, 475
switches, 17, 470, 475, 496
10BASE-2, 473–474
10BASE-T, 473–474
10GBASE-T, 474–475
EWMA (exponential weighted moving
average), 240
expedited forwarding PHB, 651
explicit forward congestion indication bit.
See EFCI
explicit rate field. See ER (explicit rate)
field
exponential weighted moving average. See
EWMA
extended FSM (finite-state machine), 220,
223
Extensible Authentication Protocol. See
EAP
external BGP session. See eBGP session
832 INDEX
F
fairness
AIMD algorithm, 280
congestion-control mechanism, 280
parallel TCP connections, 282
TCP (Transmission Control Protocol),
279–282
TDM (time-division multiplexing), 448
UDP (User Datagram Protocol), 282
Fault Management, 758–759, 764
FCFS (first-come-first-served) scheduling,
329, 641
FDDI (fiber distributed data interface),
460, 470
FDM (frequency-division multiplexing),
28–30, 448, 549
FDM/TDM systems, 549–550
FEC (forward error correction), 442, 613,
618–619
FHSS (frequency-hopping spread
spectrum), 544
fiber distributed data interface protocol. See
FDDI
fiber optics, 20–21
100 Mbps Ethernet, 475
fiber to the home. See FTTH
FIFO (first-in-first-out), 637–638, 641–642
file sharing, 86
file transfer, 97, 116–118
filtering, 476–477
FIN bit, 235, 254
finite-state machine. See FSM
FIOS service, 15–16
firewalls, 673, 730–738
application gateways, 732, 736–738
authorized traffic, 731
blocking packets, 355, 596
connection table, 732–736
filtering, 733–736
malicious packet attacks, 355
first-come-first-served scheduling.
See FCFS
first-in-first-out order. See FIFO
fixed-length labels, 487
fixed playout delay, 615
flag field, 235, 334–336
flooding, 401–405
flow control, 240
different from congestion control, 220
TCP, 250
flow (of packets), 311, 357
foreign address, 559
foreign agent, 557, 566
COA (care-of-address), 567
mobile IP, 565
receiving and decapsulating datagram,
561, 562
registration with home agent, 562,
568–569
foreign networks, 557–559
forwarding, 305, 308–310, 321–322, 322,
476–477, 649
forwarding function, 320–321
forwarding plane, 321
forwarding tables, 26–27, 308–309,
317–318, 322–323
adding entries, 396–397
configuring, 308–309, 364
datagram networks, 319
directing datagrams to foreign
network, 558
routing algorithms, 364
VC networks, 319
4G systems, 18, 553–554
fourth-generation of wide-area wireless
networks. See 4G
fragmentation of datagrams, 334–338
frames, 52, 433–434, 436
frequency-division multiplexing. See FDM
frequency-hopping spread spectrum. See
FHSS
FSM (finite-state machine), 206–207
extended, 220, 223
initial state, 207
FTP (File Transfer Protocol), 51, 86,
97–98, 116–118
FTTH (fiber to the home), 15–16
full-duplex service, 232
full-table block ciphers, 679–680
fully connected topology, 493–494
INDEX 833
G
Gateway GPRS Support Nodes. See
GGSNs
gateway routers, 380–381
import policy, 395
packet filtering, 732
prefixes, 393
GBN (Go-Back-N) protocol, 218–224
Generalized Packet Radio Service. See
GPRS
generator, 443
geographically closest, 606
geostationary satellites, 21
GET method, 104–105, 115
GGSNs (Gateway GPRS Support Nodes),
552
Gigabit Ethernet, 475
global routing algorithm, 365–366
Global System for Mobile
Communications standards. See
GSM
global transit ISPs (Internet Service
Providers), 32–33
GMSC (Gateway Mobile services
Switching Center), 570
Go-Back-N protocol. See GBN protocol
Google, 34–25, 64–66, 86, 114, 431,
603, 610
GPRS (Generalized Packet Radio
Service), 552
graphs, 364–365
ground stations, 21
GSM (Global System for Mobile
Communications), 547, 549–550,
570
anchor MSC, 574
BTS (base transceiver station),
548–549
cells, 548–549
encoding audio, 628
handoffs, 572–574
home network, 570
home PLMN (home public land mobile
network), 570
indirect routing, 570
mobility, 576
routing cells to mobile user, 571–572
visited network, 570
guaranteed delivery, 311
guided media, 19
H
handoff, 517–518
GSM, 572–574
handshake, 94, 231–232, 252–253, 713,
716
hard guarantee, 634
hardware data plane, 331
hardware-implemented protocols, 9
hash functions, 152, 707, 714
digital signatures, 695–696
HDLC (high-level data link control), 445
header checksum, 334
header fields, 55
head-of-line blocking. See HOL blocking
HFC (hybrid fiber coax), 14
hidden terminals, 521, 534–537
hierarchical routing, 379–383
higher-layer protocols and wireless networks
and mobility, 575–577
high-level data link control. See HDLC
HLR (home location register), 570, 572
HMAC standard, 692–693
HOL (head-of-the-line) blocking, 330–331
home agents, 557–562, 565–566, 568–569
home location register. See HLR
home MSC, 574
home networks, 13, 557
Ethernet, 17
GMSC (Gateway Mobile services
Switching Center), 570
GSM (Global System for Mobile
Communications), 570
HLR (home location register), 570
home agent, 558–559
IP addresses, 349–352
mobile nodes, 559
PLMN (home public land mobile network),
570
WiFi, 17
834 INDEX
hostnames, 130
alias, 132
canonical, 131
translating to IP addresses, 131, 132,
133–139
hosts, 2, 4, 10–11
aliasing, 131–132, 140
ARP table, 466
assigning IP addresses to, 345–349
canonical hostname, 140
changing base station, 517–518
connected into network, 338
data centers, 490
default router, 364
DHCP discovery message, 530
dial-up modem connection, 486
hostnames, 130–131
interfaces, 338
IP addresses, 90, 130–131, 190,
465
link-layer addresses, 462–463
load balancer, 491–492
local DNS server, 136
MAC addresses, 465
monitoring, 756
moving datagrams between, 51–52
network-layer addresses, 462, 465
network-layer protocols, 471–472
process-to-process delivery service,
191–198
storing routing information, 380
wireless, 514
hot-potato routing, 382
HSP (High Speed Packet Access),
552
HTML, 64, 100–101
HTTP (HyperText Transfer Protocol), 51,
97–100, 127
client program, 98
conditional GET, 114–116
cookies, 108–110
GET request, 103–105, 499–500, 596,
601, 610
If-Modified-Since: header line, 115
message format, 98, 103–108
non-persistent connections,
100–103
persistent connections, 103, 124
pull protocol, 124
POST method, 104–105
PUT method, 105
response message, 105–108, 124, 500,
596
server program, 98, 100
SMTP comparison with, 124
stateless, 108
SSL security, 712
stateless protocols, 100, 117
TCP and, 99–100, 116, 200
Web caching, 110–114
HTTP streaming, 593, 597–600, 611
prefetching video, 596–597
hubs, 470
Hulu, streaming stored video, 591
human protocols, 7–8
human speech, 590
hybrid fiber coax. See HFC
hypertext, 64
HyperText Transfer Protocol. See HTTP
I
iBGP (internal BGP), 393–395, 397
IBM SNA architecture, 62, 266
ICANN (Internet Corporation for
Assigned Names and Numbers),
142, 345
ICMP (Internet Control Message
Protocol), 306, 353–355, 359
datagram, 258
messages, 353–354
IPv6, 359
IDEA, 710
identification field (IP), 336
IDSs (intrusion detection systems), 355,
673, 739–742
IEEE 802.3 CSMA/CD (Ethernet) working
group, 474
IEEE 802 LAN/MAN Standards
Committee, 5
IEEE managing MAC address space, 464
INDEX 835
IETF (Internet Engineering Task Force), 5,
668
Address Lifetime Expectations working
group, 356
standards for CAs, 699
If-Modified-Since: header line, 115
IGMP (Internet Group Management
Protocol), 359, 407–409
IKE (Internet Key Exchange), 725
IMAP (Internet Mail Access Protocol),
127, 129
IMAP server, 129
import policy, 395
in-band, 117
indirect routing,
GSM (Global System for Mobile
Communications), 570
mobile IP standard, 562
mobile node, 559–562
triangle routing problem, 563
infrastructure wireless networks, 517, 528
in-order packet delivery, 311
input ports, 320
packet queues, 327–331
processing, 322–324
switching to output ports, 324–326
input processing, 322–324
instantaneous throughput, 44
instant messaging, 65, 83, 632
Intel 8254x controller, 437
interactivity, 591
inter-AS routing, 390–391, 393–399
inter-AS routing protocols, 382, 398
BGP (Border Gateway Protocol),
390–391, 393–399
interconnection network, 326
interfaces
assigning IP addresses to, 345–349
IP addresses, 338–339
routers, 468
interior gateway protocols, 384
interleaving, 618–620
Intermediate-System-to-Intermediate-
System routing algorithm. See IS-IS
routing algorithm
internal BGP session. See iBGP session
International Organization for
Standardization. See ISO
Internet
access networks, 12–18
address assignment strategy, 342
addressing, 331–363
best-effort service model, 190,
311–313, 612–614, 636
cellular access, 546–554
commercialization, 64
connecting end systems, 5
connectivity, 392
delivering data, 6
difficulty making changes to, 303
distributed applications, 5–6
e-mail, 64, 118–130, 705–711
end systems, 2, 4, 10
flag day, 359
forwarding, 331–363
global traffic, 4
history of, 60–66
hosts, 2, 4
inter-AS routing, 390–391, 393–399
inter-domain protocol, 498
intra-AS routing, 384–390
ISPs (Internet Service Providers), 4,
32–35
IXPs (Internet exchange points), 34
layered architecture, 47–53
link-layer overview, 434–438
link-layer switches, 4, 476–482
mobile devices, 550–552
multicast routing, 411–412
network layer, 332
network of networks, 3–5, 32–35
obtaining presence on, 392–393
packet forwarding, 26–27
protocols, 5, 9
protocol stack, 50
root DNS servers, 135
routers, 4
routing protocols, 27, 383–399
service classes, 636–640
service provider private networks, 66
services description, 5–7
standards, 5
836 INDEX
3G and 4G cellular networks, 65,
547–554
TLD (top-level domain) servers, 135
transport layer overview, 189–191
transport services, 93–96
Web, 64, 98–100, 111
wireless access, 17–18
Internet API, 6, 156–158
Internet applications, 83, 100
application-layer protocols, 96–97
Internet checksum, 203, 334, 442
Internet commerce, 64, 86
cloud, 66
Internet Control Message Protocol. See
ICMP
Internet Corporation for Assigned Names
and Numbers. See ICANN
Internet Engineering Task Force. See IETF
Internet exchange points. See IXPs
Internet Group Management Protocol. See
IGMP
Internet hosts. See hosts
Internet Key Exchange. See IKE
Internet Mail Access Protocol. See IMAP
Internet phone application, 612–614,
618–621
Internet Protocol. See IP
Internet registrar, 392
Internet routing protocols, 353
Internet Service Providers. See ISPs
Internet-Standard Management
Framework, 764–778
Internet telephony, 87, 612–623
Skype, 621–623
internetting, 62
Internet transport protocols, 95
intra-AS (autonomous system) routing
protocols, 380, 381, 397–398
intra-AS routing, 388–390
intra-domain routing and DNS servers,
498–499
intrusion detection, 758
intrusion prevention system. See IPS
intrusion detection systems. See IDSs
IP addresses, 26, 90, 130–131
adapters, 465
address aggregation, 342
allocating, 345
authoritative DNS server, 142
block, 345
CIDR (Classless Interdomain Routing),
342, 344
Classful addressing, 344
destination, 334–335
distinguishing among devices, 344
DNS, 130–144
dotted-decimal notation, 339
globally unique, 339
hierarchical structure, 130, 464
home networks, 349–352
increasing size of, 356
interfaces, 338–339
IP broadcast, 344
lease time, 347
local DNS server, 136
mobility, 556–559
obtaining, 495
prefix, 342, 344
private addresses, 349–350
range, 392
routers, 465, 468
setting up call to known, 627–629
source, 334–335
subnets, 340, 342, 344
translating hostnames to, 131–139
IP anycast, 607–608
IP broadcast address 344, 347
IP datagrams, 233, 496
attacks and, 355
encryption of payloads, 363
Ethernet frames, 471
fragmentation, 335–338
MTU (maximum transmission unit), 335
security, 718
transport-layer protocols, 334
IP header, 202, 335
IP-in-IP encapsulation, 567
IP (Internet Protocol), 5, 51–52, 190, 306,
331–363, 353, 387
best-effort delivery service, 190,
311–312, 612–614, 633–634
confining Internet’s development, 512
INDEX 837
IP (Internet Protocol) (continued)
datagrams, 242, 332–338
de facto standard for internetworking,
512
Internet addressing and forwarding,
331–363
multicasting, 593
network layer, 305–307, 332
pervasiveness, 431
routers, 53
security, 362–363
separation from TCP, 62
IP multicast, 412
IPng (Next Generation IP), 356
IP protocol version 4. See IPv4
IP protocol version 6. See IPv6
IPsec, 362, 72–724
IPS (intrusion prevention system), 355, 740
IP spoofing, 59–60, 701
IP subnet, 541–542
IP tunnels, 303
IPTV, 87
IPv4 addresses, 334, 356
IPv6 addresses, 356, 512
IPv4 addressing, 338–352
IPv4 datagrams, 332–335
format, 306
IPv6 datagrams, 356–359
IPv4 header, 636–637
IPv6 header, 359
IPv4 (IP protocol version 4), 160, 165,
331, 336
IPsec, 362
transitioning to IPv6, 359–362
IPv6 (IP protocol version 6), 306, 332,
356–362
dual-stack approach, 360
flow labeling and priority, 357
IP addresses, 351, 356
transitioning from IPv4, 359–362
tunneling, 360–361
IPv6/IPv4 nodes, 359–360
IS-IS (Intermediate-System-to-
Intermediate-System), 405, 498
IS-IS protocol, 384
ISO (International Organization for
Standardization), 52, 758, 770
ISPs (Internet Service Providers), 32–35
access networks, 12–18
ASs (autonomous systems), 383
customer-provider relationship, 33
end-to-end service, 651–652
global transit, 32
low-tier and upper-tier, 5
multi-homing, 33–34
naming and addressing conventions, 5
obtaining set of addresses from, 345
peer, 34
peering agreements, 399
iterative queries, 137–138
ITU-T (International Telecommunication
Union), 699, 770
IXPs (Internet exchange points), 33, 34
J
Jacobson, Van, 302, 303, 584
Java and client-server programming, 157
jitter, 614–618
K
Kademlia DHT, 156
Kahn, Robert, 62, 231, 431–432, 511
KanKan, 87, 588, 611–612
key derivation, 713–714
key management, 726
Kleinrock, Leonard, 60–62, 80–82, 431, 511
known-plaintext attack, 677
L
label-switched routers, 488
Lam, Simon S., 511–512
LANs (local area networks), 16
broadcast address, 464
configured hierarchically, 482–483
Ethernet, 470
hub-based star topology, 470
MAC address, 463
switches, 17, 470, 475, 483, 496
VLANs (virtual local area networks),
482–486
838 INDEX
Last-Modified: header line, 106, 115
layered architecture, 47–48, 51–53
layer-2 packet switch, 480
layer-4 switch, 492
LDNS (Local DNS Server), 136–137,
605–606
leaky bucket mechanism, 646–648
least-cost path, 365, 367–368
Bellman-Ford equation, 371–372
LEO (low-earth orbiting) satellites, 21–22
Limelight, 35, 114, 604, 609
limited-scope flooding, 405
link-cost changes and DV (distancevector)
algorithm, 376–377
link-layer acknowledgments, 532
link-layer addressing, 462–469
link-layer frames, 55, 434, 436, 438–445,
461–462
link-layer protocols, 52
cable Internet access, 460–461
error detection and correction, 437
services 436–437
link layer, 52, 433–436
bit-level error detection and correction,
438–445
broadcast channels, 433
CRC (cyclic redundancy check) codes,
443–445
error-detection and-correction techniques,
438–445
IEEE protocols, 444
implementation, 437–438
link-layer frame, 434
multiple access protocols, 445–461
networks as, 486–490
point-to-point communication link, 434
services, 436–437
switches, 461
wireless links, 17–18, 519–522
link-layer switches, 4, 22, 53–54, 310,
476–482
link rates, 515
links, 434
broadcast, 445
heterogeneous, 479
MPLS header format, 488
point-to-point, 445
transmission rates, 4
link-scheduling disciplines
FIFO (first-in-first-out), 641–642
priority queuing, 642–643
round robin queuing discipline, 643–644
WFQ (weighted fair queuing), 644–645
work-conserving round robin discipline,
644
link-state advertisements. See LSAs
link-state broadcast, 366–367
link-state messages, 689
link-state protocols, 388, 400
link-state routing algorithms, 366–371
link virtualization, 486–490
link weights, 390
load balancer, 491–492
load distribution, 132–133
load-insensitive routing algorithm, 366
local area networks. See LANs
Local DNS Server. See LDNS
local ISP, 392
logical communication between processes,
186
longest prefix matching rule, 318–319
Long-Term Evolution. See LTE
loss recovery schemes, 618
loss-tolerant, 91, 592–593
lost packets, 262–263
low-earth orbiting satellites. See LEO
satellites
LSAs (link-state advertisements), 405
LTE (Long-Term Evolution), 18, 553–554
M
MAC addresses, 463–465, 497
adapters, 464–465, 465, 471
AP (access point), 528
BSS, 539
802.11 wireless LAN, 528
flat structure, 464
no two adapters have same, 464
permanent, 463–464
switches, 477, 480
INDEX 839
MAC-based VLANs (virtual local area
networks), 486
MAC (message authentication code),
691–693, 777
compared with digital signatures,
696–697
MAC (multiple access protocols), 436,
464, 531
802.11 wireless LANs, 531–537
mail access protocols, 125–130
mailbox, 120–121
mail clients, 97, 125
mail servers, 97, 119–121, 125–126
main-in-the-middle attack and DNS
(domain name system), 143
malicious packet attacks, 355
malware, 56–57
managed device, 761
managed objects, 761
Management Information Base. See MIB
managing entity, 761
MANETs (mobile ad hoc networks), 518
manifest file, 601, 610
MAP message, 460
Master Key. See MK
Master Secret. See MS
maximum segment size. See MSS
maximum transmission unit. See MTU
MBone multicast network, 411
MCR (minimum cell transmission rate), 313
MD5, 710
authentication, 389
MDCs (modular data centers), 494
MD5 hash algorithm, 690
message authentication code. See MAC
message digests, 707
message integrity, 688–693, 706
cryptographic techniques, 675
digital signatures, 695
secure e-mail system, 707–708
message queue, 121
messages, 51
application-layer, 51
authenticating, 689
breaking into shorter segments, 51
confidentiality, 672–673
eavesdropping, 673
encrypted, 672
HTTP format, 103–108
integrity, 673
segmentation, 77
semantics of fields, 97
syntax, 96
transmitting and receiving, 7–8
Metcalfe, Bob, 454, 470, 473
metering function, 650
method field, 104
MIB (Management Information Base),
761–762, 765, 770
MIB modules, 765, 770–772
MIB objects, 765
Microsoft, 64–66, 66, 114
middleboxes, 303
MIMO (multiple-input, multiple output)
antennas, 553
minimum cell transmission rate. See MCR
minimum spanning tree, 403
Minitel project, 63–64
MK (Master Key), 730
mobile ad hoc networks. See MANETs
mobile devices, 81
Internet, 550–552
power management, 543–544
mobile IP, 349, 563–569, 576, 568–569
mobile IP standard, 562
mobile nodes, 554–564
COA (care-of-address), 559
direct routing, 563–564
foreign address, 559
foreign network, 559
home network, 559
indirect routing, 559–562
location protocol, 563
permanent address, 559
permanent home, 557
registering with foreign agent, 566–567
routing to, 559–564
sending datagrams to correspondent, 561
mobile-node-to-foreign-agent protocol,
562
840 INDEX
mobile phones and wireless LAN base stations,
548
mobile station roaming number. See
MSRN
mobile switching center. See MSC
mobility, 513–514
addressing, 556–559
cellular network management, 570–577
GSM (Global System for Mobile
Communications), 576
in IP subnet, 541–542
management, 555–564
mobile IP, 576
network-layer functionality required,
561–562
routing to mobile node, 559–564
scalability, 558
wireless networks, 575–577
mobility-related services, 513
modems, 15
modular arithmetic, 685
modular data centers. See MDCs
modulo arithmetic, 687–688
MPEG, 624
MPLS (Multiprotocol Label Switching),
487–490
MPLS paths, 303
MP3 (MPEG 1 layer 3), 590
MSC (mobile switching center), 550
MSDP (Multicast Source Discovery
Protocol), 412
MS (Master Secret), 714, 716
MSRN (mobile station roaming
number), 571–572
MSS (maximum segment size), 232–234
MTU (maximum transmission unit),
232–233
Ethernet, 471
IP datagrams, 335
multicast routing, 405–412
multicast addresses, 356
multicast group, 406
multicast packets, 405–412
multicast protocols, 362
multicast routing, 399, 407–412
Multicast Source Discovery Protocol. See
MSDP
multicast trees and RTP packets, 624–625
multi-homed stub network, 397
multi-homing, 33
multi-hop wireless networks, 518
multihop paths, 263–265
multimedia
network supported for, 632–655
streaming stored video, 593–612
system-level approach for delivering,
633
VoIP (Voice-over-IP), 612–623
multimedia applications, 588
audio properties, 590–591
bandwidth sensitive, 92
conversational voice, 587, 592–593
improving quality, 635
Internet telephony, 592–593
streaming live audio and video, 587,
593
streaming stored audio and video, 587,
591–592
TCP (Transmission Control Protocol),
200
types, 591–593
UDP (User Datagram Protocol),
200–201, 282
video over IP, 592–593
video properties, 588–589
multipath propagation, 519
multi-player online games, 83
multiple access links, 445–461
multiple access problem, 445
multiple access protocols, 445–461
ALOHA protocol, 63
CDMA (code division multiple access),
449
channel partitioning protocols, 447–448
characteristics, 447–448
FDM (frequency-division multiplexing),
448
random access protocols, 447
taking-turns protocols, 447, 459–460
TDM (time-division multiplexing), 448
INDEX 841
multiple classes of service, 636–640
multiple-input, multiple output antennas.
See MIMO antennas
multiplexing, 191–198
circuit-switched networks, 28–30
connectionless, 193–194
connection-oriented, 194–197
multiplexing/demultiplexing service and
transport layer, 198–199
Multiprotocol Label Switching. See MPLS
multi-tier hierarchy, 33
MX record, 140, 142
N
NAKs (negative acknowledgments), 207,
208
named data, 585
name translation and SIP (Session
Initiation Protocol), 630–632
Napster, 65
NAT (network address translation), 306,
322–324, 349–352
Skype, 622
NCP (network-control protocol), 62
negative acknowledgements. See NAKs
neighbors, 364
neighbor-to-neighbor communication,
372–375
Netflix, 65, 83, 588, 608–610
Netscape Communications Corporation,
64
network adapter, 437–438
network address translation. See NAT
network applications, 83
application-layer protocols, 97
architectures, 86–88
client program, 156
communication between client and
server, 156
creation, 156–168
principles of, 84–98
processes communicating, 88
proprietary, 156–157
server program, 156
Web, 98–116
network-assisted congestion control,
266–269
network attacks, 55–60
network cache servers, 106
network-control protocol. See NCP
network core
circuit switching, 27–32
network of networks, 32–35
packet switching, 22–27
network dimensioning, 634–635
network interface card. See NIC
network-layer addresses, 462, 465
network-layer components, 266
network-layer datagram, 55
network-layer multicast routing
algorithms, 408
network-layer packets, 51–52
network-layer protocols
constrained by service model, 189
difficulty changing, 362
hosts supporting multiple, 471–472
IP (Internet Protocol), 190
logical communication between hosts,
186, 188–189
network layer, 50–53, 189–190
best-effort service, 311–312
broadcast protocols, 405
complexity, 305
confidentiality, 718
connectionless service, 313
connection service, 313
connection setup, 310
datagram networks, 313
datagram service, 364
encapsulating transport-layer segment
into IP datagram, 199
extracting transport-layer segment from
datagram, 186
forwarding, 305, 308–310, 315
guaranteed delivery, 311
host-to-host communication, 305
host-to-host services, 313
ICMP (Internet Control Message
Protocol), 353
in-order packet delivery, 311
842 INDEX
Internet, 332
Internet routing protocols, 353
IP protocol, 51–52, 332, 353
mobility, 561–562
passing segments to, 191–198
path between sender and receiver, 315
process-to-process delivery service,
191–198
reporting errors in datagrams, 332
reserving resources, 316
routing, 51–52, 305–306, 308–310
security, 705, 718–725
services offered by, 310–313
transport layer relationship, 186–189
VC (virtual-circuit) networks, 313–315
network-layer service models, 319
network management
accounting management, 759
Comcast case study, 763–764
configuration management, 759
definition, 759
fault management, 758–759
host monitoring, 756
infrastructure, 760–762
intrusion detection, 758
managed device, 761
managed objects, 761
managing entity, 761
MIB (Management Information Base),
761–762
monitoring traffic, 756–758
network management protocol, 762
performance management, 758
security management, 759
SLAs (Service Level Agreements), 758
standards, 762
network management agent, 762
network management protocol, 762
network mapping, 740
network of networks, 32–35, 62
network prefix, 342
network protocols, 8–9
networks, 340
access networks, 12–18
attacks, 57–58
circuit-switched, 27–30
cellular, 546–554
components, 9–21
crossbar, 326
datagram, 317–319
differentiated service, 634, 648–652
dimensioning best-effort, 634–636
DMZ (demilitarized zone), 741
foreign, 557
growth of, 62
interconnecting, 62
linking universities, 63
as link layer, 486–490
links, 635
mobility, 513–514
MPLS (Multiprotocol Label Switching)
networks, 487–490
multiple classes of service,
636–640
packet delay and loss, 35–44, 635
packet-switched, 4, 22–27
per-connection QoS (Quality-of-
Service) guarantees, 652–655
physical media, 18–22
private, 718
programs communicating on, 84
routes, 4
scheduling mechanisms, 640–645
security, 55–56, 671–674
sockets, 89–90
switched local area networks,
461–486
VC (virtual-circuit), 314–317
visited, 557
wireless LANS, 515, 518, 526–546
network service models, 310–313
NEXT-HOP attribute, 394–395, 397
NIC (network Interface card), link layer
implementation, 437–438
NI (no increase) bit, 268–269
nmap port scanning tool, 196, 258
NOC (network operations center), 755
nodal processing, 36
no increase bit. See NI bit
nomadic computing, 81
INDEX 843
nonces, 704–705, 716–717
non-persistent connections, 100–103, 198
nonpreemptive priority queuing
discipline, 643
non-real-time applications, 92–93
nonrepudiation and cryptographic
techniques, 675
nslookup program, 141–142
N-way-unicast, 400–401
O
OBJECT IDENTIFIER data type, 766
objects, 99, 103
OC (Optical Carrier) standard link, 21
odd parity schemes, 440
offered load, 261
OLT (optical line terminator), 16
ONT (optical network terminator), 16
Open-Shortest Path First. See OSPF
Open Systems Interconnection model. See
OSI model
operational security, 673, 731–742
Optical Carrier standard link. See OC
standard link
optical distribution network, 15–16
optical line terminator. See OLT
optical network terminator. See ONT
options field, 235
origin authentication, 363
orthogonal frequency division multiplexing.
See OFDM
OSI (Open Systems Interconnection)
model, 52–53
OSPF (Open-Shortest Path First),
366–367, 384, 388–390, 498
LSAs (link-state advertisements), 405
router authentication, 388–389
OS vulnerability attacks, 740
out-of-band, 117
out-of-order segments, 236
output buffers, 25
output ports, 320–326
packet queues, 327–331
packet scheduler, 329
processing, 326
output queue, 25
overlapping fragments, 338
overlay network, 154, 486
P
packet classification, 648–649
packet delay, 35–44, 102, 635
packet-discarding policy, 641
packet filtering, 732, 737
packet forwarding, 26–27
packet loss, 25, 41–42, 91, 259, 613, 635
error concealment, 620–621
FEC (forward error correction),
618–619
interleaving, 618–620
predicting imminent, 278
recovering from, 213, 618–621
packet marking, 638
packet-radio networks, 62, 511
packet repetition, 621
packets, 4, 22
average rate, 645
bit errors, 207
buffering, 218
burst size, 646
controlled flooding, 401–403
cumulative acknowledgment, 222
delays, 36–37
delivering, 204–205
destination, 35
destination IP address, 158
detecting loss, 212–215
dropping, 41, 329
duplicate, 210, 213–214
duplicate ACKs, 211
end-to-end delay, 612
FIFO (first-in-first-out), 641–642
format of, 5
forwarding, 4, 308–310, 321–322, 649
header fields, 55
IP spoofing, 59–60
jitter, 614
moving between nodes, 52
path, 35
payload fields, 55
844 INDEX
peak rate, 646
prefix destination address, 318
priority queuing, 642–643
processing delays, 36–37
queuing delays, 25, 37, 39–42
round-robin queuing, 643–644
round-trip delays, 42–43
routing, 308–310
RTT (round-trip time), 102–103
same priority class, 642
sender’s source address, 158
sending, 24
sending multiple, 218
sequence numbers, 210, 218, 220
sockets, 158
source, 35
source address, 162
switching, 324–326
tracing, 42–43
transmitting, 213–214
uncontrolled flooding, 401
VC number, 315
WFQ (weighted fair queuing),
644–645
what to do when loss occurs, 212–215
where queuing occurs, 327–331
Packet Satellite, 511
packet scheduler, 329
packet sniffers, 58–59, 78
packet-switched networks, 4, 25
ARPAnet, 61
comparing transmission and propagation
delay, 38–39
delays, 35–39
end-to-end delays, 42–44
packet loss, 41–42
processing delays, 36–37
propagation delays, 37–38
queuing delays, 37, 39–42
sending packets, 28
transmission delays, 37
packet switches, 4, 310
facilitating exchange of data, 6
link-layer switches, 22, 53, 310
output buffers, 25
routers, 22, 53, 310
store-and-forward transmission,
22, 24
packet switching, 30–31
alternative to circuit switching, 60
forwarding tables, 26–27
packet loss, 25
queuing delays, 25
queuing theory, 60
routing protocols, 26–27
secure voice over military networks, 60
store-and-forward transmission,
22, 24
VC (virtual-circuit) approach, 267
packet-switching networks, 62
paging, 550
parity checks, 440–442
passive optical networks. See PONs
passive spanning, 530
passwords, 703, 710
paths, 4, 365
least-cost, 365
multihop, 263–265
payload fields, 55
PBXs (private branch exchanges), 627
PCM µ-law, 628–629
PCM (pulse code modulation), 590
PDU and SNMP applications, 776–777
peak rate, 646
peer, 34
peer churn and DHTs (distributed hash
tables), 155–156
peering, 33
peers, 86, 144–145
DHT, 155–156
file sharing, 145–151
torrent, 149
peer-to-peer applications. See P2P applications
per-connection QoS (Quality-of-Service)
guarantees, 634, 652–655
per-connection throughput, 260
perfectly reliable channel, 206–207
Performance Management, 758, 763–764
per-hop behavior. See PHB
INDEX 845
permanent address, 559
persistent connections, 100–103
persistent HTTP, 198
personal area networks
Bluetooth, 544–545
Zigbee, 545–546
PGP (Pretty Good Privacy), 678, 706,
709–711
PHB (Per-hop behavior), 649–651
physical address, 463
physical layer, 50, 52–53
physical media, 4
coaxial cable, 20
costs, 19
fiber optics, 20–21
guided media, 19
radio channels, 21
satellite radio channels, 21–22
twisted-pair copper wire, 19–20
unguided media, 19
piconet, 544
piggybacked acknowledgment, 237
PIM (Protocol-Independent Multicast),
411–412, 584
ping program, 353
pipelined reliable data transfer protocols,
215–218
pipelining, 218
persistent connections, 103
TCP (Transmission Control Protocol),
240
plaintext, 675
playback attacks, 703, 777
playout delay, 614
plug-and-play protocol, 346
plug-and-play switches, 479–480
PMS (Pre-Master Secret), 716
points of presence. See PoPs
point-to-point, 232
point-to-point communication link, 434
point-to-point links, 436, 445
Point-to-Point Protocol. See PPP
poisoned reverse, 377–378
poisoning attack, 143
policing disciplines, 645–648
policing mechanisms, 640
polling protocols, 459
polls, 459
polyalphabetic encryption, 678
polynomial codes, 443
PONs (passive optical networks), 15–16
POP3 (Post Office Protocol-Version 3),
127–129
POP3 server, 127, 129
PoPs (points of presence), 33
POP3 user agent, 128
port-based VLAN, 483–484
port numbers, 90, 158
addressing processes, 351
destination, 234
protocols, 90
source, 234
Web servers, 197–198
well-known, 192, 196
port-based VLAN, 483–484
port scanners, 196
port scans, 196, 740
postive acknowledgment. See ACK
POST method, 104–105
Post Office Protocol-Version 3. See POP3
power management, 543–544
P2P (peer to peer)
applications, 97–98
architecture, 86–88, 144–148
BitTorrent, 149–151
connection reversal, 352
DHTs (distributed hash tables), 145,
151–156
file distribution, 83, 88, 45–151
NAT, 351–352
Skype, 621–622
video streaming applications, 592
PPP (point-to-point protocol), 434, 445
PPstream, 87
PPTV and P2P delivery, 611
prefetching, 592
video, 596–597
prefixes, 342, 344, 393
awareness of, 396–397
BGP attributes, 394
846 INDEX
forwarding table, 396–397
gateway routers, 393
Pre-Master Secret. See PMS
prerecorded video, 591
presentation layer, 53
presentation service, 780
Pretty Good Privacy. See PGP
priority queuing, 642–643
privacy
cookies, 108
proxy servers, 738
QQ, 623
Skype, 623
SSL (Secure Sockets Layer), 738
Web sites, 738
private branch exchanges. See PBXs
private CDNs (Content Distribution
Networks), 603
private key, 684–685, 693, 708
passwords, 710
private networks, 66, 718
processes, 88–90
communicating by sending messages to
sockets, 157
communicating using UDP sockets,
158
connection sockets, 198
handshaking, 231
logical communication between, 186
sockets, 191
processing delays, 36–37
programming, event-based, 223
propagation delays, 24, 36–39, 456
proprietary network applications,
156–157
Protocol-Independent Multicast. See PIM
protocols, 5, 68
alternating-bit, 214
application-layer, 49–50
congestion-control, 9
defining, 7–9
hardware-implemented, 9
human analogy, 7–8
interior gateway, 384
Internet, 9
IP (Internet Protocol), 5
layering, 49–50
nonce, 704–705
packet sizes, 335
plug-and-play, 346
port numbers, 90
real-time interactive applications,
623–632
routing, 51–52
RTP, 623–626
SIP, 626–632
soft state, 408–409
SR (selective repeat), 223–230
stateless, 100
stop-and-wait, 209, 215, 217
TCP, 5
UDP, 5
transmission and receipt of messages,
7–8
transport-layer, 50
protocol stack, 50
provider, 32
proxy servers, 106, 110, 738
public key algorithm, 716
public key certification, 697–699
public-key cryptography, 708
digital signatures, 693–694
private key, 693
public key, 693, 697
secure e-mail system, 706–707
public key encryption, 683–688
public-key encryption algorithm, 687
public keys, 684–685, 693, 706–708,
713–714
binding to particular entity, 697–698
certifying, 708
encryption/decryption algorithms,
684
public key systems, 676
pull protocol, 124
pulse code modulation. See PCM (pulse
code modulation)
push protocol, 124
PUT method, 105
Python, 157, 160, 193
INDEX 847
Q
QAM16 modulation, 521
Q2931b protocol, 654
QoS (Quality-of-Service), 329, 653–654
QQ, 592, 623
Quality-of-Service. See QoS
quantization, 590
query
ARP message, 467
information about, 141
query messages, 140–142
queues
FIFO (first-in-first-out), 641–642
packet-discarding policy, 641
priority queuing, 642–643
provable maximum delay, 647–648
round robin queuing discipline,
643–644
WFQ (weighted fair queuing),
644–645
work-conserving round robin
discipline, 644
queuing, 327–331
queuing delays, 25, 36–37, 39–42, 60
R
radio channels, 21
Radio Network Controller. See RNC
RADIUS protocol, 530, 730
random access protocols, 447, 473
Aloha protocol, 452–453
CSMA (carrier sense multiple access)
protocol, 453–456
CSMA/CD (carrier sense multiple
access with collision detection),
455–459
slotted ALOHA protocol, 450–452
Random Early Detection algorithm. See
RED algorithm
rarest first, 149
rate adaptation, 542–543
RC4 algorithm, 727–728
RCP (Routing Control Platform), 786
rdt (reliable data transfer protocol), 204
building, 206–215
packet reordering, 229–230
pipelined, 215–218
TCP (Transmission Control Protocol),
204
unreliable layer below, 204
real-time applications
timing, 92
UDP (User Datagram Protocol), 200
real-time interactive applications
protocols, 623–632
RTP (Real-Time Transport Protocol),
623–626
SIP, 626–632
real-time measurements of delay and loss
performance, 606
Real-Time Streaming Protocol. See RTSP
Real-Time Transport Protocol. See RTP
receive buffer, 233
receiver authentication, 706
receiver-based recovery, 621
receiver feedback, 208
receivers
ACK generation policy, 247
defining operation, 206
sequence number of packet acknowledged
by ACK message, 212
receiver-side transport layer, 54
receive window, 250–252
receive window field, 234
receiving adapter, 472
receiving processes addresses, 90
records, inserting in DNS database, 142,
144
recursive queries and DNS servers,
137–138, 140
RED (Random Early Detection)
algorithm, 329
regional ISPs, 33
registrars, 142
registration with home agent, 568–569
relays, 622–623
reliable channel, 204
reliable data transfer, 91, 190
application layer, 204
channel with bit errors, 207–212
848 INDEX
link layer, 204
lossy channel with bit errors, 212–215
perfectly reliable channel, 206–207
principles, 204–230
reliable channel, 204
TCP (Transmission Control Protocol),
230–231, 240, 242–250
transport layer, 204
transport-layer protocols, 91
reliable data transfer protocol. See rdt
reliable data transfer service, 235
reliable delivery, 436
reliable transport service, 269
remote host, transferring files, 116–118
rendezvous point, 404
repeater, 474
replicated servers, 132
reply messages and DNS (domain name
system), 140–142
repositioning video, 600
request messages and HTTP, 103–105
request-response mode, 772
requests for comments. See RFCs
Request to Send control frame. See RTS
control frame
residential ISPs, 87
resource-management cells. See RM cells
resource records. See RRs
resource reservation protocols, 362
resources
admitting or blocking flows, 653
efficient use of, 640
reservations, 653–654
response ARP, 467
response messages and HTTP, 105–108
retransmission, 208, 212
retransmitting data, 241, 262
retransmitting packets, 259, 261–263
reverse path broadcast. See RPB
reverse path forwarding. See RPF
Rexford, Jennifer, 786–787
RFCs (requests for comments), 5
RIP advertisements, 384–385
RIP request message, 387
RIP response message, 384
RIP routers, 386–387
RIP (Routing Information Protocol), 384,
498
hops, 384
implementation aspects, 386–388
IP network-layer protocol, 387
lower-tier ISPs, 388
modifying local routing table and propagating
information, 387
RIP messages, 384–385
RIP table, 385–386
routing updates, 384
UDP transport-layer protocol, 387
UNIX implementation, 387–388
Rivest, Ron, 684, 690
RM (resource-management cells),
267–269
RNC (Radio Network Controller), 552
roaming number, 572
Roberts, Larry, 61, 511
root DNS servers, 134–136
round robin queuing discipline, 643–644
round-trip delays, 43
round-trip time. See RTT
route aggregation, 342
route attributes, 395
router control plane functions, 322
router discovery message, 566–567
router forwarding plane, 321
routers, 4, 12, 22, 53, 303, 310
access control lists, 734
adapters, 468
address of, 43
administrative autonomy, 380
area border, 389
ARP modules, 468
AS-PATH attribute, 394
ASs (autonomous systems), 380
authenticated and encrypted channel
between, 725
buffering packet bits, 24
buffer sizing, 328–329
connected into network, 338
connection state information, 315
control functions, 321–322
INDEX 849
routers (continued)
control plane implemented in, 331
data center hierarchy, 492–493
default, 364
destination, 364
finite buffers, 261–265
firewalls, 355, 481
first-hop, 364
fixed-length labels, 487
forwarding function, 320–322
forwarding table, 26, 308–309,
317–318, 322–323, 394, 396–397,
469
gateway, 380–381
implementing layers 1 through 3, 53
incident links, 22
input ports, 320
input processing, 322–324
interfaces, 338, 468
intra-AS routing protocols, 397
IP addresses, 394, 465, 468
IP protocol, 53
label-switched, 488
layer-2 packet switch, 480
link-layer and MAC addresses,
462–463, 465
longest prefix matching rule, 318–319
lookup, 323–324
looping advertisements, 394
memory access times, 324
network core, 4
network-layer addresses, 462, 465
output ports, 320–321
output processing, 326
packet-forwarding decisions, 364
packet loss, 327
packets not cycling through, 481
physical links between, 364
plug-and-play, 481
primary role, 306
processing datagrams, 480
processing packets, 351
protocols, 9
queuing, 327–331
routing control plane, 331
routing packets, 380–382
routing processor, 321
routing tables, 385–386
scale, 379–380
self-synchronizing, 371
source, 364
spanning tree, 481
store-and-forward, 22, 24
store-and-forward packet switches, 480
versus switches, 480–482
switching, 320, 324–326
terminating incoming physical link, 320
VC setup, 316
routes, 4, 394–396
route summarization, 342
routing, 305–306, 308–310
advertising information, 382–383
broadcast, 399–405
calls to mobile user, 571–572
distance vector, 384
hierarchical, 379–383
hot-potato, 382
to mobile node, 559–564
multicast, 399, 405–412
storing information, 379–380
routing algorithms, 309, 363–383
ARPAnet, 366
circuit-switched, 379
decentralized, 366
DV (distance-vector) algorithm, 366,
371–379
dynamic, 366
forwarding tables, 364
global, 365–366
hierarchical routing, 379–383
least costly paths, 365
load-sensitive, 366
LS (link-state) algorithms, 366–371
path from source to destination router,
364
scale of routers, 379–380
static, 366
switches, 494–495
viewing packet traffic flows, 379
routing control plane, 331
850 INDEX
Routing Control Platform. See RCP
routing daemons, 674
Routing Information Protocol. See RIP
routing loop, 377
routing protocols, 26–27, 51–52
BGP (Border Gateway Protocol),
390–399, 498–499
DV (distance vector) algorithms,
374–375
executing, 321
inter-AS, 382
Internet, 383–399
intra-AS, 380–381
IS-IS, 384
messages, 309
OSPF (Open-Shortest Path First), 384
RIP (Routing Information Protocol),
384
RPB (reverse path broadcast), 402
RPF (reverse path forwarding), 402–403,
411
RRs (resource records), 139–141
RSA algorithm, 684–688, 710
RST flag bit and segment 235, 258
RSVP, RSVP-TE protocol, 489, 654
RTP packets, 624–625
RTP (Real-Time Transport Protocol), 588,
623–626, 668
UDP streaming, 595
RTS/CTS exchange, 537
RTS frame, 536–537
RTSP (Real-Time Streaming Protocol),
117, 595, 668
RTS (Request to Send) control frame,
535–537
RTT (round-trip time), 102–103
EWMA (exponential weighted
moving average), 240
TCP (Transmission Control Protocol),
238–241
S
SAD (Security Association Database), 721
SA (security association), 720–721
satellite links, 16, 21–22
scalability and P2P architecture, 145–148
scheduling mechanisms, 640–645
Schulzrinne, Henning, 623, 632, 668–670
SDN (Software Defined Networking), 786
secure communication, 672–674
secure e-mail system, 706–708
Secure Hash Algorithm. See SHA-1
secure networking protocols and
message integrity, 689
Secure Network Programming, 511
Secure Sockets Layer. See SSL
security, 55–56
application-layer protocol, 705
attacks, 674
cryptography, 675–688
data link layer, 705
digital signatures, 688–699
e-mail, 705–711
end-point authentication, 700–705
IEEE 802.11i, 728–731
IP datagrams, 718
IP (Internet Protocol), 362–363
IPsec, 362
message integrity, 688–693
mobile IP, 566
network layer, 705, 718–725
networks, 671–674
operational, 673, 731–742
OSPF (Open-Shortest Path First),
388–389
P2P architecture, 88
public key encryption, 683–688
RSA, 687
SNMPv3, 775–778
switches, 479
TCP connection, 711–717
transport-layer protocols, 93, 705
transport services, 93
user-based, 777
WEP (Wired Equivalent Privacy),
726–728
wireless LANs, 726–731
security and administration capabilities,
765
security association. See SA
INDEX 851
Security Association Database. See SAD
Security Management, 759, 764
Security Policy Database. See SPD
segments, 51, 186, 189
acknowledgment number, 236
destination port number field, 192
fast retransmit, 248
fields, 191–192
out-of-order, 236
piggybacked acknowledgment, 237
sequence numbers, 235–238
source port number field, 192
TCP (Transmission Control Protocol),
233
unique identifiers, 192
selective acknowledgment, 250
selective repeat protocols. See SR
protocols
self-learning, 478–479, 497, 542
self-replicating, 56
self-scalability, 87
send buffer, 232
sender
countdown timer, 214
defining operation, 206
detecting and recovering from lost
packets, 212–215
leftmost state, 208
receive window, 250
rightmost state, 208
sending multiple packets without
acknowledgments, 218
sequence number of packet, 212
utilization, 217
sender authentication, 706–708
sender-to-receiver channel, 213–214
sending rates, 260
send side states rdt2.0 protocols, 208
sequence-number-controlled flooding,
401–403, 405
sequence numbers, 210, 212, 218–220,
234, 614–615, 618, 717
IPsec, 724
RTP packets, 625
SSL (Secure Sockets Layer), 715
SYN segment, 252–253
TCP segments, 235–236
TCP (Transmission Control Protocol),
244, 249
Telnet, 237–238
server authentication, 712
server processes, 88, 164, 232
server program, 156, 163
servers, 2, 10–11, 88–89
always on, 86
dedicated socket, 167
hostname of, 160
IP addresses, 86, 160, 161, 163
network attacks, 57–58
non-persistent connections, 198
persistent HTTP, 198
port number, 161, 167
TCP socket creation, 167
Web caches as, 111
server SMTP, 122
server socket TCP connection, 163
server-to-client throughput, 44–45
Service Level Agreements. See SLAs
service model, 49
service providers and private
networks, 66
services, 49
description of Internet, 5–7
DNS (domain name system),
131–133
flow of packets, 311
transport layer, 186
transport protocols, 189
Service Set Identifier. See SSID
Serving GPRS Support Nodes. See SGSNs
session encryption key, 714
Session Initiation Protocol. See SIP
session keys, 687, 707, 714
session layer, 53
SGMP (Simple Gateway Monitoring
Protocol), 764
SGSNs (Serving GPRS Support Nodes),
552
SHA, 710
Shamir, Adi, 684
852 INDEX
Shannon, Claude, 80, 82
shared medium, 20
SHA-1 (Secure Hash Algorithm), 691
shortest paths, 365
SIFS (Shorter Inter-frame Spacing), 532
signaling messages, 316
signaling protocols, 317
signal-to-noise ratio. See SNR
signature-based IDSs (intrusion detection
systems), 741–742
silent periods, 29–30
simple authentication, 389
Simple Gateway Monitoring Protocol. See
SGMP
Simple Mail Transfer Protocol. See
SMTP
Simple Network Management Protocol.
See SNMP
single-hop, wireless networks, 518
SIP (Session Initiation Protocol), 588,
626–632, 668–669
Skype, 65, 83, 87, 588, 621–623
conversational voice and voice, 592
proprietary application-layer protocols,
97
UDP (User Datagram Protocol), 613
SLAs (Service Level Agreements), 758
sliding-window protocol, 220
slotted ALOHA protocol, 450–452
node’s decision to transmit, 453–455
small office, home office subnets. See
SOHO subnets
SMI (Structure of Management
Information), 765, 766–769
SMTP clients, 122–123
SMTP servers, 123
SMTP (Simple Mail Transfer Protocol),
51, 97, 117, 120–127
SNMP applications, 776–777
SNMP messages, 777
SNMP (Simple Network Management
Protocol), 758–759, 762, 764–778
SNMPv3, 765, 775–778
SNMPv2 (Simple Network Management
Protocol version 2), 772, 773–775
Snort IDS system, 740–742
SNR (signal-to-noise ratio), 520–521
social networking, 83, 86
social networks, 64–65, 100
socket interface, 100
socket module, 160
socket programming
TCP (Transmission Control Protocol),
158, 163
UDP, 157–158
sockets, 89–91, 91, 191
assigning port number, 162
port number, 158
soft guarantee, 634
soft state protocols, 408–409
software control plane, 331
Software Defined Networking. See SDN
SOHO (small office, home office) subnets
and IP addresses, 349–352
source
host and source router, 364
total delay to destination, 42–44
source port numbers, 192, 194, 196, 234
source quench message, 353
source router, 364
source-specific congestion-control actions,
267
source-specific multicast. See SSM
spam, 56
spanning-tree broadcast, 403–405
spanning trees, 403–405, 481
spatial redundancy, 589
SPD (Security Policy Database), 724
special socket server program, 163
speed-matching service, 250
SPI (Security Parameter Index), 721
split-connection approaches, 577
Sprint, 5, 33, 758
spyware, 56
SRAM, 324
SR (selective repeat) protocols,
223–230
SSH protocol, 237
SSID (Service Set Identifier), 529
SSL record, 715–716
INDEX 853
SSL (Secure Sockets Layer), 711
anonymity, 738
API (Application Programmer
Interface) with sockets, 712
block ciphers, 678
breaking data stream into records, 714
connection closure, 717
cryptographic algorithms, 716
data transfer, 713–715
designed by Netscape, 711
handshake, 713–714, 716–717
HTTP transactions security, 712
key derivation, 713–714
nonces, 717
popularity, 711
privacy, 738
public key certification, 697
sequence numbers, 715
SSL classes/libraries, 712
SSL record, 715–716
transport protocols, 712
SSM (source-specific multicast), 412
state, 117
stateful packet filters, 732, 735–736
stateless protocols, 100
static routing algorithm, 366
stations, 531–533
status line in HTTP response messages,
106
steaming prerecorded videos, 591
stop-and-wait protocols, 209–210, 215,
217
store-and-forward packet switches, 22, 24,
480
stream ciphers, 678
streaming, 591
live audio and video, 587, 593
stored audio and video, 587, 591–592
video, 589
streaming stored video, 593–612
adaptive HTTP streaming, 593
adaptive streaming, 600–601
bandwidth, 594
CDNs (content distribution
networks), 602–608
client buffering, 594–595
continuous playout, 591–592
DASH (Dynamic Adaptive Streaming
over HTTP), 600–601
end-to-end delays, 594
HTTP streaming, 593, 596–600
interactivity, 591
KanKan, 611–612
Netflix, 608–610
streaming, 591
UDP streaming, 593, 595–596
YouTube, 610–611
streaming video, 592
TCP (Transmission Control Protocol),
596
Structure of Management Information. See
SMI
stub network, 397–398
multi-homed, 397
subnet mask, 340
subnets, 340
advertising existence to Internet,
391
class A, B and C networks, 344
defining, 341
DHCP offer message, 347
DHCP servers, 346
IP addresses, 340, 342, 345
IP definition of, 340–341
prefixes, 393
sending datagrams off, 468–469
shortest-path tree, 388
successful slots, 451
switched Ethernet, 470
switched-LANs
ARP (Address Resolution Protocol),
465–468
Ethernet, 469–476
link-layer addressing, 462–469
link-layer switches, 476–482
MAC addresses, 463–465
switch poisoning, 480
VLANs (virtual local area networks),
482–486
switched networks, 481
854 INDEX
switches, 80
aging time, 478
broadcasting frames, 464
broadcast storms, 481
collisions elimination, 479
congestion-related information, 268
data center hierarchy, 492–493
enhanced security, 479
Ethernet, 470, 475
filtering, 476–477
filtering frame, 477
forwarding, 476–477
gathering statistics, 479
heterogeneous links, 479
high filtering and forwarding rates, 480
link-layer, 461, 476–482
link-layer addresses, 462
link-layer frames, 476
MAC addresses, 480
management, 479
plug-and-play devices, 479–480
processing frames, 480
versus routers, 480–482
routing algorithms, 494–495
self-learning, 478–479, 497, 542
small networks, 482
store-and-forward packet switches,
480
switch table, 476
tracking behavior of senders, 267
transparent, 476
trunk port to interconnect, 484
VLANs (virtual local area
networks), 483–484
switch fabric, 320, 322, 327, 329–330
switching and routers, 324–326
switch output interfaces buffers, 476
switch poisoning, 480
switch table, 476–477
symmetric algorithm, 716
symmetric key, 706–707, 707
symmetric key algorithm
block ciphers, 678–681
Caesar cipher, 676
monoalphabetic cipher, 676–677
polyalphabetic encryption, 678
stream ciphers, 678
symmetric key encryption and CBC
(cipher-block chaining), 681–682
SYNACK segment, 257–258
SYN bit, 235, 253
SYN cookies, 257
SYN flood attack, 252, 253, 257
SYN packet, 258
SYN segments, 252–254, 257–258
SYN_SENT state, 254
T
taking-turns protocols, 447, 459–460
TCAMs (Ternary Content Address
Memories), 324
TCP buffers, 597–598
TCPClient.py client program, 164–166
TCP clients, 195, 253–255
TCP congestion-control algorithm,
272–277, 279
TCP connections, 57, 94
allocating buffers and variables, 253
bandwidth, 281
bottleneck link, 279–281
buffers, 233
between client and server, 166
client process, 232
client-side TCP sending TCP
segment to server-side TCP,
252–253
client socket, 163
connection-granted segment, 253
ending, 253–254
establishing, 232, 252–253, 713
full-duplex service, 232
HTTP server, 596
management, 252–256, 258
out-of-order segments, 236
packet loss, 281
parallel and fairness, 282
point-to-point, 232
processes sending data, 232–233
receive buffer, 233, 250
regulating rate of traffic, 190
INDEX 855
TCP connections (continued)
security, 711–717
send buffer, 232
server process, 232
server socket, 163
socket connection to process, 233
split-connection approaches, 577
three-way handshake, 102–103, 166,
232
throughput, 280
transporting request message and
response message, 101
variables, 233
TCP header, 234–235
TCP/IP (Transmission Control
Protocol/Internet Protocol), 5, 63,
93, 231, 431
TCP ports, 258
TCP Reno, 276, 278
TCP segments, 233–236, 253
with different source IP addresses,
194–195
header overhead, 200
loss, 266
reordering, 715
structure, 233–238
TCP sender, 242–243, 269, 270
awareness of wireless links, 577
congestion control, 250
TCP server, 163, 195
TCPServer.py server program, 166–168
TCP sockets, 165–166, 497, 499
server-side connection socket, 163
welcoming socket, 163
TCP splitting, 273
TCP streaming and prefetching video, 597
TCP SYNACK segment, 499
TCP SYN segment, 499
TCP Tahoe, 276
TCP (Transmission Control Protocol), 5,
51, 93, 189, 313, 338
acknowledgment numbers, 244
block ciphers, 678
buffer and out-of-order segments, 249
buffer overflow, 251
byte stream, 242
checksum, 334
client-server application, 157
congestion avoidance, 272–276
congestion control, 95, 190, 199–200,
240, 247, 269–272, 274–283,
576–577, 596, 613
congestion window, 269–270,
276–277, 576
connection-establishment delays, 200
connection-oriented, 94, 163, 230–238
connection state, 200, 231
continued evolution of, 279
cumulative acknowledgments, 236,
243, 248–249
duplicate ACK, 247–248
early versions, 62
end-to-end congestion control, 266,
269
extending IP’s delivery service, 190
fairness, 279–282
fast retransmit, 247–248
flow control, 240, 250–252
full-duplex, 235
GBN (Go-Back-N) protocol, 248–250
high-bandwidth paths, 279
host-based congestion control, 63
HTTP and, 116, 200
implicit NAK mechanism, 240
integrity checking, 190
Internet checksum, 442
lost acknowledgment, 244
lost segments, 238
MSS (maximum segment size),
232–234
MTU (maximum transmission unit),
232–233
multimedia applications, 200
negative acknowledgments, 248
packet loss, 247–248, 613
pipelining, 240
positive acknowledgments, 240
receive buffer, 270
receiver-so-sender ACK, 576
receive window, 251
856 INDEX
reliable data transfer, 96, 190,
230–231, 240
reliable data transfer service, 95, 100,
123, 163, 199–200, 235, 242–250
resending segment until acknowledged,
199
retransmission timeout interval, 241
retransmission timer, 242
retransmitting data, 473
retransmitting segments, 239–240, 246,
249, 575–576
RST segment, 258
RTT (round-trip time) estimation,
238–241
security services, 95
segments, 189
selective acknowledgment, 250
separation of IP, 62
sequence numbers, 244, 249
server-to-client transmission rate, 596
services, 94–95
socket programming, 158, 163
states, 254
state variable, 243
steady-state behavior, 278–279
streaming media, 200–201
streaming video, 596
SYNACK segment, 258
SYN segments, 257–258
TCP Reno, 276, 278
TCP segments, 233
TCP Tahoe, 276
TCP Vegas, 278
32-bit sequence number, 220
three-way handshake, 163, 200, 253
throughput macroscopic description,
278–279
timeout, 238–241, 243
timeout, 244–247
timeout/retransmit mechanism, 238
transmission rate, 278
Web servers, 197–198
window size, 266
wireless networks, 575–577
TCP Vegas, 278
TDM (time-division multiplexing), 28–30,
31, 448, 549
telco (telephone company), 13–14
Telenet, 62
telephone company. See telco
telephone networks, 27
circuit switching, 60
complexity, 319
frequency band, 29
packet switching, 31
Telnet, 86
blocked, 737
sending message to mail server, 125
SMTP server, 124
TCP example, 234, 237–238
temporary IP address, 346
10BASE-2, 473–474
10BASE-T, 473–474
10GBASE-T, 474–475
Ternary Content Address Memories. See
TCAMs
3GPP (3rd Generation Partnership
Project), 550, 552, 362
third-party CDNs (Content Distribution
Networks), 603
3DES, 680
3G cellular data networks, 550–552
3G cellular mobile systems versus wireless
LANs, 548
3G core network, 550–552
3G networks, 669
3G radio access networks, 552
3G systems, 547
3G UMTS and DS-WCDMA (Direct
Sequence Wideband CDMA), 552
three-way handshake, 102–103, 232, 253,
499, 735
throughput, 260
average, 44
end-to-end, 44–47
fluctuations in, 92
instantaneous, 44
macroscopic description for TCP,
278–279
server-to-client, 44–45
INDEX 857
throughput (continued)
streaming video, 592
TCP connection, 280
transmission rates of links, 47
transport-layer protocols, 92
zero in heavy traffic, 265
tier-1 ISPs, 33–34
time-division multiplexing. See TDM
timeout
doubling interval, 246–247
event, 222, 244
length of intervals, 238–239
setting and managing interval, 241
TCP (Transmission Control Protocol),
238–241, 243
timer management and overhead, 242
time-sensitive applications, 95
time-sharing networks, 62
time slots, 448
timestamps, 614–615, 617, 625
time-to-live field. See TTL (time-to-live)
field
timing guarantees, 92–93
TLD (top-level domain) DNS servers,
134–136, 143
DNS servers, 134
TLS (Transport Layer Security), 711
TLV (Type, Length, Value) approach, 780
token-passing protocol, 459–460
top-down approach, 50
top-level domain DNS servers. See TLD
DNS servers
top-level domains, 135
Top of Rack switch. See TOR switch
top-tier switch, 492
TOR anonymizing and privacy service,
738
torrents, 149
TOR (Top of Rack) switch, 490, 492
TOS (type of service) bits, 333
total nodal delay, 36
Traceroute program, 27, 353–355
end-to-end delays, 42–43
tracker, 149
traditional packet filters, 732–734
traffic
bursty, 60
conditioning, 648–649
intensity, 40
traffic engineering, 489
traffic isolation, 638–640
traffic policing, 638–639
traffic profile, 650
transferring files, 116–118
transfer time, 45
Transmission Control Protocol. See TCP
Transmission Control Protocol/Internet
Protocol.
transmission delays, 36–39
transmission rates, 4, 45–46
transmitting
frames, 532
packets in datagram networks, 317
transport layer, 51, 53, 185
application-layer message, 54
automatically assigning port number,
193–194
checksumming, 442–443
congestion control, 266
connectionless service, 313
connection-oriented service, 313–314
datagram passed, 337
delivering data to socket, 191
demultiplexing, 191–198
destination host, 191
error checking, 203
multiplexing, 191–198
multiplexing/demultiplexing
service, 198–199
network layer relationship, 186–189
overview, 189–191
process-to-process communication,
305, 313
reliable data transfer, 204
responsibility of delivering data to
appropriate application, 191
segments, 189
services, 186
transport-layer multiplexing, 192
transport-layer packets, 186
858 INDEX
transport-layer protocols, 50, 91
end systems implementation, 186
IP datagrams, 334
living in end systems, 188
logical communication between
processes, 186, 188–189
reliable data transfer, 91
reliable delivery, 436
security, 93, 705
TCP (Transmission Control Protocol),
189
throughput, 92
timing, 92–93
UDP (User Datagram Protocol), 189
Transport Layer Security. See TLS
transport-layer segments, 54–55, 186
datagrams, 242
delivering data to correct socket,
191–198
fields, 191
unreliability, 242
transport mode, 721
transport protocols
Internet applications, 96
services, 189
SSL (Secure Sockets Layer), 712
TCP, 51
UDP, 51
transport services
available to applications, 91–93
connection-oriented service, 94
provided by Internet, 93–96
reliable data transfer, 91
security, 93
TCP services, 94–95
throughput, 92
timing, 92–93
UDP, 95
trap messages, 773
tree-join messages, 404–405
triangle routing problem, 563
triple-DES, 710
truncation attack, 717
TTL (time-to-live) field, 139–140, 334
tunneling, 360–361, 561
tunnel mode, 721–722
twisted-pair copper wire, 19–20, 475
Twitter, 65, 83, 86
two-dimensional parity scheme, 441–442
2G cellular networks architecture,
548–550
Type, Length, Value approach. See TLV
approach
type of service bits. See TOS bits
U
UDP checksum, 202–204
UDPClient.py client program, 158–161
UDP header, 202
UDP packet, 258, 346, 595
UDP ports, 258
UDP segments, 202–204, 495–497, 613
UDPServer.py server program, 158, 161,
194
UDP sockets
communicating to processes, 158
creation, 161
identifying, 194
port numbers, 193–194
UDP streaming, 593, 595–596
UDP (User Datagram Protocol), 51, 93,
189, 387
checksum, 208, 334
client-server application, 157
congestion control, 201, 282
connection establishment, 200
connectionless transport, 95, 198–204
connection state, 200
datagrams, 189
delays, 200
destination port number, 199
development, 62
directly talking with IP, 199
discarding damaged segment, 204
DNS and, 199–200
end-to-end principle, 203
end-to-end throughput, 95
error checking, 199
error detection, 202–204
extending IP’s delivery service, 190
INDEX 859
UDP (User Datagram Protocol)
(continued)
fairness, 282
finer application-level control over
data, 199
flow control, 252
gaps in data, 473
handshaking, 199
header overhead, 200
integrity checking, 190
Internet checksum, 442
Internet telephony applications, 96
multimedia applications, 200–201,
282
multiplexing/demultiplexing
function, 199
network management data, 200
no-frills segment-delivery service, 199
packet loss, 613
passing damaged segment to
application, 204
real-time applications, 200
reliable data transfer, 201
RIP routing table updates, 200
RTP and, 624
segments, 189
small packet header overhead, 200
socket programming, 157–158
transport services, 95
unreliability, 95, 190
wireless networks, 575–577, 301
UMTS (Universal Mobile
Telecommunications Service)
3G standards, 550
unchoked, 150
uncontrolled flooding, 401
undetected bit errors, 440
unguided media, 19
unicast addresses, 356
unicast applications and RTP packets,
624
unicast communication and IP addresses,
406
unidirectional data transfer, 205
Universal Plug and Play. See UPnP
UNIX
BSD (Berkeley Software Distribution)
version, 384
nslookup program, 141–142
RIP implemented in, 387–388
Snort, 742
unreliable data transfer, 206
unreliable service, 190
unshielded twisted pair. See UTP
UPnP (Universal Plug and Play), 352
urgent data pointer field, 235
URL field, 104
URLs, 99
US Department of Defense Advanced
Research Projects Agency. See
DARPA
user agents, 119–121, 126–127
user-based security, 777
user-server interaction and HTTP
(HyperText Transfer Protocol),
108–110
utilization, 217
UTP (unshielded twisted pair), 19–20
V
VANET (vehicular ad hoc network), 518
variables and TCP connection, 233
VC networks, 314–317, 319–320
VC (virtual-circuit), 267, 314
roots in telephony world, 319
terminating, 316
vehicular ad hoc network. See VANET
Verizon, 758
FIOS service and PONs (passive optical
networks), 15–16
version number, 333
video, 588–589
P2P delivery, 611
prefetching, 596–597
prerecorded, 591
repositioning, 600
streaming stored, 593–612
timing considerations and tolerance of
data loss, 592
traversing firewalls and NATs, 596
860 INDEX
video conferencing, 83
video over IP, 592–593
video stream, 625
virtual-circuit. See VC (virtual-circuit)
virtual local area networks. See VLANs
virtual private networks. See VPNs
viruses, 56, 740
visited MSC, 574
visited networks, 557, 570
visitor location register. See VLR
VLANs (virtual local area networks),
482–486
VLAN tag, 484–486
VLAN trunking, 484–485
VLR (visitor location register), 570
voice and video applications, 83
VoIP (Voice-over-IP), 83
adaptive playout delay, 615–618
end-to-end delay, 613–614
enhancing over best-effort network, 612
fixed playout delay, 615
jitter and audio, 614–618
media packetization delays, 44
packet loss, 613
recovering from packet loss, 618–621
sequence numbers, 615
timestamps, 615
wireless systems, 668
VPNs (virtual private networks), 362
confidentiality, 720
end points, 725
IPsec, 718–720
IPv4, 719
MPLS (Multiprotocol Label
Switching), 489–490
SA (security association), 720
tunnel mode, 721
vulnerability attacks, 57
W
Web, 64, 86, 97
client-server application architecture,
100
HTTP (HyperText Transfer Protocol),
98–100
network applications, 98–116
operating on demand, 98
platform for applications emerging
after 2003, 98
terminology, 98–99
Web applications, 97
client and server processes, 88
client-server architecture, 86
Web-based e-mail, 86, 129–130
Web browsers, 97
client side of HTTP, 99
GUI interfaces, 64
Web caches, 59, 110–115
Web client-server interaction, 499
web of trust, 710
Web pages, 99
displaying, 101
requests, 495–500
Web proxy caches, 104
Web servers, 89, 97
deleting objects, 105
initial versions, 64
IP addresses, 392
port numbers, 197–198
server processes, 88
server side of HTTP, 99
spawning new process for connections,
198
TCP (Transmission Control Protocol),
197–198
uploading objects to, 105
Web sites, 108
anonymity, 738
privacy, 738
weighted fair queuing. See WFQ
well-known port number, 192
WEP (Wired Equivalent Privacy), 726–728
WFQ (weighted fair queuing), 329, 644–645
leaky bucket, 647–648
wide-area wireless access, 18
WiFi, 17, 52, 526–546
high-speed, 65
home networks, 17
hotspots, 515, 546
public access, 515
INDEX 861
WiMAX (World Interoperability for
Microwave Access), 554, 668
Windows
nslookup program, 141–142
Snort, 742
Wireshark packet sniffer, 78
window size, 220
wired-access ISPs tiered levels of
service, 636
wired broadcast links, 521
wired environments and packet sniffer,
58–59
Wired Equivalent Privacy. See WEP
wired link differences from wireless links,
519
wired networks, 519
wireless, 513–514
wireless communication links, 515–516
wireless devices, 58–59
wireless hosts, 514, 516–517, 530
wireless LANs, 445
access point, 17
LAN base stations, 548
DHCP (Dynamic Host Configuration
Protocol), 346
versus 3G cellular mobile systems, 548
IEEE 802.11 technology, 17
security, 726–731
WiFi, 17
wireless LANs and 802.11 standards, 526
wireless links
bit errors, 519
decreasing signal strength, 519
differences from wired links, 519
fading signal’s strength, 521–522
hidden terminal problem, 521
interference from other sources, 519
multipath propagation, 519
TCP sender awareness, 577
undetectable collisions, 521–522
wireless mesh networks, 518
wireless networks, 513
application layer, 575
base station, 516–518
CDMA (code division multiple access)
protocol, 522–526
characteristics, 519–526
802.11 wireless LANs, 526–546
link layer, 575
link rates, 515
mobility, 575–577
multi-hop, infrastructure-based, 518
multi-hop, infrastructure-less, 518
network infrastructure, 518
network layer, 575
single-hop, infrastructure-based, 518
single-hop, infrastructure-less, 518
TCP (Transmission Control Protocol),
575–577
UDP (User Datagram Protocol),
575–577
wireless communication links, 515–516
wireless hosts, 514
wireless personal area network. See WPAN
Wireless Philadelphia, 515
wireless station, 529–530
Wireshark labs, 59, 78
work-conserving round robin discipline, 644
workload model, 635
World Wide Web. See Web
worms, 56–57, 740
WPAN (wireless personal area network),
544
X
X.25, 512
XNS (Xerox Network Systems)
architecture, 384
Y
Yahoo!, 65, 86, 130
YouTube, 65, 588, 610–611
HTTP streaming (over TCP), 596
streaming stored video, 591
video, 602
Z
Zigbee, 545–546
862 INDEX







